<html><head></head><body>
  <div id="_idContainer074" class="Basic-Text-Frame">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-66" class="chapterTitle">Word2vec – Learning Word Embeddings</h1>
    <p class="normal">In this chapter, we will discuss a topic of paramount importance in NLP—Word2vec, a data-driven technique for learning powerful numerical representations (that is, vectors) of words or tokens in a language. Languages are complex. This warrants sound language understanding capabilities in the models we build to solve NLP problems. When transforming words to a numerical representation, a lot of methods aren’t able to sufficiently capture the semantics and contextual information that word carries. For example, the feature representation of the word <em class="italic">forest</em> should be very different from <em class="italic">oven</em> as these words are rarely used in similar contexts, whereas the representations of <em class="italic">forest</em> and <em class="italic">jungle</em> should be very similar. Not being able to capture this information leads to underperforming models. </p>
    <p class="normal">Word2vec tries to overcome this problem by learning word representations by consuming large amounts of text.</p>
    <div class="note">
      <p class="normal">Word2vec is called a <em class="italic">distributed representation</em>, as the semantics of the word are captured<a id="_idIndexMarker238"/> by the activation pattern of the full representation vector, in contrast to a single element of the representation vector (for example, setting a single element in the vector to 1 and rest to 0 for a single word).</p>
    </div>
    <p class="normal">In this chapter, we will learn the mechanics of several Word2vec algorithms. But first, we will discuss the classical approaches to solving this problem and their limitations. This then motivates us to look at learning neural-network-based Word2vec algorithms that deliver state-of-the-art performance when finding good word representations. </p>
    <p class="normal">We will train a model on a dataset and analyze the representations learned by the model. We visualize (using t-SNE, a visualization technique for high-dimensional data) these learned word embeddings for a set of words on a 2D canvas in <em class="italic">Figure 3.1</em>. If you take a closer look, you will see that similar things are placed close to each other (for example, numbers in the cluster in the middle):</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_01.png" alt="C:\Users\gauravg\Desktop\14070\CH03\B08681_03_01.png"/></figure>
    <p class="packt_figref">Figure 3. 1: An example visualization of learned word embeddings using t-SNE</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">t-Distributed Stochastic Neighbor Embedding</strong> (<strong class="keyWord">t-SNE</strong>)</p>
      <p class="normal">This is a dimensionality<a id="_idIndexMarker239"/> reduction technique that projects high-dimensional data to a two-dimensional space. This allows us to imagine how high-dimensional data is distributed in space, because humans are generally not so good at intuitively understanding data in more than three dimensions. You will learn about t-SNE in more detail in the next chapter.</p>
    </div>
    <p class="normal">This chapter covers this information through the following main topics:</p>
    <ul>
      <li class="bulletList">What is a word representation or meaning?</li>
      <li class="bulletList">Classical approaches to learning word representations</li>
      <li class="bulletList">Word2vec – a neural network-based approach to learning word representation</li>
      <li class="bulletList">The skip-gram algorithm</li>
      <li class="bulletList">The Continuous Bag-of-Words algorithm</li>
    </ul>
    <p class="normal">By the end of this chapter, you will have gained a thorough understanding of how the history of word representations has led to Word2vec, how to utilize two different Word2vec algorithms, and the vital importance of Word2vec for NLP.</p>
    <h1 id="_idParaDest-67" class="heading-1">What is a word representation or meaning?</h1>
    <p class="normal">What is meant by the word <em class="italic">meaning</em>? This is more of a philosophical<a id="_idIndexMarker240"/> question than a technical one. So, we will not try to discern the best answer for this question, but accept a more modest answer, that is, <em class="italic">meaning</em> is the idea conveyed by or some representation associated with a word. For example, when you hear the word “cat” you conjure up a mental picture of something that meows, has four legs, has a tail, and so on; then, if you hear the word “dog,” you again formulate a mental image of something that barks, has a bigger body than a cat, has four legs, has a tail, and so on. In this new space (that is, the mental pictures), it is easier for you to understand that cats and dogs are similar than by just looking at the words. Since the primary objective of NLP is to achieve human-like performance in linguistic tasks, it is sensible to explore principled ways of representing words for machines. To achieve this, we will use algorithms that can analyze a given text corpus and come up with good numerical representations of words (that is, word embeddings) such that words that fall within similar contexts (for example, <em class="italic">one</em> and <em class="italic">two</em>, <em class="italic">I </em>and <em class="italic">we</em>) will have similar numerical representations compared to words that are unrelated (for example, <em class="italic">cat</em> and <em class="italic">volcano</em>).</p>
    <p class="normal">First, we will discuss some classical approaches to achieve this and then move on to understanding recent, more sophisticated methods that use neural networks to learn feature representations and deliver state-of-the-art performance.</p>
    <h1 id="_idParaDest-68" class="heading-1">Classical approaches to learning word representation</h1>
    <p class="normal">In this section, we will discuss<a id="_idIndexMarker241"/> some of the classical approaches used for numerically representing words. It is important to have an understanding of the alternatives to word vectors, as these methods are still used in the real world, especially when limited data is available. </p>
    <p class="normal">More specifically, we will discuss common representations, such as <strong class="keyWord">one-hot encoding </strong>and <strong class="keyWord">Term Frequency-Inverse Document Frequency </strong>(<strong class="keyWord">TF-IDF</strong>).</p>
    <h2 id="_idParaDest-69" class="heading-2">One-hot encoded representation</h2>
    <p class="normal">One of the simpler ways<a id="_idIndexMarker242"/> of representing words is to use<a id="_idIndexMarker243"/> the one-hot encoded representation. This means that if we have a vocabulary of size<em class="italic"> V</em>, for each <em class="italic">i</em><sup class="superscript">th</sup> word <em class="italic">w</em><sub class="subscript">i</sub>, we will represent the word <em class="italic">w</em><sub class="subscript">i</sub> with a <em class="italic">V</em>-length vector [0, 0, 0, …, 0, 1, 0, …, 0, 0, 0] where the <em class="italic">i</em><sup class="superscript">th</sup> element is 1 and other elements are 0. As an example, consider this sentence:</p>
    <p class="normal"><em class="italic">Bob and Mary are good friends.</em></p>
    <p class="normal">The one-hot encoded representation of each word might look like this:</p>
    <p class="normal"><em class="italic">Bob</em>: [1,0,0,0,0,0]</p>
    <p class="normal"><em class="italic">and</em>: [0,1,0,0,0,0]</p>
    <p class="normal"><em class="italic">Mary</em>: [0,0,1,0,0,0]</p>
    <p class="normal"><em class="italic">are</em>: [0,0,0,1,0,0]</p>
    <p class="normal"><em class="italic">good</em>: [0,0,0,0,1,0]</p>
    <p class="normal"><em class="italic">friends</em>: [0,0,0,0,0,1]</p>
    <p class="normal">However, as you might have already figured out, this representation has many drawbacks.</p>
    <p class="normal">This representation does not encode the similarity between words in any way and completely ignores<a id="_idIndexMarker244"/> the context in which the words are used. Let’s consider<a id="_idIndexMarker245"/> the dot product between the word vectors as the similarity measure. The more similar two vectors are, the higher the dot product is for those two vectors. For example, the representation of the words <em class="italic">car</em> and <em class="italic">automobile</em> will have a similarity distance of 0, while <em class="italic">car</em> and <em class="italic">pencil</em> will also have the same value.</p>
    <p class="normal">This method becomes extremely ineffective for large vocabularies. Also, for a typical NLP task, the vocabulary easily can exceed 50,000 words. Therefore, the word representation matrix for 50,000 words will result in a very sparse 50,000 × 50,000 matrix.</p>
    <p class="normal">However, one-hot encoding plays an important role even in state-of-the-art word embedding learning algorithms. We use one-hot encoding to represent words numerically and feed them into neural networks so that the neural networks can learn better and smaller numerical feature representations of the words.</p>
    <div class="note">
      <p class="normal">One-hot encoding<a id="_idIndexMarker246"/> is also known as a localist representation (the opposite to the distributed representation), as the feature representation is decided by the activation of a single element in the vector.</p>
    </div>
    <p class="normal">We will now discuss another technique for representing words, known as the TF-IDF method.</p>
    <h2 id="_idParaDest-70" class="heading-2">The TF-IDF method</h2>
    <p class="normal"><strong class="keyWord">TF-IDF</strong> is a frequency-based method<a id="_idIndexMarker247"/> that takes into account<a id="_idIndexMarker248"/> the frequency with which a word appears in a corpus. This is a word representation in the sense that it represents the importance of a specific word in a given document. Intuitively, the higher the frequency of the word, the more important that word is in the document. For example, in a document about cats, the word <em class="italic">cats</em> will appear more often than in a document that isn’t about cats. However, just calculating the frequency would not work because words such as <em class="italic">this</em> and <em class="italic">is</em> are very frequent in documents but do not contribute much information. TF-IDF takes this into consideration and gives values of near-zero for such common words.</p>
    <p class="normal">Again, <em class="italic">TF</em> stands for term frequency and <em class="italic">IDF</em> stands for inverse document frequency:</p>
    <p class="normal"><em class="italic">TF(w</em><sub class="subscript">i</sub><em class="italic">)</em> = number of times w<sub class="subscript">i</sub> appear / total number of words</p>
    <p class="normal"><em class="italic">IDF(w</em><sub class="subscript">i</sub><em class="italic">)</em> = log(total number of documents / number of documents with w<sub class="subscript">i</sub> in it)</p>
    <p class="normal"><em class="italic">TF-IDF(w</em><sub class="subscript">i</sub><em class="italic">)</em> = TF(w<sub class="subscript">i</sub>) x IDF(w<sub class="subscript">i</sub>)</p>
    <p class="normal">Let’s do a quick exercise. Consider two documents:</p>
    <ul>
      <li class="bulletList">Document 1: <em class="italic">This is about cats. Cats are great companions</em>.</li>
      <li class="bulletList">Document 2: <em class="italic">This is about dogs. Dogs are very loyal</em>.</li>
    </ul>
    <p class="normal">Now let’s crunch some numbers:</p>
    <p class="normal"><em class="italic">TF-IDF (cats, doc1)</em> = (2/8) * log(2/1) = 0.075</p>
    <p class="normal"><em class="italic">TF-IDF (this, doc2)</em> = (1/8) * log(2/2) = 0.0</p>
    <p class="normal">Therefore, the word <em class="italic">cats</em> is informative, while <em class="italic">this</em> is not. This is the desired<a id="_idIndexMarker249"/> behavior we needed<a id="_idIndexMarker250"/> in terms of measuring the importance of words.</p>
    <h2 id="_idParaDest-71" class="heading-2">Co-occurrence matrix</h2>
    <p class="normal">Co-occurrence matrices, unlike one-hot-encoded<a id="_idIndexMarker251"/> representations, encode the context<a id="_idIndexMarker252"/> information of words, but require maintaining a V × V matrix. To understand the co-occurrence matrix, let’s take two example sentences:</p>
    <ul>
      <li class="bulletList"><em class="italic">Jerry and Mary are friends</em>.</li>
      <li class="bulletList"><em class="italic">Jerry buys flowers for Mary</em>.</li>
    </ul>
    <p class="normal">The co-occurrence matrix will look like the following matrix. We only show one half of the matrix, as it is symmetrical:</p>
    <table id="table001-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">Jerry</p>
          </td>
          <td class="table-cell">
            <p class="normal">and</p>
          </td>
          <td class="table-cell">
            <p class="normal">Mary</p>
          </td>
          <td class="table-cell">
            <p class="normal">are</p>
          </td>
          <td class="table-cell">
            <p class="normal">friends</p>
          </td>
          <td class="table-cell">
            <p class="normal">buys</p>
          </td>
          <td class="table-cell">
            <p class="normal">flowers</p>
          </td>
          <td class="table-cell">
            <p class="normal">for</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Jerry</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">and</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Mary</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">are</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">friends</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">buys</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">flowers</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">for</p>
          </td>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">However, it is not hard<a id="_idIndexMarker253"/> to see that maintaining such a co-occurrence matrix comes at a cost<a id="_idIndexMarker254"/> as the size of the matrix grows polynomially with the size of the vocabulary. Furthermore, it is not straightforward to incorporate a context window size larger than 1. One option is to have a weighted count, where the weight for a word in the context deteriorates with the distance from the word of interest.</p>
    <p class="normal">As you can see, these methods are very limited in their representational power. </p>
    <p class="normal">For example, in the one-hot encoded method, all words will have the same vector distance to each other. The TF-IDF method represents a word with a single number and is unable to capture the semantics of words. Finally calculating the co-occurrence matrix is very expensive and provides limited information about a word’s context.</p>
    <p class="normal">We end our discussion about simple representations of words here. In the following section, we will first develop an intuitive understanding of word embeddings by working through an example. Then we will define a loss function so that we can use machine learning to learn word embeddings. Also, we will discuss two Word2vec algorithms, namely, the <strong class="keyWord">skip-gram </strong>and <strong class="keyWord">Continuous Bag-of-Words </strong>(<strong class="keyWord">CBOW</strong>) algorithms.</p>
    <h1 id="_idParaDest-72" class="heading-1">An intuitive understanding of Word2vec – an approach to learning word representation</h1>
    <blockquote class="packt_quote">
      <p class="quote"> “You shall know a word by the company it keeps.”</p>
      <p class="quote">– J.R. Firth</p>
    </blockquote>
    <p class="normal">This statement, uttered by J. R. Firth in 1957, lies at the very foundation of Word2vec, as Word2vec techniques<a id="_idIndexMarker255"/> use the context of a given word to learn its semantics.</p>
    <p class="normal">Word2vec is a groundbreaking approach that allows computers to learn the meaning of words without any human intervention. Also, Word2vec learns numerical representations of words by looking at the words surrounding a given word.</p>
    <p class="normal">We can test the correctness of the preceding quote by imagining a real-world scenario. Imagine you are sitting an exam and you find this sentence in your first question: “Mary is a very stubborn child. Her pervicacious nature always gets her in trouble.” Now, unless you are very<a id="_idIndexMarker256"/> clever, you might not know what <em class="italic">pervicacious</em> means. In such a situation, you automatically will be compelled to look at the phrases surrounding the word of interest. In our example, <em class="italic">pervicacious</em> is surrounded by <em class="italic">stubborn</em>, <em class="italic">nature</em>, and <em class="italic">trouble</em>. Looking at these three words is enough to determine that pervicacious in fact means the state of being stubborn. I think this is adequate evidence to observe the importance of context for a word’s meaning.</p>
    <p class="normal">Now let’s discuss the basics of Word2vec. As already mentioned, Word2vec learns the meaning of a given word by looking at its context and representing it numerically. </p>
    <p class="normal">By <em class="italic">context</em>, we refer to a fixed number of words in front of and behind the word of interest. Let’s take a hypothetical corpus with <em class="italic">N</em> words. Mathematically, this can be represented by a sequence of words denoted by <em class="italic">w</em><sub class="subscript">0</sub>, <em class="italic">w</em><sub class="subscript">1</sub>, …, <em class="italic">w</em><sub class="subscript">i</sub>, and <em class="italic">w</em><sub class="subscript">N</sub>, where <em class="italic">w</em><sub class="subscript">i</sub> is the <em class="italic">i</em><sup class="superscript">th</sup> word in the corpus.</p>
    <p class="normal">Next, if we want to find a good algorithm that is capable of learning word meanings, given a word, our algorithm should be able to predict the context words correctly.</p>
    <p class="normal">This means that the following probability should be high for any given word <em class="italic">w</em><sub class="subscript">i</sub>:</p>
    <p class="center"><img src="../Images/B14070_03_001.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">To arrive at the right-hand side of the equation, we need to assume that given the target word (<em class="italic">w</em><sub class="subscript">i</sub>), the context words are independent of each other (for example, <em class="italic">w</em><sub class="subscript">i-2</sub> and <em class="italic">w</em><sub class="subscript">i-1</sub> are independent). Though not entirely true, this approximation makes the learning problem practical and works well in practice. Let’s go through an example to understand the computations.</p>
    <h2 id="_idParaDest-73" class="heading-2">Exercise: does queen = king – he + she?</h2>
    <p class="normal">Before proceeding further, let’s do a small exercise<a id="_idIndexMarker257"/> to understand how maximizing<a id="_idIndexMarker258"/> the previously mentioned probability leads to finding good meaning (or representations) of words. Consider the following very small corpus:</p>
    <p class="normal"><em class="italic">There was a very rich king. He had a beautiful queen. She was very kind</em>.</p>
    <p class="normal">To keep the exercise simple, let’s do some manual preprocessing and remove the punctuation and the uninformative words:</p>
    <p class="normal"><em class="italic">was rich king he had beautiful queen she was kind</em></p>
    <p class="normal">Now let’s form a set of tuples<a id="_idIndexMarker259"/> for each word with their context<a id="_idIndexMarker260"/> words in the format (<em class="italic">target word --&gt; context word 1</em>, <em class="italic">context word 2</em>). We will assume a context window size of 1 on either side:</p>
    <p class="normal"><em class="italic">was --&gt; rich</em></p>
    <p class="normal"><em class="italic">rich --&gt; was, king</em></p>
    <p class="normal"><em class="italic">king --&gt; rich, he</em></p>
    <p class="normal"><em class="italic">he --&gt; king, had</em></p>
    <p class="normal"><em class="italic">had --&gt; he, beautiful</em></p>
    <p class="normal"><em class="italic">beautiful --&gt; had, queen</em></p>
    <p class="normal"><em class="italic">queen --&gt; beautiful, she</em></p>
    <p class="normal"><em class="italic">she --&gt; queen, was</em></p>
    <p class="normal"><em class="italic">was --&gt; she, kind</em></p>
    <p class="normal"><em class="italic">kind --&gt; was</em></p>
    <p class="normal">Remember, our goal is to be able to predict the words on the right given the word on the left. To do this, for a given word, the words on the right-side context should share a high numerical or geometrical similarity with the words on the left-side context. In other words, the word of interest should be conveyed by the surrounding words. Now let’s consider actual numerical vectors to understand how this works. For simplicity, let’s only consider the tuples highlighted in bold. Let’s begin by assuming the following for the word <em class="italic">rich</em>:</p>
    <p class="normal"><em class="italic">rich --&gt; [0,0]</em></p>
    <p class="normal">To be able to correctly predict <em class="italic">was</em> and <em class="italic">king</em> from <em class="italic">rich</em>, was and <em class="italic">king</em> should have high similarity with the word <em class="italic">rich</em>. The Euclidean distance will be used to measure the distance between words. </p>
    <p class="normal">Let’s try the following values for the words <em class="italic">king</em> and <em class="italic">rich</em>:</p>
    <p class="normal"><em class="italic">king --&gt; [0,1]</em></p>
    <p class="normal"><em class="italic">was --&gt; [-1,0]</em></p>
    <p class="normal">This works out fine as the following:</p>
    <p class="normal"><em class="italic">Dist(rich,king)</em> = 1.0</p>
    <p class="normal"><em class="italic">Dist(rich,was)</em> = 1.0</p>
    <p class="normal">Here, <em class="italic">Dist</em> is the Euclidean distance<a id="_idIndexMarker261"/> between two<a id="_idIndexMarker262"/> words. This is illustrated in <em class="italic">Figure 3.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_02.png" alt="C:\Users\gauravg\Desktop\14070\CH03\B08681_03_04.jpg"/></figure>
    <p class="packt_figref">Figure 3.2: The positioning of word vectors for the words “rich”, “was” and “king”</p>
    <p class="normal">Now let’s consider the following tuple:</p>
    <p class="normal"><em class="italic">king --&gt; rich, he</em></p>
    <p class="normal">We have established the relationship between <em class="italic">king</em> and <em class="italic">rich</em> already. However, it is not done yet; the more we see a relationship, the closer these two words should be. So, let’s first adjust the vector of king so that it is a bit closer to <em class="italic">rich</em>:</p>
    <p class="normal"><em class="italic">king --&gt; [0,0.8]</em></p>
    <p class="normal">Next, we will need to add the word <em class="italic">he</em> to the picture. The word <em class="italic">he</em> should be closer to <em class="italic">king</em>. This is all the information that we have right now about the word <em class="italic">he: he --&gt; [0.5,0.8]</em>.</p>
    <p class="normal">At this moment, the graph with the words looks like <em class="italic">Figure 3.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_03.png" alt="C:\Users\gauravg\Desktop\14070\CH03\B08681_03_05.jpg"/></figure>
    <p class="packt_figref">Figure 3.3: The positioning of word vectors for the words “rich”, “was”, “king,” and “he”</p>
    <p class="normal">Now let’s proceed<a id="_idIndexMarker263"/> with the next<a id="_idIndexMarker264"/> two tuples: <em class="italic">queen --&gt; beautiful, she</em> and <em class="italic">she --&gt; queen, was</em>. Note that I have swapped the order of the tuples as this makes it easier for us to understand the example:</p>
    <p class="normal"><em class="italic">she --&gt; queen, was</em></p>
    <p class="normal">Now, we will have to use our prior knowledge of English to proceed further.</p>
    <p class="normal">It is a reasonable decision to place the word <em class="italic">she</em> the same distance from <em class="italic">was</em> that <em class="italic">he</em> is from <em class="italic">was</em>, because their usage in the context of the word <em class="italic">was</em> is equivalent. Therefore, let’s use this:</p>
    <p class="normal"><em class="italic">she --&gt; [0.5,0.6]</em></p>
    <p class="normal">Next, we will use the word <em class="italic">queen</em> close to the word <em class="italic">she: queen --&gt; [0.0,0.6]</em>.</p>
    <p class="normal">This is illustrated in <em class="italic">Figure 3.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_04.png" alt="C:\Users\gauravg\Desktop\14070\CH03\B08681_03_06.jpg"/></figure>
    <p class="packt_figref">Figure 3.4: The positioning of word vectors for the words “rich,” “was,” “king,” “he,” “she,” and “queen”</p>
    <p class="normal">Next, we only have the following tuple:</p>
    <p class="normal"><em class="italic">queen --&gt; beautiful, she</em></p>
    <p class="normal">Here, the word <em class="italic">beautiful</em> is found. It should be approximately<a id="_idIndexMarker265"/> the same distance<a id="_idIndexMarker266"/> from the words <em class="italic">queen</em> and <em class="italic">she</em>. Let’s use the following:</p>
    <p class="normal"><em class="italic">beautiful --&gt; [0.25,0]</em></p>
    <p class="normal">Now we have the following graph depicting the relationships between words. When we observe <em class="italic">Figure 3.6</em>, it seems to be a very intuitive representation of the meanings of words:</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_05.png" alt="C:\Users\gauravg\Desktop\14070\CH03\B08681_03_07.jpg"/></figure>
    <p class="packt_figref">Figure 3.5: The positioning of word vectors for the words “rich,” “was,” “king,” “he,” “she,” “queen,” and “beautiful”</p>
    <p class="normal">Now, let’s look at the question that has been lurking in our minds since the beginning of this exercise. Are the quantities in this equation equivalent: <em class="italic">queen</em> = <em class="italic">king</em> - <em class="italic">he</em> + <em class="italic">she</em>? Well, we’ve got all the resources that we’ll need to solve this mystery now. Let’s try the right-hand side of the equation first:</p>
    <p class="normal">= <em class="italic">king</em> – <em class="italic">he</em> + <em class="italic">she</em></p>
    <p class="normal">= <em class="italic">[0,0.8]</em> – <em class="italic">[0.5,0.8]</em> + <em class="italic">[0.5,0.6]</em></p>
    <p class="normal">= <em class="italic">[0,0.6]</em></p>
    <p class="normal">It all works out in the end. If you look at the word vector we obtained for the word <em class="italic">queen</em>, you see that this is exactly the same as the answer we deduced earlier.</p>
    <p class="normal">Note that this is a crude way to show how word embeddings are learned, and this might differ from the exact positions of word embeddings learned using an algorithm.</p>
    <p class="normal">Also keep in mind that this is an unrealistically scaled-down exercise with regard to what a real-world corpus might look like. So, you will not be able to work out these values by hand just by crunching a dozen numbers. Sophisticated function approximators such as neural networks do this job for us. But, to use neural networks, we need to formulate our problem in a mathematically assertive way. However, this is a good exercise to show the power of word vectors.</p>
    <p class="normal">Now that we have a good<a id="_idIndexMarker267"/> understanding of how Word2vec enables<a id="_idIndexMarker268"/> us to learn word representations, let’s look at the actual algorithms Word2vec utilizes in the next two sections.</p>
    <h1 id="_idParaDest-74" class="heading-1">The skip-gram algorithm</h1>
    <p class="normal">The first algorithm<a id="_idIndexMarker269"/> we will talk about is known as the <strong class="keyWord">skip-gram algorithm</strong>: a type of Word2vec algorithm. As we have discussed in numerous places, the meaning of a word can be elicited from the contextual words surrounding it. However, it is not entirely straightforward to develop a model that exploits this way of learning word meanings. The skip-gram algorithm, introduced by Mikolov et al. in 2013, is an algorithm that does exploit the context of the words in a written text to learn good word embeddings.</p>
    <p class="normal">Let’s go through the skip-gram algorithm step by step. First, we will discuss the data preparation process. Understanding the format of the data puts us in a great position to understand the algorithm. We will then discuss the algorithm itself. Finally, we will implement the algorithm using TensorFlow.</p>
    <h2 id="_idParaDest-75" class="heading-2">From raw text to semi-structured text</h2>
    <p class="normal">First, we need to design<a id="_idIndexMarker270"/> a mechanism to extract a dataset that can be fed to our learning model. Such a dataset should be a set of tuples of the format (target, context). Moreover, this needs to be created in an unsupervised manner. That is, a human should not have to manually engineer the labels for the data. In summary, the data preparation process<a id="_idIndexMarker271"/> should do the following:</p>
    <ul>
      <li class="bulletList">Capture the surrounding words of a given word (that is, the context)</li>
      <li class="bulletList">Run in an unsupervised manner</li>
    </ul>
    <p class="normal">The skip-gram model uses the following approach to design a dataset:</p>
    <ul>
      <li class="bulletList">For a given word <em class="italic">w</em><sub class="subscript">i</sub>, a context window size of <em class="italic">m</em> is assumed. By <em class="italic">context window size</em>, we mean the number of words considered as context on a single side. Therefore, for <em class="italic">w</em><sub class="subscript">i</sub>, the context window (including the target word <em class="italic">w</em><sub class="subscript">i</sub>) will be of size <em class="italic">2m+1</em> and will look like this: <em class="italic">[w</em><sub class="subscript">i-m</sub><em class="italic">, …, w</em><sub class="subscript">i-1</sub><em class="italic">, w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">i+1</sub><em class="italic">, …, w</em><sub class="subscript">i+m</sub><em class="italic">]</em>.</li>
      <li class="bulletList">Next, (target, context) tuples are formed as <em class="italic">[…, (w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">i-m</sub><em class="italic">), …, (w</em><sub class="subscript">i</sub><em class="italic">,w</em><sub class="subscript">i-1</sub><em class="italic">), (w</em><sub class="subscript">i</sub><em class="italic">,w</em><sub class="subscript">i+1</sub><em class="italic">), …, (w</em><sub class="subscript">i</sub><em class="italic">,w</em><sub class="subscript">i+m</sub><em class="italic">), …]</em>; here, <img src="../Images/B14070_03_002.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> and <em class="italic">N</em> is the number of words in the text. Let’s use the following sentence and a context window size (<em class="italic">m</em>) of 1:</li>
    </ul>
    <p class="normal"><em class="italic">The dog barked at the mailman</em>.</p>
    <p class="normal">For this example, the dataset would be as follows:</p>
    <p class="normal"><em class="italic">[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the, mailman)]</em></p>
    <p class="normal">Once the data is in the <em class="italic">(target, context)</em> format, we<a id="_idIndexMarker272"/> can use a neural network to learn the word embeddings.</p>
    <h2 id="_idParaDest-76" class="heading-2">Understanding the skip-gram algorithm</h2>
    <p class="normal">First, let’s identify the variables<a id="_idIndexMarker273"/> and notation we need to learn the word embeddings. To store the word embeddings, we need two V × D matrices, where <em class="italic">V</em> is the vocabulary size and <em class="italic">D</em> is the dimensionality of the word embeddings (that is, the number of elements in the vector that represent a single word). <em class="italic">D</em> is a user-defined hyperparameter. The higher <em class="italic">D</em> is, the more expressive the word embeddings learned will be. We need two matrices, one to represent the context words and one to represent the target words. These matrices will be referred to as the <em class="italic">context</em> <em class="italic">embedding space (or context embedding layer)</em> and the <em class="italic">target</em> <em class="italic">embedding space (or target embedding layer)</em>, or in general as the embedding space (or the embedding layer).</p>
    <p class="normal">Each word will be represented with a unique ID in the range [1, <em class="italic">V+1</em>]. These IDs are passed to the embedding layer to look up corresponding vectors. To generate these IDs, we will use<a id="_idIndexMarker274"/> a special object called a Tokenizer that’s available in TensorFlow. Let’s refer to an example target-context tuple (w<sub class="subscript">i</sub>, w<sub class="subscript">j</sub>), where the target word ID is <em class="italic">w</em><sub class="subscript">i</sub>, and one of the context words is w<sub class="subscript">j</sub>. The corresponding target embedding of <em class="italic">w</em><sub class="subscript">i</sub> is <em class="italic">t</em><sub class="subscript">i</sub>, and the corresponding context embedding of <em class="italic">w</em><sub class="subscript">j</sub> is <em class="italic">c</em><sub class="subscript">j</sub>. Each target-context tuple is accompanied by a label (0 or 1), denoted by <em class="italic">y</em><sub class="subscript">i</sub>, where true target-context pairs will get a label of 1, and negative (or false) target-context candidates will get a label of 0. It is easy to generate negative target-context candidates by sampling a word that does not appear in the context of a given target as the context word. We will talk about this in more detail later.</p>
    <p class="normal">At this point, we have defined<a id="_idIndexMarker275"/> the necessary variables. Next, for each input <em class="italic">w</em><sub class="subscript">i</sub>, we will look up the embedding vectors from the context embedding layer corresponding to the input. This operation provides us with <em class="italic">c</em><sub class="subscript">i</sub>, which is a <em class="italic">D</em>-sized vector (that is, a <em class="italic">D</em>-long embedding vector). We do the same for the input <em class="italic">w</em><sub class="subscript">j</sub>, using the context embedding space to retrieve <em class="italic">c</em><sub class="subscript">j</sub>. Afterward, we calculate the prediction output for (<em class="italic">w</em><sub class="subscript">i</sub><em class="italic"> ,w</em><sub class="subscript">i</sub><em class="italic">)</em> using the following transformation:</p>
    <p class="center"><em class="italic">logit(w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">i</sub><em class="italic">)</em> = <em class="italic">c</em><sub class="subscript">i</sub><em class="italic"> .t</em><sub class="subscript">j</sub></p>
    <p class="center"><em class="italic">ŷ</em><sub class="subscript">ij</sub> = <em class="italic">sigmoid(logit(w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">i</sub><em class="italic">))</em></p>
    <p class="normal">Here, <em class="italic">logit(w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">i</sub><em class="italic">) </em> represents the unnormalized scores (that is, logits), <em class="italic">ŷ</em><sub class="subscript">i</sub> is a single-valued predicted output (representing the probability of context word belonging in the context of the target word). </p>
    <p class="normal">We will visualize both the conceptual (<em class="italic">Figure 3.7</em>) and implementation (<em class="italic">Figure 3.8</em>) views of the skip-gram model. Here is a summary of the notation:</p>
    <ul>
      <li class="bulletList"><em class="italic">V</em>: This is the size of the vocabulary</li>
      <li class="bulletList"><em class="italic">D</em>: This is the dimensionality of the embedding layer</li>
      <li class="bulletList"><em class="italic">w</em><sub class="subscript">i</sub>: Target word</li>
      <li class="bulletList"><em class="italic">w</em><sub class="subscript">j</sub>: Context word</li>
      <li class="bulletList"><em class="italic">t</em><sub class="subscript">i</sub>: Target embedding of the word <em class="italic">w</em><sub class="subscript">i</sub></li>
      <li class="bulletList"><em class="italic">c</em><sub class="subscript">j</sub>: Context embedding of the word <em class="italic">w</em><sub class="subscript">j</sub></li>
      <li class="bulletList"><em class="italic">y</em><sub class="subscript">i</sub>: This is the one-hot-encoded output word corresponding to <em class="italic">x</em><sub class="subscript">i</sub></li>
      <li class="bulletList"><em class="italic">ŷ</em><sub class="subscript">i</sub>: This is the predicted output for <em class="italic">x</em><sub class="subscript">i</sub></li>
      <li class="bulletList"><em class="italic">logit(w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">j</sub><em class="italic">)</em>: This is the unnormalized score for the input <em class="italic">x</em><sub class="subscript">i</sub></li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B14070_03_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.6: The conceptual skip-gram model</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.7: The implementation of the skip-gram model</p>
    <p class="normal">Using both the existing<a id="_idIndexMarker276"/> and derived entities, we can now use the cross-entropy loss function to calculate the loss for a given data point <em class="italic">[(w</em><sub class="subscript">i</sub><em class="italic">, w</em><sub class="subscript">j</sub><em class="italic">), y</em><sub class="subscript">i</sub><em class="italic">]</em>.</p>
    <p class="normal">For binary labels, the cross-entropy loss for a single sample <img src="../Images/B14070_03_003.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> is computed as:</p>
    <p class="center"><img src="../Images/B14070_03_004.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Where <img src="../Images/B14070_03_005.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> is the predicted label for <img src="../Images/B14070_03_006.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>. For multi-class classification problems, we generalize the loss by computing the term <img src="../Images/B14070_03_007.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> for each class:</p>
    <p class="center"><img src="../Images/B14070_03_008.png" alt="" style="height: 3.33em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">Where <img src="../Images/B14070_03_009.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> represents the value of the <img src="../Images/B14070_03_010.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> index of the <img src="../Images/B14070_03_011.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/>, where <img src="../Images/B14070_03_011.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> is a one hot encoded vector representing the label of the data point. </p>
    <p class="normal">Typically, when training neural networks, this loss is computed for each sample in a given batch, then averaged to compute the loss of the batch. Finally, the batch losses are averaged over all the batches in the dataset to compute the final loss.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Why does the original word embeddings paper use two embedding layers?</strong></p>
      <p class="normal">The original paper (by Mikolov et al., 2013) uses two distinct V × D embedding spaces to denote<a id="_idIndexMarker277"/> words in the target space (words when used as the target) and words in the contextual space (words used as context words). One motivation to do this is that a word does not occur in its own context often. So, we want to minimize the probability of such things happening.</p>
      <p class="normal">For example, for the target word <em class="italic">dog</em>, it is highly unlikely that the word <em class="italic">dog</em> is also found in its context (<em class="italic">P(dog</em>|<em class="italic">dog) ~ 0</em>). Intuitively, if we feed the (<em class="italic">w</em><sub class="subscript">i</sub>=<em class="italic">dog</em> and <em class="italic">w</em><sub class="subscript">j</sub>=<em class="italic">dog</em>) data point to the neural network, we are asking the neural network to give a higher loss if the neural network predicts <em class="italic">dog</em> as a context word of <em class="italic">dog</em>. </p>
      <p class="normal">In other words, we are asking the word embedding of the word <em class="italic">dog</em> to have a very high distance to the word embedding of the word <em class="italic">dog</em>. This creates a strong contradiction as the distance between the embeddings of the same word will be 0. Therefore, we cannot achieve this if we only have a single embedding space. </p>
      <p class="normal">However, having two separate embedding spaces for target words and contextual words allows us to have this property because this way we have two separate embedding vectors for the same word. In practice, as long as you avoid feeding input-output tuples, having the same word as input and output allows us to work with a single embedding space and eliminates the need for two distinct embedding layers.</p>
    </div>
    <p class="normal">Let’s now implement the data generation process with TensorFlow.</p>
    <h2 id="_idParaDest-77" class="heading-2">Implementing and running the skip-gram algorithm with TensorFlow</h2>
    <p class="normal">We are now going to get<a id="_idIndexMarker278"/> our hands dirty with TensorFlow<a id="_idIndexMarker279"/> and implement the algorithm<a id="_idIndexMarker280"/> from end to end. First, we will discuss<a id="_idIndexMarker281"/> the data we’re going to use and how TensorFlow can help us to get that data in the format the model accepts. We will implement the skip-gram algorithm with TensorFlow and finally train the model and evaluate it on data that was prepared.</p>
    <h3 id="_idParaDest-78" class="heading-3">Implementing the data generators with TensorFlow</h3>
    <p class="normal">First, we will investigate<a id="_idIndexMarker282"/> how data can be generated in the correct format<a id="_idIndexMarker283"/> for the model. For this exercise, we are going to use the BBC news articles dataset available at <a href="http://mlg.ucd.ie/datasets/bbc.html"><span class="url">http://mlg.ucd.ie/datasets/bbc.html</span></a>. It contains 2,225 news articles belonging to 5 topics, business, entertainment, politics, sport, and tech, which were published on the BBC website between 2004-2005.</p>
    <p class="normal">We write the function <code class="inlineCode">download_data()</code> below to download the data to a given folder and extract it from its compressed format:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">download_data</span>(<span class="hljs-params">url, data_dir</span>):
    <span class="hljs-string">"""Download a file if not present, and make sure it's the right</span>
<span class="hljs-string">    size."""</span>
  
    os.makedirs(data_dir, exist_ok=<span class="hljs-literal">True</span>)
    file_path = os.path.join(data_dir, <span class="hljs-string">'bbc-fulltext.zip'</span>)
  
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(file_path):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Downloading file...'</span>)
        filename, _ = urlretrieve(url, file_path)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"File already exists"</span>)
  
    extract_path = os.path.join(data_dir, <span class="hljs-string">'bbc'</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(extract_path):
        
        <span class="hljs-keyword">with</span> zipfile.ZipFile(
            os.path.join(data_dir, <span class="hljs-string">'bbc-fulltext.zip'</span>),
        <span class="hljs-string">'r'</span>
<span class="hljs-string">        </span>) <span class="hljs-keyword">as</span> zipf:
            zipf.extractall(data_dir)
  
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"bbc-fulltext.zip has already been extracted"</span>)
</code></pre>
    <p class="normal">The function first creates the <code class="inlineCode">data_dir</code> if it doesn’t exist. Next, if the <code class="inlineCode">bbc-fulltext.zip</code> file does not exist, it will be downloaded from the provided URL. If <code class="inlineCode">bbc-fulltext.zip</code> has not been extracted yet, it will be extracted to <code class="inlineCode">data_dir</code>. </p>
    <p class="normal">We can call this function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">url = <span class="hljs-string">'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'</span>
download_data(url, <span class="hljs-string">'data'</span>)
</code></pre>
    <p class="normal">With that, we are going<a id="_idIndexMarker284"/> to focus on reading the data contained<a id="_idIndexMarker285"/> in the news articles (in <code class="inlineCode">.txt</code> format) into the memory. To do that, we will define the <code class="inlineCode">read_data()</code> function, which takes a data directory path (<code class="inlineCode">data_dir</code>), and reads the <code class="inlineCode">.txt</code> files (except for the README file) found in the data directory:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">read_data</span>(<span class="hljs-params">data_dir</span>):
    news_stories = []
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Reading files"</span>)
    <span class="hljs-keyword">for</span> root, dirs, files <span class="hljs-keyword">in</span> os.walk(data_dir):
        <span class="hljs-keyword">for</span> fi, f <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(files):
            <span class="hljs-keyword">if</span> <span class="hljs-string">'README'</span> <span class="hljs-keyword">in</span> f:
                <span class="hljs-keyword">continue</span>
            <span class="hljs-built_in">print</span>(<span class="hljs-string">"."</span>*fi, f, end=<span class="hljs-string">'\r'</span>)
            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(root, f), encoding=<span class="hljs-string">'latin-1'</span>) <span class="hljs-keyword">as</span> f:
                story = []
                <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> f:               
                    story.append(row.strip())
                story = <span class="hljs-string">' '</span>.join(story)                        
                news_stories.append(story)                
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\nDetected </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(news_stories)}</span><span class="hljs-string"> stories"</span>)
    <span class="hljs-keyword">return</span> news_stories
</code></pre>
    <p class="normal">With the <code class="inlineCode">read_data()</code> function defined, let’s use it to read in the data and print some samples as well as some statistics:</p>
    <pre class="programlisting code"><code class="hljs-code">news_stories = read_data(os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'bbc'</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{</span><span class="hljs-built_in">sum</span><span class="hljs-subst">([</span><span class="hljs-built_in">len</span><span class="hljs-subst">(story.split(</span><span class="hljs-string">' '</span><span class="hljs-subst">)) </span><span class="hljs-keyword">for</span><span class="hljs-subst"> story </span><span class="hljs-keyword">in</span><span class="hljs-subst"> news_stories])}</span><span class="hljs-string"> words found in the total news set"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Example words (start): '</span>,news_stories[<span class="hljs-number">0</span>][:<span class="hljs-number">50</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Example words (end): '</span>,news_stories[-<span class="hljs-number">1</span>][-<span class="hljs-number">50</span>:])
</code></pre>
    <p class="normal">This will print the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Reading files
............. 361.txt
Detected 2225 stories
865163 words found in the total news set
Example words (start):  Windows worm travels with Tetris  Users are being 
Example words (end):  is years at Stradey as "the best time of my life."
</code></pre>
    <p class="normal">As we said at the beginning<a id="_idIndexMarker286"/> of this section, there are 2,225 stories<a id="_idIndexMarker287"/> with close to a million words. In the next step, we need to tokenize each story (in the form of a long string) to a list of tokens (or words). Along with that, we will perform some preprocessing on the text:</p>
    <ul>
      <li class="bulletList">Lowercase all the characters</li>
      <li class="bulletList">Remove punctuation</li>
    </ul>
    <p class="normal">All of these can be achieved with the <code class="inlineCode">tensorflow.keras.preprocessing.text.Tokenizer</code> object. We can define a Tokenizer as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
tokenizer = Tokenizer(
    num_words=<span class="hljs-literal">None</span>,
    filters=<span class="hljs-string">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_'{|}~\t\n'</span>,
    lower=<span class="hljs-literal">True</span>,
split=<span class="hljs-string">' '</span> 
)
</code></pre>
    <p class="normal">Here, you can see some of the most popular keyword arguments and their default values used when defining a Tokenizer:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">num_words</code> – Defines the size of the vocabulary. Defaults to <code class="inlineCode">None</code>, meaning it will consider all the words appearing in the text corpus. If set to the integer n, it will only consider the n most common words appearing in the corpus.</li>
      <li class="bulletList"><code class="inlineCode">filters</code> – Defines any characters that need to be omitted during preprocessing. By default, it defines a string containing most of the common punctuation marks and symbols.</li>
      <li class="bulletList"><code class="inlineCode">lower</code> – Defines whether the text needs to be converted to lowercase.</li>
      <li class="bulletList"><code class="inlineCode">split</code> – Defines the character that the words will be tokenized on.</li>
    </ul>
    <p class="normal">Once the Tokenizer<a id="_idIndexMarker288"/> is defined, you can call its <code class="inlineCode">fit_on_texts()</code> method<a id="_idIndexMarker289"/> with a list of strings (where each string is a news article) so that the Tokenizer will learn the vocabulary and map the words to unique IDs:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenizer.fit_on_texts(news_stories)
</code></pre>
    <p class="normal">Let’s take a moment to analyze what the Tokenizer has produced after it has been fitted on the text. Once it has been fitted, the Tokenizer will have two important attributes populated: <code class="inlineCode">word_index</code> and <code class="inlineCode">index_word</code>. Here <code class="inlineCode">word_index</code> is a dictionary that maps each word to a unique ID. The <code class="inlineCode">index_word</code> attribute is the opposite of <code class="inlineCode">word_index</code>, that is, a dictionary that maps each unique word ID to the corresponding word:</p>
    <pre class="programlisting code"><code class="hljs-code">n_vocab = <span class="hljs-built_in">len</span>(tokenizer.word_index.items())+<span class="hljs-number">1</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Vocabulary size: </span><span class="hljs-subst">{n_vocab}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nWords at the top"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'\t'</span>, <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">list</span>(tokenizer.word_index.items())[:<span class="hljs-number">10</span>]))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nWords at the bottom"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'\t'</span>, <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">list</span>(tokenizer.word_index.items())[-<span class="hljs-number">10</span>:]))
</code></pre>
    <p class="normal">Note how we are using the length of the <code class="inlineCode">word_index</code> dictionary to derive the vocabulary size. We need an additional 1 as the ID 0 is a reserved ID and will not be used for any word. This will output the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Vocabulary size: 32361
Words at the top
    {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}
Words at the bottom
    {'counsellor': 32351, "'frag'": 32352, 'relasing': 32353, "'real'": 32354, 'hrs': 32355, 'enviroment': 32356, 'trifling': 32357, '24hours': 32358, 'ahhhh': 32359, 'lol': 32360}
</code></pre>
    <p class="normal">The more frequent<a id="_idIndexMarker290"/> a word is in the corpus, the lower<a id="_idIndexMarker291"/> the ID will be. Words such as “the”, “to” and “of” which tend to be common (and are called stop words) are in fact the most<a id="_idIndexMarker292"/> common words. As the next step, we are going to refine our Tokenizer object to have a limited-sized vocabulary. Because we are working with a relatively small corpus, we have to make sure the vocabulary is not too large, as it can lead to poorly learned word vectors due to the lack of data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
tokenizer = Tokenizer(
    num_words=<span class="hljs-number">15000</span>,
    filters=<span class="hljs-string">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_'{|}~\t\n'</span>,
    lower=<span class="hljs-literal">True</span>, split=<span class="hljs-string">' '</span>, oov_token=<span class="hljs-string">''</span>,    
)
tokenizer.fit_on_texts(news_stories)
</code></pre>
    <p class="normal">Since we have a total vocabulary of more than 30,000 words, we’ll restrict the size of the vocabulary to 15,000. This means the Tokenizer will only keep the most common 15,000 words as the vocabulary. When we restrict a vocabulary this way, a new problem arises. As the Tokenizer’s vocabulary does not encompass all possible words in the true vocabulary, out-of-vocabulary words (or OOV words) can rear their heads. Some solutions are to replace OOV words with a special token (such as &lt;<code class="inlineCode">UNK</code>&gt;) or remove them from the corpus. This is possible by passing the string you want to replace OOV tokens with to the <code class="inlineCode">oov_token</code> argument in the Tokenizer. In this case, we will remove OOV words. If we are careful when setting the size of the vocabulary, omitting some of the rare words would not harm learning the context of words accurately.</p>
    <p class="normal">We can have a look at the transformation done on the text by the Tokenizer as follows. Let’s convert a string of the first 100 characters of the first story in our corpus (stored in the <code class="inlineCode">news_stories </code>variable):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Original: </span><span class="hljs-subst">{news_stories[</span><span class="hljs-number">0</span><span class="hljs-subst">][:</span><span class="hljs-number">100</span><span class="hljs-subst">]}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">Then we can call the <code class="inlineCode">tokenizer</code>'s <code class="inlineCode">texts_to_sequences()</code> method to convert a list of documents (where each document is a string) to a list<a id="_idIndexMarker293"/> of list of word<a id="_idIndexMarker294"/> IDs (that is, each document is converted to a list of word IDs).</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Sequence IDs: </span><span class="hljs-subst">{tokenizer.texts_to_sequences([news_stories[</span><span class="hljs-number">0</span><span class="hljs-subst">][:</span><span class="hljs-number">100</span><span class="hljs-subst">]])[</span><span class="hljs-number">0</span><span class="hljs-subst">]}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">This will print out:</p>
    <pre class="programlisting con"><code class="hljs-con">Original: Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped 76% to $1.1
Sequence IDs: [4223, 187, 716, 66, 3596, 1050, 3938, 626, 21, 49, 303, 717, 8263, 2972, 5321, 3, 108, 108]
</code></pre>
    <p class="normal">We now have our Tokenizer sorted. There’s nothing left to do but to convert all of our news articles to sequences of word IDs with a single line of code:</p>
    <pre class="programlisting code"><code class="hljs-code">news_sequences = tokenizer.texts_to_sequences(news_stories)
</code></pre>
    <p class="normal">Let’s move on to generating skip-grams using the <code class="inlineCode">tf.keras.preprocessing.sequence.skipgrams()</code> function, provided by TensorFlow. We call the function on a sample phrase representing the first 5 words extracted from the first article in the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">sample_word_ids = news_sequences[<span class="hljs-number">0</span>][:<span class="hljs-number">5</span>]
sample_phrase = <span class="hljs-string">' '</span>.join([tokenizer.index_word[wid] <span class="hljs-keyword">for</span> wid <span class="hljs-keyword">in</span> sample_word_ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Sample phrase: </span><span class="hljs-subst">{sample_phrase}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Sample word IDs: </span><span class="hljs-subst">{sample_word_ids }</span><span class="hljs-string">\n"</span>)
</code></pre>
    <p class="normal">This will output:</p>
    <pre class="programlisting con"><code class="hljs-con">Sample phrase: ad sales boost time warner
Sample word IDs: [4223, 187, 716, 66, 3596]
</code></pre>
    <p class="normal">Let’s consider a window size of 1. This means, for a given target word, we define the context as one word from each side of the target word.</p>
    <pre class="programlisting code"><code class="hljs-code">window_size = <span class="hljs-number">1</span> <span class="hljs-comment"># How many words to consider left and right.</span>
</code></pre>
    <p class="normal">We have all the ingredients to define extract skip-grams from the sample phrase we chose as follows. When run, this function<a id="_idIndexMarker295"/> will output data in the exact format<a id="_idIndexMarker296"/> we need the data in, that is, (target-context) tuples as inputs and corresponding labels (0 or 1) as outputs:</p>
    <pre class="programlisting code"><code class="hljs-code">inputs, labels = tf.keras.preprocessing.sequence.skipgrams(
    sequence=sample_word_ids, 
    vocabulary_size=n_vocab, 
    window_size=window_size, 
    negative_samples=<span class="hljs-number">1.0</span>, 
    shuffle=<span class="hljs-literal">False</span>,
    categorical=<span class="hljs-literal">False</span>, 
    sampling_table=<span class="hljs-literal">None</span>, 
    seed=<span class="hljs-literal">None</span>
)
</code></pre>
    <p class="normal">Let’s take a moment to reflect on some of the important arguments that have been used:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">sequence</code> <code class="inlineCode">(list[str]</code> or <code class="inlineCode">list[int])</code> – A list of words or word IDs.</li>
      <li class="bulletList"><code class="inlineCode">vocabulary_size</code> <code class="inlineCode">(int)</code> – Size of the vocabulary.</li>
      <li class="bulletList"><code class="inlineCode">window_size</code> <code class="inlineCode">(int)</code> – Size of the window to be considered for the context. <code class="inlineCode">window_size</code> defines the length on each side.</li>
      <li class="bulletList"><code class="inlineCode">negative_samples</code> <code class="inlineCode">(int)</code> – Fraction of negative candidates to generate. For example, a value of 1 means there will be an equal number of positive and negative skipgram candidates. A value of 0 means there will not be any negative candidates.</li>
      <li class="bulletList"><code class="inlineCode">shuffle</code> <code class="inlineCode">(bool)</code> – Whether to shuffle the generated inputs or not.</li>
      <li class="bulletList"><code class="inlineCode">categorical (bool)</code> – Whether to produce labels as categorical (that is, one-hot encoded) or integers.</li>
      <li class="bulletList"><code class="inlineCode">sampling_table</code> <code class="inlineCode">(np.ndarray)</code> – An array of the same size as the vocabulary. An element in a given position in the array represents the probability of sampling the word indexed by that position in the Tokenizer’s word ID to word mapping. As we will see soon, this is a handy way to avoid common uninformative words being over-sampled much.</li>
      <li class="bulletList"><code class="inlineCode">seed</code> <code class="inlineCode">(int)</code> – If shuffling is enabled, this is the random seed to be used for shuffling.</li>
    </ul>
    <p class="normal">With the inputs and labels generated, let’s print some data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">"Sample skip-grams"</span>)
<span class="hljs-keyword">for</span> inp, lbl <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(inputs, labels):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\tInput: </span><span class="hljs-subst">{inp}</span><span class="hljs-string"> (</span><span class="hljs-subst">{[tokenizer.index_word[wi] </span><span class="hljs-keyword">for</span><span class="hljs-subst"> wi </span><span class="hljs-keyword">in</span><span class="hljs-subst"> inp]}</span><span class="hljs-string">) /</span>
<span class="hljs-string">    Label: </span><span class="hljs-subst">{lbl}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">This will produce:</p>
    <pre class="programlisting con"><code class="hljs-con">Sample skip-grams
    Input: [4223, 187] (['ad', 'sales']) / Label: 1
    Input: [187, 4223] (['sales', 'ad']) / Label: 1
    Input: [187, 716] (['sales', 'boost']) / Label: 1
    Input: [716, 187] (['boost', 'sales']) / Label: 1
    Input: [716, 66] (['boost', 'time']) / Label: 1
    Input: [66, 716] (['time', 'boost']) / Label: 1
    Input: [66, 3596] (['time', 'warner']) / Label: 1
    Input: [3596, 66] (['warner', 'time']) / Label: 1
    Input: [716, 9685] (['boost', "kenya's"]) / Label: 0
    Input: [3596, 12251] (['warner', 'rear']) / Label: 0
    Input: [4223, 3325] (['ad', 'racing']) / Label: 0
    Input: [66, 7978] (['time', 'certificate']) / Label: 0
    Input: [716, 12756] (['boost', 'crushing']) / Label: 0
    Input: [66, 14543] (['time', 'touchy']) / Label: 0
    Input: [187, 3786] (['sales', '9m']) / Label: 0
    Input: [187, 3917] (['sales', 'doherty']) / Label: 0
</code></pre>
    <p class="normal">For example, since the word “sales” appears in the context of the word “ad”, it is considered a positive<a id="_idIndexMarker297"/> candidate. On the other hand, since<a id="_idIndexMarker298"/> the word “racing” (randomly sampled from the vocabulary) does not appear in the context of the word “ad”, it is added as a negative candidate.</p>
    <p class="normal">When selecting negative candidates, the <code class="inlineCode">skipgrams()</code> function selects them randomly, giving uniform weights to all the words in the vocabulary. However, the original paper explains that this can lead to poor performance. A better strategy is to use the unigram distribution as a prior for selecting negative context words.</p>
    <p class="normal">You might be wondering what a unigram distribution is. It represents the frequency counts of unigrams (or tokens) found in the text. Then the frequency counts are easily converted to probabilities (or normalized frequencies) by dividing them by the sum of all frequencies. The most amazing thing is that you don’t have to compute this by hand for every corpus of text! It turns out that if you take any sufficiently large corpus of text, compute the normalized frequencies of unigrams, and order them from high to low, you’ll see that the corpus<a id="_idIndexMarker299"/> approximately follows a certain<a id="_idIndexMarker300"/> constant distribution. For the word with rank <em class="italic">math</em> in a corpus of <em class="italic">math</em> unigrams, the normalized frequency <em class="italic">f</em><sub class="subscript">k</sub> is given by:</p>
    <p class="center"><img src="../Images/B14070_03_013.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">Here, <em class="italic">math</em> is a hyperparameter that can be tuned to<a id="_idIndexMarker301"/> match the true distribution more closely. This is known as <em class="italic">Zipf’s law</em>. In other words, if you have a vocabulary where words are ranked (ID-ed) from most common to least common, you can approximate the normalized frequency of each word using Zipf’s law. We will be sampling words according to the probabilities output through Zipf’s law instead of giving equal probabilities to the words. This means words are sampled according to their presence (that is, the more frequent, the higher the chance of being sampled) in the corpus.</p>
    <p class="normal">To do that, we can use the <code class="inlineCode">tf.random.log_uniform_candidate_sampler()</code> function. This function takes a batch of positive context candidates of shape <code class="inlineCode">[b, num_true]</code>, where <code class="inlineCode">b</code> is the batch size and <code class="inlineCode">num_true</code> is the number of true candidates per example (1 for the skip-gram model), and it outputs a [<code class="inlineCode">num_sampled</code>] sized array, where <code class="inlineCode">num_sampled</code> is the number of negative samples we need. We will discuss the nitty-gritty of this function soon, while going through an exercise. But let’s first generate some positive candidates using the <code class="inlineCode">tf.keras.preprocessing.sequence.skipgrams()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">inputs, labels = tf.keras.preprocessing.sequence.skipgrams(
    sample_phrase_word_ids, 
    vocabulary_size=<span class="hljs-built_in">len</span>(tokenizer.word_index.items())+<span class="hljs-number">1</span>, 
    window_size=window_size, 
    negative_samples=<span class="hljs-number">0</span>, 
    shuffle=<span class="hljs-literal">False</span>    
)
inputs, labels = np.array(inputs), np.array(labels)
</code></pre>
    <p class="normal">Note that we’re specifying <code class="inlineCode">negative_samples=0</code>, as we will be generating negative samples with the candidate sampler. Let’s now discuss how we can use the <code class="inlineCode">tf.random.log_uniform_candidate_sampler()</code> function to generate negative candidates. Here we will first use this function to generate negative candidates for a single word:</p>
    <pre class="programlisting code"><code class="hljs-code">negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(
    true_classes=inputs[:<span class="hljs-number">1</span>, <span class="hljs-number">1</span>:], <span class="hljs-comment"># [b, 1] sized tensor</span>
    num_true=<span class="hljs-number">1</span>, <span class="hljs-comment"># number of true words per example</span>
    num_sampled=<span class="hljs-number">10</span>,
    unique=<span class="hljs-literal">True</span>,
    range_max=n_vocab,            
    name=<span class="hljs-string">"</span><span class="hljs-string">negative_sampling"</span>
)
</code></pre>
    <p class="normal">This function<a id="_idIndexMarker302"/> takes the following<a id="_idIndexMarker303"/> arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">true_classes</code> <code class="inlineCode">(np.ndarray</code> or <code class="inlineCode">tf.Tensor)</code> – A tensor containing true target words. This needs to be a [<code class="inlineCode">b, num_true</code>] sized array, where <code class="inlineCode">num_true</code> denotes the number of true context candidates per example. Since we have one context word per example, this is 1.</li>
      <li class="bulletList"><code class="inlineCode">num_true</code> <code class="inlineCode">(int)</code> – The number of true context terms per example.</li>
      <li class="bulletList"><code class="inlineCode">num_sampled</code> <code class="inlineCode">(int)</code> – The number of negative samples to generate.</li>
      <li class="bulletList"><code class="inlineCode">unique</code> <code class="inlineCode">(bool)</code> – Whether to generate unique samples or with replacement.</li>
      <li class="bulletList"><code class="inlineCode">range_max</code> <code class="inlineCode">(int)</code> – The size of the vocabulary.</li>
    </ul>
    <p class="normal">It returns:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">sampled_candidates</code> <code class="inlineCode">(tf.Tensor)</code> – A tensor of size [<code class="inlineCode">num_sampled</code>] containing negative candidates</li>
      <li class="bulletList"><code class="inlineCode">true_expected_count</code> <code class="inlineCode">(tf.Tensor)</code> – A tensor of size [<code class="inlineCode">b, num_true</code>]; the probability of each true candidate being sampled (according to Zipf’s law)</li>
      <li class="bulletList"><code class="inlineCode">sampled_expected_count</code> <code class="inlineCode">(tf.Tensor)</code> – A tensor of size [<code class="inlineCode">num_sampled</code>]; the probabilities of each negative sample occurring along with true candidates, if sampled from the corpus</li>
    </ul>
    <p class="normal">We will not worry too much about the latter two entities. The most important to us is <code class="inlineCode">sampled_candidates</code>. When calling the function, we have to make sure <code class="inlineCode">true_classes</code> has the shape <code class="inlineCode">[b, num_true]</code>. In our case, we will run this in a single input word ID, which will be in the shape [1, 1]. It returns the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Positive sample: [[187]]
Negative samples: [   1   10 9744 3062  139    5   14   78 1402  115]
true_expected_count: [[0.00660027]]
sampled_expected_count: [4.0367463e-01 1.0333969e-01 1.2804421e-04 4.0727769e-04 8.8460185e-03
 1.7628242e-01 7.7631921e-02 1.5584969e-02 8.8879210e-04 1.0659459e-02]
</code></pre>
    <p class="normal">Now, putting everything<a id="_idIndexMarker304"/> together, let’s write a data<a id="_idIndexMarker305"/> generator function that generates batches of data for the model. This function, named <code class="inlineCode">skip_gram_data_generator()</code>, takes the following arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">sequences</code> <code class="inlineCode">(List[List[int]])</code> – A list of list of word IDs. This is the output generated by the Tokenizer’s <code class="inlineCode">texts_to_sequences()</code> function.</li>
      <li class="bulletList"><code class="inlineCode">window_size</code> <code class="inlineCode">(int)</code> – The window size for the context.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> <code class="inlineCode">(int)</code> – The batch size.</li>
      <li class="bulletList"><code class="inlineCode">negative_samples</code> <code class="inlineCode">(int)</code> – The number of negative samples per example to generate.</li>
      <li class="bulletList"><code class="inlineCode">vocabulary_size</code> <code class="inlineCode">(int)</code> – The vocabulary size.</li>
      <li class="bulletList"><code class="inlineCode">seed </code>– The random seed.</li>
    </ul>
    <p class="normal">It will return a batch of data containing:</p>
    <ul>
      <li class="bulletList">A batch of target word IDs</li>
      <li class="bulletList">A batch of corresponding context word IDs (both positive and negative)</li>
      <li class="bulletList">A batch of labels (0 and 1)</li>
    </ul>
    <p class="normal">The function signature looks as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">skip_gram_data_generator</span>(<span class="hljs-params">sequences, window_size, batch_size, negative_samples, vocab_size, seed=</span><span class="hljs-literal">None</span>):
</code></pre>
    <p class="normal">First, we are going to shuffle the news articles so that every time we generate data, they are fetched in a different order. This helps the model to generalize better:</p>
    <pre class="programlisting code"><code class="hljs-code">    rand_sequence_ids = np.arange(<span class="hljs-built_in">len</span>(sequences))
    np.random.shuffle(rand_sequence_ids)
</code></pre>
    <p class="normal">Next, for each text sequence in the corpus we generate positive skip grams. <code class="inlineCode">positive_skip_grams</code> contains tuples of (target, context) word pairs in that order:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> si <span class="hljs-keyword">in</span> rand_sequence_ids:
        
        positive_skip_grams, _ = 
        tf.keras.preprocessing.sequence.skipgrams(
            sequences[si], 
            vocabulary_size=vocab_size, 
            window_size=window_size, 
            negative_samples=<span class="hljs-number">0.0</span>, 
            shuffle=<span class="hljs-literal">False</span>,
            sampling_table=sampling_table,
            seed=seed
        )
</code></pre>
    <p class="normal">Note that we are passing a <code class="inlineCode">sampling_table</code> argument. This is another strategy to enhance the performance<a id="_idIndexMarker306"/> of Word2vec models. <code class="inlineCode">sampling_table </code>is simply<a id="_idIndexMarker307"/> an array that is the same size as your vocabulary and specifies a probability at each index of the array with which the word indexed by that index will be sampled during skip gram<a id="_idIndexMarker308"/> generation. This technique is known as subsampling. Each word <em class="italic">w</em><sub class="subscript">i</sub> is sampled with the probability given by the following equation:</p>
    <p class="center"><img src="../Images/B14070_03_014.png" alt="" style="height: 3.55em !important; vertical-align: 0.07em !important;"/></p>
    <p class="normal">Here, <em class="italic">t</em> is a tunable parameter. It defaults to 0.00001 for a large enough corpus. In TensorFlow, you can generate this table easily as follows. </p>
    <p class="normal">You don’t need the exact frequencies to compute the sampling table, as we can leverage Zipf’s law to approximate those frequencies:</p>
    <pre class="programlisting code"><code class="hljs-code">sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(
    n_vocab, sampling_factor=<span class="hljs-number">1e-05</span>
)
</code></pre>
    <p class="normal">For each tuple contained in <code class="inlineCode">positive_skip_grams</code>, we generate <code class="inlineCode">negative_samples</code> number of negative candidates. We then populate targets, contexts, and label lists with both positive and negative candidates:</p>
    <pre class="programlisting code"><code class="hljs-code">        targets, contexts, labels = [], [], []
        
        <span class="hljs-keyword">for</span> target_word, context_word <span class="hljs-keyword">in</span> positive_skip_grams:
            context_class = tf.expand_dims(tf.constant([context_word], 
            dtype=<span class="hljs-string">"int64"</span>), <span class="hljs-number">1</span>)
            
            negative_sampling_candidates, _, _ = 
            tf.random.log_uniform_candidate_sampler(
              true_classes=context_class,
              num_true=<span class="hljs-number">1</span>,
              num_sampled=negative_samples,
              unique=<span class="hljs-literal">True</span>,
              range_max=vocab_size,
              name=<span class="hljs-string">"negative_sampling"</span>)
            <span class="hljs-comment"># Build context and label vectors (for one target word)</span>
            context = tf.concat(
                [tf.constant([context_word], dtype=<span class="hljs-string">'int64'</span>), 
                negative_sampling_candidates],
                axis=<span class="hljs-number">0</span>
            )
            label = tf.constant([<span class="hljs-number">1</span>] + [<span class="hljs-number">0</span>]*negative_samples, 
            dtype=<span class="hljs-string">"int64"</span>)
            <span class="hljs-comment"># Append each element from the training example to global</span>
<span class="hljs-comment">            # lists.</span>
            targets.extend([target_word]*(negative_samples+<span class="hljs-number">1</span>))
            contexts.append(context)
            labels.append(label)
</code></pre>
    <p class="normal">We will then convert<a id="_idIndexMarker309"/> these to arrays as follows<a id="_idIndexMarker310"/> and randomly shuffle the data. When shuffling, you have to make sure all the arrays are consistently shuffled. Otherwise, you will corrupt the labels associated with the inputs:</p>
    <pre class="programlisting code"><code class="hljs-code">        contexts, targets, labels = np.concatenate(contexts), 
        np.array(targets), np.concatenate(labels)
        
        <span class="hljs-comment"># If seed is not provided generate a random one</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> seed:
            seed = random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10e6</span>)
        np.random.seed(seed)
        np.random.shuffle(contexts)
        np.random.seed(seed)
        np.random.shuffle(targets)
        np.random.seed(seed)
        np.random.shuffle(labels)
</code></pre>
    <p class="normal">Finally, batches of data are generated as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">for</span> eg_id_start <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, contexts.shape[<span class="hljs-number">0</span>], batch_size): 
            <span class="hljs-keyword">yield</span> (
                targets[eg_id_start: <span class="hljs-built_in">min</span>(eg_id_start+batch_size, 
                inputs.shape[<span class="hljs-number">0</span>])], 
                contexts[eg_id_start: <span class="hljs-built_in">min</span>(eg_id_start+batch_size, 
                inputs.shape[<span class="hljs-number">0</span>])]
            ), labels[eg_id_start: <span class="hljs-built_in">min</span>(eg_id_start+batch_size, 
               inputs.shape[<span class="hljs-number">0</span>])]
</code></pre>
    <p class="normal">Next, we will<a id="_idIndexMarker311"/> look at the specifics<a id="_idIndexMarker312"/> of the model we’re going to use.</p>
    <h3 id="_idParaDest-79" class="heading-3">Implementing the skip-gram architecture with TensorFlow</h3>
    <p class="normal">We will now walk through an implementation<a id="_idIndexMarker313"/> of the skip-gram algorithm<a id="_idIndexMarker314"/> that uses the TensorFlow library. The full exercise is available in <code class="inlineCode">ch3_word2vec.ipynb</code> in the <code class="inlineCode">Ch03-Word-Vectors</code> exercise directory.</p>
    <p class="normal">First, let’s define the hyperparameters of the model. You are free to change these hyperparameters to see how they affect final performance (for example, <code class="inlineCode">batch_size = 1024</code> or <code class="inlineCode">batch_size = 2048</code>). However, since this is a simpler problem than the more complex real-world problems, you might not see any significant differences (unless you change them to extremes, for example, <code class="inlineCode">batch_size = 1</code> or <code class="inlineCode">num_sampled = 1</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">4096</span> <span class="hljs-comment"># Data points in a single batch</span>
embedding_size = <span class="hljs-number">128</span> <span class="hljs-comment"># Dimension of the embedding vector.</span>
window_size=<span class="hljs-number">1</span> <span class="hljs-comment"># We use a window size of 1 on either side of target word</span>
negative_samples = <span class="hljs-number">4</span> <span class="hljs-comment"># Number of negative samples generated per example</span>
epochs = <span class="hljs-number">5</span> <span class="hljs-comment"># Number of epochs to train for</span>
<span class="hljs-comment"># We pick a random validation set to sample nearest neighbors</span>
valid_size = <span class="hljs-number">16</span> <span class="hljs-comment"># Random set of words to evaluate similarity on.</span>
<span class="hljs-comment"># We sample valid datapoints randomly from a large window without always</span>
<span class="hljs-comment"># being deterministic</span>
valid_window = <span class="hljs-number">250</span>
<span class="hljs-comment"># When selecting valid examples, we select some of the most frequent words # as well as</span> <span class="hljs-comment">some moderately rare words as well</span>
np.random.seed(<span class="hljs-number">54321</span>)
random.seed(<span class="hljs-number">54321</span>)
valid_term_ids = np.array(random.sample(<span class="hljs-built_in">range</span>(valid_window), valid_size))
valid_term_ids = np.append(
    valid_term_ids, random.sample(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>+valid_window), 
    valid_size),
    axis=<span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal">Next, we define<a id="_idIndexMarker315"/> the model. To do this, we will be relying<a id="_idIndexMarker316"/> on the Functional API of Keras. We need to go beyond the simplest API, that is, the Sequential API, as this model requires two input streams (one for the context and one for the target).</p>
    <p class="normal">We will start off with an import. Then we will clear any current running sessions, to make sure there aren’t any other models occupying the hardware:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
K.clear_session()
</code></pre>
    <p class="normal">We will define two input layers:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Inputs - skipgrams() function outputs target, context in that order</span>
input_1 = tf.keras.layers.Input(shape=(), name=<span class="hljs-string">'target'</span>)
input_2 = tf.keras.layers.Input(shape=(), name=<span class="hljs-string">'context'</span>)
</code></pre>
    <p class="normal">Note how the shape is defined as <code class="inlineCode">()</code>. When defining the <code class="inlineCode">shape</code> argument, the actual output shape will have a new undefined dimension (i.e. <code class="inlineCode">None</code> sized) added. In other words, the final output shape will be <code class="inlineCode">[None]</code>.</p>
    <p class="normal">Next, we define two embedding layers: a target embedding layer and a context embedding layer. These layers will be used to look up the embeddings for target and context word IDs that will be generated by the input generation function.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Two embeddings layers are used one for the context and one for the</span>
<span class="hljs-comment"># target</span>
target_embedding_layer = tf.keras.layers.Embedding(
    input_dim=n_vocab, output_dim=embedding_size, 
    name=<span class="hljs-string">'target_embedding'</span>
)
context_embedding_layer = tf.keras.layers.Embedding(
    input_dim=n_vocab, output_dim=embedding_size, 
    name=<span class="hljs-string">'context_embedding'</span>
)
</code></pre>
    <p class="normal">With the embedding <a id="_idIndexMarker317"/>layers defined, let’s look up the embeddings<a id="_idIndexMarker318"/> for the word IDs that will be fed to the input layers:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Lookup outputs of the embedding layers</span>
target_out = target_embedding_layer(input_1)
context_out = context_embedding_layer(input_2)
</code></pre>
    <p class="normal">We now need to compute the dot product of <code class="inlineCode">target_out</code> and <code class="inlineCode">context_out</code>. </p>
    <p class="normal">To do that, we are going to use the <code class="inlineCode">tf.keras.layers.Dot</code> layer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Computing the dot product between the two </span>
out = tf.keras.layers.Dot(axes=-<span class="hljs-number">1</span>)([context_out, target_out])
</code></pre>
    <p class="normal">Finally, we define our model as a <code class="inlineCode">tf.keras.models.Model</code> object, where we specify <code class="inlineCode">inputs</code> and <code class="inlineCode">outputs</code> arguments. <code class="inlineCode">inputs</code> need to be one or more input layers, and <code class="inlineCode">outputs</code> can be one or more outputs produced by a series of <code class="inlineCode">tf.keras.layers</code> objects:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Defining the model</span>
skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name=<span class="hljs-string">'</span><span class="hljs-string">skip_gram_model'</span>)
</code></pre>
    <p class="normal">We compile the model using a loss function and an optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Compiling the model</span>
skip_gram_model.<span class="hljs-built_in">compile</span>(loss=tf.keras.losses.BinaryCrossentropy(from_logits=<span class="hljs-literal">True</span>), optimizer=<span class="hljs-string">'adam'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Let’s see a summary of our model by calling the following:</p>
    <pre class="programlisting code"><code class="hljs-code">skip_gram_model.summary()
</code></pre>
    <p class="normal">This will output:</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "skip_gram_model"
________________________________________________________________________
Layer (type)                    Output Shape     Param   # Connected to
========================================================================
context (InputLayer)            [(None,)]        0         
_______________________________________________________________________
target (InputLayer)             [(None,)]        0         
_______________________________________________________________________
context_embedding (Embedding)   (None, 128)     1920128    context[0][0]
_______________________________________________________________________
target_embedding (Embedding)    (None, 128)     1920128    target[0][0]
_______________________________________________________________________
dot (Dot)                        (None, 1)      0  context_embedding[0][0]
                                                   target_embedding[0][0]
=======================================================================
Total params: 3,840,256
Trainable params: 3,840,256
Non-trainable params: 0
________________________________________________________________________
</code></pre>
    <p class="normal">Training<a id="_idIndexMarker319"/> and evaluating<a id="_idIndexMarker320"/> the model will be the next item on our agenda.</p>
    <h3 id="_idParaDest-80" class="heading-3">Training and evaluating the model</h3>
    <p class="normal">Our training process<a id="_idIndexMarker321"/> is going to be very simple as we have defined a function<a id="_idIndexMarker322"/> to generate batches of data in the exact format the model needs them in. But before we go ahead with training the model, we need to think about how we evaluate word vector models. The idea of word vectors is that words sharing semantic similarity will have a smaller distance between them, whereas words with no similarity will be far apart. To compute the similarities between words, we can use the cosine distance. We picked a set of random word IDs and stored them in <code class="inlineCode">valid_term_ids</code> during our hyperparameter discussion. We will implement a way to compute the closest <code class="inlineCode">k</code> words to each of those terms at the end of every epoch.</p>
    <p class="normal">For this, we utilize Keras callbacks. Keras callbacks give you<a id="_idIndexMarker323"/> a way to execute some<a id="_idIndexMarker324"/> important operation(s) at the end of every training iteration, epoch, prediction<a id="_idIndexMarker325"/> step, and so on. You can see a full list of the available callbacks at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/callbacks</span></a>. Since we need a bespoke evaluation mechanism designed for word vectors, we will need to implement our own callback. Our callback will take a list of word IDs intended as the validation words, a model containing the embedding matrix, and a Tokenizer to decode word IDs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ValidationCallback</span>(tf.keras.callbacks.Callback):
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, valid_term_ids, model_with_embeddings, tokenizer</span>):
        
        self.valid_term_ids = valid_term_ids
        self.model_with_embeddings = model_with_embeddings
        self.tokenizer = tokenizer
        
        <span class="hljs-built_in">super</span>().__init__()
        
    <span class="hljs-keyword">def</span> <span class="hljs-title">on_epoch_end</span>(<span class="hljs-params">self, epoch, logs=</span><span class="hljs-literal">None</span>):
        <span class="hljs-string">""" Validation logic """</span>
        <span class="hljs-comment"># We will use context embeddings to get the most similar words</span>
        <span class="hljs-comment"># Other strategies include: using target embeddings, mean</span>
<span class="hljs-comment">        # embeddings after avaraging context/target</span>
        embedding_weights = 
        self.model_with_embeddings.get_layer(
            <span class="hljs-string">"context_embedding"</span>
<span class="hljs-string">        </span>).get_weights()[<span class="hljs-number">0</span>]
        normalized_embeddings = embedding_weights / 
        np.sqrt(np.<span class="hljs-built_in">sum</span>(embedding_weights**<span class="hljs-number">2</span>, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))
        
        <span class="hljs-comment"># Get the embeddings corresponding to valid_term_ids</span>
        valid_embeddings = normalized_embeddings[self.valid_term_ids, 
        :]
        
        <span class="hljs-comment"># Compute the similarity between valid_term_ids and all the</span>
<span class="hljs-comment">        # embeddings</span>
        <span class="hljs-comment"># V x d (d x D) =&gt; V x D</span>
        top_k = <span class="hljs-number">5</span> <span class="hljs-comment"># Top k items will be displayed</span>
        similarity = np.dot(valid_embeddings, normalized_embeddings.T)
        
        <span class="hljs-comment"># Invert similarity matrix to negative</span>
        <span class="hljs-comment"># Ignore the first one because that would be the same word as the</span>
<span class="hljs-comment">        # probe word</span>
        similarity_top_k = np.argsort(-similarity, axis=<span class="hljs-number">1</span>)[:, <span class="hljs-number">1</span>: 
        top_k+<span class="hljs-number">1</span>]
                
        <span class="hljs-comment"># Print the output</span>
        <span class="hljs-keyword">for</span> i, term_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_term_ids):
            similar_word_str = <span class="hljs-string">', '</span>.join([self.tokenizer.index_word[j] 
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> similarity_top_k[i, :] <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">1</span>])
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{self.tokenizer.index_word[term_id]}</span><span class="hljs-string">: </span>
<span class="hljs-string">            </span><span class="hljs-subst">{similar_word_str }</span><span class="hljs-string">"</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'\n'</span>)
</code></pre>
    <p class="normal">The evaluation will be done<a id="_idIndexMarker326"/> at the end of a training epoch, therefore<a id="_idIndexMarker327"/> we will override the <code class="inlineCode">on_epoch_end() </code>function. The function extracts the embeddings from the context embedding layer. </p>
    <p class="normal">Then the embeddings are normalized to have a unit length. Afterward, embeddings corresponding to validation words are extracted to a separate matrix called <code class="inlineCode">valid_embeddings</code>. Then the cosine distance is computed between the validation embeddings and all word embeddings, which results in a <code class="inlineCode">[valid_size, vocabulary size] </code>sized matrix. From this, we extract the top <code class="inlineCode">k</code> similar words and display them through <code class="inlineCode">print</code> statements.</p>
    <p class="normal">Finally, the model can be trained as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)
<span class="hljs-keyword">for</span> ei <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Epoch: </span><span class="hljs-subst">{ei+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">/</span><span class="hljs-subst">{epochs}</span><span class="hljs-string"> started"</span>)
    
    news_skip_gram_gen = skip_gram_data_generator(
        news_sequences, window_size, batch_size, negative_samples, 
        n_vocab
    )
    
    skip_gram_model.fit(
        news_skip_gram_gen, epochs=<span class="hljs-number">1</span>, 
        callbacks=skipgram_validation_callback,
    )
</code></pre>
    <p class="normal">We are simply defining<a id="_idIndexMarker328"/> an instance of the callback first. Next, we train the model for several<a id="_idIndexMarker329"/> epochs. In each, we generate skip gram data (while shuffling the order of the articles) and call <code class="inlineCode">skip_gram_model.fit()</code> on the data. Here’s the result after five epochs of training:</p>
    <pre class="programlisting con"><code class="hljs-con">Epoch: 5/5 ended
2233/2233 [==============================] - 146s 65ms/step - loss: 0.4842 - accuracy: 0.8056
months: days, weeks, years, detained, meaning
were: are, was, now, davidson, widened
mr: resignation, scott, tony, stead, article
champions: premier, pottage, kampala, danielli, dominique
businesses: medium, port, 2002's, tackling, doug
positive: electorate, proposal, bolz, visitors', strengthen
pop: 'me', style, lacks, tourism, tuesdays
</code></pre>
    <p class="normal">Here, we denote some of the most sensible word vectors learned. For example, we can see that two of the most similar words to the word “months” are “days” and “weeks”. The title “mr” is accompanied by male names such as “scott” and “tony”. The word “premier” appears as a similar word to “champion”. You can further experiment with:</p>
    <ul>
      <li class="bulletList">Different<a id="_idIndexMarker330"/> negative candidate sampling methods available at <a href="https://www.tensorflow.org/api_docs/python/tf/random"><span class="url">https://www.tensorflow.org/api_docs/python/tf/random</span></a></li>
      <li class="bulletList">Different hyperparameter choices (such as the embedding size and the number of negative samples)</li>
    </ul>
    <p class="normal">In this section, we discussed the skip-gram algorithm from end to end. We saw how we can use functions in TensorFlow to transform data. Then we implemented the skip-gram architecture using layers in Keras and the Functional API. Finally, we trained the model and visually inspected its performance on some test data. We will now discuss another popular Word2vec algorithm known as the <strong class="keyWord">Continuous Bag-of-Words</strong> (<strong class="keyWord">CBOW</strong>) model.</p>
    <h1 id="_idParaDest-81" class="heading-1">The Continuous Bag-of-Words algorithm</h1>
    <p class="normal">The CBOW model works<a id="_idIndexMarker331"/> in a similar way to the skip-gram algorithm, with one significant change in the problem formulation. In the skip-gram model, we predict the context words from the target word. However, in the CBOW model, we predict the target word from contextual words. Let’s compare what data looks like for the skip-gram algorithm and the CBOW model by taking the previous example sentence:</p>
    <p class="normal"><em class="italic">The dog barked at the mailman.</em></p>
    <p class="normal">For the skip-gram<a id="_idIndexMarker332"/> algorithm, the data tuples—<em class="italic">(input word, output word)</em>—might look like this:</p>
    <p class="normal"><em class="italic">(dog, the)</em>, <em class="italic">(dog, barked)</em>, <em class="italic">(barked, dog)</em>, and so on</p>
    <p class="normal">For CBOW, the data tuples would look like the following:</p>
    <p class="normal"><em class="italic">([the, barked], dog)</em>, <em class="italic">([dog, at], barked)</em>, and so on</p>
    <p class="normal">Consequently, the input of the CBOW has a dimensionality of 2 × m × D, where <em class="italic">m</em> is the context window size and <em class="italic">D</em> is the dimensionality of the embeddings. The conceptual model of CBOW is shown in <em class="italic">Figure 3.13</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_03_08.png" alt="C:\Users\gauravg\Desktop\14070\CH03\B08681_03_29.png"/></figure>
    <p class="packt_figref">Figure 3.8: The CBOW model</p>
    <p class="normal">We will not go into great detail about the intricacies of CBOW as it is quite similar to skip-gram. For example, once the embeddings are aggregated (that is, concatenated or summed), they flow through a softmax layer to finally compute the same loss as we did with the skip-gram algorithm. However, we will discuss the algorithm’s implementation (though not in depth) to get a clear understanding of how to properly implement CBOW. The full implementation of CBOW<a id="_idIndexMarker333"/> is available at <code class="inlineCode">ch3_word2vec.ipynb</code> in the <code class="inlineCode">Ch03-Word-Vectors</code> exercise folder.</p>
    <h2 id="_idParaDest-82" class="heading-2">Generating data for the CBOW algorithm</h2>
    <p class="normal">Unfortunately, unlike<a id="_idIndexMarker334"/> for the skip-gram algorithm, we do<a id="_idIndexMarker335"/> not have a handy function to generate data for the CBOW algorithm at our disposal. Therefore, we will need to implement this function ourselves. </p>
    <p class="normal">You can find the implementation of this function (named <code class="inlineCode">cbow_grams()</code>) in <code class="inlineCode">ch3_word2vec.ipynb</code> in the <code class="inlineCode">Ch03-Word-Vectors</code> folder. The procedure will be quite similar to the one we used for skip-grams. However, the format of the data will be slightly different. Therefore, we will discuss the format of the data returned by this function.</p>
    <p class="normal">The function takes the same arguments as the <code class="inlineCode">skip_gram_data_generator()</code> function we discussed earlier:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">sequences</code> <code class="inlineCode">(List[List[int]])</code> – A list of list of word IDs. This is the output generated by Tokenizer’s <code class="inlineCode">texts_to_sequences()</code> function.</li>
      <li class="bulletList"><code class="inlineCode">window_size</code> <code class="inlineCode">(int)</code> – The window size for the context.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> <code class="inlineCode">(int)</code> – The batch size.</li>
      <li class="bulletList"><code class="inlineCode">negative_samples</code> <code class="inlineCode">(int)</code> – The number of negative samples per example to generate.</li>
      <li class="bulletList"><code class="inlineCode">vocabulary_size</code> <code class="inlineCode">(int)</code> – The vocabulary size.</li>
      <li class="bulletList"><code class="inlineCode">seed </code>– The random seed.</li>
    </ul>
    <p class="normal">The data returned also has a slightly different format. It will return a batch of data containing:</p>
    <ul>
      <li class="bulletList">A batch of target word IDs, these target words are both positive and negative.</li>
      <li class="bulletList">A batch of corresponding context word IDs. Unlike skip-grams, for CBOW, we need all the words in the context, not just one. For example, if we define a batch size of <code class="inlineCode">b</code> and window size of <code class="inlineCode">w</code>, this will be a <code class="inlineCode">[b, 2w]</code> sized tensor.</li>
      <li class="bulletList">A batch or labels (0 and 1).</li>
    </ul>
    <p class="normal">We will now learn<a id="_idIndexMarker336"/> about the specifics<a id="_idIndexMarker337"/> of the algorithm.</p>
    <h2 id="_idParaDest-83" class="heading-2">Implementing CBOW in TensorFlow</h2>
    <p class="normal">We will use<a id="_idIndexMarker338"/> the same hyperparameters<a id="_idIndexMarker339"/> as before:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">4096</span> <span class="hljs-comment"># Data points in a single batch</span>
embedding_size = <span class="hljs-number">128</span> <span class="hljs-comment"># Dimension of the embedding vector.</span>
window_size=<span class="hljs-number">1</span> <span class="hljs-comment"># We use a window size of 1 on either side of target word</span>
epochs = <span class="hljs-number">5</span> <span class="hljs-comment"># Number of epochs to train for</span>
negative_samples = <span class="hljs-number">4</span> <span class="hljs-comment"># Number of negative samples generated per example</span>
<span class="hljs-comment"># We pick a random validation set to sample nearest neighbors</span>
valid_size = <span class="hljs-number">16</span> <span class="hljs-comment"># Random set of words to evaluate similarity on.</span>
<span class="hljs-comment"># We sample valid datapoints randomly from a large window without always</span>
<span class="hljs-comment"># being deterministic</span>
valid_window = <span class="hljs-number">250</span>
<span class="hljs-comment"># When selecting valid examples, we select some of the most frequent words</span>
<span class="hljs-comment"># as well as</span> <span class="hljs-comment">some moderately rare words as well</span>
np.random.seed(<span class="hljs-number">54321</span>)
random.seed(<span class="hljs-number">54321</span>)
valid_term_ids = np.array(random.sample(<span class="hljs-built_in">range</span>(valid_window), valid_size))
valid_term_ids = np.append(
    valid_term_ids, random.sample(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>+valid_window), 
    valid_size),
    axis=<span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal">Just as before, let’s first clear out any remaining sessions, if there are any:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
K.clear_session()
</code></pre>
    <p class="normal">We define two input layers. Note how the second input layer is defined to have <code class="inlineCode">2 x window_size</code> dimensions. This means the final shape of that layer will be<code class="inlineCode"> [None, 2 x window_size]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Inputs</span>
input_1 = tf.keras.layers.Input(shape=())
input_2 = tf.keras.layers.Input(shape=(window_size*<span class="hljs-number">2</span>,))
</code></pre>
    <p class="normal">Let’s now define two<a id="_idIndexMarker340"/> embedding layers: one<a id="_idIndexMarker341"/> for the context words and one for the target words. We will feed the inputs from the input layers and produce <code class="inlineCode">context_out</code> and <code class="inlineCode">target_out</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">context_embedding_layer = tf.keras.layers.Embedding(
    input_dim=n_vocab+<span class="hljs-number">1</span>, output_dim=embedding_size, 
    name=<span class="hljs-string">'context_embedding'</span>
)
target_embedding_layer = tf.keras.layers.Embedding(
    input_dim=n_vocab+<span class="hljs-number">1</span>, output_dim=embedding_size, 
    name=<span class="hljs-string">'target_embedding'</span>
)
context_out = context_embedding_layer(input_2)
target_out = target_embedding_layer(input_1)
</code></pre>
    <p class="normal">If you look at the shape of <code class="inlineCode">context_out</code>, you will see that it has the shape <code class="inlineCode">[None, 2, 128]</code>, where <code class="inlineCode">2</code> is <code class="inlineCode">2 x window_size</code>, due to taking the whole context around a word. This needs to be reduced to <code class="inlineCode">[None, 128]</code> by taking the average of all the context words. This is done by using a Lambda layer:</p>
    <pre class="programlisting code"><code class="hljs-code">mean_context_out = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.reduce_mean(x, axis=<span class="hljs-number">1</span>))(context_out)
</code></pre>
    <p class="normal">We pass a <code class="inlineCode">Lambda</code> function to the <code class="inlineCode">tf.keras.layers.Lambda</code> layer to reduce the <code class="inlineCode">context_out</code> tensor on the second dimension to produce a <code class="inlineCode">[None, 128]</code> sized tensor. With both the <code class="inlineCode">target_out</code> and <code class="inlineCode">mean_context_out</code> tensors having the shape <code class="inlineCode">[None, 128]</code>, we can compute the dot product of the two to produce an output tensor <code class="inlineCode">[None, 1]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">out = tf.keras.layers.Dot(axes=-<span class="hljs-number">1</span>)([context_out, target_out])
</code></pre>
    <p class="normal">With that, we can define the final model as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">cbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name=<span class="hljs-string">'cbow_model'</span>)
</code></pre>
    <p class="normal">Similar to <code class="inlineCode">skip_gram_model</code>, we will compile <code class="inlineCode">cbow_model</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">cbow_model.<span class="hljs-built_in">compile</span>(
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=<span class="hljs-literal">True</span>), 
    optimizer=<span class="hljs-string">'adam'</span>, 
    metrics=[<span class="hljs-string">'accuracy'</span>]
)
</code></pre>
    <p class="normal">Again, if you would<a id="_idIndexMarker342"/> like to see the summary<a id="_idIndexMarker343"/> of the model, you can run <code class="inlineCode">cbow_model.summary()</code>.</p>
    <h2 id="_idParaDest-84" class="heading-2">Training and evaluating the model</h2>
    <p class="normal">The model training is identical<a id="_idIndexMarker344"/> to how we trained the skip-gram<a id="_idIndexMarker345"/> model. First, let’s define a callback to find the top k words similar to the words defined in the <code class="inlineCode">valid_term_ids</code> set:</p>
    <pre class="programlisting code"><code class="hljs-code">cbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer)
</code></pre>
    <p class="normal">Next, we train <code class="inlineCode">cbow_model</code> for several epochs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> ei <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Epoch: </span><span class="hljs-subst">{ei+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">/</span><span class="hljs-subst">{epochs}</span><span class="hljs-string"> started"</span>)
    news_cbow_gen = cbow_data_generator(
        news_sequences, 
        window_size, 
        batch_size, 
        negative_samples
    )
    cbow_model.fit(
        news_cbow_gen, 
        epochs=<span class="hljs-number">1</span>, 
        callbacks=cbow_validation_callback,
    )
</code></pre>
    <p class="normal">The output should look like the following. We have cherry-picked some of the most sensible word vectors learned:</p>
    <pre class="programlisting con"><code class="hljs-con">months: years, days, weeks, minutes, seasons
you: we, they, i, don't, we'll
were: are, aren't, have, because, need
music: terrestrial, cameras, casual, divide, camera
also: already, previously, recently, rarely, reportedly
best: supporting, actress, category, fiction, contenders
him: them, me, themselves, won't, censors
mr: tony, gordon, resignation, cherie, jack
5bn: 5m, 7bn, 4bn, 8bn, 8m
champions: premier, rugby, appearances, irish, midfielder
deutsche: austria, austria's, butcher, violence, 1989
files: movies, collections, vast, habit, ballad
pop: fiction, veteran, scrubs, wars, commonwealth
</code></pre>
    <p class="normal">From visual inspection, it seems CBOW has learned some effective word vectors. Similar to the skip-gram model, it has picked<a id="_idIndexMarker346"/> words like “years” and “days” as similar<a id="_idIndexMarker347"/> to “months”. Numerical values such as “5bn” have “5m” and “7bn” around them. But it’s important to remember that visual inspection is just a quick and dirty way to evaluate word vectors.</p>
    <p class="normal">Typically, word vectors are evaluated on some downstream tasks. One of the popular tasks is the word analogical reasoning task. It focuses on answering questions like:</p>
    <p class="normal"><em class="italic">Athens is to Greece as Baghdad to ____</em></p>
    <p class="normal">The answer is <code class="inlineCode">Iraq</code>. How is the answer computed? If the word vectors are sensible, then:</p>
    <p class="normal"><code class="inlineCode">Word2vec(Athens) – Word2vec(Greece) = Word2vec(Baghdad) – Word2vec(Iraq)</code></p>
    <p class="normal">or</p>
    <p class="normal"><code class="inlineCode">Word2vec(Iraq) = Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)</code></p>
    <p class="normal">The answer is computed as the vector given by <code class="inlineCode">Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)</code>. The next step for this analogy task would be to see if the most similar vector to the resulting vector is given by the word Iraq. This way, accuracy can be computed for an analogy reasoning task. However, we will not utilize this task in this chapter, as our dataset is not big enough to perform well in this task.</p>
    <p class="normal">Here, we conclude<a id="_idIndexMarker348"/> our discussion on the CBOW<a id="_idIndexMarker349"/> algorithm. Though CBOW shares similarities with the skip-gram algorithm, it had some architectural differences as well as differences in data.</p>
    <h1 id="_idParaDest-85" class="heading-1">Summary</h1>
    <p class="normal">Word embeddings have become an integral part of many NLP tasks and are widely used for tasks such as machine translation, chatbots, image caption generation, and language modeling. Not only do word embeddings act as a dimensionality reduction technique (compared to one-hot encoding), they also give a richer feature representation than other techniques. In this chapter, we discussed two popular neural-network-based methods for learning word representations, namely the skip-gram model and the CBOW model.</p>
    <p class="normal">First, we discussed the classical approaches to this problem to develop an understanding of how word representations were learned in the past. We discussed various methods, such as using WordNet, building a co-occurrence matrix of the words, and calculating TF-IDF.</p>
    <p class="normal">Next, we explored neural-network-based word representation learning methods. First, we worked out an example by hand to understand how word embeddings or word vectors can be calculated to help us understand the computations involved.</p>
    <p class="normal">Next, we discussed the first word-embedding learning algorithm—the skip-gram model. We then learned how to prepare the data to be used for learning. Later, we examined how to design a loss function that allows us to use word embeddings using the context words of a given word. Finally, we discussed how to implement the skip-gram algorithm using TensorFlow.</p>
    <p class="normal">Then we reviewed the next choice for learning word embeddings—the CBOW model. We also discussed how CBOW differs from the skip-gram model. Finally, we discussed a TensorFlow implementation of CBOW as well.</p>
    <p class="normal">In the next chapter, we will learn several other word embedding learning techniques known as Global Vectors, or GloVe, and Embeddings from Language Models, or ELMo.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"> <img src="../Images/QR_Code5143653472357468031.png" alt=""/></figure>
  </div>
</body></html>