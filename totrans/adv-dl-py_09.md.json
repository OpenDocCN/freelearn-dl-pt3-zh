["```\nimport logging\nimport pprint  # beautify prints\n\nimport gensim\nimport nltk\n```", "```\nlogging.basicConfig(level=logging.INFO)\n```", "```\nclass TokenizedSentences:\n    \"\"\"Split text to sentences and tokenize them\"\"\"\n\n    def __init__(self, filename: str):\n        self.filename = filename\n\n    def __iter__(self):\n        with open(self.filename) as f:\n            corpus = f.read()\n\n        raw_sentences = nltk.tokenize.sent_tokenize(corpus)\n        for sentence in raw_sentences:\n            if len(sentence) > 0:\n                yield gensim.utils.simple_preprocess(sentence, min_len=2, max_len=15)\n```", "```\nsentences = TokenizedSentences('war_and_peace.txt')\n```", "```\nmodel = gensim.models.word2vec. \\\n    Word2Vec(sentences=sentences,\n             sg=1,  # 0 for CBOW and 1 for Skip-gram\n             window=5,  # the size of the context window\n             negative=5,  # negative sampling word count\n             min_count=5,  # minimal word occurrences to include\n             iter=5,  # number of epochs\n             )\n```", "```\n[('sister', 0.9024157524108887),\n ('daughter', 0.8976515531539917),\n ('brother', 0.8965438008308411),\n ('father', 0.8935455679893494),\n ('husband', 0.8779271245002747)]\n```", "```\n[('heiress', 0.9176832437515259), ('admirable', 0.9104862213134766), ('honorable', 0.9047746658325195), ('creature', 0.9040032625198364), ('depraved', 0.9013445973396301)]\n```", "```\nimport logging\nimport pprint  # beautify prints\n\nimport gensim.downloader as gensim_downloader\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.manifold import TSNE\n\nlogging.basicConfig(level=logging.INFO)\n```", "```\nmodel = Word2Vec(\n    sentences=gensim_downloader.load('text8'),  # download and load the text8 dataset\n    sg=0, size=100, window=5, negative=5, min_count=5, iter=5)\n```", "```\n[('queen', 0.6532326936721802), ('prince', 0.6139929294586182), ('empress', 0.6126195192337036), ('princess', 0.6075714230537415), ('elizabeth', 0.588543176651001), ('throne', 0.5846244692802429), ('daughter', 0.5667101144790649), ('son', 0.5659586191177368), ('isabella', 0.5611927509307861), ('scots', 0.5606790781021118)]\n```", "```\ntarget_words = ['mother', 'car', 'tree', 'science', 'building', 'elephant', 'green']\nword_groups, embedding_groups = list(), list()\n\nfor word in target_words:\n    words = [w for w, _ in model.most_similar(word, topn=5)]\n    word_groups.append(words)\n\n    embedding_groups.append([model.wv[w] for w in words])\n```", "```\n# Train the t-SNE algorithm\nembedding_groups = np.array(embedding_groups)\nm, n, vector_size = embedding_groups.shape\ntsne_model = TSNE(perplexity=8, n_components=2, init='pca', n_iter=5000)\n\n# generate 2d embeddings from the original 100d ones\nembeddings_2d = tsne_model.fit_transform(embedding_groups.reshape(m * n, vector_size))\nembeddings_2d = np.array(embeddings_2d).reshape(m, n, 2)\n```", "```\n# Plot the results\nplt.figure(figsize=(16, 9))\n# Different color and marker for each group of similar words\ncolor_map = plt.get_cmap('Dark2')(np.linspace(0, 1, len(target_words)))\nmarkers = ['o', 'v', 's', 'x', 'D', '*', '+']\n```", "```\n# Iterate over all groups\nfor label, similar_words, emb, color, marker in \\\n        zip(target_words, word_groups, embeddings_2d, color_map, markers):\n    x, y = emb[:, 0], emb[:, 1]\n\n    # Plot the points of each word group\n    plt.scatter(x=x, y=y, c=color, label=label, marker=marker)\n\n    # Annotate each point with its corresponding caption\n    for word, w_x, w_y in zip(similar_words, x, y):\n        plt.annotate(word, xy=(w_x, w_y), xytext=(0, 15),\n                     textcoords='offset points', ha='center', va='top', size=10)\n```", "```\nplt.legend()\nplt.grid(True)\nplt.show()\n```"]