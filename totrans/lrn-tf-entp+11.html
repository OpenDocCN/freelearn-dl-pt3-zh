<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer116">&#13;
			<p id="_idParaDest-106" class="chapter-number"><a id="_idTextAnchor200"/>Chapter 7: </p>&#13;
			<h1 id="_idParaDest-107"><a id="_idTextAnchor201"/>Model Optimization</h1>&#13;
			<p>In this chapter, we will learn about the concept of model optimization through a technique known as quantization. This is important because even though capacity, such as compute and memory, are less of an issue in a cloud environment, latency and throughput are always a factor in the quality and quantity of the model's output. Therefore, model optimization to reduce latency and maximize throughput can help reduce the compute cost. In the edge environment, many of the constraints are related to resources such as memory, compute, power consumption, and bandwidth. </p>&#13;
			<p>In this chapter, you will learn how to make your model as lean and mean as possible, with acceptable or negligible changes in the model's accuracy. In other words, we will reduce the model size so that we can have the model running on less power and fewer compute resources without overly impacting its performance. In this chapter, we are going to take a look at recent advances and a method available for TensorFlow: TFLite Quantization.</p>&#13;
			<p>In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Understanding the quantization concept</li>&#13;
				<li>Preparing a full original model for scoring</li>&#13;
				<li>Converting a full model to a reduced float16 model</li>&#13;
				<li>Converting a full model to a reduced hybrid quantization model</li>&#13;
				<li>Converting a full model to an integer quantization model</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-108"><a id="_idTextAnchor202"/>Technical requirements</h1>&#13;
			<p>You will find all the source code in <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise.git">https://github.com/PacktPublishing/learn-tensorflow-enterprise.git</a>.</p>&#13;
			<p>You may clone it with a <strong class="source-inline">git</strong> command in your command terminal:</p>&#13;
			<p class="source-code">git clone https://github.com/PacktPublishing/learn-tensorflow-enterprise.git</p>&#13;
			<p>All the resources for this chapter are available in the <strong class="source-inline">chapter_07</strong> folder in the GitHub link for the book.</p>&#13;
			<h1 id="_idParaDest-109"><a id="_idTextAnchor203"/><a id="_idTextAnchor204"/>Understanding the quantization concept</h1>&#13;
			<p>Quantization is a technique <a id="_idIndexMarker358"/>whereby the model size is reduced and its efficiency therefore improved. This technique is helpful in building models for mobile or edge deployment, where compute resources or power supply are constrained. Since our aim is to make the model run as efficiently as possible, we are also accepting the fact that the model has to become smaller and therefore less precise than the original model. This means that we are transforming the model into a lighter version of its original self, and that the transformed model is an approximation of the original one.</p>&#13;
			<p>Quantization may be applied to a trained model. This is known as a post-training quantization <a id="_idIndexMarker359"/>API. Within this type of quantization, there are three approaches: </p>&#13;
			<ul>&#13;
				<li><strong class="bold">Reduced float quantization</strong>: Convert <strong class="source-inline">float 32 bits</strong> ops to <strong class="source-inline">float 16</strong> ops.</li>&#13;
				<li><strong class="bold">Hybrid quantization</strong>: Convert weights to <strong class="source-inline">8 bits</strong>, while keeping biases and activation as <strong class="source-inline">32 bits</strong> ops.</li>&#13;
				<li><strong class="bold">Integer quantization</strong>: Convert everything to integer ops. Weights are converted to <strong class="source-inline">8 bits</strong>, while biases and activations may be <strong class="source-inline">8</strong> or <strong class="source-inline">16 bits</strong>. </li>&#13;
			</ul>&#13;
			<p>The preceding approaches are applicable to a TensorFlow model that was built and trained using traditional means. Another approach is to train the model while performing <a id="_idIndexMarker360"/>optimization. This is known as <strong class="bold">quantization-aware training</strong>, in which we apply the API to emulate the quantization operations during the forward pass of the deep learning training. </p>&#13;
			<p>The result model contains quantized values. This is relatively new and only an integer quantization API is available. Quantization-aware training currently only works for custom built models, not models from TensorFlow Hub, which are pre-trained. If you wish to use a quantized version of those famous pre-trained models, you can find these models here: <a href="https://www.tensorflow.org/lite/guide/hosted_models">https://www.tensorflow.org/lite/guide/hosted_models</a>. </p>&#13;
			<h2 id="_idParaDest-110"><a id="_idTextAnchor205"/>Training a baseline model</h2>&#13;
			<p>Let's begin start by <a id="_idIndexMarker361"/>training an image classification model with five classes of flowers. We will leverage a pre-trained ResNet feature vector hosted in TensorFlow Hub (<a href="https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4">https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4</a>) and you can download the flower images in TFRecord format from here: <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN</a>.</p>&#13;
			<p>Alternatively, if you cloned the repository for this book, the source code and TFRecord dataset for training a baseline model can be found at <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model">https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model</a>.</p>&#13;
			<p>The following is a training script <strong class="source-inline">default_trainer.py</strong> file that trains this model with the TFRecord dataset:</p>&#13;
			<ol>&#13;
				<li value="1">We start this training script with an <strong class="source-inline">import</strong> statement for all the libraries we will require:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">import os</p><p class="source-code">import IPython</p><p class="source-code">import time</p><p class="source-code">from absl import flags</p><p class="source-code">from absl import logging</p><p class="source-code">from absl import app</p><p>A <strong class="source-inline">Absl</strong> is a useful library. In this library, the <strong class="source-inline">flags</strong> API is used to define user input. This is especially handy because we invoke this script through a user command instead of running it as a notebook. </p></li>&#13;
				<li>Equipped with a <strong class="source-inline">flags</strong> API in the <strong class="source-inline">import</strong> statement, we will define a short-hand alias for <strong class="source-inline">flags.FLAGS</strong>, and then define a series of user inputs that we will pass to the script. This is accomplished with the help of the <strong class="source-inline">tf.compat.v1.flags</strong> API. Notice that we can define a data type for user inputs and provide a default <a id="_idIndexMarker362"/>value so that users do not have to specify every input:<p class="source-code">FLAGS = flags.FLAGS</p><p class="source-code"># flag name, default value, explanation/help.</p><p class="source-code">tf.compat.v1.flags.DEFINE_string('model_dir', 'default_model_dir', 'Directory or bucket for storing checkpoint model.')</p><p class="source-code">tf.compat.v1.flags.DEFINE_bool('fine_tuning_choice', False, 'Retrain base parameters')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('train_batch_size', 32, 'Number of samples in a training batch')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('validation_batch_size', 40, 'Number of samples in a validation batch')</p><p class="source-code">tf.compat.v1.flags.DEFINE_string('distribution_strategy', 'tpu', 'Distribution strategy for training.')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('train_epochs', 3, 'Number of epochs for training')</p><p class="source-code">tf.compat.v1.flags.DEFINE_string('data_dir', 'tf_datasets/flower_photos', 'training data path')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('num_gpus', 4, 'Number of GPU per worker')</p><p class="source-code">tf.compat.v1.flags.DEFINE_string('cache_dir', '../imagenet_resnet_v2_50_feature_vector_4' , 'Location of cached model')</p><p>There are a couple of user flags that are worthy of further explanation. The <strong class="source-inline">data-dir</strong> flag defines the file path for training data. In this case, it is pointing to a folder path, <strong class="source-inline">tf_datasets/flower_photos</strong>, from the current directory, <strong class="source-inline">train_base_model</strong>. The other user flag is <strong class="source-inline">cache_dir</strong>. This is the path to our downloaded ResNet feature vector model. While we can access the TensorFlow Hub directly through the internet, there are occasions where connectivity may be an issue. Therefore, downloading the model and putting it in a local environment is a good idea.</p></li>&#13;
				<li>We may wrap the <a id="_idIndexMarker363"/>model architecture and compilation in the following function:<p class="source-code">def model_default():</p><p class="source-code">    flags_obj = flags.FLAGS</p><p class="source-code">    os.environ['TFHUB_CACHE_DIR'] = flags_obj.cache_dir</p><p class="source-code">    IMAGE_SIZE = (224, 224)</p><p class="source-code">    model = tf.keras.Sequential([</p><p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), </p><p class="source-code">    hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=flags_obj.fine_tuning_choice),</p><p class="source-code">    tf.keras.layers.Flatten(),</p><p class="source-code">    tf.keras.layers.Dense(units = 64, </p><p class="source-code">                                     activation = 'relu', </p><p class="source-code">                    kernel_initializer='glorot_uniform'),</p><p class="source-code">    tf.keras.layers.Dense(5, activation='softmax', </p><p class="source-code">                                   name = 'custom_class')</p><p class="source-code">    ])</p><p class="source-code">    model.build([None, 224, 224, 3])</p><p class="source-code">     model.compile(</p><p class="source-code">        optimizer=tf.keras.optimizers.SGD(lr=1e-2, 	                 </p><p class="source-code">                                           momentum=0.5), </p><p class="source-code">        loss=tf.keras.losses.CategoricalCrossentropy(</p><p class="source-code">                  from_logits=True, label_smoothing=0.1),</p><p class="source-code">        metrics=['accuracy'])</p><p class="source-code">  return model</p><p>This function is responsible for building the model and compiling it with proper <strong class="source-inline">optimizer</strong> and <strong class="source-inline">loss</strong> functions. It returns a model object for training.</p></li>&#13;
				<li>As for the <a id="_idIndexMarker364"/>image data's input pipeline, the pipeline needs to handle data parsing. This is accomplished with the following function:<p class="source-code">def decode_and_resize(serialized_example):</p><p class="source-code">    # resized image should be [224, 224, 3] and normalized to value range [0, 255] </p><p class="source-code">    # label is integer index of class.</p><p class="source-code">        parsed_features = tf.io.parse_single_example(</p><p class="source-code">    serialized_example,</p><p class="source-code">    features = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([],  	 	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([], 	 	     	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">    })</p><p class="source-code">    image = tf.io.decode_jpeg(parsed_features[</p><p class="source-code">                            'image/encoded'], channels=3)</p><p class="source-code">    label = tf.cast(parsed_features['image/class/label'],  </p><p class="source-code">                                                tf.int32)</p><p class="source-code">    label_txt = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/text'], tf.string)</p><p class="source-code">    label_one_hot = tf.one_hot(label, depth = 5)</p><p class="source-code">    resized_image = tf.image.resize(image, [224, 224], </p><p class="source-code">                                         method='nearest')</p><p class="source-code">    return resized_image, label_one_hot</p><p>As the function's <a id="_idIndexMarker365"/>name suggests, this function takes a sample that is stored in TFRecord. It is parsed with a feature description and the sample image (which is a <strong class="source-inline">byte string</strong>) decoded as a JPEG image. As for the image label, the function also parses the label name (<strong class="source-inline">image</strong>/<strong class="source-inline">class</strong>/<strong class="source-inline">text</strong>) and converts it into a one-hot vector. The jpeg image is resized to <strong class="source-inline">224</strong> by <strong class="source-inline">224</strong>. As a result, this function returns a tuple. This tuple consists of one resized image and its label.</p></li>&#13;
				<li>We also need to normalize the image pixel value to a range of [0, 1.0]. This is done through the following function:<p class="source-code">def normalize(image, label):</p><p class="source-code">    #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p><p class="source-code">    image = tf.cast(image, tf.float32) / 255.</p><p class="source-code">    return image, label</p><p>In the <strong class="source-inline">normalize</strong> function, a JPEG image, represented as a NumPy array, is normalized to a range of <strong class="source-inline">[0, 1.0]</strong>. At the same time, although we are not doing anything <a id="_idIndexMarker366"/>with the label, it is a good idea to pass the label along with the image and return them as a tuple so that you keep track of the image and label together.</p></li>&#13;
				<li>Then we apply shuffle and batch ops to the training data in the following function:<p class="source-code">def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):</p><p class="source-code">    # This is a small dataset, only load it once, and keep it in memory.</p><p class="source-code">    # use `.cache(filename)` to cache preprocessing work for datasets that don't</p><p class="source-code">    # fit in memory.</p><p class="source-code">    flags_obj = flags.FLAGS</p><p class="source-code">    if cache:</p><p class="source-code">        if isinstance(cache, str):</p><p class="source-code">            ds = ds.cache(cache)</p><p class="source-code">        else:</p><p class="source-code">            ds = ds.cache()</p><p class="source-code">    ds = ds.shuffle(buffer_size=shuffle_buffer_size)</p><p class="source-code">    # Repeat forever</p><p class="source-code">    ds = ds.repeat()</p><p class="source-code">    ds = ds.batch(flags_obj.train_batch_size)</p><p class="source-code">    # `prefetch` lets the dataset fetch batches in the background while the model</p><p class="source-code">    # is training.</p><p class="source-code">    AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">    ds = ds.prefetch(buffer_size=AUTOTUNE)</p><p class="source-code">return ds</p><p>This function returns a dataset with shuffle, repeat, batch, and prefetch ops attached. This is a standard approach <a id="_idIndexMarker367"/>for getting the dataset ready for training.</p></li>&#13;
				<li>Now we come to the main driver of this code:<p class="source-code">def main(_):</p><p class="source-code">    flags_obj = flags.FLAGS</p><p class="source-code">       if flags_obj.distribution_strategy == 'tpu':</p><p class="source-code">        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()</p><p class="source-code">        tf.config.experimental_connect_to_cluster(resolver)</p><p class="source-code">        tf.tpu.experimental.initialize_tpu_system(resolver)</p><p class="source-code">        strategy = tf.distribute.experimental.TPUStrategy(resolver)</p><p class="source-code">        strategy_scope = strategy.scope()</p><p class="source-code">        print('All devices: ', tf.config.list_logical_devices('TPU'))</p><p class="source-code">    elif flags_obj.distribution_strategy == 'gpu':</p><p class="source-code">        strategy = tf.distribute.MirroredStrategy()</p><p class="source-code">        strategy_scope = strategy.scope()</p><p class="source-code">        devices = ['device:GPU:%d' % i for i in </p><p class="source-code">                    range(flags_obj.num_gpus)]</p><p class="source-code">    else:</p><p class="source-code">        strategy = tf.distribute.MirroredStrategy()</p><p class="source-code">        strategy_scope = strategy.scope()</p><p class="source-code"> print('NUMBER OF DEVICES: ', </p><p class="source-code">                           strategy.num_replicas_in_sync)</p><p>In this section of the <strong class="source-inline">main()</strong> function, we provide the logic for a distributed training strategy.</p></li>&#13;
				<li>Continuing with <strong class="source-inline">main()</strong>, the data paths are <a id="_idIndexMarker368"/>identified and handled by the <strong class="source-inline">tf.data</strong> API: <p class="source-code">    ## identify data paths and sources</p><p class="source-code">    root_dir = flags_obj.data_dir # this is gs://&lt;bucket&gt;/folder or file path where tfrecord is found</p><p class="source-code">    file_pattern = '{}/image_classification_builder-train*.tfrecord*'.format(root_dir)</p><p class="source-code">    val_file_pattern = '{}/image_classification_builder-validation*.tfrecord*'.format(root_dir)</p><p class="source-code">    file_list = tf.io.gfile.glob(file_pattern)</p><p class="source-code">    all_files = tf.data.Dataset.list_files( </p><p class="source-code">                          tf.io.gfile.glob(file_pattern))</p><p class="source-code">    val_file_list = tf.io.gfile.glob(val_file_pattern)</p><p class="source-code">    val_all_files = tf.data.Dataset.list_files( </p><p class="source-code">                      tf.io.gfile.glob(val_file_pattern))</p><p class="source-code">    train_all_ds = tf.data.TFRecordDataset(all_files, </p><p class="source-code">        num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p class="source-code">val_all_ds = tf.data.TFRecordDataset(val_all_files, </p><p class="source-code">        num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p>With the preceding code, all three data sources – training, validation, and testing – are identified and referenced. Recall that the wildcard symbol, <strong class="source-inline">*</strong>, in the filename pattern helps this pipeline to be scalable. It doesn't matter how many TFRecord data parts you have; this pipeline can handle it. </p></li>&#13;
				<li>Continuing with <strong class="source-inline">main()</strong>, now we need to apply feature engineering and normalization <a id="_idIndexMarker369"/>functions to every sample in the training and validation datasets. This is done through the <strong class="source-inline">map</strong> function:<p class="source-code">    # perform data engineering </p><p class="source-code">    dataset = train_all_ds.map(decode_and_resize)</p><p class="source-code">    val_dataset = val_all_ds.map(decode_and_resize)</p><p class="source-code">    # Create dataset for training run</p><p class="source-code">    BATCH_SIZE = flags_obj.train_batch_size</p><p class="source-code">    VALIDATION_BATCH_SIZE = </p><p class="source-code">                          flags_obj.validation_batch_size</p><p class="source-code">    dataset = dataset.map(normalize, </p><p class="source-code">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</p><p class="source-code">    val_dataset = val_dataset.map(normalize, </p><p class="source-code">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</p><p class="source-code">val_ds = val_dataset.batch(VALIDATION_BATCH_SIZE)</p><p class="source-code">    AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">    train_ds = prepare_for_training(dataset)</p><p>Training and validation datasets are batched according to the respective user flags. If none are given, default values are used.</p></li>&#13;
				<li> Now we <a id="_idIndexMarker370"/>need to set up some parameters for training and validation:<p class="source-code">    NUM_CLASSES = 5</p><p class="source-code">    IMAGE_SIZE = (224, 224)</p><p class="source-code">        train_sample_size=0</p><p class="source-code">    for raw_record in train_all_ds:</p><p class="source-code">        train_sample_size += 1</p><p class="source-code">    print('TRAIN_SAMPLE_SIZE = ', train_sample_size)</p><p class="source-code">    validation_sample_size=0</p><p class="source-code">    for raw_record in val_all_ds:</p><p class="source-code">        validation_sample_size += 1</p><p class="source-code">    print('VALIDATION_SAMPLE_SIZE = ', </p><p class="source-code">                                  validation_sample_size)</p><p class="source-code">    STEPS_PER_EPOCHS = train_sample_size // BATCH_SIZE</p><p class="source-code">VALIDATION_STEPS = validation_sample_size </p><p class="source-code">                                 // VALIDATION_BATCH_SIZE</p><p>In the preceding code, we set the number of classes and image size in variables to be passed into the model training process. Then we determine the number of steps for each epoch of training and cross-validation.</p></li>&#13;
				<li>Continuing with <strong class="source-inline">main()</strong>, we can now create the model by invoking the <strong class="source-inline">model_default</strong> function:<p class="source-code">    model = model_default()</p><p class="source-code">    checkpoint_prefix = os.path.join(flags_obj.model_dir, </p><p class="source-code">                                    'train_ckpt_{epoch}')</p><p class="source-code">    callbacks = [</p><p class="source-code">    tf.keras.callbacks.TensorBoard(log_dir=os.path.join(flags_obj.model_dir, 'tensorboard_logs')),</p><p class="source-code">    tf.keras.callbacks.ModelCheckpoint(</p><p class="source-code">                              filepath=checkpoint_prefix,</p><p class="source-code">        save_weights_only=True)]</p><p>In the preceding <a id="_idIndexMarker371"/>code, we invoke the <strong class="source-inline">model_default</strong> function to build and compile our model. We also set up callbacks for the training checkpoint.</p></li>&#13;
				<li>Continuing with <strong class="source-inline">main()</strong>, we can now launch the training process: <p class="source-code">     model.fit(</p><p class="source-code">        train_ds,</p><p class="source-code">        epochs=flags_obj.train_epochs, </p><p class="source-code">        steps_per_epoch=STEPS_PER_EPOCHS,</p><p class="source-code">        validation_data=val_ds,</p><p class="source-code">        validation_steps=VALIDATION_STEPS,</p><p class="source-code">        callbacks=callbacks)</p><p>In the preceding code, we pass the training and validation datasets into the <strong class="source-inline">fit</strong> function. The number of epochs is determined by the user input. If none is given, then the default value is used.</p></li>&#13;
				<li>Continuing with <strong class="source-inline">main()</strong>, we may log output as <strong class="source-inline">STDOUT</strong> in the terminal where this script is <a id="_idIndexMarker372"/>executed:<p class="source-code">    logging.info('INSIDE MAIN FUNCTION user input model_dir %s', flags_obj.model_dir)</p><p class="source-code">        timestr = time.strftime('%Y%m%d-%H%M%S')</p><p class="source-code">    output_folder = flags_obj.model_dir + '-' + timestr</p><p class="source-code">    if not os.path.exists(output_folder):</p><p class="source-code">        os.mkdir(output_folder)</p><p class="source-code">        print('Directory ' , output_folder , ' Created ')</p><p class="source-code">    else:</p><p class="source-code">        print('Directory ' , output_folder , ' already exists')   </p><p class="source-code">    model_save_dir = os.path.join(output_folder, </p><p class="source-code">                                            'save_model')</p><p class="source-code">    model.save(model_save_dir)</p><p class="source-code">if __name__ == '__main__':</p><p class="source-code">    app.run(main)</p><p>In the preceding code, we also leverage a timestamp value to build a folder name, where the models built each time may be saved according to the time of training completion.</p></li>&#13;
			</ol>&#13;
			<p>This concludes <strong class="source-inline">main()</strong>. The model is saved in <strong class="source-inline">model_save_dir</strong>. To invoke this script, you simply have to run the following command in your Python environment:</p>&#13;
			<p class="source-code">python3 default_trainer.py \</p>&#13;
			<p class="source-code">--distribution_strategy=default \</p>&#13;
			<p class="source-code">--fine_tuning_choice=False \</p>&#13;
			<p class="source-code">--train_batch_size=32 \</p>&#13;
			<p class="source-code">--validation_batch_size=40 \</p>&#13;
			<p class="source-code">--train_epochs=5 \</p>&#13;
			<p class="source-code">--data_dir=tf_datasets/flower_photos \</p>&#13;
			<p class="source-code">--model_dir=trained_resnet_vector</p>&#13;
			<p>From the directory where this script is stored, you will find a subfolder with the prefix name <strong class="source-inline">trained_resnet_vector</strong>, followed by a date and time stamp such as <strong class="source-inline">20200910-213303</strong>. This subfolder contains the saved model. We will use this model as our baseline model. Once training is complete, you will find the saved model in the following directory:</p>&#13;
			<p><strong class="source-inline">trained_resnet_vector-20200910-213303/save_model/assets</strong></p>&#13;
			<p>This saved model is in <a id="_idIndexMarker373"/>the same directory where <strong class="source-inline">default_trainer.py</strong> is stored. Now that we have a trained TensorFlow model, in the next section, we are going score our test data with the<a id="_idTextAnchor206"/> <a id="_idTextAnchor207"/>trained model. </p>&#13;
			<h1 id="_idParaDest-111"><a id="_idTextAnchor208"/>Preparing a full original model for scoring</h1>&#13;
			<p>After training <a id="_idIndexMarker374"/>for a full model is complete, we will use a <strong class="source-inline">Scoring</strong> Jupyter notebook in this repository to demonstrate scoring with a full model. This notebook can be found in <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_07/train_base_model/Scoring.ipynb">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_07/train_base_model/Scoring.ipynb</a>.</p>&#13;
			<p>For the original model, it is stored in the <strong class="source-inline">savedModel</strong> Protobuf format. We need to load it as follows:</p>&#13;
			<p class="source-code">import tensorflow as tf</p>&#13;
			<p class="source-code">import numpy as np</p>&#13;
			<p class="source-code">import matplotlib.pyplot as plt</p>&#13;
			<p class="source-code">from PIL import Image, ImageOps</p>&#13;
			<p class="source-code">import IPython.display as display</p>&#13;
			<p class="source-code">path_saved_model = 'trained_resnet_vector-unquantized/save_model'</p>&#13;
			<p class="source-code">trained_model = tf.saved_model.load(path_saved_model)</p>&#13;
			<p>The full model we just trained is now loaded in our Jupyter notebook's runtime as <strong class="source-inline">trained_model</strong>. For scoring, a few more steps are required. We have to find the model signature for prediction:</p>&#13;
			<p class="source-code">signature_list = list(trained_model.signatures.keys())</p>&#13;
			<p class="source-code">signature_list</p>&#13;
			<p>It shows that there is <a id="_idIndexMarker375"/>only one signature in this list:</p>&#13;
			<p class="source-code">['serving_default']</p>&#13;
			<p>We will create an <strong class="source-inline">infer</strong> wrapper function and pass the signature into it:</p>&#13;
			<p class="source-code">infer = trained_model.signatures[signature_list[0]]</p>&#13;
			<p>Here, <strong class="source-inline">signature_list[0]</strong> is equivalent to <strong class="source-inline">serving_default</strong>. Now let's print the output:</p>&#13;
			<p class="source-code">print(infer.structured_outputs)</p>&#13;
			<p>Let's take a look at the output of the preceding function:</p>&#13;
			<p class="source-code">{'custom_class': TensorSpec(shape=(None, 5), dtype=tf.float32, name='custom_class')}</p>&#13;
			<p>The output is a NumPy array of <strong class="source-inline">shape=(None, 5)</strong>. This array will hold the probability of classes predicted by the model. </p>&#13;
			<p>Now let's work on the test data. The test data provided in this case is in TFRecord format. We are going to convert it to a batch of images expressed as a NumPy array in the dimensions of <strong class="source-inline">[None, 224, 224, 3]</strong>.</p>&#13;
			<h2 id="_idParaDest-112"><a id="_idTextAnchor209"/>Preparing test data</h2>&#13;
			<p>This part is very similar to what we saw in <a href="B16070_06_Final_JM_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 6</em></a>, <em class="italic">Hyperparameter Tuning</em>, where we used TFRecord extensively <a id="_idIndexMarker376"/>as the input format for model training. The TFRecord used here is available in <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model/tf_datasets/flower_photos">https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model/tf_datasets/flower_photos</a>.</p>&#13;
			<h2 id="_idParaDest-113"><a id="_idTextAnchor210"/>Loading test data </h2>&#13;
			<p>Let's start by <a id="_idIndexMarker377"/>loading the TFRecord data:</p>&#13;
			<p class="source-code">root_dir = ' tf_datasets/flower_photos'</p>&#13;
			<p class="source-code">test_pattern = '{}/image_classification_builder-test.tfrecord*'.format(root_dir)</p>&#13;
			<p class="source-code">test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_pattern))</p>&#13;
			<p class="source-code">test_all_ds = tf.data.TFRecordDataset(test_all_files,</p>&#13;
			<p class="source-code">num_parallel_reads=tf.data.experimental.AUTOTUNE)</p>&#13;
			<p>We will check the sample size of the image with the following code:</p>&#13;
			<p class="source-code">sample_size = 0</p>&#13;
			<p class="source-code">for raw_record in test_all_ds:</p>&#13;
			<p class="source-code">    sample_size += 1</p>&#13;
			<p class="source-code">print('Sample size: ', sample_size)</p>&#13;
			<p>Here is the output:</p>&#13;
			<p class="source-code">Sample size:  50</p>&#13;
			<p>This shows that we have 50 samples in our test data.</p>&#13;
			<p>There are <strong class="source-inline">50</strong> images in this test dataset. We will reuse the helper function from <a href="B16070_06_Final_JM_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 6</em></a>, <em class="italic">Hyperparameter Tuning</em>, to decode the TFRecord and the metadata within and then normalize the <a id="_idIndexMarker378"/>pixel values:</p>&#13;
			<p class="source-code">def decode_and_resize(serialized_example):</p>&#13;
			<p class="source-code">    # resized image should be [224, 224, 3] and normalized to value range [0, 255] </p>&#13;
			<p class="source-code">    # label is integer index of class.</p>&#13;
			<p class="source-code">        parsed_features = tf.io.parse_single_example(</p>&#13;
			<p class="source-code">    serialized_example,</p>&#13;
			<p class="source-code">    features = {</p>&#13;
			<p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/format' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p>&#13;
			<p class="source-code">    })</p>&#13;
			<p class="source-code">    image = tf.io.decode_jpeg(parsed_features['image/encoded'], 	                                                    channels=3)</p>&#13;
			<p class="source-code">    label = tf.cast(parsed_features['image/class/label'],         	                                                      tf.int32)</p>&#13;
			<p class="source-code">    label_txt = tf.cast(parsed_features['image/class/text'], 	                                                     tf.string)</p>&#13;
			<p class="source-code">    label_one_hot = tf.one_hot(label, depth = 5)</p>&#13;
			<p class="source-code">    resized_image = tf.image.resize(image, [224, 224],  	     	                                              method='nearest')</p>&#13;
			<p class="source-code">    return resized_image, label_one_hot</p>&#13;
			<p class="source-code">def normalize(image, label):</p>&#13;
			<p class="source-code">    #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p>&#13;
			<p class="source-code">    image = tf.cast(image, tf.float32) / 255. </p>&#13;
			<p class="source-code">   return image, label</p>&#13;
			<p>The <strong class="source-inline">decode_and_resize</strong> function parses an image, resizes it to <strong class="source-inline">224</strong> by <strong class="source-inline">224</strong> pixels, and, at the same time, one-hot encodes the image's label. <strong class="source-inline">decode_and_resize</strong> then returns the image and corresponding label as a tuple, so that the image and label are always kept together.</p>&#13;
			<p>The <strong class="source-inline">normalize</strong> function divides the image pixel value by <strong class="source-inline">255</strong> in order to bring the pixel range to <strong class="source-inline">[0, 1.0]</strong>. And even though nothing is done in relation to the label, it is necessary to keep <a id="_idIndexMarker379"/>track of the image and label as a tuple so that they are always kept together.</p>&#13;
			<p>Now we may apply the preceding helper functions to decode, standardize, and normalize images in the TFRecord dataset:</p>&#13;
			<p class="source-code">decoded = test_all_ds.map(decode_and_resize)</p>&#13;
			<p class="source-code">normed = decoded.map(normalize)</p>&#13;
			<p>Notice that we introduced an additional dimension as the first dimension through <strong class="source-inline">np.expand_dims</strong>. This extra dimension is intended for the variable batch size:</p>&#13;
			<p class="source-code">np_img_holder = np.empty((0, 224, 224,3), float)</p>&#13;
			<p class="source-code">np_lbl_holder = np.empty((0, 5), int)</p>&#13;
			<p class="source-code">for img, lbl in normed:</p>&#13;
			<p class="source-code">    r = img.numpy() # image value extracted</p>&#13;
			<p class="source-code">    rx = np.expand_dims(r, axis=0) </p>&#13;
			<p class="source-code">    lx = np.expand_dims(lbl, axis=0) </p>&#13;
			<p class="source-code">    np_img_holder = np.append(np_img_holder, rx, axis=0) </p>&#13;
			<p class="source-code">np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) </p>&#13;
			<p>The test data is now in NumPy format with standardized dimensions, pixel values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, and is batched, as are the labels.</p>&#13;
			<p>We will now inspect these images. In order to do so, we may display the NumPy array, <strong class="source-inline">np_img_holder</strong>, as <a id="_idIndexMarker380"/>images with the following code in Figure 7.1:</p>&#13;
			<p class="source-code">%matplotlib inline</p>&#13;
			<p class="source-code">plt.figure()</p>&#13;
			<p class="source-code">for i in range(len(np_img_holder)):</p>&#13;
			<p class="source-code">    plt.subplot(10, 5, i+1)</p>&#13;
			<p class="source-code">    plt.axis('off')</p>&#13;
			<p class="source-code">plt.imshow(np.asarray(np_img_holder[i]))</p>&#13;
			<p>In the preceding code snippet, we iterate through our image array and place each image in one of the subplots. There are 50 images (10 rows, with each row having five subplots), as can be seen in th<a id="_idTextAnchor211"/>e following figure:</p>&#13;
			<div>&#13;
				<div id="_idContainer112" class="IMG---Figure">&#13;
					<img src="Images/image0013.jpg" alt="Figure 7.1 – 50 images within the test dataset of five flower classes&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 7.1 – 50 images within the test dataset of five flower classes</p>&#13;
			<h2 id="_idParaDest-114"><a id="_idTextAnchor212"/>Scoring a single image with a full model</h2>&#13;
			<p>Let's now take a look at <a id="_idIndexMarker381"/>the shape of the test <a id="_idIndexMarker382"/>data, and understand what it takes to transform test data into the shape expected by the model:</p>&#13;
			<ol>&#13;
				<li value="1">We will first test our scoring routine with just a single image. Just like how we created an image batch by adding a new dimension as the first dimension, we will do the same to create an image batch with a sample size of <strong class="source-inline">1</strong>:<p class="source-code">x = np.expand_dims(np_img_holder[0], axis=0)</p><p class="source-code">x.shape</p><p class="source-code">(1, 224, 224, 3)</p></li>&#13;
				<li>Now the dimension is correct, which is a batch of one image. Let's convert this to a tensor with a type of <strong class="source-inline">float32</strong>:<p class="source-code">xf = tf.dtypes.cast(x, tf.float32)</p><p>Then, pass this to the <strong class="source-inline">infer</strong> function for scoring:</p><p class="source-code">prediction = infer(xf)</p><p>You will recall that the last layer of our model is a dense layer named <strong class="source-inline">custom_class</strong>. With five nodes, and softmax as the activation function in each node, we will get the probability for each of the five classes. </p></li>&#13;
				<li>We will now inspect the content of the prediction:<p class="source-code">prediction.get('custom_class')</p><p>The output should appear similar to this:</p><p class="source-code">&lt;tf.Tensor: shape=(1, 5), dtype=float32, numpy=</p><p class="source-code">array([[1.5271275e-04, 2.0515859e-05, 1.0230409e-06, 2.9591745e-06, 9.9982280e-01]], dtype=float32)&gt;</p><p>These values in the <a id="_idIndexMarker383"/>array represent probability. Each <a id="_idIndexMarker384"/>position in the array represents a class of flower type. As you can see, the highest probability is in the very last position of the array; the index corresponding to this position is <strong class="source-inline">4</strong>. We need to map <strong class="source-inline">4</strong> to the plaintext name.</p><p>Now we will convert it to a NumPy array so that we may find the index where the maximum probability is predicted:</p><p class="source-code">predicted_prob_array = prediction.get('custom_class').numpy()</p><p class="source-code">idx = np.argmax(predicted_prob_array)</p><p class="source-code">print(idx)</p></li>&#13;
				<li>The fourth position index is where maximum probability is predicted. Now we need to know what this represents by mapping this index to a label. We need to create a reverse lookup dictionary to map probability back to the label. We just found the index where the maximum probability is located. The next step is to map <strong class="source-inline">idx</strong> to the correct flower type. In order to do this, we need to extract this information from TFRecord:<p class="source-code">feature_description = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([], 	 	 	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">}</p><p class="source-code">def _parse_function(example_proto):</p><p class="source-code">  return tf.io.parse_single_example(example_proto, 	 	                                     feature_description)</p><p class="source-code">parsd_ds = test_all_ds.map(_parse_function)</p><p class="source-code">val_label_map = {}</p><p class="source-code"># getting label mapping</p><p class="source-code">for image_features in parsd_ds.take(50):</p><p class="source-code">    label_idx = image_features[</p><p class="source-code">                             'image/class/label'].numpy()</p><p class="source-code">    label_str = image_features[</p><p class="source-code">                     'image/class/text'].numpy().decode()</p><p class="source-code">    if label_idx not in val_label_map:</p><p class="source-code">        val_label_map[label_idx] = label_str</p><p>In the preceding code, we used the same feature description (<strong class="source-inline">feature_description</strong>) to parse <strong class="source-inline">test_all_ds</strong>. Once it is parsed using <strong class="source-inline">_parse_function</strong>, we iterate through the entire test dataset. The information we want is in <strong class="source-inline">image/class/label</strong> and <strong class="source-inline">image/class/text</strong>. </p></li>&#13;
				<li>We simply <a id="_idIndexMarker385"/>create a dictionary, where <a id="_idIndexMarker386"/>the key is <strong class="source-inline">label_idx</strong> and the value is <strong class="source-inline">label_str</strong>. The result is <strong class="source-inline">val_label_map</strong>. If we inspect it as follows:<p class="source-code">val_label_map</p><p>The output is as follows:</p><p class="source-code">{4: 'tulips', 3: 'dandelion', 1: 'sunflowers', 2: 'daisy', 0: 'roses'}</p><p>Then we evaluate <strong class="source-inline">idx</strong>:</p><p class="source-code">print(val_label_map.get(idx))</p><p>Here is the output:</p><p class="source-code">tulip</p><p>This maps our image to the <strong class="source-inline">tulip</strong> class.</p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-115"><a id="_idTextAnchor213"/>Scoring batch images with a full model</h2>&#13;
			<p>In the <a id="_idIndexMarker387"/>previous section, we looked at how to score one image. Now we want to score a batch of images. In <a id="_idIndexMarker388"/>our test data, there are 50 images:</p>&#13;
			<ol>&#13;
				<li value="1">In the previous section, we created the image batch in the proper shape of <strong class="source-inline">[50, 224, 224, 3]</strong>. This is ready for scoring:<p class="source-code">batched_input = tf.dtypes.cast(np_img_holder, tf.float32)</p><p class="source-code">batch_predicted_prob_array = infer(batched_input)</p><p>Let's create a function that assists in looking up the label name when given a NumPy array and a lookup dictionary:</p><p class="source-code">def lookup(np_entry, dictionary):</p><p class="source-code">  	class_key = np.argmax(np_entry)</p><p class="source-code">	return dictionary.get(class_key)</p><p>This function takes a NumPy array, maps the position where the maximum value exists, and then maps that position with a dictionary.</p></li>&#13;
				<li>This is a list holding our ground truth labels as indicated by <strong class="source-inline">np_lbl_holder</strong>:<p class="source-code">actual = []</p><p class="source-code">for i in range(len(np_lbl_holder)):</p><p class="source-code">    plain_text_label = lookup(np_lbl_holder[i], </p><p class="source-code">                                           val_label_map)</p><p class="source-code">    actual.append(plain_text_label)</p><p><strong class="source-inline">actual</strong> holds the actual plaintext labels of all 50 test samples.</p></li>&#13;
				<li>This is how we can get a list holding the predicted label:<p class="source-code">predicted_label = []</p><p class="source-code">for i in range(sample_size):</p><p class="source-code">batch_prediction = batch_predicted_prob_array.get('custom_class').numpy()</p><p class="source-code">plain_text_label = lookup(batch_prediction[i], </p><p class="source-code">                                           val_label_map)</p><p class="source-code">predicted_label.append(plain_text_label)</p><p><strong class="source-inline">predicted_label</strong> holds the predictions for all 50 test samples in plaintext because we leverage the <strong class="source-inline">lookup</strong> function to map the probability to the flower type name.</p></li>&#13;
				<li>We will compare <strong class="source-inline">predicted_label</strong> and <strong class="source-inline">actual</strong> to get the accuracy of the model:<p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">accuracy=accuracy_score(actual, predicted_label)</p><p class="source-code">print(accuracy)</p><p class="source-code">0.82</p></li>&#13;
			</ol>&#13;
			<p>This shows that our full <a id="_idIndexMarker389"/>model's accuracy is 82%. This is <a id="_idIndexMarker390"/>simply done by comparing <strong class="source-inline">actual</strong> with <strong class="source-inline">predicted_label</strong> using the <strong class="source-inline">accuracy_score</strong> API from <strong class="source-inline">sklearn</strong>.</p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">It's expected that your model accuracy will be slightly different from the nominal value printed here. Every time a base model is trained, the model accuracy will not be identical. However, it should not be too dissimilar to the nominal value. Another factor that impacts reproducibility in terms of model accuracy is the number of epochs used in training; in this case, only five epochs for demonstration and didactic purposes. More training epochs will give you a better and tighter varia<a id="_idTextAnchor214"/>nce in terms of model accuracy.</p>&#13;
			<h1 id="_idParaDest-116"><a id="_idTextAnchor215"/>Converting a full model to a reduced float16 model</h1>&#13;
			<p>In this section, we <a id="_idIndexMarker391"/>are going to load the model <a id="_idIndexMarker392"/>we just trained and quantize it into a reduced <strong class="source-inline">float16</strong> model. For the convenience of step-by-step explanations and your learning experience, it is recommended that you use JupyterLab or Jupyter Notebook to follow along with the explanation here:</p>&#13;
			<ol>&#13;
				<li value="1">Let's start by loading the trained model:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pathlib</p><p class="source-code">import os</p><p class="source-code">import numpy as np</p><p class="source-code">from matplotlib.pyplot import imshow</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">root_dir = '../train_base_model'</p><p class="source-code">model_dir = ' trained_resnet_vector-unquantized/save_model'</p><p class="source-code">saved_model_dir = os.path.join(root_dir, model_dir)</p><p class="source-code">trained_model = tf.saved_model.load(saved_model_dir)</p><p>The <strong class="source-inline">tf.saved_model.load</strong> API helps us to load the saved model we built and trained. </p></li>&#13;
				<li>Then we will create a <strong class="source-inline">converter</strong> object to refer to the <strong class="source-inline">savedModel</strong> directory with the following line of code:<p class="source-code">converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)</p><p>For the <strong class="source-inline">converter</strong> object, we will select the <strong class="source-inline">DEFAULT</strong> optimization strategy for the converter to best improve model size and latency:</p><p class="source-code">converter.optimizations = [tf.lite.Optimize.DEFAULT]</p><p>The alternatives are <strong class="source-inline">OPTIMIZE_FOR_LATENCY</strong> or <strong class="source-inline">OPTIMIZE_FOR_SIZE</strong>. Refer to <a href="https://www.tensorflow.org/api_docs/python/tf/lite/Optimize">https://www.tensorflow.org/api_docs/python/tf/lite/Optimize</a> for information.</p></li>&#13;
				<li>Next, we will set <strong class="source-inline">float16</strong> as the target type for the model parameters and start the conversion process:<p class="source-code">converter.target_spec.supported_types = [tf.float16]</p><p class="source-code">tflite_model = converter.convert()</p><p>We will set up a directory designation for saving the quantized model using the following code:</p><p class="source-code">root_dir = ''</p><p class="source-code">tflite_model_dir = trained_resnet_vector-unquantized</p><p class="source-code">to_save_tflite_model_dir = os.path.join(root_dir, </p><p class="source-code">                                        tflite_model_dir)</p></li>&#13;
				<li>Now, we <a id="_idIndexMarker393"/>will create a <strong class="source-inline">pathlib</strong> object to represent the <a id="_idIndexMarker394"/>directory where we want to save our quantized model:<p class="source-code">saved_tflite_model_dir = pathlib.Path(</p><p class="source-code">                                to_save_tflite_model_dir)</p><p>Let's create the directory for saving the quantized model:</p><p class="source-code">saved_tflite_model_dir.mkdir(exist_ok=True, parents=True) </p></li>&#13;
				<li>We will now create a <strong class="source-inline">pathlib</strong> object, <strong class="source-inline">tgt</strong>, to represent the quantized model file:<p class="source-code">tgt = pathlib.Path(tflite_models_dir, </p><p class="source-code">                        'converted_model_reduced.tflite')</p><p>We will now write the quantized model using the <strong class="source-inline">pathlib</strong> object, <strong class="source-inline">tgt</strong>:</p><p class="source-code">tgt.write_bytes(tflite_model)</p><p>This will show the output in terms of the size of bytes written: </p><p class="source-code">47487392</p><p>With the last command, you will see that the quantized model size is slightly more than <strong class="source-inline">47</strong> MB, at exactly <strong class="source-inline">47,487,392</strong> Bytes. Go to the following directory:<strong class="source-inline">../trained_resnet_vector-unquantized/save_model/variables.</strong></p><p>This shows that the original model's weight and bias file is slightly more than 95 MB (results may vary and won't be exactly the same if you train it again; however, it should be very close to 95 MB) as shown in the following figure:</p></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer113" class="IMG---Figure">&#13;
					<img src="Images/image0032.jpg" alt="Figure 7.2 – Original model’s weight and bias file size&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 7.2 – Original model's weight and bias file size</p>&#13;
			<p>The quantized <a id="_idIndexMarker395"/>model is about half the size of the original model. This is <a id="_idIndexMarker396"/>as expected, as the model was converted from <strong class="source-inline">float32</strong> to <strong class="source-inline">float16</strong> format. Next, we are going to score our test data with the reduced <strong class="source-inline">float16</strong> model.</p>&#13;
			<h2 id="_idParaDest-117"><a id="_idTextAnchor216"/>Preparing the reduced float16 model for scoring</h2>&#13;
			<p>In this section, we will <a id="_idIndexMarker397"/>use the quantized model (reduced <strong class="source-inline">float16</strong>) to score the same test dataset used in the previous section. We will execute scoring (inferencing) with the TensorFlow Lite interpreter interface:</p>&#13;
			<ol>&#13;
				<li value="1">We will load the quantized model from the file path represented by <strong class="source-inline">tflite_models_dir</strong>. In the previous section, we created a <strong class="source-inline">pathlib</strong> object, <strong class="source-inline">tgt</strong>, to represent the quantized model file:<p class="source-code">tgt = pathlib.Path(tflite_models_dir, </p><p class="source-code">                        'converted_model_reduced.tflite')</p></li>&#13;
				<li>Then we need to get the <strong class="source-inline">input_details</strong> and <strong class="source-inline">output_details</strong> tensors:<p class="source-code">input_details = interpreter.get_input_details()</p><p class="source-code">output_details = interpreter.get_output_details()</p></li>&#13;
				<li>From these tensors, we will inspect the shape of the NumPy arrays in both the input and output:<p class="source-code">input_details[0]['shape']</p><p class="source-code">array([  1, 224, 224,   3], dtype=int32)</p><p class="source-code">output_details[0]['shape']</p><p class="source-code">array([1, 5], dtype=int32)</p></li>&#13;
			</ol>&#13;
			<p>We verified that the <a id="_idIndexMarker398"/>model input and output are expected to be a batch because there are four dimensions in these tensors. Next, we are going to see how well this model performs by scoring the test data.</p>&#13;
			<h2 id="_idParaDest-118"><a id="_idTextAnchor217"/>Scoring a single image with a quantized model</h2>&#13;
			<p>Now we may <a id="_idIndexMarker399"/>start the scoring process with a TFLite <a id="_idIndexMarker400"/>quantized model. In the following steps, we first expand the sample to include a dimension for the batch, pass the input data to the interpreter, perform scoring of input data, and then get the output of the prediction:</p>&#13;
			<p class="source-code">input_data = np.array(np.expand_dims(np_img_holder[0], axis=0), dtype=np.float32)</p>&#13;
			<p class="source-code">interpreter.set_tensor(input_details[0]['index'], input_data)</p>&#13;
			<p class="source-code">interpreter.invoke()</p>&#13;
			<p class="source-code">output_data = interpreter.get_tensor(output_details[0]['index'])</p>&#13;
			<p class="source-code">print(output_data)</p>&#13;
			<p>Here is the output:</p>&#13;
			<p class="source-code">[[1.5181543e-04 2.0090181e-05 1.0022727e-06 2.8991076e-06 9.9982423e-01]]</p>&#13;
			<p>To map <strong class="source-inline">output_data</strong> back to the original labels, execute the following command:</p>&#13;
			<p class="source-code">lookup(output_data, val_label_map)</p>&#13;
			<p>Here is the output:</p>&#13;
			<p class="source-code">'tulips'</p>&#13;
			<h2 id="_idParaDest-119"><a id="_idTextAnchor218"/>Scoring a batch image with a quantized model</h2>&#13;
			<p>Currently, batch <a id="_idIndexMarker401"/>scoring in the TFLite model is supported through the iterative scoring process of a single image. For our example of 50 <a id="_idIndexMarker402"/>test images, we may create a helper function to encapsulate the entire single image scoring process: </p>&#13;
			<ol>&#13;
				<li value="1">This is a function that handles batch scoring:<p class="source-code">def batch_predict(input_raw, input_tensor, output_tensor, dictionary):</p><p class="source-code">    input_data = np.array(np.expand_dims(input_raw, </p><p class="source-code">                               axis=0), dtype=np.float32)</p><p class="source-code">    interpreter.set_tensor(input_tensor[0]['index'], </p><p class="source-code">                                              input_data)</p><p class="source-code">    interpreter.invoke()</p><p class="source-code">    interpreter_output = interpreter.get_tensor(</p><p class="source-code">                               output_tensor[0]['index'])</p><p class="source-code">    plain_text_label = lookup(interpreter_output, </p><p class="source-code">                                               dictionary)</p><p class="source-code">    return plain_text_label</p><p>This function expands raw image dimensions to batches, and then passes the batched image to the interpreter for scoring. The interpreter's output is then mapped to a plaintext name by means of the <strong class="source-inline">lookup</strong> function and the plaintext is returned as the predicted label.</p></li>&#13;
				<li>Next, we will iterate through our test data to call on <strong class="source-inline">batch_predict</strong>:<p class="source-code">batch_quantized_prediction = []</p><p class="source-code">for i in range(sample_size):</p><p class="source-code">    plain_text_label = batch_predict(np_img_holder[i], input_details, output_details, val_label_map)</p><p class="source-code">    batch_quantized_prediction.append(plain_text_label)</p><p>The result is stored in the <strong class="source-inline">batch_quantized_prediction</strong> list. </p></li>&#13;
				<li>And just like how we measure the <a id="_idIndexMarker403"/>prediction accuracy of our original model, we may <a id="_idIndexMarker404"/>use <strong class="source-inline">accuracy_score</strong> to get the accuracy of the TFLite quantized model:<p class="source-code">quantized_accuracy = accuracy_score(actual, batched_quantized_prediiction)</p><p class="source-code">print(quantized_accuracy)</p><p>The output is as follows:</p><p class="source-code">0.82</p><p>The output here is shown to also be 82%. Results may vary if you retrained the model, but in my experience, it is identical to the accuracy of the base model.</p><p class="callout-heading">Note</p><p class="callout">It's expected that your model accuracy will be slightly different from the nominal value printed here. Every time a base model is trained, the model accuracy will not be identical. However, it should not be too dissimilar to the nominal value. Another factor that impacts reproducibility in terms of model accuracy is the number of epochs used in training; in this case, only five epochs for demonstration and didactic purposes. More training epochs will give you a better and tighter variance in terms of model accuracy.</p></li>&#13;
			</ol>&#13;
			<p>The functions, routines, and workflow developed up to this point will be used in the remaining sections of this chapter to demonstrate the process and outcome of model optimization. We have <a id="_idIndexMarker405"/>learned how to score the original model, convert the original model to the TFLite <a id="_idIndexMarker406"/>quantized model, and score the quantized model. Next, we will convert the original model to different formats using <a id="_idTextAnchor219"/>the same conversion and evaluation processes. </p>&#13;
			<h1 id="_idParaDest-120"><a id="_idTextAnchor220"/>Converting a full model to a reduced hybrid quantization model</h1>&#13;
			<p>In the <a id="_idIndexMarker407"/>previous section, we converted a full model into a reduced <strong class="source-inline">float16</strong> TFLite model, and demonstrated its scoring <a id="_idIndexMarker408"/>and evaluation processes. Now we will try the second type of supported quantization, which is a hybrid approach. </p>&#13;
			<p>Hybrid quantization optimizes the model by converting the model to 8-bit integer weights, 32-bit float biases, and activations. Since it contains both integer and floating-point <a id="_idIndexMarker409"/>computations, it is known as hybrid quantization. This is intended for a trade-off between accuracy and optimization.</p>&#13;
			<p>There is only one small difference that we need to make for hybrid quantization. There is only one line of difference, as explained below. In the previous section, this is how we quantized the full model to a reduced <strong class="source-inline">float16</strong> TFLite model:</p>&#13;
			<p class="source-code">converter.optimizations = [tf.lite.Optimize.DEFAULT]</p>&#13;
			<p class="source-code">converter.target_spec.supported_types = [tf.float16]</p>&#13;
			<p class="source-code">tflite_model = converter.convert()</p>&#13;
			<p>For hybrid quantization, we will simply remove the middle line about <strong class="source-inline">supported_types</strong>:</p>&#13;
			<p class="source-code">converter.optimizations = [tf.lite.Optimize.DEFAULT]</p>&#13;
			<p class="source-code">tflite_model = converter.convert()</p>&#13;
			<p>Everything else remains pretty much the same. Following is the complete notebook for hybrid quantization and scoring:</p>&#13;
			<ol>&#13;
				<li value="1">As usual, we will specify the necessary libraries and path to the model: <p class="source-code">import tensorflow as tf</p><p class="source-code">import pathlib</p><p class="source-code">import os</p><p class="source-code">import numpy as np</p><p class="source-code">from matplotlib.pyplot import imshow</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">root_dir = ''</p><p class="source-code">model_dir = 'trained_resnet_vector-unquantized/save_model'</p><p class="source-code">saved_model_dir = os.path.join(root_dir, model_dir)</p><p>Now, the <a id="_idIndexMarker410"/>path to the model is <a id="_idIndexMarker411"/>specified in <strong class="source-inline">saved_model_dir</strong>.</p></li>&#13;
				<li>Then we create a <strong class="source-inline">converter</strong> object for <strong class="source-inline">saved_model_dir</strong> and use it to convert our model:<p class="source-code">converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)</p><p class="source-code">converter.optimizations = [tf.lite.Optimize.DEFAULT]</p><p class="source-code">tflite_model = converter.convert()</p><p>Now, the converter converts the full model to a hybrid quantization model.</p></li>&#13;
				<li>Now we will save our hybrid quantization model: <p class="source-code">root_dir = ''</p><p class="source-code">tflite_models_dir = 'trained_resnet_vector-unquantized/tflite_hybrid_model'</p><p class="source-code">to_save_tflite_model_dir = os.path.join(root_dir,           tflite_models_dir)</p><p class="source-code">saved_tflite_models_dir = pathlib.Path(to_save_tflite_model_dir) </p><p class="source-code">saved_tflite_models_dir.mkdir(exist_ok=True, parents=True</p><p class="source-code">tgt = pathlib.Path(to_save_tflite_model_dir, 'converted_model_reduced.tflite')</p><p class="source-code">tgt.write_bytes(tflite_model)</p><p>The output shows the model size in bytes:</p><p class="source-code">24050608</p></li>&#13;
			</ol>&#13;
			<p>This is significantly <a id="_idIndexMarker412"/>smaller than the 95 <a id="_idIndexMarker413"/>MB of the original base model. Next, let's see how well this smaller, hybrid quantized model performs with test data.</p>&#13;
			<h2 id="_idParaDest-121"><a id="_idTextAnchor221"/>Preparing test data for scoring</h2>&#13;
			<p>We will begin by <a id="_idIndexMarker414"/>loading the test data, as we did with the reduced <strong class="source-inline">float16</strong> model:</p>&#13;
			<ol>&#13;
				<li value="1">We can load TFRecord data, as we have done previously:<p class="source-code">root_dir = '../train_base_model/tf_datasets/flower_photos'</p><p class="source-code">test_pattern = '{}/image_classification_builder-test.tfrecord*'.format(root_dir)</p><p class="source-code">test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_pattern))</p><p class="source-code">test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p>Now, <strong class="source-inline">test_all_ds</strong> represents the dataset object that points to the path of our test data.</p></li>&#13;
				<li>We may determine sample size by iterating through the dataset and keeping track of the sample count:<p class="source-code">sample_size = 0</p><p class="source-code">for raw_record in test_all_ds:</p><p class="source-code">    sample_size += 1</p><p class="source-code">print('Sample size: ', sample_size)</p><p>This will show the sample size as <strong class="source-inline">50</strong>.</p></li>&#13;
				<li>We use the <a id="_idIndexMarker415"/>same helper functions seen in the reduced <strong class="source-inline">float16</strong> model section to standardize the image size and pixel values:<p class="source-code">def decode_and_resize(serialized_example):</p><p class="source-code">    # resized image should be [224, 224, 3] and normalized to value range [0, 255] </p><p class="source-code">    # label is integer index of class.</p><p class="source-code">        parsed_features = tf.io.parse_single_example(</p><p class="source-code">    serialized_example,</p><p class="source-code">    features = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], 	  	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([], 	 	       	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">    })</p><p class="source-code">    image = tf.io.decode_jpeg(parsed_features[</p><p class="source-code">                            'image/encoded'], channels=3)</p><p class="source-code">    label = tf.cast(parsed_features['image/class/label'], </p><p class="source-code">                                                 tf.int32)</p><p class="source-code">    label_txt = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/text'], tf.string)</p><p class="source-code">    label_one_hot = tf.one_hot(label, depth = 5)</p><p class="source-code">    resized_image = tf.image.resize(image, [224, 224], </p><p class="source-code">                                         method='nearest')</p><p class="source-code">    return resized_image, label_one_hot</p><p class="source-code">def normalize(image, label):</p><p class="source-code">    #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p><p class="source-code">    image = tf.cast(image, tf.float32) / 255. </p><p class="source-code">    return image, label</p><p>The <strong class="source-inline">decode_and_resize</strong> function parses an image, resizes it to <strong class="source-inline">224</strong> by <strong class="source-inline">224</strong> pixels, and, at the same time, one-hot encodes the image's label. <strong class="source-inline">decode_and_resize</strong> then returns the image and corresponding label as a tuple, so that the image and label are always kept together.</p><p>The <strong class="source-inline">normalize</strong> function divides the image pixel value by <strong class="source-inline">255</strong> in order to bring the pixel range to <strong class="source-inline">[0, 1.0]</strong>. And even though nothing is done in relation to the label, it is necessary to keep track of the image and label as a tuple so that they are always kept together.</p></li>&#13;
				<li>Next, we will <a id="_idIndexMarker416"/>apply the transformation with the following helper functions:<p class="source-code">decoded = test_all_ds.map(decode_and_resize)</p><p class="source-code">normed = decoded.map(normalize)</p></li>&#13;
				<li>Let's convert TFRecord to a NumPy array for scoring:<p class="source-code">np_img_holder = np.empty((0, 224, 224,3), float)</p><p class="source-code">np_lbl_holder = np.empty((0, 5), int)</p><p class="source-code">for img, lbl in normed:</p><p class="source-code">    r = img.numpy() # image value extracted</p><p class="source-code">    rx = np.expand_dims(r, axis=0) </p><p class="source-code">    lx = np.expand_dims(lbl, axis=0) </p><p class="source-code">    np_img_holder = np.append(np_img_holder, rx, axis=0) </p><p class="source-code">    np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) </p><p>Now, all test images are in NumPy format with standard <strong class="source-inline">(224, 224, 3)</strong> dimensions, the pixel values are between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, and images are batched. Labels are batched as well. </p></li>&#13;
				<li>We now need to extract the ground truth labels so that we can measure our prediction accuracy:<p class="source-code">actual = []</p><p class="source-code">for i in range(len(np_lbl_holder)):</p><p class="source-code">    class_key = np.argmax(np_lbl_holder[i])</p><p class="source-code">    actual.append(val_label_map.get(class_key))</p><p>In the preceding code, <strong class="source-inline">actual</strong> is a list that contains class names for each test image. </p></li>&#13;
				<li>We may inspect the NumPy array, <strong class="source-inline">np_img_holder</strong>, as images with the following code, and <a id="_idIndexMarker417"/>this will produce the images seen in <em class="italic">Figure 7.1</em>:<p class="source-code">%matplotlib inline</p><p class="source-code">plt.figure()</p><p class="source-code">for i in range(len(np_img_holder)):</p><p class="source-code">    plt.subplot(10, 5, i+1)</p><p class="source-code">    plt.axis('off')</p><p class="source-code">imshow(np.asarray(np_img_holder[i]))</p><p>In the preceding code snippet, we iterate through our image array, and place each in one of the subplots, while there are 50 images (10 rows, with each row having 5 subplots). The output images should appear in 10 rows with 5 images in each row, as seen in <em class="italic">Figure 7.1</em>.</p><p>For single test file scoring, we need to add a dimension to a sample. Since a given image is of the shape <strong class="source-inline">(224, 224, 3)</strong>, we need to make it into <strong class="source-inline">(1, 224, 224, 3)</strong> so that it will be accepted by the model for scoring. This is why we used <strong class="source-inline">np.expand_dim</strong> when we converted TFRecord to NumPy. As the model is built to handle batch scoring, it is expecting four dimensions with the first dimension being the sample size. </p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-122"><a id="_idTextAnchor222"/>Mapping a prediction to a class name</h2>&#13;
			<p>From TFRecord, we need to create a reverse lookup dictionary to map probability back to the label. In other <a id="_idIndexMarker418"/>words, we need to find the index where maximum probability is positioned in the array. We will then map this position index to the flower type. </p>&#13;
			<p>To create the lookup dictionary, we will parse the TFRecord with feature descriptions to extract the label indices and names as shown in the following code:</p>&#13;
			<p class="source-code">feature_description = {</p>&#13;
			<p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/format' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p>&#13;
			<p class="source-code">}</p>&#13;
			<p class="source-code">def _parse_function(example_proto):</p>&#13;
			<p class="source-code">  return tf.io.parse_single_example(example_proto, </p>&#13;
			<p class="source-code">                                           feature_description)</p>&#13;
			<p class="source-code">parsd_ds = test_all_ds.map(_parse_function)</p>&#13;
			<p class="source-code">val_label_map = {}</p>&#13;
			<p class="source-code"># getting label mapping</p>&#13;
			<p class="source-code">for image_features in parsd_ds.take(50):</p>&#13;
			<p class="source-code">    label_idx = image_features['image/class/label'].numpy()</p>&#13;
			<p class="source-code">    label_str = image_features['image/class/text'].numpy().decode()</p>&#13;
			<p class="source-code">    if label_idx not in val_label_map:</p>&#13;
			<p class="source-code">        val_label_map[label_idx] = label_str</p>&#13;
			<p>In the preceding code, we used <strong class="source-inline">feature_description</strong> to parse <strong class="source-inline">test_all_ds</strong>. Once it is parsed using <strong class="source-inline">_parse_function</strong>, we iterate through the entire test dataset. The <a id="_idIndexMarker419"/>information we want can be found in <strong class="source-inline">image/class/label</strong> and <strong class="source-inline">image/class/text</strong>. </p>&#13;
			<p>We can also inspect <strong class="source-inline">val_label_map</strong>:</p>&#13;
			<p class="source-code">{4: 'tulips', 3: 'dandelion', 1: 'sunflowers', 2: 'daisy', 0: 'roses'}</p>&#13;
			<p>This is the lookup table that maps the index to a plaintext name.</p>&#13;
			<h2 id="_idParaDest-123"><a id="_idTextAnchor223"/>Scoring with a hybrid quantization model</h2>&#13;
			<p>As we did for the <a id="_idIndexMarker420"/>reduced <strong class="source-inline">float16</strong> model, we want to see how a hybrid quantization model performs with test data. Now we can start the process of scoring test images with the hybrid quantization model:</p>&#13;
			<ol>&#13;
				<li value="1">We will begin by loading the model and allocating tensors as usual with the help of the following lines of code:<p class="source-code">interpreter = tf.lite.Interpreter(model_path=str(tgt))</p><p class="source-code">interpreter.allocate_tensors()</p><p>Now the hybrid quantization model is loaded.</p></li>&#13;
				<li>To ascertain the input and output shape of the tensors that the model operates with, we may obtain input and output tensors in the following way:<p class="source-code">input_details = interpreter.get_input_details()</p><p class="source-code">output_details = interpreter.get_output_details()</p><p>In the preceding code, the <strong class="source-inline">get_input_details</strong> and <strong class="source-inline">get_output_details</strong> methods <a id="_idIndexMarker421"/>will retrieve these tensor's details, such as <strong class="source-inline">name</strong>, <strong class="source-inline">shape</strong>, and data type, and store these in <strong class="source-inline">input_details</strong> and <strong class="source-inline">output_details</strong>, respectively.</p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-124"><a id="_idTextAnchor224"/>Scoring a single image</h2>&#13;
			<p>We will score a single <a id="_idIndexMarker422"/>image by expanding its dimension, as if this is a batch of a single image, pass it to the TFLite interpreter, and then get the output:</p>&#13;
			<ol>&#13;
				<li value="1">We may begin by handling the image array and expanding its dimensions for batch:<p class="source-code">input_data = np.array(np.expand_dims(np_img_holder[0], axis=0), dtype=np.float32)</p><p class="source-code">interpreter.set_tensor(input_details[0]['index'], input_data)</p><p class="source-code">interpreter.invoke()</p><p class="source-code">output_data = interpreter.get_tensor(output_details[0]['index'])</p><p class="source-code">print(output_data)</p><p>The preceding code expands the image to a batch dimension, and then passes it to the interpreter for prediction. The output of the preceding code is here:</p><p class="source-code">[[1.1874483e-04 1.3445899e-05 8.4869811e-07 2.8064751e-06 9.9986410e-01]]</p><p>These are probabilities for each flower type. We need to map the position of highest probability to its plaintext name. That's where we will use the <strong class="source-inline">lookup</strong> function again.</p></li>&#13;
				<li>We use a helper function (<strong class="source-inline">lookup</strong>) to convert probability into the most likely class name:<p class="source-code">def lookup(np_entry, dictionary):</p><p class="source-code">    class_key = np.argmax(np_entry)</p><p class="source-code">    return dictionary.get(class_key)</p><p class="source-code">lookup(output_data, val_label_map)</p><p>The output is as follows:</p><p class="source-code">'tulips'</p><p>In the <strong class="source-inline">lookup</strong> function, the NumPy array, <strong class="source-inline">np_entry</strong>, is the output of our model. It contains the probability for each class. We want to map the position index of the array with the highest <a id="_idIndexMarker423"/>probability to the class name. To achieve this, this function maps it to the dictionary by key. In this case, it is the last position (which corresponds to position <strong class="source-inline">4</strong>) in the probability array that has the highest probability. <strong class="source-inline">4</strong> is mapped to <strong class="source-inline">tulips</strong>.</p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-125"><a id="_idTextAnchor225"/>Scoring batch images</h2>&#13;
			<p>Currently, batch <a id="_idIndexMarker424"/>scoring in the TFLite model is supported through the iterative scoring process of a single image. For our example of 50 test images, we may create a helper function to encapsulate the entire single image scoring process that we just went through in the previous Scoring a single image section with the hybrid quantization model:</p>&#13;
			<ol>&#13;
				<li value="1">We will iterate the entire dataset to score the batch with the help of the following code:<p class="source-code">def batch_predict(input_raw, input_tensor, output_tensor, dictionary):</p><p class="source-code">    input_data = np.array(np.expand_dims(input_raw, 	  	                               axis=0), dtype=np.float32)</p><p class="source-code">    interpreter.set_tensor(input_tensor[0]['index'], 	 	                                              input_data)</p><p class="source-code">    interpreter.invoke()</p><p class="source-code">    interpreter_output = interpreter.get_tensor(</p><p class="source-code">                               output_tensor[0]['index'])</p><p class="source-code">    plain_text_label = lookup(interpreter_output, 	 	                                               dictionary)</p><p class="source-code">    return plain_text_label</p><p>The <strong class="source-inline">batch_predict()</strong> function expands the raw image dimensions to batches, and then passes the batched image to the interpreter for scoring. The interpreter's output is then mapped to a <a id="_idIndexMarker425"/>plaintext name by means of the <strong class="source-inline">lookup</strong> function and the plaintext is returned as the predicted label.</p></li>&#13;
				<li>We then need to iterate through our test data to call on <strong class="source-inline">batch_predict</strong>:<p class="source-code">batch_quantized_prediction = []</p><p class="source-code">for i in range(sample_size):</p><p class="source-code">    plain_text_label = batch_predict(np_img_holder[i], input_details, output_details, val_label_map)</p><p class="source-code">batch_quantized_prediction.append(plain_text_label)</p><p>We may evaluate the model's accuracy using the <strong class="source-inline">accuracy_score</strong> function in the sklearn library:</p><p class="source-code">accuracy=accuracy_score(actual, batch_quantized_prediction)</p><p class="source-code">print(accuracy)</p><p>Its output is as follows:</p><p class="source-code">0.82</p><p>The output here is shown to also be 82%. Results may vary if you retrained the model, but in my experience, it is identical to the accuracy of the base model.</p><p class="callout-heading">Note</p><p class="callout">It's expected that your model accuracy will be slightly different from the nominal value printed here. Every time a base model is trained, the model accuracy will not be identical. However, it should not be too dissimilar to the nominal value. Another factor that impacts reproducibility in terms of model accuracy is the number of epochs used in training; in this case, only five epochs for demonstration and didactic purposes. More training epochs will give you a better and tighter variance in terms of model accuracy.</p></li>&#13;
			</ol>&#13;
			<p>So far, we have learned about two types of post-training quantization techniques, namely, reduced <strong class="source-inline">float16</strong> quantization and hybrid quantization. Both techniques make the TFLite model significantly smaller than the original model. This is important when deploying the model in edge devices or devices with low compute or power resources. </p>&#13;
			<p>In these two <a id="_idIndexMarker426"/>strategies, we quantized the middle layers and left the input and output untouched. Therefore, the input and output are not quantized and keep their respective original data types. However, in some devices that are optimized for speed and being lightweight, such as an edge TPU or devices that can only handle integer ops, we need to quantize the input and output layers to an integer type.</p>&#13;
			<p>In the next section, we are going to learn the third quantization strate<a id="_idTextAnchor226"/>gy, which is integer quantization, which would do precisely this. </p>&#13;
			<h1 id="_idParaDest-126"><a id="_idTextAnchor227"/>Converting a full model to an integer quantization model</h1>&#13;
			<p>This <a id="_idIndexMarker427"/>strategy requires <strong class="bold">TensorFlow 2.3</strong>. This quantization strategy is suitable for an environment where compute resources are really constrained, or where the compute node only operates in integer mode, such as edge devices or TPUs. As a result, all parameters are changed to <strong class="source-inline">int8</strong> representation. This quantization strategy will try to use <strong class="source-inline">int8</strong> representation for all ops or operations as the goal. When this is not possible, the ops are left as the original precision (in other words, <strong class="source-inline">float32</strong>). </p>&#13;
			<p>This quantization strategy requires some representative data. This data represents the type of data that the model typically expects in terms of a range of values. In other words, we need to provide either some training or validation data to the integer quantization process. This may be the data already used, such as a subset of the training or validation data. Usually, around 100 samples are recommended. We are going to use 80 samples from the validation data because this will suffice in this case.</p>&#13;
			<p>In this section, we will build a model with a pre-trained ResNet feature vector from TensorFlow Hub. Once the training run <a id="_idIndexMarker428"/>is complete, we will use cross-validation data again as the representative dataset. This dataset will help the model to adjust parameters in both the input and output layers to integers. </p>&#13;
			<h2 id="_idParaDest-127"><a id="_idTextAnchor228"/>Training a full model</h2>&#13;
			<p>We are going to <a id="_idIndexMarker429"/>use the same flower dataset as in <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model/tf_datasets">https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model/tf_datasets</a>.</p>&#13;
			<p>This is the same dataset that you used for reduced <strong class="source-inline">float16</strong> and hybrid quantization Let's get started:.</p>&#13;
			<ol>&#13;
				<li value="1">As usual, we begin by importing libraries and loading the datasets:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">import numpy as np</p><p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">root_dir = '../train_base_model/tf_datasets/flower_photos'</p><p class="source-code">file_pattern = '{}/image_classification_builder-train*.tfrecord*'.format(root_dir)</p><p class="source-code">val_file_pattern = '{}/image_classification_builder-validation*.tfrecord*'.format(root_dir)</p><p class="source-code">file_list = tf.io.gfile.glob(file_pattern)</p><p class="source-code">all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(file_pattern))</p><p class="source-code">val_file_list = tf.io.gfile.glob(val_file_pattern)</p><p class="source-code">val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))</p><p class="source-code">train_all_ds = tf.data.TFRecordDataset(all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p class="source-code">val_all_ds = tf.data.TFRecordDataset(val_all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p>In the preceding code, we use the <strong class="source-inline">tf.io</strong> API to encapsulate the file path and all the filenames we will use, which are training, validation, and test data. Once we have the file paths encoded, we use <strong class="source-inline">tf.data.TFRecordDatasedt</strong> to reference these files. This <a id="_idIndexMarker430"/>process is performed for the training data, which is referenced by <strong class="source-inline">train_all_ds</strong>, and for the validation data, which is referenced by <strong class="source-inline">val_all_ds</strong>.</p></li>&#13;
				<li>Then we will require the following helper functions to decode and standardize images, normalize pixel values, and set up a training dataset:<p class="source-code">def decode_and_resize(serialized_example):</p><p class="source-code">    # resized image should be [224, 224, 3] and normalized to value range [0, 255] </p><p class="source-code">    # label is integer index of class.</p><p class="source-code">        parsed_features = tf.io.parse_single_example(</p><p class="source-code">    serialized_example,</p><p class="source-code">    features = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([],  	  	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([],  	   	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([],  	 	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">    })</p><p class="source-code">    image = tf.io.decode_jpeg(parsed_features[</p><p class="source-code">                            'image/encoded'], channels=3)</p><p class="source-code">    label = tf.cast(parsed_features['image/class/label'],  	                                                tf.int32)</p><p class="source-code">    label_txt = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/text'], tf.string)</p><p class="source-code">    label_one_hot = tf.one_hot(label, depth = 5)</p><p class="source-code">    resized_image = tf.image.resize(image, [224, 224], 	                                         method='nearest')</p><p class="source-code">    return resized_image, label_one_hot</p><p class="source-code">def normalize(image, label):</p><p class="source-code">    #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p><p class="source-code">    image = tf.cast(image, tf.float32) / 255. + 0.5</p><p class="source-code">    return image, label</p><p>The <strong class="source-inline">decode_and_resize</strong> function parses an image, resizes it to <strong class="source-inline">224</strong> by <strong class="source-inline">224</strong> pixels, and, at the same time, one-hot encodes the image's label. <strong class="source-inline">decode_and_resize</strong> then returns the image and corresponding label as a tuple, so that the image and label are always kept together.</p><p>The <strong class="source-inline">normalize</strong> function divides the image pixel value by <strong class="source-inline">255</strong> in order to bring the pixel range to <strong class="source-inline">[0, 1.0]</strong>. And even though nothing is done in relation to <a id="_idIndexMarker431"/>the label, it is necessary to keep track of the image and label as a tuple so that they are always kept together.</p></li>&#13;
				<li>We now need to define a function to shuffle and fetch the training dataset. Here is the function to achieve this:<p class="source-code">def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):</p><p class="source-code">    # This is a small dataset, only load it once, and</p><p class="source-code">    # keep it in memory.</p><p class="source-code">    # use `.cache(filename)` to cache preprocessing work</p><p class="source-code">    # for datasets that don't fit in memory.</p><p class="source-code">    if cache:</p><p class="source-code">        if isinstance(cache, str):</p><p class="source-code">            ds = ds.cache(cache)</p><p class="source-code">        else:</p><p class="source-code">            ds = ds.cache()</p><p class="source-code">    ds = ds.shuffle(buffer_size=shuffle_buffer_size)</p><p class="source-code">    # Repeat forever</p><p class="source-code">    ds = ds.repeat()</p><p class="source-code">    ds = ds.batch(32)</p><p class="source-code">    # `prefetch` lets the dataset fetch batches in the</p><p class="source-code">    # background while the model is training.</p><p class="source-code">    AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">    ds = ds.prefetch(buffer_size=AUTOTUNE)</p><p class="source-code">    return ds</p><p>This function returns a dataset with shuffle, repeat, batch, and prefetch ops attached. This is a standard approach for getting the dataset ready for training.</p></li>&#13;
				<li>Now we <a id="_idIndexMarker432"/>may apply the following steps to each element in the training dataset:<p class="source-code"># perform data engineering </p><p class="source-code">dataset = train_all_ds.map(decode_and_resize)</p><p class="source-code">val_dataset = val_all_ds.map(decode_and_resize)</p><p>So now, <strong class="source-inline">decode_and_resize</strong> is applied to each image in <strong class="source-inline">train_all_ds</strong> and <strong class="source-inline">val_all_ds</strong>. The resulting datasets are <strong class="source-inline">dataset</strong> and <strong class="source-inline">val_dataset</strong>, respectively. </p></li>&#13;
				<li>We also need to normalize the validation dataset and finalize the training dataset for the training run process:<p class="source-code"># Create dataset for training run</p><p class="source-code">BATCH_SIZE = 32</p><p class="source-code">VALIDATION_BATCH_SIZE = 40</p><p class="source-code">dataset = dataset.map(normalize, </p><p class="source-code">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</p><p class="source-code">val_dataset = val_dataset.map(normalize, </p><p class="source-code">        num_parallel_calls=tf.data.experimental.AUTOTUNE)</p><p class="source-code">val_ds = val_dataset.batch(VALIDATION_BATCH_SIZE)</p><p class="source-code">    AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">train_ds = prepare_for_training(dataset)</p><p>In the preceding code, we use the <strong class="source-inline">map</strong> function to apply the <strong class="source-inline">decode_and_resize</strong> function to each image in the dataset. For the training dataset, we also apply <strong class="source-inline">prepare_for_training</strong> to <a id="_idIndexMarker433"/>prefetch the dataset and for the ingestion process.</p></li>&#13;
				<li>Now we will set up the parameters for cross-validation:<p class="source-code">NUM_CLASSES = 5</p><p class="source-code">IMAGE_SIZE = (224, 224)</p><p class="source-code">    train_sample_size=0</p><p class="source-code">for raw_record in train_all_ds:</p><p class="source-code">    train_sample_size += 1</p><p class="source-code">print('TRAIN_SAMPLE_SIZE = ', train_sample_size)</p><p class="source-code">validation_sample_size=0</p><p class="source-code">for raw_record in val_all_ds:</p><p class="source-code">    validation_sample_size += 1</p><p class="source-code">print('VALIDATION_SAMPLE_SIZE = ', </p><p class="source-code">                                  validation_sample_size)</p><p class="source-code">STEPS_PER_EPOCHS = train_sample_size // BATCH_SIZE</p><p class="source-code">VALIDATION_STEPS = validation_sample_size // </p><p class="source-code">                                    VALIDATION_BATCH_SIZE</p><p>In the preceding code, we set the number of classes and the image size in variables to be passed to the model training process. Then we determine the number of steps for each epoch of training and cross-validation.</p><p>The output should be as follows:</p><p class="source-code">TRAIN_SAMPLE_SIZE =  3540</p><p class="source-code">VALIDATION_SAMPLE_SIZE =  80</p><p>This indicates that we have a training data sample size of <strong class="source-inline">3540</strong>, while the cross-validation data sample size is <strong class="source-inline">80</strong>.</p></li>&#13;
				<li>Now we will build the model <a id="_idIndexMarker434"/>with the help of the following code:<p class="source-code">model = tf.keras.Sequential([</p><p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),</p><p class="source-code">    hub.KerasLayer(</p><p class="source-code">    https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/4',</p><p class="source-code">    trainable=False),</p><p class="source-code">    tf.keras.layers.Dense(NUM_CLASSES, </p><p class="source-code">             activation='softmax', name = 'custom_class')</p><p class="source-code">])</p><p class="source-code">model.build([None, 224, 224, 3])</p><p class="source-code">model.compile(</p><p class="source-code">  optimizer=tf.keras.optimizers.SGD(lr=0.005, </p><p class="source-code">                                           momentum=0.9), </p><p class="source-code">  loss=tf.keras.losses.CategoricalCrossentropy(</p><p class="source-code">                  from_logits=True, label_smoothing=0.1),</p><p class="source-code">  metrics=['accuracy'])</p><p class="source-code">model.summary()</p><p>In the preceding code, we built and compiled our model using TensorFlow Hub's ResNet feature vector as the middle layer, and the output is a classification layer denoted by a dense layer with five outputs, with each output node providing a probability for one of the five flower types.</p><p>Here is the model <a id="_idIndexMarker435"/>summary, and it consists of a layer from the <em class="italic">resnet_v1_101</em> feature<a id="_idTextAnchor229"/> vector, followed by a classification head, as indicated in <em class="italic">Figure 7.3</em>:</p><div id="_idContainer114" class="IMG---Figure"><img src="Images/image0052.jpg" alt="Figure 7.3 – Model summary for flower type classification&#13;&#10;"/></div><p class="figure-caption">Figure 7.3 – Model summary for flower type classification</p><p>We will then use the <strong class="source-inline">fit</strong> API to train this model with the training and cross-validation data provided. </p></li>&#13;
				<li>The results of the model weights and biases are saved in the <strong class="source-inline">checkpoint_prefix</strong> directory. This is how we start the training process for the model to recognize five different types of flower images:<p class="source-code">checkpoint_prefix = os.path.join('trained_resnet_vector', 'train_ckpt_{epoch}')</p><p class="source-code">callbacks = [</p><p class="source-code">    tf.keras.callbacks.ModelCheckpoint(</p><p class="source-code">                              filepath=checkpoint_prefix,</p><p class="source-code">    save_weights_only=True)]</p><p class="source-code">model.fit(</p><p class="source-code">        train_ds,</p><p class="source-code">        epochs=3, </p><p class="source-code">        steps_per_epoch=STEPS_PER_EPOCHS,</p><p class="source-code">        validation_data=val_ds,</p><p class="source-code">        validation_steps=VALIDATION_STEPS,</p><p class="source-code">        callbacks=callbacks)</p><p>In the preceding code, the <strong class="source-inline">fit</strong> API is called to train the model. <strong class="source-inline">train_ds</strong> and <strong class="source-inline">val_ds</strong> are the training and <a id="_idIndexMarker436"/>cross-validation data, respectively. At each epoch, the weights and biases are stored as a checkpoint. This is specified by the callbacks. To save training time, we will only train it for three epochs.</p></li>&#13;
				<li>Next, we will save the model using the following lines of code:<p class="source-code">saved_model_path = os.path.join(root_dir, 'custom_cnn/full_resnet_vector_saved_model')</p><p class="source-code">tf.saved_model.save(model, saved_model_path)</p><p>We can inspect the weight matrix file to get an idea of the model size using the following command:</p><p class="source-code"><strong class="bold">!ls -lrt &lt;YOUR-HOME-PATH&gt;/custom_cnn/full_resnet_vector_saved_model/variables</strong></p><p>You can expect a result similar to this. The w<a id="_idTextAnchor230"/>eight and biases matrix is approximately 170 MB, as shown in <em class="italic">Figure 7.4</em>:</p><div id="_idContainer115" class="IMG---Figure"><img src="Images/image0072.jpg" alt="Figure 7.4 – Model file size for the ResNet feature vector-based classification model&#13;&#10;"/></div><p class="figure-caption">Figure 7.4 – Model file size for the ResNet feature vector-based classification model</p><p>In this section, we leveraged the transfer learning technique to reuse the pre-trained ResNet feature vector with a multiclass classification head. The model is saved as a <strong class="source-inline">SavedModel</strong> format. </p><p>In order to properly quantize the input and output layers, we need to provide some typical data. We will use the <a id="_idIndexMarker437"/>validation data, which contains 80 samples of 5 classes of flower images: </p></li>&#13;
				<li>Let's standardize and normalize the validation images:<p class="source-code">decoded = val_all_ds.map(decode_and_resize)</p><p class="source-code">normed = decoded.map(normalize)</p></li>&#13;
				<li>Next, we expand by one dimension to batch the images. This extra dimension is intended for a variable batch size:<p class="source-code">np_img_holder = np.empty((0, 224, 224,3), float)</p><p class="source-code">np_lbl_holder = np.empty((0, 5), int)</p><p class="source-code">for img, lbl in normed:</p><p class="source-code">    r = img.numpy()</p><p class="source-code">    rx = np.expand_dims(r, axis=0) </p><p class="source-code">    lx = np.expand_dims(lbl, axis=0) </p><p class="source-code">    np_img_holder = np.append(np_img_holder, rx, axis=0) </p><p class="source-code">    np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) </p><p>The image is now expanded by one dimension to indicate that the first dimension holds the number of images, which is the size of the image batch, and the normalized images are iterated through. As we iterate through each image, we capture the image value as a NumPy array and the corresponding label, and append <strong class="source-inline">np_img_holder</strong> and <strong class="source-inline">np_lbl_holder</strong>, respectively.</p></li>&#13;
				<li>Now that we have images <a id="_idIndexMarker438"/>as a NumPy array, we need to build a generator that feeds this representative data into the conversion process:<p class="source-code">def data_generator():</p><p class="source-code">  for input_tensor in tf.data.Dataset.from_tensor_slices(np_img_holder.astype(np.float32)).batch(1).take(sample_size):</p><p class="source-code">    yield [input_tensor]</p><p>We need to specify a function that is a generator to stream the representative data during the conversion process. This is done through the <strong class="source-inline">data_generator</strong> function. This function invokes the generator that streams a NumPy array. </p></li>&#13;
				<li>Let's confirm our sample size:<p class="source-code">sample_size = 0</p><p class="source-code">for raw_record in val_all_ds:</p><p class="source-code">    sample_size += 1</p><p class="source-code">print('Sample size: ', sample_size)</p><p>The output from the preceding <strong class="source-inline">print</strong> statement is as follows:</p><p class="source-code">Sample size:  80</p><p>The preceding code iterates through a validation dataset and keeps track of the sample count as it goes over a <strong class="source-inline">for</strong> loop. For every encounter of an image, a counter (<strong class="source-inline">sample_size</strong>, which is initialized to <strong class="source-inline">0</strong>) is incremented by 1. Currently this is the only way to find out about sample sizes in a dataset. We have just confirmed that there are 80 samples in our validation data. </p></li>&#13;
				<li>Now we may start the conversion process:<p class="source-code">converter = tf.lite.TFLiteConverter.from_keras_model(model)</p><p class="source-code">converter.optimizations = [tf.lite.Optimize.DEFAULT]</p><p class="source-code">converter.representative_dataset = data_generator</p><p>In the preceding code, we set up the converter instance and optimizer as in hybrid quantization, and <a id="_idIndexMarker439"/>then we set up a data generator object for the representative dataset.</p></li>&#13;
				<li>We also want to throw an error flag if there are any ops that failed to be quantized:<p class="source-code">converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]</p><p>In the preceding code, the supported data type we want for our model is set as an 8-bit integer (<strong class="source-inline">INT8</strong>). </p></li>&#13;
				<li>Now we designate the input and output tensors to be <strong class="source-inline">INT8</strong>:<p class="source-code">converter.inference_input_type = tf.uint8</p><p class="source-code">converter.inference_output_type = tf.uint8</p><p class="source-code">tflite_model_quant = converter.convert()</p><p>Now the model is converted to an integer quantization model. The model expects an input data type of an 8-bit integer (<strong class="source-inline">INT8</strong>) and will output the data type of an 8-bit integer (<strong class="source-inline">INT8</strong>).</p></li>&#13;
				<li> Once the preceding code finishes the execution, we may inspect and verify the data type now associated with the input and output layer as unsigned <strong class="source-inline">INT8</strong>:<p class="source-code">interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)</p><p class="source-code">input_type = interpreter.get_input_details()[0]['dtype']</p><p class="source-code">print('input: ', input_type)</p><p class="source-code">output_type = interpreter.get_output_details()[0]['dtype']</p><p class="source-code">print('output: ', output_type)</p><p>In the preceding code, we first have to get the interpreter interface to the TFLite model. An interpreter <a id="_idIndexMarker440"/>object is the component in the TFLite model that executes the inference. It has methods such as <strong class="source-inline">get_input_details</strong> and <strong class="source-inline">get_output_details</strong>, which help us to look at the data types expected by the model during inference.</p><p>The following is the output of the preceding code:</p><p class="source-code">input:  &lt;class 'numpy.uint8'&gt;</p><p class="source-code">output:  &lt;class 'numpy.uint8'&gt;</p><p>The model expects an input data type of an 8-bit integer (<strong class="source-inline">INT8</strong>) and will output the data type of an 8-bit integer (<strong class="source-inline">INT8</strong>).</p></li>&#13;
				<li>Now we can save the quantized model:<p class="source-code">tflite_models_dir = 'quantized_resnet_vector/tflite_int8_model'</p><p class="source-code">to_save_tflite_model_dir = os.path.join(root_dir, tflite_models_dir)</p><p class="source-code">saved_tflite_models_dir = pathlib.Path(to_save_tflite_model_dir) </p><p class="source-code">saved_tflite_models_dir.mkdir(exist_ok=True, parents=True) </p><p class="source-code">tgt = pathlib.Path(to_save_tflite_model_dir, 'converted_model_reduced.tflite')</p><p class="source-code">tgt.write_bytes(tflite_model_quant)</p><p>Now, with the help of the preceding code, we set up a directory path and encode the path to a <strong class="source-inline">string</strong>. This string represents the path where we will write our integer quantized <a id="_idIndexMarker441"/>model. Finally, the <strong class="source-inline">write_bytes</strong> API completes the write process and saves our integer quantized model in the path as defined by the string, <strong class="source-inline">tflite_models_dir</strong>.</p><p>This shows the model size to be the following:</p><p class="source-code">44526000</p><p>The preceding output shows that our integer quantization model is approximately 44 MB. </p></li>&#13;
			</ol>&#13;
			<p>Next, we are going to see how well this model performs by scoring the test data.</p>&#13;
			<h2 id="_idParaDest-128"><a id="_idTextAnchor231"/>Scoring with an integer quantization model</h2>&#13;
			<p>For scoring, we need to <a id="_idIndexMarker442"/>prepare the test dataset and a lookup table that maps the model output to a class name. Our test dataset contains labels encoded as an index and the corresponding class name. Therefore, we will use labels and class names from the test dataset as the ground truth. This will be compared to the model predictions. </p>&#13;
			<h2 id="_idParaDest-129"><a id="_idTextAnchor232"/>Preparing a test dataset for scoring</h2>&#13;
			<p>As we did for the reduced <strong class="source-inline">float16</strong> and hybrid quantization models, we want to see how an integer <a id="_idIndexMarker443"/>quantization model performs with test data. Now we can start the process of scoring test images with the integer quantization model:</p>&#13;
			<ol>&#13;
				<li value="1">We will proceed by loading the TFRecord test:<p class="source-code">test_pattern = '{}/image_classification_builder-test.tfrecord*'.format(root_dir)</p><p class="source-code">test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_pattern))</p><p class="source-code">test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p>In the preceding code, we use the <strong class="source-inline">tf.io</strong> API to encapsulate the file path and all the filenames we will use, which is the test data. Once we have the file paths encoded, we use <strong class="source-inline">tf.data.TFRecordDatasedt</strong> to reference the data. This process is done for the test data, which is referenced by <strong class="source-inline">test_all_ds</strong>.</p></li>&#13;
				<li>Next, we can verify the sample size:<p class="source-code">sample_size = 0</p><p class="source-code">for raw_record in test_all_ds:</p><p class="source-code">    sample_size += 1</p><p class="source-code">print('Sample size: ', sample_size)</p><p>This will show that the sample size is <strong class="source-inline">50</strong>. The preceding code iterates through the validation dataset and keeps track of the sample count as it goes over a <strong class="source-inline">for</strong> loop. For every encounter of an image, a counter (<strong class="source-inline">sample_size</strong>, which is initialized to <strong class="source-inline">0</strong>) is incremented by <strong class="source-inline">1</strong>. Currently, this is the only way to find out about sample size in a dataset. We have just confirmed that there are 80 samples in our validation data. </p></li>&#13;
				<li>As our model was quantized to handle integer ops, we don't want to normalize pixel values into floating-point values. We only need to standardize the image size:<p class="source-code">decoded = test_all_ds.map(decode_and_resize)</p><p>Then we convert TFRecord to NumPy arrays of image data and labels. </p></li>&#13;
				<li>We also need to expand the data dimensions to handle the batch of images:<p class="source-code">np_img_holder = np.empty((0, 224, 224,3), float)</p><p class="source-code">np_lbl_holder = np.empty((0, 5), int)</p><p class="source-code">for img, lbl in decoded:</p><p class="source-code">    r = img.numpy() </p><p class="source-code">    rx = np.expand_dims(r, axis=0) </p><p class="source-code">    lx = np.expand_dims(lbl, axis=0) </p><p class="source-code">    np_img_holder = np.append(np_img_holder, rx, axis=0) </p><p class="source-code">    np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) </p><p>The image is now <a id="_idIndexMarker444"/>expanded by one dimension to indicate that the first dimension holds the number of images, which is the size of the image batch, and the normalized images are iterated through. As we iterate through each image, we capture the image value as a NumPy array and the corresponding label, and append <strong class="source-inline">np_img_holder</strong> and <strong class="source-inline">np_lbl_holder</strong>, respectively.</p></li>&#13;
				<li>To create a lookup dictionary to map the label index to the class name, we may iterate through the TFRecord dataset to create a dictionary, <strong class="source-inline">val_label_map</strong>, but first, we need to know how to parse the TFRecord dataset. This means that we need to capture the tensors in the TFRecord dataset correctly. Therefore, we need to use the following <strong class="source-inline">feature_description</strong>:<p class="source-code">feature_description = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([], 	 	   	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">}</p><p><strong class="source-inline">The</strong> <strong class="source-inline">feature_description</strong> in the preceding code is a <a id="_idIndexMarker445"/>collection of key-value pairs. Each pair delineates a piece of metadata represented as a tensor:</p><p class="source-code">def _parse_function(example_proto):</p><p class="source-code">  return tf.io.parse_single_example(example_proto, </p><p class="source-code">                                     feature_description)</p><p class="source-code">parsd_ds = test_all_ds.map(_parse_function)</p><p>The preceding code shows how to parse <strong class="source-inline">test_all_ds</strong> with the <strong class="source-inline">feature_description</strong> provided. The result is a parsed dataset (<strong class="source-inline">parsd_ds</strong>) with all the necessary tensors defined and parsed:</p><p class="source-code">val_label_map = {}</p><p class="source-code"># getting label mapping</p><p class="source-code">for image_features in parsd_ds.take(30):</p><p class="source-code">    label_idx = image_features[</p><p class="source-code">                             'image/class/label'].numpy()</p><p class="source-code">    label_str = image_features[</p><p class="source-code">                     'image/class/text'].numpy().decode()</p><p class="source-code">    if label_idx not in val_label_map:</p><p class="source-code">        val_label_map[label_idx] = label_str</p><p>We now need to find out how the dataset assigns indices to class labels. One way of doing this is to iterate through the whole dataset, or a portion of it. At each iteration, we <a id="_idIndexMarker446"/>capture both the label index and the corresponding plaintext for the label, and update this as a key-value pair in a dictionary such as <strong class="source-inline">val_label_map</strong>. This is shown as the preceding code.</p></li>&#13;
				<li>We may inspect the dictionary by typing <strong class="source-inline">val_label_map</strong> in a notebook cell:<p class="source-code">val_label_map</p><p>You may find <strong class="source-inline">val_label_map</strong> to be a dictionary such as this:</p><p class="source-code">{0: 'roses', 1: 'sunflowers', 2: 'daisy', 3: 'dandelion', 4: 'tulips'}</p><p>Keys are indexes of flower classes, and the values are plaintext names of flower classes.</p></li>&#13;
				<li>We will create a helper function to handle the lookup:<p class="source-code">def lookup(np_entry, dictionary):</p><p class="source-code">    class_key = np.argmax(np_entry)</p><p class="source-code">    return dictionary.get(class_key)</p><p>In the <strong class="source-inline">lookup</strong> function, the NumPy array <strong class="source-inline">np_entry</strong> is the output of our model. It contains the probability for each class. We want to map the position index of the array with the highest probability to the class name. To achieve this, this function maps it to the dictionary by key. </p></li>&#13;
				<li>Next, we create a list that contains the ground truth flower class names:<p class="source-code">actual = []</p><p class="source-code">for i in range(len(np_lbl_holder)):</p><p class="source-code">    class_key = np.argmax(np_lbl_holder[i])</p><p class="source-code">    actual.append(val_label_map.get(class_key))</p><p>We can create a table that <a id="_idIndexMarker447"/>maps the integer value of the label to the corresponding plaintext name. In the preceding code, we first set up an empty list, <strong class="source-inline">actual</strong>, and then we use a <strong class="source-inline">for</strong> loop to iterate through the entire label holder, <strong class="source-inline">np_lbl_holder</strong>. The next step is to find the position where the maximum value occurs in this record, and assign it to <strong class="source-inline">class_key</strong>. <strong class="source-inline">class_key</strong> is the index that is used for looking up <strong class="source-inline">val_label_map</strong>, which maps the key to the corresponding plaintext name. The plaintext name is then added to <strong class="source-inline">actual</strong>. Then, the <strong class="source-inline">for</strong> loop starts over again with the next record it finds in <strong class="source-inline">np_lbl_holder</strong>. </p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-130"><a id="_idTextAnchor233"/>Scoring batch images</h2>&#13;
			<p>A helper <a id="_idIndexMarker448"/>function is required for batch scoring. This is similar to what we used in the hybrid and reduced <strong class="source-inline">float16</strong> quantization models. The only difference lies in the data type for the NumPy array dimension expansion. Since we are using a model built by integer quantization, we need to cast the data type to an unsigned 8-bit integer (<strong class="source-inline">uint8</strong>):</p>&#13;
			<ol>&#13;
				<li value="1">This is a <strong class="source-inline">batch_predict</strong> function that treats the input NumPy array as an unsigned 8-bit integer (<strong class="source-inline">uint8</strong>):<p class="source-code">def batch_predict(input_raw, input_tensor, output_tensor, dictionary):</p><p class="source-code">    input_data = np.array(np.expand_dims(input_raw, 	</p><p class="source-code">                                 axis=0), dtype=np.uint8)</p><p class="source-code">    interpreter.set_tensor(input_tensor['index'], </p><p class="source-code">                                              input_data)</p><p class="source-code">    interpreter.invoke()</p><p class="source-code">    interpreter_output = interpreter.get_tensor(</p><p class="source-code">                                  output_tensor['index'])</p><p class="source-code">    plain_text_label = lookup(interpreter_output, </p><p class="source-code">                                               dictionary)</p><p class="source-code">    return plain_text_label</p><p>This concludes <a id="_idIndexMarker449"/>the <strong class="source-inline">batch_predict</strong> function. This function takes the <strong class="source-inline">input_raw</strong> array and scores it using our interpreter. The interpreter's output is then mapped to a plaintext label with the <strong class="source-inline">lookup</strong> function. </p></li>&#13;
				<li>Let's now load the integer quantization model and set up the input and output tensors:<p class="source-code">interpreter = tf.lite.Interpreter(model_path=str(tgt))</p><p class="source-code">interpreter.allocate_tensors()</p><p class="source-code"># Get input and output tensors.</p><p class="source-code">input_details = interpreter.get_input_details()[0]</p><p class="source-code">output_details = interpreter.get_output_details()[0]</p><p>In the preceding code, we initialized our quantization model and allocated memory for input tensors as per this model. The <strong class="source-inline">get_input_details</strong> and <strong class="source-inline">get_output_details</strong> methods will then retrieve these tensors' details, such as the name, shape, and data type.</p></li>&#13;
				<li>Then we may perform batched prediction:<p class="source-code">batch_quantized_prediction = []</p><p class="source-code">for i in range(sample_size):</p><p class="source-code">    plain_text_label = batch_predict(np_img_holder[i], </p><p class="source-code">            input_details, output_details, val_label_map)</p><p class="source-code">    batch_quantized_prediction.append(plain_text_label)</p><p>In the preceding code, we iterate through the test images, score them, and then store the results in a <a id="_idIndexMarker450"/>list defined as <strong class="source-inline">batch_quantized_prediction</strong>. </p></li>&#13;
				<li>We can calculate accuracy using <strong class="source-inline">accuracy_score</strong> from <strong class="source-inline">sklearn</strong>:<p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">accuracy_score(actual, batch_quantized_prediction)</p><p>The preceding function basically compares the <strong class="source-inline">actual</strong> list with the <strong class="source-inline">batch_quantized_prediciton</strong> list.</p><p>In this particular case, the accuracy is as follows:</p><p class="source-code">0.86</p><p class="callout-heading">Note</p><p class="callout">It's expected that your model accuracy will be slightly different from the nominal value printed here. Every time a base model is trained, the model accuracy will not be identical. However, it should not be too dissimilar to the nominal value. Another factor that impacts reproducibility in terms of model accuracy is the number of epochs used in training; in this case, only five epochs for demonstration and didactic purposes. More training epochs will give you a better and tighter variance in terms of model accuracy.</p></li>&#13;
			</ol>&#13;
			<p>This result may vary if you retrained the full model over again, but it shouldn't be too dissimilar to this value. Furthermore, based on my experience with this data, integer quantized model <a id="_idIndexMarker451"/>performance is on a par with that of the original full model. The preceding code shows that our TFLite model performed just as well as the original model. As we reduce the model size through quantization, we are still able to preserve the model's accuracy. In this example, the accuracy is not impacted just because the model is now more compact.</p>&#13;
			<h1 id="_idParaDest-131"><a id="_idTextAnchor234"/>Summary</h1>&#13;
			<p>In this chapter, we learned to optimize a trained model by making it smaller and therefore more compact. Therefore, we have more flexibility when it comes to deploying these models in various hardware or resource constrained conditions. Optimization is important for model deployment in a resource constrained environment such as edge devices with limited compute, memory, or power resources. We achieved model optimization by means of quantization, where we reduced the model footprint by altering the weight, biases, and activation levels' data type. </p>&#13;
			<p>We learned about three quantization strategies: reduced <strong class="source-inline">float16</strong>, hybrid quantization, and integer quantization. Of these three strategies, integer quantization currently requires an upgrade to TensorFlow 2.3.</p>&#13;
			<p>Choosing a quantization strategy depends on factors such as target compute, resource, model size limit, and model accuracy. Furthermore, you have to consider whether or not the target hardware requires integer ops only (in other words, TPU). If so, then integer quantization is the obvious choice. With all the examples, we learned that model accuracy is not impacted by model optimization strategies. After quantization, model size is a fraction of the original. This demonstrates the value of model optimization, especially when the deployment scenarios require efficient use of the compute and power resources. </p>&#13;
			<p>In the next chapter, we are going to take a closer look at some common practices in the model building process. This practice involves data ingestion pipeline design and how to avoid model overfitting.</p>&#13;
		</div>&#13;
	</div></body></html>