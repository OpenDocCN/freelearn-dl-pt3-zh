- en: Playing Pacman Using Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning refers to a paradigm where an agent learns from environment
    feedback by virtue of receiving observations and rewards in return for actions
    it takes. The following diagram captures the feedback-based learning loop of reinforcement
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/185ca50f-32f0-4e67-9d9b-200eb4c15771.png)'
  prefs: []
  type: TYPE_IMG
- en: Although mostly applied to learn how to play games, reinforcement learning has
    also been successfully applied in digital advertising, stock trading, self-driving
    cars, and industrial robots.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use reinforcement learning to create a PacMan game
    and learn about reinforcement learning in the process. We will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning  versus supervised and unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PacMan game in OpenAI Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DQN for deep reinforcement learning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q Learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q Network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying DQN to a PacMan game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a type of machine learning in which an agent learns
    from the environment. The agent takes actions and, as a result of the actions,
    the environment returns observations and rewards. From the observation and rewards,
    the agent learns the policy and takes further actions, thus continuing the sequence
    of actions, observations, and rewards. In the long run, the agent has to learn
    the policy such that, when it takes actions based on the policy, it does so in
    such a way as to maximize the long-term rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning versus supervised and unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning solutions can be of three major types: supervised learning,
    unsupervised learning, and reinforcement learning. So how is reinforcement learning
    different from the other two types?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: In supervised learning, the agent learns the model
    from a training dataset consisting of features and labels. The two most common
    types of supervised learning problems are regression and classification. Regression
    refers to predicting the future values based on the model, and classification
    refers to predicting the categories of the input values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: In unsupervised learning, the agent learns the model
    from a training dataset consisting of only features. The two most common types
    of unsupervised learning problems are dimensionality reduction and clustering.
    Dimensionality reduction refers to reducing the number of features or dimensions
    in a dataset without altering its natural distribution. Clustering refers to dividing
    the input data into multiple groups, thus producing clusters or segments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: In reinforcement learning, the agent starts with
    an initial model and then continuously learns the model based on feedback from
    the environment. An RL agent updates the model by applying supervised or unsupervised
    learning techniques on a sequence of actions, observations, and rewards. The agent
    only learns from a reward signal, not from a loss function as in other machine
    learning approaches. The agent receives the feedback after it has already taken
    the action, while, in other ML approaches, the feedback is provided at the time
    of training in terms of loss or error. The data is not i.i.d. (independent and
    identically distributed) because it depends on previous actions taken, while in
    other ML approaches data is i.i.d.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Components of Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In any RL formalization, we talk in terms of a **state space** and an **action
    space**. Action space is a set of finite numbers of actions that can be taken
    by the agent, represented by *A*. State space is a finite set of states that the
    environment can be in, represented by *S*.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the agent is to learn a policy, denoted by ![](img/2d80d9e2-ee59-47b1-90c6-5050457942b1.png).
    A **policy** can be **deterministic** or **stochastic**. A policy basically represents
    the model, using which the agent to  select the best action to take. Thus, the
    policy maps the rewards and observations received from the environment to actions.
  prefs: []
  type: TYPE_NORMAL
- en: When an agent follows a policy, it results in a sequence of state, action, reward,
    state, and so on. This sequence is known as a **trajectory** or an **episode**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important component of reinforcement learning formalizations is the **return**.
    The return is the estimate of the total long-term reward. Generally, the return
    can be represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d82b310-321c-4ff0-82b6-6ca9742f366b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](img/77a691d7-b626-4fd0-98fc-5acebc7745d2.png) is a discount factor
    with values between (0,1), and ![](img/48e152e1-7cdc-4d4d-9d55-68e496294f47.png) is
    the reward at time step *t*. The discount factor represents how much importance
    should be given to the reward at later time steps. If ![](img/5187e85e-81f4-4c40-81a5-a70743fec3cd.png) is
    0 then only the rewards from the next action are considered, and if it is 1 then
    the future rewards have the same weight as the rewards from the next action.
  prefs: []
  type: TYPE_NORMAL
- en: However, since it is difficult to compute the value of the return, hence it
    is estimated with **state-value** or **action-value** functions. We shall talk
    about action-value functions further in the q-learning section in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For simulating our agent which will play the PacMan game, we shall be using
    the OpenAI Gym. Let's learn about OpenAI Gym now.
  prefs: []
  type: TYPE_NORMAL
- en: You can follow along with the code in the Jupyter Notebook  `ch-14_Reinforcement_Learning` in
    the code bundle of this book.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym is a Python-based toolkit for the development of reinforcement learning
    algorithms. It provides more than 700 open source contributed environments at
    the time of writing this book. Custom environments for OpenAI can also be created.
    OpenAI Gym provides a unified interface for working with reinforcement learning
    environments and takes care of running the simulation, while the user of OpenAI
    can focus on designing and implementing the reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The original research paper on OpenAI Gym is available at the following link: [http://arxiv.org/abs/1606.01540](http://arxiv.org/abs/1606.01540).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following steps to learn how to install and explore
    OpenAI Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install OpenAI Gym using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If the preceding command does not work, then refer to the following link for
    further help with installation: [https://github.com/openai/gym#installation](https://github.com/openai/gym#installation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the number of available environments in the OpenAI Gym with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the list of all environments, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The partial list from the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Each environment, represented by the `env` object, has a standardized interface:'
  prefs: []
  type: TYPE_NORMAL
- en: An `env` object can be created with the `env.make(<game-id-string>)` function
    by passing the `id` string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each `env` object contains the following main functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `step()` function takes an action object as an argument and returns four
    objects:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`observation`: An object implemented by the environment, representing the observation
    of the environment.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: A signed float value indicating the gain (or loss) from the previous
    action.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done`: A Boolean value representing whether or not the scenario is finished.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`: A Python dictionary object representing the diagnostic information.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `render()` function creates a visual representation of the environment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `reset()` function resets the environment to the original state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each `env` object comes with well-defined actions and observations, represented
    by `action_space` and `observation_space`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Pacman game in OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the PacMan game as an example, known as **MsPacman-v0**.
    Let''s explore this game a bit further:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `env` object with the standard `make` function, as shown in the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the action space of the game with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`Discrete 9` refers to the nine actions, such as up, down, left, and right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now see the observation space, as shown in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the observation space has three color channels and is of size 210 x 160\.
    The observation space gets rendered as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5d91e17-360f-4dec-97be-4aad1eb1ad46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The number of episodes is the number of game plays. We shall set it to one,
    for now, indicating that we just want to play the game once. Since every episode
    is stochastic, in actual production runs you will run over several episodes and
    calculate the average values of the rewards. Let''s run the game for one episode
    while randomly selecting one of the actions during the gameplay with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run, the game `500` times and see what maximum, minimum, and average
    scores we get. This is demonstrated in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Randomly picking an action and applying it is probably not the best strategy.
    There are many algorithms for finding solutions to make the agent learn from playing
    the game and apply the best actions. In this chapter, we shall apply Deep Q Network
    for learning from the game. The reader is encouraged to explore other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: DQN for deep reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Deep Q Networks** (**DQN**) are based on Q-learning. In this section,
    we will explain both of them before we implement the DQN in Keras to play the
    PacMan game.
  prefs: []
  type: TYPE_NORMAL
- en: '**Q-learning**: In Q-learning, the agent learns the action-value function,
    also known as the Q-function. The *Q* function denoted with *q(s*,*a)* is used
    to estimate the long-term value of taking an action *a* when the agent is in state
    *s*. The *Q* function maps the state-action pairs to the estimates of long-term
    values, as shown in the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eab73493-acce-4360-8164-a86d11c95447.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, under a policy, the *q*-value function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28dd49bb-5f0b-43ea-96dc-6a94209695cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *q* function can be recursively written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3041f8dd-a123-4a77-bcf0-0206b13f832a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expectation can be expanded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c71a670c-3b18-47a3-8821-2c15ac30b20e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An optimal *q* function is the one that returns the maximum value, and an optimal
    policy is the one that applies the optimal *q* function. The optimal *q* function
    can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4c59cf4-9276-49c5-b00a-950c70652361.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation represents the **Bellman Optimality Equation**. Since directly
    solving this equation is difficult, Q-learning is one of the methods used to approximate
    the value of this function.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in Q-learning, a model is built that can predict this value, given the
    state and action. Generally, this model is in the form of a table that contains
    all the possible combinations of state *s* and action *a*, and the expected value
    from that combination. However, for situations with a large number of state-action
    combinations, this table becomes cumbersome to maintain. The DQN helps to overcome
    this shortcoming of table-based Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: The DQN: In DQN, instead of tables, a neural network model is built that learns
    from the state-action-reward-next state tuples and predicts the approximate q-value
    based on the state and action provided. Since the sequence of states-action-rewards
    is correlated in time, deep learning faces the challenge, since, in deep learning,
    the input samples need to be i.i.d. Thus, in DQN algorithms, **experience replay**
    is used to alleviate that. In the experience replay, the previous actions and
    their results are sampled randomly to train the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic Deep Q-learning algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the play in its initial state
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select to explore or exploit
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you selected exploit, then predict the action with the neural network and
    take the predicted action
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you selected explore, then randomly select an action
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Record the previous state, action, rewards, and next state in the experience
    buffer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the `q_values` using `bellman` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the neural network with `states`, `actions`, and `q_values`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from *step 2*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To improve the performance, and implement experience replay, one of the things
    you can do is to randomly select the training data in *step 7*.
  prefs: []
  type: TYPE_NORMAL
- en: Applying DQN to a game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have randomly picked an action and applied it to the game. Now, let's
    apply DQN for selecting actions for playing the PacMan game.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the `q_nn` policy function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we modify the `episode` function to incorporate calculation of `q_values`
    and `train` the neural network on the sampled experience buffer. This is shown
    in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an `ex``periment` function that will run for a specific number of episodes;
    each episode runs until the game is lost, namely when `done` is `True`. We use `rewards_max` to
    indicate when to break out of the loop as we do not wish to run the experiment
    forever, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a simple MLP network with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an empty list to contain the game memory and define other hyperparameters
    and run the experiment for one episode, as shown in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result we get is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: That is definitely an improvement in our case, but, in your case, it might be
    different. In this case, our game has only learned from a limited memory and only
    from game replay in one episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run it for `100` episodes, as shown in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus we see that, on average, the results did not improve, although we reached
    a high max reward. Tuning the network architecture, features, and hyperparameters
    might produce better results. We would encourage you to modify the code. As an
    example, instead of MLP, you can use the simple one-layer convolutional network,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the network summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what reinforcement learning is. Reinforcement learning
    is an advanced technique that you will find is often used to solve complex problems. We
    learned about OpenAI Gym, a framework that provides an environment for simulating
    many popular games in order to implement and practice reinforcement learning algorithms.
    We touched on deep reinforcement learning concepts, and we encourage you to explore
    books (mentioned in the further reading) specifically written about reinforcement
    learning to learn deeply about the theories and concepts.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to play the PacMan game in OpenAI Gym. We implemented DQN and
    used it to learn to play the PacMan game. We only used an MLP network to keep
    things simple, but, for complex examples, you may end up using complex CNN, RNN,
    or Sequence-to-Sequence models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall learn about future opportunities in the fields
    of machine learning and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning Hands-On by Maxim Lapan, Packt Publications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G.
    Barto'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Statistical Reinforcement Learning: Modern Machine Learning Approaches by Masashi
    Sugiyama'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for reinforcement learning by Csaba Szepesvari
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
