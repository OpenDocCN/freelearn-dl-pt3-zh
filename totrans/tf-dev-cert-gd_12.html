<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer194">
<h1 class="chapter-number" id="_idParaDest-209"><a id="_idTextAnchor291"/>12</h1>
<h1 id="_idParaDest-210"><a id="_idTextAnchor292"/>Introduction to Time Series, Sequences, and Predictions</h1>
<p><strong class="bold">Time series</strong> cut across <a id="_idIndexMarker678"/>various industries, sectors, and aspects of our lives. Finance, healthcare, social sciences, physics – you name it, time series data is there. It’s in sensors monitoring our environment, social media platforms tracking our digital footprint, online transactions recording our financial behavior, and many more avenues. This sequential data represents dynamic processes that evolve over time, and as we increasingly digitize our planet, the volume, and thereby the importance, of this data type is set to <span class="No-Break">grow exponentially.</span></p>
<p>Time series follow a chronological order, capturing events as they occur in time. This temporal nature of time series bestows a unique quality that differentiates it from cross-sectional data. When we turn on the searchlight on time series data, we can observe attributes such as trends, seasonality, noise, cyclicity, and autocorrelations. These unique characteristics endow time series data with rich information, but they also present us with a unique set of challenges that we must overcome to harness the gains inherent in this data type. With frameworks such as <strong class="bold">TensorFlow</strong>, we can <a id="_idIndexMarker679"/>leverage patterns from the past to make informed decisions about <span class="No-Break">the future.</span></p>
<p>In this chapter, we will be covering the <span class="No-Break">following topics:</span></p>
<ul>
<li>Time series analysis – characteristics, applications, and <span class="No-Break">forecasting techniques</span></li>
<li>Statistical techniques for forecasting <span class="No-Break">time series</span></li>
<li>Preparing data for forecasting with <span class="No-Break">neural networks</span></li>
<li>Sales forecasting with <span class="No-Break">neural networks</span></li>
</ul>
<p>By the end of this chapter, you will have gained theoretical insight and hands-on experience in building, training, and evaluating time series forecasting models using statistical and deep <span class="No-Break">learning techniques.</span></p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor293"/>Time series analysis – characteristics, applications, and forecasting techniques</h1>
<p>We know time<a id="_idIndexMarker680"/> series data<a id="_idIndexMarker681"/> is defined by <a id="_idIndexMarker682"/>the ordering of data points in a sequence over time. Imagine we are forecasting energy consumption patterns in London. Over the years, there has been a growing increase in energy consumption, perhaps due to urbanization – this signifies a positive upward trend. During winter each year, we expect energy consumption to rise as more people will need to heat up their homes and offices to stay warm. This seasonal change in the weather also accounts for seasonality in energy utilization. Again, we could also witness an unusual surge in energy consumption due to a major sporting event, leading to a large influx of guests during the period. This causes noise in the data as such events are one-offs or occur at <span class="No-Break">irregular intervals.</span></p>
<p>In the following sections, let us explore the characteristics of time series, types, applications, and techniques for modeling time <span class="No-Break">series data.</span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor294"/>Characteristics of time series</h2>
<p>To effectively build <a id="_idIndexMarker683"/>efficient forecasting models, we need to gain a clear understanding of the underlying nature of time series data. We may find ourselves working with time series data that has a positive upward trend, monthly seasonality, noise, and autocorrelation, while the next time series we work on may have yearly seasonality and noise but no visible sign of autocorrelation or trends in <span class="No-Break">the data.</span></p>
<p>Understanding these data properties of time series data arms us with the requisite details to make informed preprocessing decisions. For example, if we are working with a dataset with high volatility, our knowledge of this may inform our decision to apply smoothing during our preprocessing steps. Later in this chapter, where we will be building statistical and deep learning models for forecasting time series, we will see how our understanding of the properties of time series data will guide our decisions with respect to engineering new features, choosing optimal hyperparameter values, and making model selection decisions. Let us examine the characteristics of <span class="No-Break">time se<a id="_idTextAnchor295"/>ries:</span></p>
<ul>
<li><strong class="bold">Trend</strong>: Trends refer to the general direction in which a time series is moving over the long term. We could look at a trend as the overall big picture of our time series data. Trends can be linear, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em>, or nonlinear (quadratic and exponential); they can also be in a positive (upward) or negative (<span class="No-Break">downward) direction:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Figure 12.1 – A plot showing a positive stock price trend over time" height="525" src="image/B18118_12_001.jpg" width="879"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – A plot showing a positive stock price trend over time</p>
<p class="list-inset">Trend analysis <a id="_idIndexMarker684"/>empowers data professionals, businesses, and policymakers to make informed decisions about <span class="No-Break">the future.</span></p>
<ul>
<li><strong class="bold">Seasonality</strong>: Seasonality refers to repetitive cycles occurring at regular intervals over a specific period, such as on a daily, weekly, monthly, or yearly basis. These variations are often byproducts of seasonal fluctuations; for example, a retail store in a residential area might get higher sales on weekends compared to weekdays (weekly seasonality). The same store could also witness a surge in sales during the holiday season in December and a drop in sales shortly after the festive period (annual seasonality), as illustrated by <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="Figure 12.2 – A plot showing annual seasonality for a retail store" height="404" src="image/B18118_12_002.jpg" width="1017"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – A plot showing annual seasonality for a retail s<a id="_idTextAnchor296"/>tore</p>
<ul>
<li><strong class="bold">Cyclicality</strong>: Cyclicality refers to irregular cycles that occur in a time series over a long period of time. Unlike seasonality, these cycles are long-term in nature and their duration and magnitude are irregular, making them harder to predict when compared to seasonality. Economic cycles are a good example of cyclicity. These cycles are influenced by several factors such as inflation, interest rates, and government policies. Due to its irregular nature, predicting the timing, duration, and magnitude of cycles can be quite challenging. Advanced statistical and machine learning models are often required to model and forecast cyclic <span class="No-Break">patterns accura<a id="_idTextAnchor297"/>tely.</span></li>
<li><strong class="bold">Autocorrelation</strong>: Autocorrelation is a statistical concept that refers to the correlation between a time series and the lagged version of itself, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 12.3 – A plot showing autocorrelation" height="535" src="image/B18118_12_003.jpg" width="766"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – A plot showing autocorrelation</p>
<p class="list-inset">It is often referred to as serial correlation and measures the degree to which a data point is related to its past values. Autocorrelations can be positive or negative, with values ranging from -1 <span class="No-Break">t<a id="_idTextAnchor298"/>o 1.</span></p>
<ul>
<li><strong class="bold">Noise</strong>: Noise is an inherent part of any real-world data. It refers to the random fluctuations in data that cannot be explained by the model, nor can it be explained by any known underlying factors, patterns, or structural influences. These fluctuations can result from various sources such as measurement errors or unexpected events. For example, in the financial markets, an unpredicted event such as a political announcement can create noise that deviates stock prices from their underlying trends. Noise displays randomness and unexplained variations that we may have to smooth out when forecasting time <span class="No-Break">series data.</span></li>
</ul>
<p>We have<a id="_idIndexMarker685"/> discussed some important characteristics that can occur individually or together in time series data. Next, let us look at the types of time <span class="No-Break">series data.</span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor299"/>Types of time series data</h2>
<p>Time series <a id="_idIndexMarker686"/>can be classified as <span class="No-Break">the following:</span></p>
<ul>
<li>Stationary <span class="No-Break">and non-stationary</span></li>
<li>Univariate <span class="No-Break">and multivariate</span></li>
</ul>
<h3>Stationary and non-stationary time series</h3>
<p>A stationary<a id="_idIndexMarker687"/> time <a id="_idIndexMarker688"/>series is a time series whose statistical properties (mean, variance, and autocorrelation) remain constant over time. It is a series that displays recurring patterns and behaviors that are likely to replicate themselves in the future. A non-stationary time series is the opposite. It is not stationary, and we typically find these types of time series in many real-world scenarios where the series may display trends or seasonality. For example, we will expect the monthly sales of a ski resort to reach their peak during winter and dip during off-seasons. This seasonal component has an impact on the statistical properties of <span class="No-Break">the series.</span></p>
<h3>Univariate and multivariate time series</h3>
<p>A univariate time<a id="_idIndexMarker689"/> series is a type of time series where<a id="_idIndexMarker690"/> we track just one metric over time. For example, we could use a smartwatch to track the number of steps we take on a daily basis. On the other hand, when we track more than one metric over time, we have a multivariate time series, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.4</em>. In the chart, we see the interaction between inflation and wage growth in the UK. Over time, we see that inflation persistently outpaces wage growth, leaving everyday people with lowered real income and <span class="No-Break">purchasing power:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 12.4 – A multivariate time series showing the relationship between inflation and wage growth" height="814" src="image/B18118_12_004.jpg" width="1217"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – A multivariate time series showing the relationship between inflation and wage growth</p>
<p>Multivariate time series analysis lets us account for the dependencies and interactions between<a id="_idIndexMarker691"/> several<a id="_idIndexMarker692"/> variables over time. Next, let us delve into the importance of time series data and various applications of <span class="No-Break">ti<a id="_idTextAnchor300"/>me series.</span></p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor301"/>Applications of time series</h2>
<p>We have discussed the <a id="_idIndexMarker693"/>types and properties of time series data. By applying machine learning techniques, we can leverage the wealth of information in these data types. Let us examine some of the important applications of machine learning in <span class="No-Break">time series:</span></p>
<ul>
<li><strong class="bold">Forecasting</strong>: We can apply <a id="_idIndexMarker694"/>machine<a id="_idIndexMarker695"/> learning models to forecast time series; for instance, we may want to forecast the future sales of a retail store to inform our inventory decisions. If we analyze the sales record of the business, we may find some patterns, such as increased sales during holiday seasons or lowered sales during specific months. We can train our models with these patterns to make informed predictions about the future sales of the store, allowing key stakeholders to effectively plan for the <span class="No-Break">expected demand.</span></li>
<li><strong class="bold">Imputed data</strong>: Missing values can pose a significant challenge when we are working on analyzing or forecasting time series data. A good solution is the application of imputation, allowing us to fill the missing data points with substitute values. For example, in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.5 (a)</em>, we see a plot of temperature values over a year. We quickly notice that some temperature recordings are missing. With the aid of imputation, we can estimate those values using adjacent<a id="_idIndexMarker696"/> days, as<a id="_idIndexMarker697"/> shown in <span class="No-Break"><em class="italic">Figure </em></span><span class="No-Break"><em class="italic">12</em></span><span class="No-Break"><em class="italic">.5 (b)</em></span><span class="No-Break">:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 12.5 – A plot displaying temperature over time (a) with missing values (b) with no missing values" height="454" src="image/B18118_12_005.jpg" width="1118"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – A plot displaying temperature over time (a) with missing values (b) with no missing values</p>
<p class="list-inset">By filling in the gaps, we now have a complete dataset that can be better utilized for analysis <span class="No-Break">and prediction.</span></p>
<ul>
<li><strong class="bold">Anomaly detection</strong>: Anomalies are data points that deviate significantly from the general norm. We can apply time series analysis to detect anomalies and potentially identify significant issues. For example, in credit card transactions, as illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.6</em>, an unusually large transaction might indicate <span class="No-Break">fraudulent activity:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 12.6 – A plot showing spikes in transaction values" height="514" src="image/B18118_12_006.jpg" width="811"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – A plot showing spikes in transaction values</p>
<p class="list-inset">By using time series analysis to identify such anomalies, the bank can swiftly take action to mitigate <span class="No-Break">potential damages.</span></p>
<ul>
<li><strong class="bold">Trend analysis</strong>: Understanding<a id="_idIndexMarker698"/> trends can <a id="_idIndexMarker699"/>provide valuable insights into the underlying phenomena. For example, the international energy agencies show that 14% of all new cars sold in 2022 were electric vehicles. This trend could indicate that people are moving toward a more sustainable option of transportation (<span class="No-Break">see </span><a href="https://www.iea.org/reports/global-ev-outlook-2023/executive-summary"><span class="No-Break">https://www.iea.org/reports/global-ev-outlook-2023/executive-summary</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Seasonality analysis</strong>: Another useful application of time series is in seasonality analysis. This could prove useful in guiding energy consumption planning and infrastructural <span class="No-Break">expansion needs.</span></li>
</ul>
<p>We have now looked at some important applications of time series data. Next, let us take a look at some important techniques for forecasting <span class="No-Break">time series.</span></p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor302"/>Techniques for forecasting time series</h2>
<p>In this book, we will<a id="_idIndexMarker700"/> examine two main techniques for forecasting time series: statistical methods and machine learning methods. Statistical methods use mathematical models to capture the trend, seasonality, and other <a id="_idIndexMarker701"/>components of the time series data, with popular models being <strong class="bold">autoregressive integrated moving average</strong> (<strong class="bold">ARIMA</strong>) and <strong class="bold">seasonal and trend decomposition using LOESS</strong> (<strong class="bold">STL</strong>). However, these<a id="_idIndexMarker702"/> methods are beyond the scope of this book. Here, we will be using simpler statistical <a id="_idIndexMarker703"/>methods<a id="_idIndexMarker704"/> such as <strong class="bold">naïve forecasting</strong> and <strong class="bold">moving averages</strong> to establish our baseline, after which we will apply different machine learning<a id="_idIndexMarker705"/> methods. In this chapter, we <a id="_idIndexMarker706"/>will focus on using <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>), and in the <a id="_idIndexMarker707"/>next chapter, we will apply <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) and <strong class="bold">long short-term memory </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LSTMs</strong></span><span class="No-Break">).</span></p>
<p>Each approach has its pros and cons, and the best approach to forecasting time series is largely dependent on the specific characteristics of the data and the problem at hand. It is important thatwe highlight here that time series forecasting is a broad field, and there are other methods that fall outside the scope of this book that you may explore at a later stage. Before we move into modeling time series problems, let us examine how we can evaluate this type <span class="No-Break">of data.</span></p>
<h2 id="_idParaDest-216">Evaluating time series <a id="_idTextAnchor303"/>forecasting techniques</h2>
<p>To effectively <a id="_idIndexMarker708"/>evaluate a time series forecasting model, we must gauge its performance with appropriate metrics. In <a href="B18118_03.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Linear Regression with TensorFlow,</em> we explored several regression metrics such as MAE, MSE, RMSE, and MAPE. We can apply these metrics to evaluate time series forecasting models. However, in this chapter, we will concentrate on the application of MAE and MSE in line with the exam requirements. We use MAE to compute the average of the absolute difference between the predicted and true values. This way, we have a sense of how wrong our predictions are. A smaller MAE indicates a better model fit. Imagine you are a stock market analyst trying to forecast the future price of a specific stock. Using MAE as your evaluation metric, you would get a clear understanding of how much, on average, your forecasts differ from the actual stock prices. This information can help you refine your model to make more accurate predictions, minimizing potential <span class="No-Break">financial risks.</span></p>
<p>On the other hand, MSE takes the average of squared discrepancies between predictions and actuals. By squaring the errors, MSE is more sensitive to large errors compared to MAE, making it useful where large discrepancies are unfavorable, such as when working with a power grid where precise load forecasting is of top priority. With this in mind, let’s <a id="_idIndexMarker709"/>now turn our attention to a sales use case and apply our learnings <a id="_idTextAnchor304"/>to forecast <span class="No-Break">future sales.</span></p>
<h1 id="_idParaDest-217"><a id="_idTextAnchor305"/>Retail store forecasting</h1>
<p>Imagine you are <a id="_idIndexMarker710"/>working as a machine learning engineer and your company just landed a new project. A rapidly growing superstore in Florida wants your help. They want to predict future reviews, as this will serve as a guide in planning the expansion of their stores to meet the expected demand. You have been saddled with the responsibility of building a forecasting model with the available historical data provided by the Tensor superstore. Let’s jump in and see how you can solve this problem, as your company is counting on you. Let’s <span class="No-Break">get started!</span></p>
<ol>
<li>We begin by importing the necessary libraries for <span class="No-Break">our project:</span><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><p class="list-inset">Here, we import <strong class="source-inline">numpy</strong> and <strong class="source-inline">matplotlib</strong> for numerical analysis and visualization purposes and <strong class="source-inline">pandas</strong> for <span class="No-Break">data transformations.</span></p></li>
<li>Next, we load the time <span class="No-Break">series data:</span><pre class="source-code">
df = pd.read_csv('/content/sales_data.csv')</pre><pre class="source-code">
df.head()</pre><p class="list-inset">Here, we load the data and use the <strong class="source-inline">head</strong> function to get a snapshot of the first five rows of the data. When we run the code, we see that the first day of sales from the data given to us is <strong class="source-inline">2013-01-01</strong>. Next, let us look at some statistics to get a sense of the data we have <span class="No-Break">in hand.</span></p></li>
<li>Use the following code to check the data type and <span class="No-Break">summary statistics:</span><pre class="source-code">
# Check data types</pre><pre class="source-code">
print(df.dtypes)</pre><pre class="source-code">
# Summary stati<a id="_idTextAnchor306"/>stics</pre><pre class="source-code">
print(df.describe())</pre><p class="list-inset">When we run the code, it returns the data type as <strong class="source-inline">float64</strong> and key summary statistics for our sales data. Using the <strong class="source-inline">describe</strong> function, we get a count of 3,653 data points. This points to daily data over a 10-year period. We also see the mean sales per day come to around $75, giving us a sense of the central tendency. We see decent variability in the daily sales with a standard deviation of <strong class="source-inline">20.2</strong>. The minimum and maximum values reveal a range from $22 to $128 in sales, signaling some significant fluctuations occurring. The 25th and 75th percentiles are <strong class="source-inline">60.27</strong> and <strong class="source-inline">89.18</strong>, respectively, showing that lower-volume days see sales ofaround $60 while higher-volume days see around $90. Let us continue to explore the<a id="_idIndexMarker711"/> data by looking at it on <span class="No-Break">a plot.</span></p></li>
<li>Let’s visualize <span class="No-Break">our data:</span><pre class="source-code">
#Sales data plot</pre><pre class="source-code">
df.set_index('Date').plot()</pre><pre class="source-code">
plt.ylabel('Sales')</pre><pre class="source-code">
plt.title('Sales Over Time')</pre><pre class="source-code">
plt.xticks(rotation=90)</pre><pre class="source-code">
plt.show()</pre><p class="list-inset">The code returns the plot shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.7</em>, which represents the company’s sales over a <span class="No-Break">10-year period:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="Figure 12.7 – A plot showing sales over time" height="508" src="image/B18118_12_007.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – A plot showing sales over time</p>
<p>From the plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.7</em>, we can observe an overall positive upward trend, potentially indicative of economic growth or successful business strategies, such as new product releases and effective marketing. A clear yearly seasonality also emerges; this may suggest that the company deals in seasonal goods with annual demand fluctuations. Also, we observe some noise exists. This could be a result of weather variability, random events, or the entry of competitors. The upward trend in the data demonstrates promising performance and growth in sales over time. However, the seasonal effects <a id="_idIndexMarker712"/>and noise elements showcase complex dynamics underneath the aggregate trend. Next, let’s explore how to data partition <a id="_idTextAnchor307"/><span class="No-Break">the data.</span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor308"/>Data partitioning</h2>
<p>In time series <a id="_idIndexMarker713"/>forecasting, we typically divide our dataset into <a id="_idIndexMarker714"/>distinct sections: a training period for training our machine learning models, a validation period for model tuning and evaluation, and a test period for assessing performance on unseen data. This process is known as fixed partitioning. An alternate method is roll-forward partitioning, which we will be <span class="No-Break">discussing shortly.</span></p>
<p>Our sales data demonstrates seasonality; hence, we have to split the data in such a way that each partition captures entire seasonal cycles. We do this to ensure we do not omit important seasonal patterns in one or more of our partitions. While this method diverges from typical data partitioning, we see that when working with other machine learning problems where random samples are taken to form training, validation, and testing sets, the fundamental purpose remains the same. We train our model on the training data, fine-tune it using the validation data, and evaluate it on the test data. We can then incorporate the validation data into the training data to benefit from the most recent information and forecast <span class="No-Break">future data.</span></p>
<p>It is generally a good idea to ensure that your training set is large enough to capture all relevant patterns in the data, including any seasonal behavior of the data. When setting the size of your validation set, you must strike a balance. While a larger validation set gives a more reliable estimate of model performance, it also reduces the size of the training set. You should also remember to retrain your final model on the entire dataset (combining the training and validation sets) before making final predictions. This strategy maximizes the amount of data the model learns from, likely improving its predictive performance on future unseen data. Again, avoid shuffling the data before splitting, as it would disrupt the temporal order, leading to misleading results. In fixed partitioning, we usually use a chronological split such that the training data should be from the earliest <a id="_idIndexMarker715"/>timestamps, followed sequentially by the validation set, and <a id="_idIndexMarker716"/>finally, the test set containing the latest timestamps. Let’s split our sales data into training and <span class="No-Break">validation sets:</span></p>
<pre class="source-code">
# Split data into training and validation sets
split_time = int(len(df) * 0.8)
train_df = df.iloc[:split_time]
valid_df = df.iloc[split_time:]</pre>
<p>Here, we split the data into training and validation sets. We take 80% of the data (<strong class="source-inline">len(df) * 0.8</strong>), which, in this case, is 8 years of data for training and the last 2 years for validation. We use the <strong class="source-inline">int</strong> function to ensure that the split time is an integer for indexing purposes. We set up our training and validation sets, using everything before the split time for training and everything after the split time <span class="No-Break">for validation.</span></p>
<p>Next, let us plot our training and <span class="No-Break">validation data:</span></p>
<pre class="source-code">
plt.figure(figsize=(12, 9))
# Plotting the training data in green
plt.plot(train_df['Date'], train_df['Sales'], 'green',
    label = 'Training Data')
plt.plot(valid_df['Date'], valid_df['Sales'], 'blue',
    label = 'Validation Data')
plt.title('Fixed Partitioning')
plt.xlabel('Date')
plt.ylabel('Sales')
all_dates = np.concatenate([train_df['Date'],
    valid_df['Date']])
plt.xticks(all_dates[::180], rotation=90)
plt.legend()
plt.tight_layout()
plt.show()</pre>
<p>This code displays <a id="_idIndexMarker717"/>the <a id="_idIndexMarker718"/>partitioning of sales data into a training set and a validation set, marking them in green and blue colors respectively, with dates along the <em class="italic">x</em> axis and sales along the <em class="italic">y</em> axis. For readability, we set our <em class="italic">x</em> ticks marks to every <span class="No-Break">180 days:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 12.8 – A plot showing fixed partitioning" height="539" src="image/B18118_12_008.jpg" width="844"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – A plot showing fixed partitioning</p>
<p>In <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.8</em>, we split our sales data into 8 years for training and 2 years for validation. In this scenario, our test set will be data from the future. This is done to ensure that the model is trained on the earliest part of our series, validated on the recent past, and tested on the future. Another method of partitioning time series data is called roll-forward partitioning, or “walk-forward” validation. In this method, we start with a short training period and gradually increase it. For each training period, the following <a id="_idIndexMarker719"/>period is <a id="_idIndexMarker720"/>used as the validation set. This mirrors a real-life situation where we continually retrain our model as new data comes in and use it to predict the next period. Let’s discuss our first method of forecasting, called <span class="No-Break">naïve forecasting.</span></p>
<h2 id="_idParaDest-219"><a id="_idTextAnchor309"/>Naïve forecasting</h2>
<p>Naïve forecasting<a id="_idIndexMarker721"/> is <a id="_idIndexMarker722"/>one of the simplest methods for forecasting in time series analysis. The principle behind naïve forecasting is to simply set all forecasts to be the value of the last observed point. This is why it’s referred to as “naïve.” It is a method that assumes that the future value is likely to be the same as the current one. Despite its simplicity, naïve forecasting can often serve as a good baseline for time series forecasting; however, its performance can vary depending on the characteristics of the <span class="No-Break">time series.</span></p>
<p>Let us<a id="_idTextAnchor310"/> see how to implement this <span class="No-Break">in code:</span></p>
<ol>
<li>Let’s implement <span class="No-Break">naïve forecasting:</span><pre class="source-code">
# Apply naive forecast</pre><pre class="source-code">
df['Naive_Forecast'] = df['Sales'].shift(1)</pre><pre class="source-code">
df.head()</pre><p class="list-inset">To implement the naïve forecasting method, each forecasted value is simply set to the actual observed value from the previous time step, achieved by shifting the <strong class="source-inline">Sales</strong> column by<a id="_idIndexMarker723"/> one unit. We use <strong class="source-inline">df.head()</strong> to display<a id="_idIndexMarker724"/> the first five rows of the DataFrame, providing a quick overview of the sales data and the <span class="No-Break">naïve forecast:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 12.9 – A snapshot of the DataFrame showing the sales and naïve forecasts" height="232" src="image/B18118_12_009.jpg" width="344"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – A snapshot of the DataFrame showing the sales and naïve forecasts</p>
<p class="list-inset">From the table in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.9</em>, we see that the first forecast is not available. This is because this method simply takes all the values in the series starting from one step before the validation data until the second-last value of the series. This effectively shifts the time series by one time step into <span class="No-Break">the future.</span></p>
<ol>
<li value="2">Let us create a function for <span class="No-Break">plotting purposes:</span><pre class="source-code">
def plot_forecast(validation_df, forecast_df,</pre><pre class="source-code">
    start_date=None, end_date=None,</pre><pre class="source-code">
    plot_title='Naive Forecasting',</pre><pre class="source-code">
    forecast_label='Naive Forecast'):</pre><pre class="source-code">
        if start_date:</pre><pre class="source-code">
            validation_df = validation_df[</pre><pre class="source-code">
                validation_df['Date'] &gt;= start_date]</pre><pre class="source-code">
            forecast_df = forecast_df[forecast_df[</pre><pre class="source-code">
                'Date'] &gt;= start_date]</pre><pre class="source-code">
        if end_date:</pre><pre class="source-code">
            validation_df = validation_df[</pre><pre class="source-code">
                validation_df['Date'] &lt;= end_date]</pre><pre class="source-code">
            forecast_df = forecast_df[forecast_df[</pre><pre class="source-code">
                'Date'] &lt;= end_date]</pre><pre class="source-code">
    # Extract the dates in the selected range</pre><pre class="source-code">
    all_dates = validation_df['Date']</pre><pre class="source-code">
    plt.figure(figsize=(12, 9))</pre><pre class="source-code">
    plt.plot(validation_df['Date'],</pre><pre class="source-code">
        validation_df['Sales'],</pre><pre class="source-code">
        label='Validation Data')</pre><pre class="source-code">
    plt.plot(forecast_df['Date'],</pre><pre class="source-code">
        forecast_df['Naive_Forecast'],</pre><pre class="source-code">
        label=forecast_label)</pre><pre class="source-code">
    plt.legend(loc='best')</pre><pre class="source-code">
    plt.title(plot_title)</pre><pre class="source-code">
    plt.xlabel('Date')</pre><pre class="source-code">
    plt.ylabel('Sales')</pre><pre class="source-code">
    # Set x-ticks to every 90th date in the selected range</pre><pre class="source-code">
    plt.xticks(all_dates[::90], rotation=90)</pre><pre class="source-code">
    plt.legend()</pre><pre class="source-code">
    plt.tight_layout()</pre><pre class="source-code">
    plt.show()</pre><p class="list-inset">We construct a utility plot to generate the graph of the true and predicted validation values. This<a id="_idIndexMarker725"/> function takes in the predicted and true <a id="_idIndexMarker726"/>values of our validation data along with the start date, end date, plot title, and plot label. This function gives us the flexibility to drill down into different areas of interest in <span class="No-Break">the plot.</span></p></li>
<li>Next, let’s plot the <span class="No-Break">naïve forecast:</span><pre class="source-code">
plot_forecast(valid_df, valid_df,</pre><pre class="source-code">
    plot_title='Naive Forecasting',</pre><pre class="source-code">
    forecast_label='Naive Forecast')</pre><p class="list-inset">We pass in the required parameters and run the code to generate the <span class="No-Break">following plot:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 12.10 – A time series forecast using the naïve method" height="889" src="image/B18118_12_010.jpg" width="1189"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – A time series forecast using the naïve method</p>
<p class="list-inset">The plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.10</em> displays the forecasted values and the true values for the <a id="_idIndexMarker727"/>validation<a id="_idIndexMarker728"/> data. Because the plot looks a bit clustered due to the closeness in values, let us zoom in to help us visually investigate how our forecasting <span class="No-Break">is doing.</span></p>
<ol>
<li value="4">Let’s look at a specific <span class="No-Break">time range:</span><pre class="source-code">
plot_forecast(valid_df, valid_df,</pre><pre class="source-code">
    start_date='2022-01-01', end_date='2022-06-30')</pre><p class="list-inset">We set the start date and end date parameters to <strong class="source-inline">2022-01-01</strong> and <strong class="source-inline">2022-06-30</strong>, respectively. The resulting plot is <span class="No-Break">as follows:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 12.11 – Zoomed-in time series forecasting using the naïve method" height="426" src="image/B18118_12_011.jpg" width="866"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Zoomed-in time series forecasting using the naïve method</p>
<p class="list-inset">From the plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.11</em>, we can see that it starts the forecast one step later because the naïve forecast is shifted one step into <span class="No-Break">the future.</span></p>
<ol>
<li value="5">Next, let <a id="_idIndexMarker729"/>us <a id="_idIndexMarker730"/>evaluate the performance of the <span class="No-Break">naïve method:</span><pre class="source-code">
# Compute and print mean squared error</pre><pre class="source-code">
mse = tf.keras.metrics.mean_squared_error(</pre><pre class="source-code">
    valid_df['Sales'], valid_df</pre><pre class="source-code">
        ['Naive_Forecast']).numpy()</pre><pre class="source-code">
print('Mean Squared Error:', mse)</pre><pre class="source-code">
# Compute and print mean absolute error</pre><pre class="source-code">
mae = tf.keras.metrics.mean_absolute_error(</pre><pre class="source-code">
    valid_df['Sales'],</pre><pre class="source-code">
    valid_df['Naive_Forecast']).numpy()</pre><pre class="source-code">
print('Mean Absolute Error:', mae)</pre><p class="list-inset">We use the <strong class="source-inline">metrics</strong> functions <a id="_idIndexMarker731"/>provided by the <strong class="bold">Keras</strong> API to calculate the MSE and MAE. The functions take in the true values and the forecasted values to generate the MSE and MAE values. We apply <strong class="source-inline">.numpy()</strong> to convert the result from a TensorFlow tensor to a NumPy array. When we run the code, we get an MSE for the naïve forecast as <strong class="source-inline">45.22</strong> and an MAE for the naïve forecast as <strong class="source-inline">5.43</strong>. Recall that for the MSE and MAE values, lower values are <span class="No-Break">always better.</span></p></li>
</ol>
<p>Let’s keep this in mind as we explore other forecasting techniques. Naïve forecasting can serve as a <a id="_idIndexMarker732"/>baseline to compare the performance of more<a id="_idIndexMarker733"/> complex models. Next, let’s examine anothe<a id="_idTextAnchor311"/>r statistical method, called <span class="No-Break">moving average.</span></p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor312"/>Moving average</h2>
<p>Moving <a id="_idIndexMarker734"/>average is a<a id="_idIndexMarker735"/> technique for smoothing time series data by replacing each point with an average of the neighboring points. In this process, we generate a new series in which the data points are averages of the raw data in our original series. The key parameter in this method <a id="_idIndexMarker736"/>is the <strong class="bold">window width</strong>; this determines the number of consecutive raw data points included in the <span class="No-Break">average calculation.</span></p>
<p>The term “moving” refers to the sliding of the window along the time series to compute average values, thereby generating a <span class="No-Break">new series.</span></p>
<h3>Types of moving averages</h3>
<p>Two main types of <a id="_idIndexMarker737"/>moving averages are commonly used in time series analysis – the centered moving average and the trailing <span class="No-Break">moving average:</span></p>
<ul>
<li><strong class="bold">Centered moving average</strong>: A centered moving average calculates the average around a central point (<em class="italic">t</em>). It uses data from both before and after the time of interest for visualization, as illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.12</em>. Centered moving averages can give a well-balanced view of data trends, but since they require future data, they’re not suitable for forecasting as we do not have access to future values when making forecasts. Centered moving averages are good for visualization and time <span class="No-Break">series analysis.</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 12.12 – A plot showing acentered moving average" height="435" src="image/B18118_12_012.jpg" width="733"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – A plot showing acentered moving average</p>
<p class="list-inset">Centered moving averages can give a well-balanced view of data trends, but since they require future data, they’re not suitable for forecasting as we do not have access to future values when making forecasts. Centered moving averages are good for visualization and time <span class="No-Break">series analysis.</span></p>
<ul>
<li><strong class="bold">Trailing moving average</strong>: A trailing moving average, also known as a rolling or running average, calculates <a id="_idIndexMarker738"/>the average using the most recent <em class="italic">n</em> data points. This method solely requires past data points, as illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.13</em>, making it ideal <span class="No-Break">for forecasting.</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="Figure 12.13 – A plot showing atrailing moving average" height="428" src="image/B18118_12_013.jpg" width="727"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – A plot showing atrailing moving average</p>
<p class="list-inset">To compute a trailing moving average, the first step is choosing the window width (<em class="italic">W</em>). This selection can depend on various factors such as the series’ patterns and how much smoothing you want to achieve. A smaller window width will track quick changes closely, but this will happen at the risk of including more noise. On the other hand, a larger window width will provide a smoother line but might miss out on some <span class="No-Break">short-term fluctuations.</span></p>
<p>Let’s see how to implement <span class="No-Break">moving averages:</span></p>
<pre class="source-code">
# Calculate moving average over a 30-day window
window =30
df['Moving_Average_Forecast'] = df['Sales'].rolling(
    window=window).mean().shift(1)</pre>
<p>The code uses pandas’ <strong class="source-inline">rolling</strong> function to calculate the moving average of the <strong class="source-inline">Sales</strong> data over a 30-day<a id="_idIndexMarker739"/> window, then shifts the outcome one step forward to simulate a forecast for the subsequent time step, storing the result in a new <strong class="source-inline">Moving_Average_Forecast</strong> column. You can think of it as using a window of 30 days of sales to predict the sales on the <span class="No-Break">31st day.</span></p>
<p>We call the <strong class="source-inline">plot_forecast</strong> function to plot both the validation data and the moving average forecast data. We can see the resulting plot in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="Figure 12.14 – A time series forecast using the moving average method" height="680" src="image/B18118_12_014.jpg" width="1378"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.14 – A time series forecast using the moving average method</p>
<p>In the plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.14</em>, the moving average forecast is computed using the past 30 days of sales data. A smaller window size, such as a 7-day window, will follow the actual sales more closely than the 30-day moving average, but it might also capture more of the noise in <span class="No-Break">the data.</span></p>
<p>The next logical step will be to evaluate our model by using the <strong class="source-inline">metric</strong> function again. This time, we pass in the<a id="_idIndexMarker740"/> moving average forecast against the true <span class="No-Break">validation values:</span></p>
<pre class="source-code">
# Compute and print mean squared error
mse = tf.keras.metrics.mean_squared_error(
    valid_df['Sales'],
    df.loc[valid_df.index,
    'Moving_Average_Forecast']).numpy()
print('Mean Squared Error:', mse)
# Compute and print mean absolute error
mae = tf.keras.metrics.mean_absolute_error(
    valid_df['Sales'],
    df.loc[valid_df.index,
    'Moving_Average_Forecast']).numpy()
print('Mean Absolute Error:', mae)
print('Mean Squared Error for moving average forecast:',
    mse)</pre>
<p>When we run the code, we achieve an MSE of <strong class="source-inline">55.55</strong> and an MAE of <strong class="source-inline">6.05</strong> for the moving average forecast. The MAE and MSE are much worse than our baseline. If you change the window size to 7 days, we end up with a much lower MSE and MAE of <strong class="source-inline">48.57</strong> and <strong class="source-inline">5.61,</strong> respectively. This is a much better result, but slightly worse than our naïve approach. You could experiment with a smaller window size and see whether your results will surpass <span class="No-Break">the baseline.</span></p>
<p>However, we need to note that the underlying assumption when using moving average is stationarity. And we know that our time series has both trend and seasonality, so how do we achieve stationarity with this data? And will this help us to achieve a much lower MAE? To achieve<a id="_idIndexMarker741"/> stationarity, we use a concept called <strong class="bold">differencing</strong>. Let us discuss differencing next, and see how to apply it and whether it will help us achieve a much lower MAE <span class="No-Break">and MSE.</span></p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor313"/>Differencing</h2>
<p>Differencing is a<a id="_idIndexMarker742"/> method used to achieve stationarity in our time series. It works by<a id="_idIndexMarker743"/> calculating the difference between consecutive observations. The logic here is that, though the original series may have a trend and is non-stationary, the difference between the values of the series can be stationary. By differencing the data, we can remove the trend and seasonality, making the series stationary and thus suitable for a moving average forecast model. This can significantly improve the accuracy of the model and, therefore, the reliability of the forecast. Let us see this in the <span class="No-Break">following code:</span></p>
<pre class="source-code">
# Perform seasonal differencing
df['Differenced_Sales'] = df['Sales'].diff(365)
# Plotting the differenced sales data
plt.figure(figsize=(12, 9))
# Plotting the differenced data
plt.plot(df['Date'], df['Differenced_Sales'],
    label = 'Differenced Sales')
plt.title('Seasonally Differenced Sales Data')
plt.xlabel('Date')
plt.ylabel('Differenced Sales')
# Select dates to be displayed on x-axis
all_dates = df['Date']
plt.xticks(all_dates[::90], rotation=90)
plt.legend()
plt.tight_layout()
plt.show()</pre>
<p>The preceding code block applies differencing to our time series data. We begin by generating a new series where each value is different between a value and the value 365 days earlier. We <a id="_idIndexMarker744"/>do this because we know our data has yearly <a id="_idIndexMarker745"/>seasonality. Next, we plot <span class="No-Break">our data:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<img alt="Figure 12.15 – A plot showing the sales time series after differencing" height="533" src="image/B18118_12_015.jpg" width="843"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.15 – A plot showing the sales time series after differencing</p>
<p>We can see that the plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.15</em> has no trend or seasonality. Hence, we have achieved the stationarity required in the underlying assumption when using <span class="No-Break">moving averages.</span></p>
<p>Let’s now restore the trend <span class="No-Break">and seasonality:</span></p>
<pre class="source-code">
window=7
# Restore trend and seasonality
df['Restored_Sales'] = df['Sales'].shift(
    365) + df['Differenced_Sales']
# Compute moving average on the restored data
df['Restored_Moving_Average_Forecast'] = df[
    'Restored_Sales'].rolling(
    window=window).mean().shift(1)
# Split into training and validation again
train_df = df.iloc[:split_time]
valid_df = df.iloc[split_time:]
# Get forecast and true values on validation set
forecast = valid_df['Restored_Moving_Average_Forecast']
true_values = valid_df['Sales']</pre>
<p>Here, we <a id="_idIndexMarker746"/>incorporate <a id="_idIndexMarker747"/>seasonality back into our time series data after it has been differenced, then apply moving average forecasting on this restored data. We use a 7-day window for the moving average computation. After restoring the trend and seasonality by adding the shifted sales data to the differenced sales data, we compute a moving average of these restored sales over our chosen window size, and then shift the resulting series one step ahead for forecasting. We then split the data again into training and validation sets. The same split time is used to ensure consistency with the earlier split. This is crucial to ensure we are evaluating our model correctly. Finally, we prepare our forecasted and true sales values for evaluation by extracting the <strong class="source-inline">Restored_Moving_Average_Forecast</strong> and <strong class="source-inline">Sales</strong> values from the <span class="No-Break">validation set:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="Figure 12.16 – A sales forecast after restoring seasonality and trend" height="877" src="image/B18118_12_016.jpg" width="1183"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.16 – A sales forecast after restoring seasonality and trend</p>
<p>In the plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.16</em>, the orange line represents the forecasts after we’ve added the past values back in to restore the trend and seasonality. Recall that we are using a window size of <strong class="source-inline">7</strong> with which we achieved lower MAE and MSE values. Now, we have essentially used the forecasts from the differenced series to forecast the changes from one year to the next and then added these changes onto the values from a year ago to get our <span class="No-Break">final forecasts.</span></p>
<p>When we compute the MSE for forecasting with restored seasonality and trend, we get <strong class="source-inline">48.57</strong>, and for theMAE for forecast with restored seasonality and trend, we get <strong class="source-inline">5.61</strong>, both of which are <a id="_idIndexMarker748"/>significantly<a id="_idIndexMarker749"/> lower than without using differencing. We can also try to smooth out the noise in the data to improve our MAE and MSE scores. Next, let us see how we can perform forecasting with machine learning, and in particular, with neural networks <span class="No-Break">with TensorFlow.</span></p>
<h1 id="_idParaDest-222"><a id="_idTextAnchor314"/>Time series forecasting with machine learning</h1>
<p>So far, we have<a id="_idIndexMarker750"/> examined statistical methods<a id="_idIndexMarker751"/> with reasonable success. Now, we will proceed with modeling time series data using deep learning techniques. We will begin with mastering how to set up a window dataset. We will also cover ideas such as shuffling and batching, and see how we can build and train a neural network for our sales forecasting problem. Let’s begin by mastering how we can prepare time series data for modeling using the windowed dataset method with the aid of <span class="No-Break">TensorFlow utilities:</span></p>
<ol>
<li>We begin by importing the <span class="No-Break">libraries required:</span><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
import numpy as np</pre><p class="list-inset">Here, we will be using NumPy and TensorFlow to prepare and manipulate our data into the required structure <span class="No-Break">for modeling.</span></p></li>
<li>Let us create a simple dataset. Here, we are assuming the data consists of temperature <a id="_idIndexMarker752"/>values for <span class="No-Break">two</span><span class="No-Break"><a id="_idIndexMarker753"/></span><span class="No-Break"> weeks:</span><pre class="source-code">
# Create an array of temperatures for 2 weeks</pre><pre class="source-code">
temperature = np.arange(1, 15)</pre><pre class="source-code">
print(temperature)</pre><p class="list-inset">When we print out the temperature, we get <span class="No-Break">the following:</span></p><pre class="source-code">
[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]</pre><p class="list-inset">We get an array of values 1-14, where we are assuming the temperature rises from 1 on the first day to 14 on the 14th day. Odd, but let’s assume this is <span class="No-Break">the case.</span></p></li>
<li>Let’s create windowed data. Now that we have our data, we need to make a “window” of <span class="No-Break">data points:</span><pre class="source-code">
window_size = 3</pre><pre class="source-code">
batch_size = 2</pre><pre class="source-code">
shuffle_buffer = 10</pre><p class="list-inset">The <strong class="source-inline">window_size</strong> parameter refers to the window of data under consideration. If we set the window size to <strong class="source-inline">3</strong>, this means we will use 3 consecutive days’ temperature values to predict the next day’s temperature. The batch size determines how many samples are processed in each iteration during training, and <strong class="source-inline">shuffle_buffer</strong> specifies the number of elements from which TensorFlow randomly samples when shuffling the data. We shuffle to avoid sequential bias; we will expand on this in the <span class="No-Break">next chapter.</span></p></li>
<li>Creating a dataset works <span class="No-Break">as follows:</span><pre class="source-code">
dataset = tf.data.Dataset.from_tensor_slices(temperature)</pre><pre class="source-code">
for element in dataset:</pre><pre class="source-code">
    print(element.numpy())</pre><p class="list-inset">This line of code is used to create a TensorFlow <strong class="source-inline">Dataset</strong> object from our temperature data. This <strong class="source-inline">Dataset</strong> API is a high-level TensorFlow API for reading data <a id="_idIndexMarker754"/>and <a id="_idIndexMarker755"/>transforming it into a form that a machine learning model can use. Next, we iterate over the dataset and print each element. We use <strong class="source-inline">numpy()</strong> to convert a TensorFlow object into a NumPy array. When we run the code, we get the numbers 1-14; however, they are now ready to <span class="No-Break">be windowed.</span></p></li>
<li>Next, let’s transform our temperature data into a “<span class="No-Break">windowed” dataset:</span><pre class="source-code">
dataset = dataset.window(window_size + 1, shift=1, </pre><pre class="source-code">
    drop_remainder=True)</pre><pre class="source-code">
for window in dataset:</pre><pre class="source-code">
    window_data = ' '.join([str(</pre><pre class="source-code">
        element.numpy()) for element in window])</pre><pre class="source-code">
    print(window_data)</pre><p class="list-inset">We apply the <strong class="source-inline">window</strong> method to create a dataset of windows, where each window is a dataset itself. The <strong class="source-inline">window_size + 1</strong> parameter means that we are considering the <strong class="source-inline">window_size</strong> elements as input and the next one as a label. The <strong class="source-inline">shift=1</strong> parameter means that the window moves one step at a time. The <strong class="source-inline">drop_remainder=True</strong>  parameter means that we will drop the last few elements if they can’t form a <span class="No-Break">complete window.</span></p><p class="list-inset">When we print out our <strong class="source-inline">window_data</strong>, we get the <span class="No-Break">following output:</span></p><pre class="source-code">
After window:</pre><pre class="source-code">
1 2 3 4</pre><pre class="source-code">
2 3 4 5</pre><pre class="source-code">
3 4 5 6</pre><pre class="source-code">
4 5 6 7</pre><pre class="source-code">
5 6 7 8</pre><pre class="source-code">
6 7 8 9</pre><pre class="source-code">
7 8 9 10</pre><pre class="source-code">
8 9 10 11</pre><pre class="source-code">
9 10 11 12</pre><pre class="source-code">
10 11 12 13</pre><pre class="source-code">
11 12 13 14</pre><p class="list-inset">We see that we <a id="_idIndexMarker756"/>now <a id="_idIndexMarker757"/>have the window size that we set to 3 values and the (<strong class="source-inline">+1</strong>) that will serve as our label. Because we set the shift value to <strong class="source-inline">1</strong>, the next window starts from the second value in our series, which in this case is <strong class="source-inline">2</strong>. Next, the window will move one more step until we make all the <span class="No-Break">windowed data.</span></p></li>
<li>Flattening the data works <span class="No-Break">as follows:</span><pre class="source-code">
dataset = dataset.flat_map(</pre><pre class="source-code">
    lambda window: window.batch(window_size + 1))</pre><pre class="source-code">
for element in dataset:</pre><pre class="source-code">
    print(element.numpy())</pre><p class="list-inset">It’s easy to view each window created in the last step as a separate dataset. With this code, we flatten the data so that each window of data is packaged as a single batch in the main dataset. We use <strong class="source-inline">flat_map</strong> to flatten it back into a dataset of tensors and <strong class="source-inline">window.batch(window_size + 1)</strong> to convert each window dataset into a batched tensor. When we run the code, we get <span class="No-Break">the following:</span></p><pre class="source-code">
After flat_map:</pre><pre class="source-code">
[1 2 3 4]</pre><pre class="source-code">
[2 3 4 5]</pre><pre class="source-code">
[3 4 5 6]</pre><pre class="source-code">
[4 5 6 7]</pre><pre class="source-code">
[5 6 7 8]</pre><pre class="source-code">
[6 7 8 9]</pre><pre class="source-code">
[ 7  8  9 10]</pre><pre class="source-code">
[ 8  9 10 11]</pre><pre class="source-code">
[ 9 10 11 12]</pre><pre class="source-code">
[10 11 12 13]</pre><pre class="source-code">
[11 12 13 14]</pre><p class="list-inset">We can  <a id="_idIndexMarker758"/>see <a id="_idIndexMarker759"/>the windowed data is now put into <span class="No-Break">batched tensors.</span></p></li>
<li>Shuffle the data <span class="No-Break">as follows:</span><pre class="source-code">
dataset = dataset.shuffle(shuffle_buffer)</pre><pre class="source-code">
print("\nAfter shuffle:")</pre><pre class="source-code">
for element in dataset:</pre><pre class="source-code">
    print(element.numpy())</pre><p class="list-inset">In this code block, we shuffle the data. This is an important step as shuffling is used to ensure that the model doesn’t accidentally learn patterns from the order in which examples are presented during training. When we run the code block, we get <span class="No-Break">the following:</span></p><pre class="source-code">
After shuffle:</pre><pre class="source-code">
[5 6 7 8]</pre><pre class="source-code">
[4 5 6 7]</pre><pre class="source-code">
[1 2 3 4]</pre><pre class="source-code">
[11 12 13 14]</pre><pre class="source-code">
[ 7  8  9 10]</pre><pre class="source-code">
[ 8  9 10 11]</pre><pre class="source-code">
[10 11 12 13]</pre><pre class="source-code">
[ 9 10 11 12]</pre><pre class="source-code">
[6 7 8 9]</pre><pre class="source-code">
[2 3 4 5]</pre><pre class="source-code">
[3 4 5 6]</pre><p class="list-inset">We can see that<a id="_idIndexMarker760"/> the <a id="_idIndexMarker761"/>mini datasets in the main dataset have been shuffled. However, take note that the features (window) and the label are unchanged in the <span class="No-Break">shuffled dataset.</span></p></li>
<li>Mapping features and labels works <span class="No-Break">as follows:</span><pre class="source-code">
dataset = dataset.map(lambda window: (window[:-1],</pre><pre class="source-code">
    window[-1]))</pre><pre class="source-code">
print("\nAfter map:")</pre><pre class="source-code">
for x,y in dataset:</pre><pre class="source-code">
    print("x =", x.numpy(), "y =", y.numpy())</pre><p class="list-inset">The <strong class="source-inline">map</strong> method applies a function to each element of the dataset. Here, we are splitting each window into features and labels. The features are all but the last element of the <strong class="source-inline">(window[:-1])</strong> window, and the label is the last element of the <strong class="source-inline">(window[-1])</strong> window. When we run the code block, we see <span class="No-Break">the following:</span></p><pre class="source-code">
After map:</pre><pre class="source-code">
features = [3 4 5] label = 6</pre><pre class="source-code">
features = [1 2 3] label = 4</pre><pre class="source-code">
features = [10 11 12] label = 13</pre><pre class="source-code">
features = [ 8  9 10] label = 11</pre><pre class="source-code">
features = [4 5 6] label = 7</pre><pre class="source-code">
features = [7 8 9] label = 10</pre><pre class="source-code">
features = [11 12 13] label = 14</pre><pre class="source-code">
features = [5 6 7] label = 8</pre><pre class="source-code">
features = [2 3 4] label = 5</pre><pre class="source-code">
features = [6 7 8] label = 9</pre><pre class="source-code">
features = [ 9 10 11] label = 12</pre><p class="list-inset">From our <a id="_idIndexMarker762"/>print <a id="_idIndexMarker763"/>result, we see the features are made up of three observations in an order and the next value is <span class="No-Break">our label.</span></p></li>
<li>Batching and prefetching the data works <span class="No-Break">as follows:</span><pre class="source-code">
dataset = dataset.batch(batch_size).prefetch(1)</pre><pre class="source-code">
print("\nAfter batch and prefetch:")</pre><pre class="source-code">
for batch in dataset:</pre><pre class="source-code">
    print(batch)</pre><p class="list-inset">The <strong class="source-inline">batch()</strong> function groups the dataset into batches of <strong class="source-inline">batch_size</strong>. In this case, we’re creating batches of size <strong class="source-inline">2</strong>. The <strong class="source-inline">prefetch(1)</strong> performance optimization function makes sure that TensorFlow always has one batch ready to go while it’s processing the current one. After these transformations, the dataset is ready to be<a id="_idIndexMarker764"/> used<a id="_idIndexMarker765"/> for training a machine learning model. Let’s print out <span class="No-Break">the batch:</span></p><pre class="source-code">
After batch and prefetch:</pre><pre class="source-code">
(&lt;tf.Tensor: shape=(2, 3), dtype=int64,</pre><pre class="source-code">
    numpy=<strong class="bold">array([[10, 11, 12],</strong></pre><pre class="source-code">
        <strong class="bold">[ 3,  4,  5]])&gt;, &lt;tf.Tensor: shape=(2,),</strong></pre><pre class="source-code">
        <strong class="bold">dtype=int64, numpy=array([13,  6])&gt;)</strong></pre><pre class="source-code">
(&lt;tf.Tensor: shape=(2, 3), dtype=int64,</pre><pre class="source-code">
    numpy=array([[ 9, 10, 11],</pre><pre class="source-code">
        [11, 12, 13]])&gt;, &lt;tf.Tensor: shape=(2,),</pre><pre class="source-code">
        dtype=int64, numpy=array([12, 14])&gt;)</pre><pre class="source-code">
(&lt;tf.Tensor: shape=(2, 3), dtype=int64,</pre><pre class="source-code">
    numpy=array([[6, 7, 8],</pre><pre class="source-code">
        [7, 8, 9]])&gt;, &lt;tf.Tensor: shape=(2,),</pre><pre class="source-code">
        dtype=int64, numpy=array([ 9, 10])&gt;)</pre><pre class="source-code">
(&lt;tf.Tensor: shape=(2, 3), dtype=int64,</pre><pre class="source-code">
    numpy=array([[1, 2, 3],</pre><pre class="source-code">
        [4, 5, 6]])&gt;, &lt;tf.Tensor: shape=(2,),</pre><pre class="source-code">
        dtype=int64, numpy=array([4, 7])&gt;)</pre><pre class="source-code">
(&lt;tf.Tensor: shape=(2, 3), dtype=int64,</pre><pre class="source-code">
    numpy=array([[2, 3, 4],</pre><pre class="source-code">
        [5, 6, 7]])&gt;, &lt;tf.Tensor: shape=(2,),</pre><pre class="source-code">
        dtype=int64, numpy=array([5, 8])&gt;)</pre><pre class="source-code">
(&lt;tf.Tensor: shape=(1, 3), dtype=int64, numpy=array(</pre><pre class="source-code">
    [[ 8,  9, 10]])&gt;, &lt;tf.Tensor: shape=(1,),</pre><pre class="source-code">
    dtype=int64, numpy=array([11])&gt;)</pre></li>
</ol>
<p>We see that each element of the dataset is a batch of features and label pairs, where the features are arrays of the <strong class="source-inline">window_size</strong> values from the original series, and the label is the next value that we want to predict. We have seen how to prepare our time series data<a id="_idIndexMarker766"/> for <a id="_idIndexMarker767"/>modeling; we have sliced, windowed, batched, shuffled, and split it into features and labels for <span class="No-Break">this purpose.</span></p>
<p>Next, let us use what we have learned here on our synthetic sales data to forecast future <span class="No-Break">sales values.</span></p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor315"/>Sales forecasting using neural networks</h2>
<p>Let’s return to the<a id="_idIndexMarker768"/> sales data that we<a id="_idIndexMarker769"/> created to forecast sales using both naïve and moving average methods. Let us now use a neural network; here, we will use <span class="No-Break">a DNN:</span></p>
<ol>
<li>Extracting the data works <span class="No-Break">as follows:</span><pre class="source-code">
time = pd.to_datetime(df['Date'])</pre><pre class="source-code">
sales = df['Sales'].values</pre><p class="list-inset">In this code, we extract the <strong class="source-inline">Date</strong> and <strong class="source-inline">Sales</strong> data from the <strong class="source-inline">Sales</strong> DataFrame. <strong class="source-inline">Date</strong> is converted into datetime format and <strong class="source-inline">Sales</strong> is converted into a <span class="No-Break">NumPy array.</span></p></li>
<li>Splitting the data works <span class="No-Break">as follows:</span><pre class="source-code">
split_time = int(len(df) * 0.8)</pre><pre class="source-code">
time_train = time[:split_time]</pre><pre class="source-code">
x_train = sales[:split_time]</pre><pre class="source-code">
time_valid = time[split_time:]</pre><pre class="source-code">
x_valid = sales[split_time:]</pre><p class="list-inset">For uniformity, we split data into a training set and a validation set using the same 80:20 split by using <strong class="source-inline">split_time</strong> <span class="No-Break">of 80%.</span></p></li>
<li>Creating the windowed dataset works <span class="No-Break">as follows:</span><pre class="source-code">
def windowed_dataset(</pre><pre class="source-code">
    series, window_size, batch_size, shuffle_buffer):</pre><pre class="source-code">
        dataset = tf.data.Dataset.from_tensor_slices(</pre><pre class="source-code">
            series)</pre><pre class="source-code">
        dataset = dataset.window(window_size + 1,</pre><pre class="source-code">
            shift=1, drop_remainder=True)</pre><pre class="source-code">
        dataset = dataset.flat_map(lambda window:</pre><pre class="source-code">
            window.batch(window_size + 1))</pre><pre class="source-code">
        dataset = dataset.shuffle(shuffle_buffer).map(</pre><pre class="source-code">
            lambda window: (window[:-1], window[-1]))</pre><pre class="source-code">
        dataset = dataset.batch(</pre><pre class="source-code">
            batch_size).prefetch(1)</pre><pre class="source-code">
    return dataset</pre><pre class="source-code">
dataset = windowed_dataset(x_train, window_size,</pre><pre class="source-code">
    batch_size, shuffle_buffer_size)</pre><p class="list-inset">We<a id="_idIndexMarker770"/> create<a id="_idIndexMarker771"/> the <strong class="source-inline">windowed_dataset</strong> function; this function takes in a series, a window size, a batch size, and a shuffle buffer. It creates windows of data for training, with each window containing a <strong class="source-inline">window_size + 1</strong> data point. These windows are then shuffled and mapped to features and labels, where the features are all data points in a window, except the last one, and the label is the last data point. The windows are then batched and prefetched for efficient <span class="No-Break">data loading.</span></p></li>
<li>Building the model works <span class="No-Break">as follows:</span><pre class="source-code">
model = tf.keras.models.Sequential([</pre><pre class="source-code">
    tf.keras.layers.Dense(10,</pre><pre class="source-code">
        input_shape=[window_size], activation="relu"),</pre><pre class="source-code">
    tf.keras.layers.Dense(10, activation="relu"),</pre><pre class="source-code">
    tf.keras.layers.Dense(1)</pre><pre class="source-code">
    ])</pre><pre class="source-code">
model.compile(loss="mse",</pre><pre class="source-code">
    optimizer=tf.keras.optimizers.SGD(</pre><pre class="source-code">
        learning_rate=1e-6, momentum=0.9))</pre><p class="list-inset">Next, we use <a id="_idIndexMarker772"/>a<a id="_idIndexMarker773"/> simple <strong class="bold">feedforward neural network</strong> (<strong class="bold">FFN</strong>) for<a id="_idIndexMarker774"/> modeling. This model contains two dense layers with ReLU activation, followed by a dense output layer with a single neuron. The model is compiled with MSE <a id="_idIndexMarker775"/>loss and <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) as <span class="No-Break">the optimizer.</span></p></li>
<li>Training the model works <span class="No-Break">as follows:</span><pre class="source-code">
model.fit(dataset, epochs=100, verbose=0)</pre><p class="list-inset">The model is trained on the windowed dataset for <span class="No-Break">100 epochs.</span></p></li>
<li>Generating predictions works <span class="No-Break">as follows:</span><pre class="source-code">
input_batches = [sales[time:time + window_size][</pre><pre class="source-code">
    np.newaxis] for time in range(len(</pre><pre class="source-code">
        sales) - window_size)]</pre><pre class="source-code">
inputs = np.concatenate(input_batches, axis=0)</pre><pre class="source-code">
forecast = model.predict(inputs)</pre><pre class="source-code">
results = forecast[split_time-window_size:, 0]</pre><p class="list-inset">Here, we generate predictions in batches to improve computational efficiency. Only the predictions for the validation period <span class="No-Break">are kept.</span></p></li>
<li>Evaluating the model works <span class="No-Break">as follows:</span><pre class="source-code">
print(tf.keras.metrics.mean_squared_error(x_valid,</pre><pre class="source-code">
    results).numpy())</pre><pre class="source-code">
print(tf.keras.metrics.mean_absolute_error(x_valid,</pre><pre class="source-code">
    results).numpy())</pre><p class="list-inset">The MSE and MAE between the true validation data and the predicted data are calculated. Here, we achieved an MSE of <strong class="source-inline">34.51</strong> and an MAE of <strong class="source-inline">4.72</strong>, which surpasses all our <a id="_idIndexMarker776"/>simple<a id="_idIndexMarker777"/> <span class="No-Break">statistical methods.</span></p></li>
<li>Visualizing the results works as follows. The true validation data and the predicted data are plotted over time to visualize the <span class="No-Break">model’s performance:</span></li>
</ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="Figure 12.17 – A time series forecast using a simple FFN" height="505" src="image/B18118_12_017.jpg" width="831"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.17 – A time series forecast using a simple FFN</p>
<p>From the plot in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.17</em>, we see that our forecasted values map closely to the ground truth on the validation set, less the noisy spikes. Our FFN has demonstrated notable achievements in this experiment. Compared to the traditional statistical methods, our model exhibits a significant improvement in performance. You could tune the hyperparameters <a id="_idIndexMarker778"/>as <a id="_idIndexMarker779"/>well as implement callbacks to improve performance. However, our job is <span class="No-Break">done here.</span></p>
<p>Let’s meet in the final chapter, where we will be predicting app stock prices. See <span class="No-Break">you there.</span></p>
<h1 id="_idParaDest-224"><a id="_idTextAnchor316"/>Summary</h1>
<p>In this chapter, we explored the concept of time series, examined the core characteristics and types of time series, and looked at some well-known applications of time series in machine learning. We also covered concepts such as trailing and centered windows and examined how to prepare time series for modeling with neural networks with the aid of utilities from TensorFlow. In our case study, we applied both statistical and deep learning techniques in order to build a sales forecasting model for a <span class="No-Break">fictional company.</span></p>
<p>In the next chapter, we will extend our modeling using more complex architectures such as RNNs, CNNs, and CNN-LSTM architecture in forecasting time series data. Also, we will explore concepts such as learning rate scheduler and Lambda layers. To conclude the final chapter of this book, we will build a forecasting model for Apple’s closing <span class="No-Break">stock price.</span></p>
<h1 id="_idParaDest-225"><a id="_idTextAnchor317"/>Questions</h1>
<ol>
<li>Apply the principle of naïve forecasting to the “<em class="italic">Air Passenger</em>” dataset using the exercise <span class="No-Break">notebook provided.</span></li>
<li>Implement the moving average technique on the same dataset and evaluate its performance by calculating the MAE and <span class="No-Break">MSE values.</span></li>
<li>Next, introduce the method of differencing to your moving average model. Again, assess the accuracy of your forecast by determining the MAE and <span class="No-Break">MSE values.</span></li>
<li>With the sample temperature dataset at hand, demonstrate how to create meaningful features and labels from <span class="No-Break">this data.</span></li>
<li>Lastly, experiment with the simple FFN model on the dataset and observe <span class="No-Break">its performance.</span></li>
</ol>
</div>
</div></body></html>