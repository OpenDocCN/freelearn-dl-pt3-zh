<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Understanding Black-Box Optimization Algorithms</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we looked at reinforcement learning algorithms, ranging from value-based to policy-based methods and from model-free to model-based methods. In this chapter, we'll provide another solution for solving sequential tasks, that is, with a class of black-box algorithms <strong>evolutionary algorithms</strong> (<strong>EA</strong>). <span>EAs are driven by evolutionary mechanisms and are sometimes preferred to <strong>reinforcement learning</strong> (<strong>RL</strong>) as they don't require backpropagation. They also offer other complementary benefits to RL. We'll start this chapter by giving you a brief recap of RL algorithms so that you'll better understand how EA fits into these sets of problems. Then, you'll learn about the basic building blocks of EA and how those algorithms work. We'll also take advantage of this introduction and look at one of the most well-known EAs, namely <strong>evolution strategies</strong> (<strong>ES</strong>), in more depth.</span></p>
<p>A recent algorithm that was developed by OpenAI caused a great boost in the adoption of ES for solving sequential tasks. They showed how ES algorithms can be massively parallelized and scaled linearly on a number of CPUs while achieving high performance. After an explanation of evolution strategies, we'll take a deeper look at this algorithm and develop it in TensorFlow so that you'll be able to apply it to the tasks you care about.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Beyond RL</li>
<li>The core of EAs</li>
<li>Scalable evolution strategies</li>
<li>Scalable ES applied to LunarLander</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Beyond RL</h1>
                </header>
            
            <article>
                
<p>RL algorithms <span><span>are the usual choice when we're faced with</span></span> sequential decision problems. Usually, it's difficult to find other ways to solve these tasks other than using <span><span>RL</span></span>. Despite the hundreds of different optimization methods that are out there, so far, only RL has worked well on problems for sequential decision-making. But this doesn't mean it's the only option. </p>
<p>We'll start this chapter by recapping on the inner workings of RL algorithms and questioning the usefulness of their components for solving sequential tasks. This brief summary will help us introduce a new type of algorithm that offers many advantages (as well as some disadvantages) that could be used as a replacement for RL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief recap of RL</h1>
                </header>
            
            <article>
                
<p><span><span>I</span></span>n the beginning, a policy is initialized randomly and used to interact with the environment for either a given number of steps, or entire trajectories, to collect data. On each interaction, the state visited, the action taken, and the reward obtained are recorded. This information provides a full description of the influence of the agent in the environment. Then, in order to improve the policy, <span>the backpropagation algorithm (</span><span>based on the loss function, in order to move the predictions to a better estimate) </span>computes the gradient of each weight of the network. These gradients are then applied with a stochastic gradient descent optimizer. This process (gathering data from the environment and optimizing the neural network with <span><strong>stochastic gradient descent</strong> (<strong>SGD</strong>)</span>) is repeated until a convergence criterion is met.</p>
<p>There are two important things to note here that will be useful in the following discussion:</p>
<ul>
<li><strong>Temporal credit assignment</strong>: Because RL algorithms optimize the policy on each step, allocating the quality of each action and state is required. This is done by <span>assigning a value </span>to each state-action pair.<span> Moreover,</span> a discount factor <span>is used </span>to minimize the influence of distant <span>actions and to give more weight to the last actions</span>. This will help us solve the problem of assigning the credit to the actions, but will also introduce inaccuracies in the system.</li>
<li><strong>Exploration</strong>: In order to maintain a degree of exploration in the actions, <span>additional noise </span>is injected into the policy of RL algorithms. The way in which the noise is injected depends on the algorithm, but usually, the actions are sampled from a stochastic distribution. By doing so, if the agent is in the same situation twice, it may take different actions that would lead to two different paths. This strategy also<span> </span>encourages exploration in deterministic environments.<span> By deviating the path each time, the agent may discover different – and potentially better – solutions. With this additional noise that asymptotically tends to 0, the agent is then able to converge to a better and final deterministic policy. </span></li>
</ul>
<p>But are backpropagation, temporal credit assignment, and stochastic actions actually a prerequisite for learning and building complex policies?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The alternative</h1>
                </header>
            
            <article>
                
<p>The answer to this question is no.</p>
<p>As we learned in <a href="e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml">Chapter 10</a>, <em>Imitation Learning with the DAgger Algorithm</em>, by reducing policy learning to an imitation problem using backpropagation and SGD, we can learn about a<span> discriminative model </span>from an expert in order to predict which actions to take next. Still, this involves backpropagation and requires an expert that may not always be available. </p>
<p>Another general subset of algorithms for global optimization does exist. They are called EAs, and they aren't based on backpropagation and don't require any of the other two principles, namely temporal credit assignment and noisy actions. Furthermore, as we said in the introduction to this chapter, these evolutionary algorithms are very general and can be used in a large variety of problems, including sequential decision tasks. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">EAs</h1>
                </header>
            
            <article>
                
<p>As you may have guessed, EAs differ in many aspects from RL algorithms and are principally inspired by biological evolution. EAs include many similar methods such as, genetic algorithms, evolution strategies, and genetic programming, which vary in their implementation details and in the nature of their representation. However, they are all mainly based on four basic mechanisms <span>– </span>reproductions, mutation, crossover, and selection <span>–</span> that are cycled in a guess-and-check process. We'll see what this means as we progress through this chapter.</p>
<p>Evolutionary algorithms are defined as black-box algorithms. These are algorithms that optimize a function, <sub><img class="fm-editor-equation" src="assets/7bf9bc1b-7d93-4527-8b6c-db935d6b9f12.png" style="width:2.08em;height:1.17em;"/></sub>, with respect to <sub><img class="fm-editor-equation" src="assets/78d5816f-a32e-4420-83f3-a85a85de01f8.png" style="width:0.75em;height:0.75em;"/></sub> without making any assumption about <sub><img class="fm-editor-equation" src="assets/0da66c3f-a9e7-4314-9b1c-8029fbd2092e.png" style="width:0.50em;height:1.00em;"/></sub>. Hence, <sub><img class="fm-editor-equation" src="assets/469deea3-9131-4342-8c89-058bb702d379.png" style="width:0.50em;height:1.00em;"/></sub> can be anything you want. We only care about the output of <sub><img class="fm-editor-equation" src="assets/7d4de719-2d7d-4b35-8071-af7c8167991a.png" style="width:0.50em;height:1.00em;"/></sub>. This has many advantages, as well as some disadvantages. The primary advantage is that we don't have to care about the structure of <sub><img class="fm-editor-equation" src="assets/434d6333-a4e5-45a8-9b64-e420e9184a95.png" style="width:0.50em;height:1.00em;"/></sub> and we are free to use what is best for us and for the problem at hand. On the other hand, the main disadvantage is that these optimization methods cannot be explained and thus their mechanism cannot be interpreted. In problems where interpretability is of great importance, these methods are not appealing.</p>
<p>Reinforcement learning has almost always been preferred for solving sequential tasks, especially for medium to difficult tasks. However, a recent paper from OpenAI highlights that the evolution strategy, which is an evolutionary algorithm, can be used as an alternative to RL. This statement is mainly due to the performance that's reached <span>asymptotically </span>by the algorithm and its incredible ability to be scaled across thousands of CPUs.</p>
<p>Before we look at how this algorithm is able to scale so well while learning good policies on difficult tasks, let's take a more in-depth look at EAs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The core of EAs</h1>
                </header>
            
            <article>
                
<p>EAs are inspired by biological evolution and implement techniques and mechanisms that simulate biological evolution. This means that EAs go through many trials to create a population of new candidate solutions. These solutions are also called <strong>individuals</strong> (in RL problems, a candidate solution is a policy) that are better than the previous generation, in a similar way to the process within nature wherein only the strongest survive and have the possibility to procreate. </p>
<p>One of the advantages of EAs is that they a<span>re derivative-free methods, meaning that they don't use the derivative to find the solution. This allows EAs to work very well with all sorts of differentiable and non-differentiable functions, including </span>deep neural networks. This combination is schematized in the following diagram. Note that each individual is a separate deep neural network, and so we'll have as many neural networks as the number of individuals at any given moment. In the following diagram, the population is composed of five individuals:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2013 image-border" src="assets/185767dc-22b5-4956-a2fe-f07e80085a75.png" style="width:26.75em;height:16.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.1. Optimization of deep neural networks through e<span>volutionary algorithms</span></div>
<p>The specificity of each type of evolutionary algorithm differs from the others, but the underlying cycle is common to all the EAs and works as follows:</p>
<ol>
<li>A population of individuals (also called <strong>candidate solutions</strong> or <strong>phenotypes</strong>) is created so that each of them has a set of different properties (called <strong>chromosomes</strong> or <strong>genotypes</strong>). The initial population is initialized randomly.</li>
<li>Each candidate solution is evaluated independently by a fitness function that determines its quality. The fitness function is usually related to the objective function and, using the terminology we've used so far, the fitness function could be the total reward accumulated by the agent (that is, the candidate solution) throughout its life.</li>
<li>Then, the fitter individuals of the population are selected, and their genome is modified in order to produce the new generation. In some cases, the less fit candidate solution can be used as a negative example to generate the next generation. This whole step varies largely, depending on the algorithm. Some algorithms, such as genetic algorithms, breed new individuals through two processes called <strong>crossover</strong> and <strong>mutation</strong>, which give birth to new individuals (called <strong>offspring</strong>). Others, such as evolution strategies, breed new individuals through mutation only. We'll explain crossover and mutation in more depth later in this chapter, but generally speaking, crossover is the process that combines genetic information from two parents, while mutation only alters some gene values in the offspring.</li>
<li>Repeat the whole process, going through steps 1-3 until a terminal condition is met. On each iteration, the population that's created is also called a <strong>generation</strong>.</li>
</ol>
<p>This iterative process, as shown in the following diagram, terminates when a given fitness level has been reached or a maximum number of generations have been produced. As we can see, the population is created by crossover and mutation, but as we habe already explained, these processes may vary, depending on the specific algorithm:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2014 image-border" src="assets/f73e8a1d-c237-49b8-9e85-d6a9565eb061.png" style="width:26.50em;height:24.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.2. The main cycle of evolutionary algorithms</div>
<p>The main body of a general EA is very simple and can be written in just a few lines of code, as shown here. To summarize this code, on each iteration, and u<span>ntil a fitted generation has been produced, new candidates are generated and evaluated. The candidates are created from the best-fitted individuals of the previous generation:</span></p>
<pre>solver = EvolutionaryAlgortihm()<br/><br/><strong>while</strong> best_fitness &lt; required_fitness:<br/>    <br/>    candidates = solver.generate_candidates() # for example from crossover and mutation<br/><br/>    fitness_values = []<br/>    <strong>for</strong> candidate <strong>in</strong> candidates:<br/>        fitness_values.append(evaluate(candidate))<br/><br/>    solver.set_fitness_values(fitness_values)<br/><br/>    best_fitness = solver.evaluate_best_candidate()</pre>
<div class="packt_infobox">Note that the implementation details of the solver are dependent on the algorithm that's used.</div>
<p>The applications of EAs are actually spread across many fields and problems, from economy to biology, and from computer program optimization to ant colony optimization.</p>
<p>Since we are mostly interested in the application of evolutionary algorithms for solving sequential decision-making tasks, we will explain the two most common EAs that are used to solve these kinds of jobs. They are known as <strong>genetic algorithms</strong> (<strong>GAs</strong>) and <span><strong>evolution strategies</strong> (<strong>ESes</strong>). Later, we'll take a step further with ES by developing a highly scalable version of it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Genetic algorithms</h1>
                </header>
            
            <article>
                
<p>The idea of GAs is very straightforward—evaluate the current generations, use only the top-performing individuals to generate the next candidate solutions, and discard the other individuals. This is shown in the preceding diagram. The survivors will generate the next population by crossover and mutation. These two processes are represented in the following diagram. Crossover is done by selecting two solutions among the survivors and combining their parameters. Mutation, on the other hand, involves changing a few random parameters on the offspring's genotype:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2074 image-border" src="assets/40983c68-bd09-46a1-8955-2f6bb61edb4d.png" style="width:22.42em;height:19.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.3. Visual illustration of mutation and crossover</div>
<p>Crossover and mutation can be approached in many different ways. In the simpler version, crossover is done by choosing <span>parts</span><span> </span><span>from the two parents randomly</span>, and mutation is done by mutating the solution that's obtained by adding Gaussian noise with a fixed standard deviation. By only keeping the best individuals and injecting their genes into the newly born individuals, the solutions will improve over time until a condition is met. However, on complex problems, this simple solution is prone to be stuck in a local optimum (meaning that the solution is only within a small set of candidate solutions). In this case, a more advanced genetic algorithm such as <strong><span>NeroEvolution of Augmenting Topologies</span></strong> (<strong><span>NEAT</span></strong>) is preferred. NEAT not only alters the weights of the network but also its structure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evolution strategies</h1>
                </header>
            
            <article>
                
<p><strong>Evolution strategies</strong> (<strong>ESes</strong>) are even easier than GAs as they are primarily based on mutation to create a new population.</p>
<p>Mutation is performed by adding values that have been sampled from a normal distribution to the genotype. A very simple version of ES is obtained by just selecting the most performant individual across the whole population and sampling the next generation from a normal distribution with a fixed standard deviation and a mean equal to that of the best-performing individual.</p>
<p>Outside of the sphere of small problems, using this algorithm is not recommended. This is because following only a single leader and using a fixed standard deviation could prevent potential solutions from exploring a more diverse search space. As a consequence, the solution to this method <span>would probably end in a narrow local minimum. An immediate and better strategy would be to generate the offspring by combining the <img class="fm-editor-equation" src="assets/09f6e608-2765-4ccf-96fa-006c8cfe4bd8.png" style="width:0.92em;height:0.92em;"/> top performing candidate solutions and weighing them by their fitness rank. Ranking the individuals according to their fitness values is called fitness ranking. This strategy is preferred to using the actual fitness values as it is invariant to the transformation of the objective function and it prevents the new generation from moving too much toward a possible outlier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CMA-ES</h1>
                </header>
            
            <article>
                
<p>The <strong>Covariance Matrix Adaptation Evolution Strategy</strong>, or <strong>CMA-ES</strong> for short, is an evolutionary strategy algorithm. Unlike the simpler version of the evolution strategy, it samples the new candidate solution according to a multivariate normal distribution. The name CMA comes from the fact that the dependencies between the variables are kept in a covariance matrix that has been adapted to increase or decrease the search space on the next generation.</p>
<p>Put simply, CMA-ES shrinks the search space by incrementally decreasing the covariance matrix in a given direction when it's confident of the space around it. Instead, CMA-ES increases the covariance matrix and thus enlarges the possible search space when it's less confident.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ES versus RL</h1>
                </header>
            
            <article>
                
<p>ESes are an interesting alternative to RL. N<span>onetheless, the pros and cons must be evaluated so that we can pick the correct approach.</span> Let's briefly look at the main advantages of ES:</p>
<ul>
<li><strong>Derivative-free methods</strong>: There's no need for backpropagation. Only the forward pass is performed for estimating the fitness function (or equivalently, the cumulative reward). This opens the door to all the non-differentiable functions, for example; hard attention mechanisms. Moreover, by avoiding backpropagation, the code gains efficiency and speed. </li>
<li><strong>Very general</strong>: The generality of ES is mainly due to its property of being a black-box optimization method. Because we don't care about the agent, the actions that it performs, or the states visited, we can abstract these and concentrate only on its evaluation. Furthermore, ES allows learning without explicit targets and also with extremely sparse feedback. Additionally, ESes are more general in the sense that they can optimize a much larger set of functions.</li>
<li><strong>Highly parallelizable and robust</strong>: As we'll soon see, ES is much easier to parallelize than RL, and the computations can be spread across thousands of workers. The robustness of evolution strategies is due to the few hyperparameters that are required to make the algorithms work. For example, in comparison to RL, there's no need to specify the length of the trajectories, the lambda value, the discount factor, the number of frames to skip, and so on. Also, the ES is very attractive for tasks with a very long horizon.</li>
</ul>
<p>On the other hand, reinforcement learning is preferred for the following key aspects:</p>
<ul>
<li><strong>Sample efficiency</strong>: RL algorithms make better use of the information that's acquired from the environment and as a consequence, they require less data and fewer steps to learn the tasks.</li>
<li><strong>Excellent performance</strong>: Overall, reinforcement learning algorithms outperform performance evolution strategies.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scalable evolution strategies</h1>
                </header>
            
            <article>
                
<p>Now that we've introduced black-box evolutionary algorithms and<span> evolution strategies</span> in particular, we are ready to put <span>what we have just learned</span> into practice<span>. The paper called <em>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</em> by OpenAI made a major contribution to the adoption of evolution strategies as an alternative to reinforcement learning algorithms.</span></p>
<p>The main contribution of this paper is in the approach that scales ES extremely well with a number of CPUs. In particular, the new approach uses a novel communication strategy across CPUs that involves only scalars, and so it is able to scale across thousands of parallel workers.</p>
<p>Generally, ES requires more experience and thus is less efficient than RL. However, by spreading the computation across so many workers (thanks to the adoption of this new strategy), the task can be solved in less wall clock time. As an example, in the paper, the authors solve the 3D Humanoid Walking pattern in just 10 minutes with 1,440 CPUs, with a linear speedup in the number of CPU cores. Because usual RL algorithms cannot reach this level of scalability, they take hours to solve the same task.</p>
<p class="mce-root">Let's look at how they are able to scale so well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The core</h1>
                </header>
            
            <article>
                
<p>In the paper, a version of ES is used that maximizes the average objective value, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ba38e6a1-fdc3-4592-afec-231c17fd721c.png" style="width:5.92em;height:1.75em;"/></p>
<p><span>It does this by searching</span> over a population, <img class="fm-editor-equation" src="assets/539e6611-0c4e-4075-9e74-35cce86c0178.png" style="width:1.25em;height:1.08em;"/>, that's parameterized by <img class="fm-editor-equation" src="assets/3a36c710-4434-455b-8b98-03c3151da93e.png" style="width:0.67em;height:0.92em;"/> <span>with stochastic gradient ascent. </span><img style="font-size: 1em;width:0.67em;height:0.75em;" class="fm-editor-equation" src="assets/051c0918-d493-43e6-a1d4-9de7f947e548.png"/><span> is the objective function (or fitness function) while </span><img style="font-size: 1em;width:0.50em;height:1.00em;" class="fm-editor-equation" src="assets/4d706f8e-4f4f-49a4-975c-6f02832c082f.png"/> is <span>the parameters of the actor. In our problems, </span><img style="font-size: 1em;width:1.92em;height:1.25em;" class="fm-editor-equation" src="assets/3bc547ab-803d-49b4-ac20-452762b6b5e6.png"/><span> is simply the stochastic return that's obtained by the agent with</span> <img style="font-size: 1em;width:0.50em;height:1.00em;" class="fm-editor-equation" src="assets/0f404f12-ce92-410d-9d06-404c260e2f6a.png"/><span> in the environment.</span></p>
<p>The population distribution, <img class="fm-editor-equation" src="assets/add68569-90f7-4ffd-bf1c-f2310e74ac9f.png" style="width:1.17em;height:1.00em;"/>,is a multivariate Gaussian with a mean, <img class="fm-editor-equation" src="assets/c7584f6b-9665-483c-9396-b5f63d319342.png" style="width:0.67em;height:0.92em;"/>, and fixed standard deviation, <img class="fm-editor-equation" src="assets/9585cbc6-5654-4b6b-bb0c-14c85f7ae8c2.png" style="width:0.67em;height:0.75em;"/>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6ac4b3a4-7bcc-4497-be9a-739357c40167.png" style="width:24.50em;height:1.75em;"/></p>
<p>From here, we can define the step update by using the stochastic gradient estimate, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f4bc3a83-dad8-43d6-bc80-3e5846b68fd5.png" style="width:21.33em;height:3.50em;"/></p>
<p>With this update, we can estimate the stochastic gradient (without performing backpropagation) using the results of the episodes from the population. We can update the parameters using one of the well-known update methods, such as Adam or RMSProp as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parallelizing ES</h1>
                </header>
            
            <article>
                
<p>It's easy to see how ES can be scaled across multiple CPUs: each worker is assigned to a separate candidate solution of the population. The evaluation can be done in complete autonomy, and as described in the paper, optimization can be done in parallel on each worker, with <span>only a few scalars shared between each CPU unit.</span></p>
<p>Specifically, the only information that's shared between workers is the scalar return, <img class="fm-editor-equation" src="assets/42692633-6bcf-401b-bded-d2cb8a1dabb8.png" style="width:4.75em;height:1.25em;"/>, of an episode and the random seed that has been used to sample <img class="fm-editor-equation" src="assets/f805a18c-a321-40fd-926a-c72d7f12f46e.png" style="width:1.00em;height:0.92em;"/>. The amount of data can be further shrunk by sending only the return, but in this case, the random seed of each worker has to be synchronized with all the others. We decided to adopt the first technique, while the paper used the second one. In our simple implementation, the difference is negligible and both techniques require extremely low bandwidth. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other tricks</h1>
                </header>
            
            <article>
                
<p>Two more techniques are used to improve the performance of the algorithm:</p>
<ul>
<li><strong>Fitness shaping</strong> <span>–</span> <strong>objective ranking</strong>: We discussed this technique previously. It's very simple. Instead of using the raw returns to compute the update, a rank transformation is used. The rank is <span>invariant to the transformation of the objective function and thus performs better with spread returns. Additionally, it removes the noise of the outliers.</span></li>
<li><strong>Mirror noise</strong>: This trick reduces the variance and involves the evaluation of the network with both noise <img class="fm-editor-equation" src="assets/c238b7ca-715c-4fe0-add9-7c75cc76cbd5.png" style="width:0.42em;height:0.50em;"/> and <img class="fm-editor-equation" src="assets/5fd782b2-9cad-48cc-959d-0ccafd86ee28.png" style="width:1.42em;height:0.92em;"/>; that is, for each individual, we'll have two mutations: <img class="fm-editor-equation" src="assets/8374e454-8f2f-42b4-8157-0055e8b86858.png" style="width:5.75em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/9acbcfa7-181f-4d62-82a0-3399abb61d07.png" style="width:5.33em;height:1.08em;"/>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pseudocode</h1>
                </header>
            
            <article>
                
<p>The parallelized evolution strategy that combines all of these features is summarized in the following pseudocode: </p>
<pre>---------------------------------------------------------------------------------<br/>Parallelized Evolution Strategy<br/>---------------------------------------------------------------------------------<br/><br/>Initialize parameters <img class="fm-editor-equation" src="assets/21613536-a521-43c9-8cfa-37df9ea74d9c.png" style="width:0.83em;height:0.92em;"/> on each worker<br/>Initialize random seed on each worker<br/><br/><strong>for</strong> <img class="fm-editor-equation" src="assets/2d091c4b-cff8-40c7-9252-55f20cacc3f7.png" style="width:7.17em;height:0.83em;"/> <strong>do:<br/>    for <img class="fm-editor-equation" src="assets/ebb094da-015f-4821-b5a7-6ee124759d3c.png" style="width:6.33em;height:0.92em;"/></strong> <strong>do:</strong><br/>        Sample <img class="fm-editor-equation" src="assets/40615ddf-8bac-4ddf-acbc-ad7278a2c144.png" style="width:4.25em;height:1.08em;"/><br/>        Evaluate individuals <img class="fm-editor-equation" src="assets/d67696f3-210f-443b-a5b8-61173dd2ee5e.png" style="width:4.50em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/5e1ee696-2aa7-4587-b72f-96f4f3ba7171.png" style="width:4.25em;height:1.17em;"/><br/><br/>    Spread returns to each other worker<br/><br/>    <strong>for</strong> <img class="fm-editor-equation" src="assets/e2c5859f-32fb-43eb-be95-aa856273794b.png" style="width:6.25em;height:0.92em;"/> <strong>do:</strong><br/>        Compute normalized rank <img class="fm-editor-equation" src="assets/1cb812dd-e39f-447d-9983-85129c485a52.png" style="width:0.58em;height:0.58em;"/> from the returns<br/>        Reconstruct <img class="fm-editor-equation" src="assets/58bb2abc-a77c-4ac4-b14c-f90add9a6b92.png" style="width:0.92em;height:0.92em;"/> from the random seeds of the other workers<br/>        <img class="fm-editor-equation" src="assets/bb4bdba8-1e6e-4652-95a3-0be875d15a09.png" style="width:11.58em;height:2.92em;"/> (maybe using Adam)</pre>
<p>Now, all that remains is to implement <span><span>this algorithm</span></span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scalable implementation</h1>
                </header>
            
            <article>
                
<p>To simplify the implementation and to make the parallelized version of ES work well with a limited number of workers (and CPUs), we will develop a structure similar to the one that's shown in the following diagram. The main process creates one worker for each CPU core and executes the main cycle. On each iteration, it waits until a given number of new candidates are evaluated by the workers. Different from the implementation provided in the paper, each worker evaluates more than one agent on each iteration. So, if we have four CPUs, four workers will be created. Then, if we want a total batch size bigger than the number of workers <span>on each iteration of the main process</span>, let's say, 40, each worker will create and evaluate 10 individuals each time. The return values and seeds are returned to the main application, which waits for <span>results from </span>all 40 individuals, before continuing with the following lines of code.</p>
<p>Then, these results are propagated in a batch to all the workers, which optimize the neural network seperately, following the update provided in the formula (11.2):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2015 image-border" src="assets/fde043f4-e707-4c42-98ec-7bb6cc7d4855.png" style="width:23.75em;height:25.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.4. Diagram showing the main components involved in the parallel version of ES</div>
<p>Following what we just described, the code is divided into three main buckets:</p>
<ul>
<li>The main process that creates and manages the queues and the workers.</li>
<li>A function that defines the task of the workers.</li>
<li>Additionally, there are some functions that perform simple tasks, such as ranking the returns and evaluating the agent.</li>
</ul>
<p>Let's explain the code of the main process so that you have a broad view of the algorithm before going into detail about the workers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The main function</h1>
                </header>
            
            <article>
                
<p>This is defined in a function called <kbd>ES</kbd> that has the following arguments: the <span>name of the G</span>ym environment, the size of the neural network's hidden layers, the total number of generations, the number of workers, the Adam learning rate, the batch size, and the standard deviation noise:</p>
<div>
<pre><span>def</span><span> </span><span>ES</span><span>(</span><span>env_name</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[</span><span>8</span><span>,</span><span>8</span><span>], </span><span>number_iter</span><span>=</span><span>1000</span><span>, </span><span>num_workers</span><span>=</span><span>4</span><span>, </span><span>lr</span><span>=</span><span>0.01</span><span>, </span><span>batch_size</span><span>=</span><span>50, std_noise=0.01</span><span>):</span></pre></div>
<p>Then, we set an initial seed that is shared among the workers to initialize the parameters with the same weights. Moreover, we calculate the number of individuals that a worker has to generate and evaluate on each iteration and create two <kbd>multiprocessing.Queue</kbd> queues. These queues are the entry and exit points for the variables that are passed to and from the workers:</p>
<div>
<pre><span>    initial_seed </span><span>=</span><span> np.random.</span><span>randint</span><span>(</span><span>1e7</span><span>)<br/></span><span>indiv_per_worker </span><span>=</span><span> </span><span>int</span><span>(batch_size </span><span>/</span><span> num_workers)<br/></span><span>    output_queue </span><span>=</span><span> mp.</span><span>Queue</span><span>(</span><span>maxsize</span><span>=</span><span>num_workers</span><span>*</span><span>indiv_per_worker)<br/></span><span>    params_queue </span><span>=</span><span> mp.</span><span>Queue</span><span>(</span><span>maxsize</span><span>=</span><span>num_workers)</span></pre></div>
<p>Next, the multiprocessing processes, <kbd>multiprocessing.Process</kbd>, are instantiated. These will run the <kbd>worker</kbd> function, which is given as the first argument to the <kbd>Process</kbd> constructor in an asynchronous way. All the other variables that are passed to the <kbd>worker</kbd> function are assigned to <kbd>args</kbd> and are pretty much the same as the parameters taken by ES, with the addition of the two queues. The processes start running when the <kbd>start()</kbd> method is called:</p>
<div>
<pre><span>    processes </span><span>=</span><span> []<br/><br/></span><span>    for</span><span> widx </span><span>in</span><span> </span><span>range</span><span>(num_workers):<br/><br/></span><span>        p </span><span>=</span><span> mp.</span><span>Process</span><span>(</span><span>target</span><span>=</span><span>worker, </span><span>args</span><span>=</span><span>(env_name, initial_seed, hidden_sizes, lr, std_noise, indiv_per_worker, </span><span>str</span><span>(widx), params_queue, output_queue))<br/></span><span>        p.</span><span>start</span><span>()<br/></span><span>        processes.</span><span>append</span><span>(p)</span></pre></div>
<p>Once the parallel workers have started, we can iterate across the generations and wait until all the individuals have been generated and evaluated separately in each worker. Remember that the total number of individuals that are created on every generation is the number of workers, <kbd><span>num_workers</span></kbd>, multiplied by the individuals generated on each worker, <span><kbd>indiv_per_worker</kbd>. This architecture is unique to our implementation as we have only four CPU cores available, compared to the implementation in the paper, which benefits from thousands of CPUs. Generally, the population that's created on every generation is usually between 20 and 1,000:</span></p>
<div>
<pre><span>    for n_iter in range(number_iter):<br/>        batch_seed </span><span>=</span><span> []<br/></span><span>        batch_return </span><span>=</span><span> []<br/><br/></span><span>        for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(num_workers</span><span>*</span><span>indiv_per_worker):<br/></span><span>            p_rews, p_seed </span><span>=</span><span> output_queue.</span><span>get</span><span>()<br/></span><span>            batch_seed.</span><span>append</span><span>(p_seed)<br/></span><span>            batch_return.</span><span>extend</span><span>(p_rews)<br/></span></pre></div>
<p>In the previous snippet, <kbd>output_queue.get()</kbd> gets an element from <kbd>output_queue</kbd>, which is populated by the workers. In our implementation, <kbd>output_queue.get()</kbd> returns two elements. The first element, <kbd>p_rews</kbd>, is the fitness value (the return value) of the agent that's generated using <kbd>p_seed</kbd>, which is given as the second element. </p>
<p>When the <kbd>for</kbd> cycle terminates, we rank the returns and put the batch returns and seeds on the <kbd>params_queue</kbd> queue, which will be read by all the workers to optimize the agent. The code for this is as follows:</p>
<div>
<pre><span>        batch_return </span><span>=</span><span> </span><span>normalized_rank</span><span>(batch_return)<br/><br/></span><span>        for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(num_workers):<br/></span><span>            params_queue.</span><span>put</span><span>([batch_return, batch_seed])</span></pre></div>
<p>Finally, when all the training iterations have been executed, we can terminate the workers:</p>
<div>
<pre><span>    for</span><span> p </span><span>in</span><span> processes:<br/></span><span>        p.</span><span>terminate</span><span>()</span></pre></div>
<p>This concludes the main function. Now, all we need to do is implement the workers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workers</h1>
                </header>
            
            <article>
                
<p>The <span>workers' </span>functionalities are defined in the <kbd>worker</kbd> function, which was previously passed as an argument to <kbd>mp.Process</kbd>. We cannot go through all the code because it'd take too much time and space to explain, but we'll explain the core components here. As always, the full implementation is available in this book's repository on GitHub. So, if you are interested in looking at it in more depth, take the time to examine the code on GitHub. </p>
<p>In the first few lines of <kbd>worker</kbd>, the computational graph is created to run the policy and optimize it. Specifically, the policy is a multi-layer perceptron with <kbd>tanh</kbd> nonlinearities as the activation function. In this case, Adam is used to apply the expected gradient that's computed following the second term of (11.2).</p>
<p>Then, <kbd>agent_op(o)</kbd> and <kbd>evaluation_on_noise(noise)</kbd> <span>are defined. </span>The former runs the policy (or candidate solution) to obtain the action for a given state or observation, <kbd>o</kbd>, and the latter evaluates the new candidate solution that is obtained by adding the perturbation <kbd>noise</kbd> (that has the same shape as the policy) to the current policy's parameters. </p>
<p>Jumping directly to the most interesting part, we create a new session by specifying that it can rely on, at most, 4 CPUs and initialize the global variables. Don't worry if you don't have 4 CPUs available. Setting <span><kbd>allow_soft_placement</kbd> to <kbd>True</kbd> tells TensorFlow to use only the supported devices:</span></p>
<div>
<pre><span>    sess </span><span>=</span><span> tf.</span><span>Session</span><span>(</span><span>config</span><span>=</span><span>tf.</span><span>ConfigProto</span><span>(</span><span>device_count</span><span>=</span><span>{</span><span>'CPU'</span><span>: </span><span>4</span><span>}, allow_soft_placement=True))<br/></span><span>    sess.</span><span>run</span><span>(tf.</span><span>global_variables_initializer</span><span>())</span></pre></div>
<p>Despite using all 4 CPUs, we allocate only one to each worker. In the definition of the computational graph, we set the device on which the computation will be performed. For example, to specify that the worker has to use only CPU 0, you can put the graph inside a <kbd>with</kbd> statement, which defines the device to use:</p>
<div>
<pre><span>with</span><span> tf.</span><span>device</span><span>(</span><span>"/cpu:0"</span><span>):<br/>    # graph to compute on the CPUs 0<br/></span></pre></div>
<p>Going back to our implementation, we can loop forever, or at least until the worker has something to do. This condition is checked later, inside the <kbd>while</kbd> cycle. </p>
<p>An important thing to note is that because we perform many calculations on the weights of the neural network, it is much easier to deal with <span>flattened </span>weights. So, for example, instead of dealing with a list of the form [8,32,32,4], we'll perform computations on a one-dimensional array of length 8*32*32*4. The functions that perform the conversion from the former to the latter, and vice versa, are defined in TensorFlow (take a look at the full implementation on GitHub if you are interested in knowing how this is done).</p>
<p>Also, before starting the <kbd>while</kbd> loop, we retrieve the shape of the <span>flattened </span>agent:</p>
<div>
<pre><span>    agent_flatten_shape </span><span>=</span><span> sess.</span><span>run</span><span>(agent_variables_flatten).shape<br/><br/>    </span><span>while</span><span> </span><span>True</span><span>:</span></pre></div>
<p><span>In the first part</span> of the <kbd>while</kbd> loop, <span>the candidates </span><span>are generated and evaluated. </span><span>The candidate solutions are built by adding a normal perturbation to the weights; that is, <img class="fm-editor-equation" src="assets/35368897-b2dd-4f34-9776-f797818d1a56.png" style="width:3.00em;height:1.08em;"/>. This is done by choosing a new random seed every time, which will uniquely sample the perturbation (or noise), <img class="fm-editor-equation" src="assets/acc67400-71b8-4b04-8fef-43469154d93a.png" style="width:0.75em;height:0.75em;"/>, from a normal distribution. This is a key part of the algorithm because, later, the other workers will have to regenerate the same perturbation from the same seed. After that, the two new offspring (there are two because we are using mirror sampling) are evaluated and the results are put in the <kbd>output_queue</kbd> queue:</span></p>
<div>
<pre><span>        for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(indiv_per_worker):<br/></span><span>            seed </span><span>=</span><span> np.random.</span><span>randint</span><span>(</span><span>1e7</span><span>)<br/><br/></span><span>            with</span><span> </span><span>temp_seed</span><span>(seed):<br/></span><span>                sampled_noise </span><span>=</span><span> np.random.</span><span>normal</span><span>(</span><span>size</span><span>=</span><span>agent_flatten_shape)<br/><br/></span><span>            pos_rew</span><span>=</span><span> </span><span>evaluation_on_noise</span><span>(sampled_noise)<br/></span><span>            neg_rew </span><span>=</span><span> </span><span>evaluation_on_noise</span><span>(</span><span>-</span><span>sampled_noise)<br/><br/></span><span>            output_queue.</span><span>put</span><span>([[pos_rew, neg_rew], seed</span><span>])</span></pre></div>
<p>Note that the following snippet (which we used previously), <span>is just a way to set the NumPy random seed, </span><kbd>seed</kbd><span>, locally:</span></p>
<pre>with temp_seed(seed):<br/>    ..</pre>
<p>Outside the <kbd>with</kbd> statement, the seed that's used to generate random values will not be <kbd>seed</kbd> anymore.</p>
<p>The second part of the <kbd><span>while</span></kbd> loop involves the acquisition of all the returns and seeds, the reconstruction of the perturbations from those seeds, the computation of the stochastic gradient estimate following the formula (11.2), and the policy's optimization. The <kbd>params_queue</kbd> queue is populated by the main process, which we saw earlier. It does this by sending the normalized ranks and seeds of the population that were generated by the workers in the first phase. The code is as follows:</p>
<div>
<pre><span>        batch_return, batch_seed </span><span>=</span><span> params_queue.</span><span>get</span><span>()<br/></span><span>        batch_noise </span><span>=</span><span> []<br/><br/>        # reconstruction of the perturbations used to generate the individuals<br/></span><span>        for</span><span> seed </span><span>in</span><span> batch_seed:<br/></span><span>            with</span><span> </span><span>temp_seed</span><span>(seed):<br/></span><span>                sampled_noise </span><span>=</span><span> np.random.</span><span>normal</span><span>(</span><span>size</span><span>=</span><span>agent_flatten_shape)<br/><br/></span><span>            batch_noise.</span><span>append</span><span>(sampled_noise)<br/></span><span>            batch_noise.</span><span>append</span><span>(</span><span>-</span><span>sampled_noise)<br/><br/>        # Computation of the gradient estimate following the formula (11.2)<br/></span><span>        vars_grads </span><span>=</span><span> np.</span><span>zeros</span><span>(agent_flatten_shape)<br/></span><span>        for</span><span> n, r </span><span>in</span><span> </span><span>zip</span><span>(batch_noise, batch_return):<br/></span><span>            vars_grads </span><span>+=</span><span> n </span><span>*</span><span> r<br/><br/></span><span>        vars_grads </span><span>/=</span><span> </span><span>len</span><span>(batch_noise) </span><span>*</span><span> </span><span>std_noise<br/><br/></span><span>        sess.</span><span>run</span><span>(apply_g, </span><span>feed_dict</span><span>=</span><span>{new_weights_ph:</span><span>-</span><span>vars_grads})</span></pre></div>
<p>The last few lines in the preceding code compute the gradient estimate; that is, they calculate the second term of formula (11.2):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/abe6a754-4493-48b2-a6a4-789975a1a0c0.png" style="width:14.75em;height:4.00em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/6fed24d8-5e14-41bd-809e-e74e3ba4eee5.png" style="width:0.92em;height:0.92em;"/> is the normalized rank of <img class="fm-editor-equation" src="assets/5307ef79-2210-4c5d-96a4-4cbc4d08cb52.png" style="width:0.33em;height:0.83em;"/> and <img class="fm-editor-equation" src="assets/ae2228ad-b5e8-4148-8a27-d20fd14fd478.png" style="width:0.83em;height:0.83em;"/> candidates their perturbation. </p>
<p><kbd>apply_g</kbd> is the operation that applies the <kbd>vars_grads</kbd> gradient (11.3) using Adam. Note that we pass <kbd>-var_grads</kbd> as we want to perform gradient ascent and not gradient descent.</p>
<p>That's all for the implementation. Now, we have to apply it to an environment and test it to see how it performs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying scalable ES to LunarLander</h1>
                </header>
            
            <article>
                
<p>How well will the scalable version of evolution strategies perform in the LunarLander environment? Let's find out!</p>
<p>As you may recall, we already used LunarLander against A2C and REINFORCE in <a href="6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml">Chapter 6</a>, <em>Learning Stochastic and PG optimization</em>. This task consists of landing a lander on the moon through continuous actions. We decided to use this environment for its medium difficulty and to compare the ES results to those that were obtained with A2C.</p>
<p>The hyperparameters that performed the best in this environment are as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign"><strong>Hyperparameter</strong></td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><strong>Variable name</strong></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign"><strong>Value</strong></td>
</tr>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign">Neural network size</td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><kbd>hidden_sizes</kbd></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign">[32, 32]</td>
</tr>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign">Training iterations (or generations)</td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><kbd>number_iter</kbd></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign">200</td>
</tr>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign">Worker's number</td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><kbd>num_workers</kbd></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign">4</td>
</tr>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign">Adam learning rate</td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><kbd>lr</kbd></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign">0.02</td>
</tr>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign">Individuals per worker</td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><kbd>indiv_per_worker</kbd></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign">12</td>
</tr>
<tr>
<td style="width: 30%" class="CDPAlignCenter CDPAlign">Standard deviation</td>
<td style="width: 35.8935%" class="CDPAlignCenter CDPAlign"><kbd>std_noise</kbd></td>
<td style="width: 31.1065%" class="CDPAlignCenter CDPAlign">0.05</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The results are shown in the following graph. What immediately catches your eye is that the curve is very stable and smooth. Furthermore, notice that it reaches an average score of about 200 after 2.5-3 million steps. Comparing the results with those obtained with A2C (in Figure 6.7), you can see that the evolution strategy took almost 2-3 times more steps than A2C and REINFORCE.</p>
<p><span>As demonstrated in the paper, b</span>y using massive parallelization (using at least hundreds of CPUs), you should be able to obtain very good policies in just minutes. Unfortunately, we don't have such computational power. However, if you do, you may want to try it for yourself:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2016 image-border" src="assets/9ee4c3a1-b67b-4d59-918d-73e3d587e9f6.png" style="width:36.58em;height:21.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.5 The performance of scalable evolution strategies</div>
<p>Overall, the results are great and show that ES is a viable solution for very long horizon problems and tasks with very sparse rewards.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned about EAs, a new class of black-box algorithms inspired by biological evolution that can be applied to RL tasks. EAs solve these problems from a different perspective compared to reinforcement learning. You saw that many characteristics that we have to deal with when we design RL algorithms are not valid in evolutionary methods. The differences are in both the intrinsic optimization method and the underlying assumptions. For example, because EAs are black-box algorithms, we can optimize whatever function we want as we are no longer constrained to using differentiable functions, like we were with RL. EAs have many other advantages, as we saw throughout this chapter, but they also have numerous downsides.</p>
<p>Next, we looked at two evolutionary algorithms: genetic algorithms and evolution strategies. Genetic algorithms are more complex as they create offspring from two parents through crossover and mutation. Evolution strategies select the best-performing individuals from a population that has been created only by mutation from the previous generation. The simplicity of ES is one of the key elements that enables the immense scalability of the algorithm across thousands of parallel workers. This scalability has been demonstrated in the paper by OpenAI, showing the ability of ES to perform at the levels of RL algorithms in complex environments. </p>
<p>To get hands-on with evolutionary algorithms, we implemented the scalable evolution strategy from the paper we cited throughout this chapter. Furthermore, we tested it on LunarLander and saw that ES is able to solve the environment with high performance. Though the results are great, ES used two to three times more steps<span> than AC and REINFORCE</span> to learn the task. This is the main drawback of ESes: they need a lot of experience. Despite this, thanks to their capacity to scale linearly to the number of workers, with enough computational power, you might be able to solve this task in a fraction of the time compared to reinforcement learning algorithms. </p>
<p>In the next chapter, we'll go back to reinforcement learning and talk about a problem known as the exploration-exploitation dilemma. We'll see what it is and why it's crucial in online settings. Then, we'll use a potential solution to the problem to develop a meta-algorithm called ESBAS, which chooses the most appropriate algorithm for each situation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are two alternative algorithms to reinforcement learning for solving sequential decision-making problems?</li>
<li>What are the processes that give birth to new individuals in evolutionary algorithms?</li>
<li>What is the source of inspiration for evolutionary algorithms such as genetic algorithms?</li>
<li>How does CMA-ES evolve evolution strategies?</li>
<li>What's one advantage and one disadvantage of evolution strategies?</li>
<li>What's the trick that's used in the <span><em>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</em> paper to reduce the variance?</span></li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><span>To read the original paper of OpenAI that proposed the scalable version of ES, that is, the <em>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</em> paper, go to <a href="https://arxiv.org/pdf/1703.03864.pdf">https://arxiv.org/pdf/1703.03864.pdf</a>.</span></li>
<li>To read the paper that presented NEAT, that is, <em><span>Evolving Neural Networks through</span></em> <span><em>Augmenting Topologies</em>, go to <a href="http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf">http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf</a>.</span></li>
</ul>


            </article>

            
        </section>
    </body></html>