["```\ndef extract_files(*data_dir*,*type* = 'bags'):\n   '''\n :param data_dir: Input directory\n :param type: bags or shoes\n   :return: saves the cropped files to the bags to shoes directory\n   '''\n\n   input_file_dir = os.path.join(os.getcwd(),*data_dir*, \"train\")\n   result_dir = os.path.join(os.getcwd(),*type*)\n   *if not* os.path.exists(result_dir):\n       os.makedirs(result_dir)\n\n   file_names= os.listdir(input_file_dir)\n   *for* file *in* file_names:\n       input_image = Image.open(os.path.join(input_file_dir,file))\n       input_image = input_image.resize([128, 64])\n       input_image = input_image.crop([64, 0, 128, 64])  # Cropping only the colored image. Excluding the edge image\n       input_image.save(os.path.join(result_dir,file))\n```", "```\ndef generator(x, initializer, s*cope_name* = 'generator',*reuse*=*False*):\n   *with* tf.variable_scope(*scope_name*) *as* scope:\n       *if* *reuse*:\n           scope.reuse_variables()\n       conv1 = tf.contrib.layers.conv2d(inputs=*x*, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\",reuse=*reuse*, activation_fn=tf.nn.leaky_relu, weights_initializer=*initializer*,\n                                        scope=\"disc_conv1\")  # 32 x 32 x 32\n       conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv2\")  # 16 x 16 x 64\n       conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv3\")  # 8 x 8 x 128\n       conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv4\")  # 4 x 4 x 256\n\n       deconv1 = tf.contrib.layers.conv2d(conv4, num_outputs=4 * 128, kernel_size=4, stride=1, padding=\"SAME\",\n                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                              weights_initializer=*initializer*, scope=\"gen_conv1\")\n       deconv1 = tf.reshape(deconv1, shape=[tf.shape(*x*)[0], 8, 8, 128])\n\n       deconv2 = tf.contrib.layers.conv2d(deconv1, num_outputs=4 * 64, kernel_size=4, stride=1, padding=\"SAME\",\n                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                              weights_initializer=*initializer*, scope=\"gen_conv2\")\n       deconv2 = tf.reshape(deconv2, shape=[tf.shape(*x*)[0], 16, 16, 64])\n\n       deconv3 = tf.contrib.layers.conv2d(deconv2, num_outputs=4 * 32, kernel_size=4, stride=1, padding=\"SAME\",\n                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                              weights_initializer=*initializer*, scope=\"gen_conv3\")\n       deconv3 = tf.reshape(deconv3, shape=[tf.shape(*x*)[0], 32, 32, 32])\n\n       deconv4 = tf.contrib.layers.conv2d(deconv3, num_outputs=4 * 16, kernel_size=4, stride=1, padding=\"SAME\",\n                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                              weights_initializer=*initializer*, scope=\"gen_conv4\")\n       deconv4 = tf.reshape(deconv4, shape=[tf.shape(*x*)[0], 64, 64, 16])\n\n       recon = tf.contrib.layers.conv2d(deconv4, num_outputs=3, kernel_size=4, stride=1, padding=\"SAME\", \\\n                                            activation_fn=tf.nn.relu, scope=\"gen_conv5\")\n\n       return recon\n\n```", "```\ndef discriminator(*x*,*initializer*, *scope_name* ='discriminator',  *reuse*=*False*):\n   *with* tf.variable_scope(*scope_name*) *as* scope:\n       *if* *reuse*:\n           scope.reuse_variables()\n       conv1 = tf.contrib.layers.conv2d(inputs=*x*, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, weights_initializer=*initializer*,\n                                        scope=\"disc_conv1\")  # 32 x 32 x 32\n       conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv2\")  # 16 x 16 x 64\n       conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv3\")  # 8 x 8 x 128\n       conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv4\")  # 4 x 4 x 256\n       conv5 = tf.contrib.layers.conv2d(inputs=conv4, num_outputs=512, kernel_size=4, stride=2, padding=\"SAME\",\n                                        reuse=*reuse*, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n                                        weights_initializer=*initializer*, scope=\"disc_conv5\")  # 2 x 2 x 512\n       fc1 = tf.reshape(conv5, shape=[tf.shape(*x*)[0], 2 * 2 * 512])\n       fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512, reuse=*reuse*, activation_fn=tf.nn.leaky_relu,\n                                               normalizer_fn=tf.contrib.layers.batch_norm,\n                                               weights_initializer=*initializer*, scope=\"disc_fc1\")\n       fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=*reuse*, activation_fn=tf.nn.sigmoid,\n                                               weights_initializer=*initializer*, scope=\"disc_fc2\")\n       return fc2\n```", "```\ndef define_network(self):\n   # generators\n   # This one is used to generate fake data\n   self.gen_b_fake = generator(self.X_shoes, self.initializer,scope_name=\"generator_sb\")\n   self.gen_s_fake =   generator(self.X_bags, self.initializer,scope_name=\"generator_bs\")\n   # Reconstruction generators\n   # Note that parameters are being used from previous layers\n   self.gen_recon_s = generator(self.gen_b_fake, self.initializer,scope_name=\"generator_sb\",  reuse=*True*)\n   self.gen_recon_b = generator(self.gen_s_fake,  self.initializer, scope_name=\"generator_bs\", reuse=*True*)\n   # discriminator for Shoes\n   self.disc_s_real = discriminator(self.X_shoes,self.initializer, scope_name=\"discriminator_s\")\n   self.disc_s_fake = discriminator(self.gen_s_fake,self.initializer, scope_name=\"discriminator_s\", reuse=*True*)\n   # discriminator for Bags\n   self.disc_b_real = discriminator(self.X_bags,self.initializer,scope_name=\"discriminator_b\")\n   self.disc_b_fake = discriminator(self.gen_b_fake, self.initializer, reuse=*True*,scope_name=\"discriminator_b\")\n```", "```\ndef define_loss(self):\n   # Reconstruction loss for generators\n   self.const_loss_s = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_s, self.X_shoes))\n   self.const_loss_b = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_b, self.X_bags))\n   # generator loss for GANs\n   self.gen_s_loss = tf.reduce_mean(\n       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.ones_like(self.disc_s_fake)))\n   self.gen_b_loss = tf.reduce_mean(\n       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.ones_like(self.disc_b_fake)))\n   # Total generator Loss\n   self.gen_loss =  (self.const_loss_b + self.const_loss_s)  + self.gen_s_loss + self.gen_b_loss\n   # Cross Entropy loss for discriminators for shoes and bags\n   # Shoes\n   self.disc_s_real_loss = tf.reduce_mean(\n       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_real, labels=tf.ones_like(self.disc_s_real)))\n   self.disc_s_fake_loss = tf.reduce_mean(\n       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.zeros_like(self.disc_s_fake)))\n   self.disc_s_loss = self.disc_s_real_loss + self.disc_s_fake_loss  # Combined\n   # Bags\n   self.disc_b_real_loss = tf.reduce_mean(\n       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_real, labels=tf.ones_like(self.disc_b_real)))\n   self.disc_b_fake_loss = tf.reduce_mean(\n       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.zeros_like(self.disc_b_fake)))\n   self.disc_b_loss = self.disc_b_real_loss + self.disc_b_fake_loss\n   # Total discriminator Loss\n   self.disc_loss = self.disc_b_loss + self.disc_s_loss\n```", "```\n*def* define_optimizer(self):\n   self.disc_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.disc_loss, var_list=self.disc_params)\n   self.gen_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.gen_loss, var_list=self.gen_params)\n```", "```\n*def* summary_(self):\n   # Store the losses\n   tf.summary.scalar(\"gen_loss\", self.gen_loss)\n   tf.summary.scalar(\"gen_s_loss\", self.gen_s_loss)\n   tf.summary.scalar(\"gen_b_loss\", self.gen_b_loss)\n   tf.summary.scalar(\"const_loss_s\", self.const_loss_s)\n   tf.summary.scalar(\"const_loss_b\", self.const_loss_b)\n   tf.summary.scalar(\"disc_loss\", self.disc_loss)\n   tf.summary.scalar(\"disc_b_loss\", self.disc_b_loss)\n   tf.summary.scalar(\"disc_s_loss\", self.disc_s_loss)\n\n   # Histograms for all vars\n   *for* var *in* tf.trainable_variables():\n       tf.summary.histogram(var.name, var)\n\n   self.summary_ = tf.summary.merge_all()\n```", "```\nprint (\"Starting Training\")\n*for* global_step *in* range(start_epoch,EPOCHS):\n   shoe_batch = get_next_batch(BATCH_SIZE,\"shoes\")\n   bag_batch = get_next_batch(BATCH_SIZE,\"bags\")\n   feed_dict_batch = {*model*.X_bags: bag_batch, *model*.X_shoes: shoe_batch}\n   op_list = [*model*.disc_optimizer, *model*.gen_optimizer, *model*.disc_loss, *model*.gen_loss, *model*.summary_]\n   _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)\n   shoe_batch = get_next_batch(BATCH_SIZE, \"shoes\")\n   bag_batch = get_next_batch(BATCH_SIZE, \"bags\")\n   feed_dict_batch = {*model*.X_bags: bag_batch, *model*.X_shoes: shoe_batch}\n   _, gen_loss = sess.run([*model*.gen_optimizer, *model*.gen_loss], feed_dict=feed_dict_batch)\n   *if* global_step%10 ==0:\n       train_writer.add_summary(summary_,global_step)\n   *if* global_step%100 == 0:\n       print(\"EPOCH:\" + str(global_step) + \"\\tgenerator Loss: \" + str(gen_loss) + \"\\tdiscriminator Loss: \" + str(disc_loss))\n   *if* global_step % 1000 == 0:\n       shoe_sample = get_next_batch(1, \"shoes\")\n       bag_sample = get_next_batch(1, \"bags\")\n       ops = [*model*.gen_s_fake, *model*.gen_b_fake, *model*.gen_recon_s, *model*.gen_recon_b]\n       gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={*model*.X_shoes: shoe_sample, *model*.X_bags: bag_sample})\n       save_image(global_step, gen_s_fake, str(\"gen_s_fake_\") + str(global_step))\n       save_image(global_step,gen_b_fake, str(\"gen_b_fake_\") + str(global_step))\n       save_image(global_step, gen_recon_s, str(\"gen_recon_s_\") + str(global_step))\n       save_image(global_step, gen_recon_b, str(\"gen_recon_b_\") + str(global_step))\n   *if* global_step % 1000 == 0:\n       *if not* os.path.exists(\"./model\"):\n           os.makedirs(\"./model\")\n       saver.save(sess, \"./model\" + '/model-' + str(global_step) + '.ckpt')\n       print(\"Saved Model\")\nprint (\"Starting Training\")\n*for* global_step *in* range(start_epoch,EPOCHS):\n   shoe_batch = get_next_batch(BATCH_SIZE,\"shoes\")\n   bag_batch = get_next_batch(BATCH_SIZE,\"bags\")\n   feed_dict_batch = {*model*.X_bags: bag_batch, *model*.X_shoes: shoe_batch}\n   op_list = [*model*.disc_optimizer, *model*.gen_optimizer, *model*.disc_loss, *model*.gen_loss, *model*.summary_]\n   _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)\n   shoe_batch = get_next_batch(BATCH_SIZE, \"shoes\")\n   bag_batch = get_next_batch(BATCH_SIZE, \"bags\")\n   feed_dict_batch = {*model*.X_bags: bag_batch, *model*.X_shoes: shoe_batch}\n   _, gen_loss = sess.run([*model*.gen_optimizer, *model*.gen_loss], feed_dict=feed_dict_batch)\n   *if* global_step%10 ==0:\n       train_writer.add_summary(summary_,global_step)\n   *if* global_step%100 == 0:\n       print(\"EPOCH:\" + str(global_step) + \"\\tgenerator Loss: \" + str(gen_loss) + \"\\tdiscriminator Loss: \" + str(disc_loss))\n   *if* global_step % 1000 == 0:\n       shoe_sample = get_next_batch(1, \"shoes\")\n       bag_sample = get_next_batch(1, \"bags\")\n       ops = [*model*.gen_s_fake, *model*.gen_b_fake, *model*.gen_recon_s, *model*.gen_recon_b]\n       gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={*model*.X_shoes: shoe_sample, *model*.X_bags: bag_sample})\n       save_image(global_step, gen_s_fake, str(\"gen_s_fake_\") + str(global_step))\n       save_image(global_step,gen_b_fake, str(\"gen_b_fake_\") + str(global_step))\n       save_image(global_step, gen_recon_s, str(\"gen_recon_s_\") + str(global_step))\n       save_image(global_step, gen_recon_b, str(\"gen_recon_b_\") + str(global_step))\n   *if* global_step % 1000 == 0:\n       *if not* os.path.exists(\"./model\"):\n           os.makedirs(\"./model\")\n       saver.save(sess, \"./model\" + '/model-' + str(global_step) + '.ckpt')\n\n       print(\"Saved Model\")\n\n```"]