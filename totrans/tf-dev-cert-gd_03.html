<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer056">
<h1 class="chapter-number" id="_idParaDest-60"><a id="_idTextAnchor065"/>3</h1>
<h1 id="_idParaDest-61"><a id="_idTextAnchor066"/>Linear Regression with TensorFlow</h1>
<p>In this chapter, we will cover the concept of linear regression and how we can implement it using TensorFlow. We will start by discussing what linear regression is, how it works, its underlying assumptions, and the type of problems that can be solved using it. Next, we will examine the various evaluation metrics used in regression modeling, such as mean squared error, mean absolute error, root mean squared error, and R-squared, and strive to understand how to interpret the results from <span class="No-Break">these metrics.</span></p>
<p>To get hands-on, we will implement linear regression by building a real-world use case where we predict employees’ salaries using various attributes. Here, we will learn in a hands-on fashion how to load and pre-process data, covering important ideas such as handling missing values, encoding categorical variables, and normalizing the data for modeling. Then, we will explore the process of building, compiling, and fitting a linear regression model with TensorFlow, as well as examine concepts such as underfitting and overfitting and their impact on our model’s performance. Before we close this chapter, you will also learn how to save and load a trained model to make predictions on <span class="No-Break">unseen data.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Linear regression <span class="No-Break">with TensorFlow</span></li>
<li>Evaluating <span class="No-Break">regression models</span></li>
<li>Salary prediction <span class="No-Break">with TensorFlow</span></li>
<li>Saving and <span class="No-Break">loading models</span></li>
</ul>
<h1 id="_idParaDest-62"><a id="_idTextAnchor067"/>Technical requirements</h1>
<p>We will use a <strong class="bold">Google Colaboratory</strong> (<strong class="bold">Google Colab</strong>) notebook as our work environment, as it is a free, cloud-based Jupyter notebook that is easy to use, providing us with GPU and TPU backends. For the coding exercises in this chapter, we will use <strong class="source-inline">python &gt;= 3.8.0</strong>, along with the following packages, which can be installed using the <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> command:</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">tensorflow&gt;=2.7.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">tensorflow-datasets==4.4.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pillow==8.4.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pandas==1.3.4</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">numpy==1.21.4</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">scipy==1.7.3</strong></span></li>
</ul>
<h1 id="_idParaDest-63"><a id="_idTextAnchor068"/>Linear regression with TensorFlow</h1>
<p><strong class="bold">Linear regression</strong> is a supervised <a id="_idIndexMarker108"/>machine learning technique that <a id="_idIndexMarker109"/>models the linear relationship between the predicted output variable (dependent variable) and one or more independent variables. When one independent variable can be used to effectively predict the output variable, we have a case of <em class="italic">simple linear regression</em>, which can be represented by the equation <em class="italic">y = wX + b</em>, where <em class="italic">y</em> is the target variable, <em class="italic">X</em> is the input variable, <em class="italic">w</em> is the weight of the feature(s), and <em class="italic">b</em> is <span class="No-Break">the bias.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 3.1 – A plot showing simple linear regression" height="920" src="image/B18118_03_001.jpg" width="1353"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – A plot showing simple linear regression</p>
<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, the straight line, referred to as the regression line (the line of best fit), is the line that optimally models the relationship between <em class="italic">X</em> and <em class="italic">y</em>. Hence, we can use it to determine the dependent variable based on the current value of the independent variable at a certain point on the plot. The objective of linear regression is to find the best values of <em class="italic">w</em> and <em class="italic">b</em>, which model the underlying relationship between <em class="italic">X</em> and <em class="italic">y</em>. The closer the predicted value is to the ground truth, the smaller <span class="No-Break">the error.</span></p>
<p>Conversely, when we have more than one input variable used to predict the output value, then we have a case of <em class="italic">multiple linear regression</em>, and we can represent it by the equation <em class="italic">y = b0 + b1X1 + b2X2 + .... + bnXn</em>, where <em class="italic">y</em> is the target variable, <em class="italic">X1</em>, <em class="italic">X2</em>, ... <em class="italic">Xn</em> are input variables, <em class="italic">b0</em> is the bias, and <em class="italic">b1</em>, <em class="italic">b2</em>, ... <em class="italic">bn</em> are the <span class="No-Break">feature weights.</span></p>
<p>Simple linear and multiple linear regression have lots of real-world applications, as they are simple to implement and computationally cheap. Hence, they can be easily applied to large datasets. However, linear regression may fail when we try to model nonlinear relationships between <em class="italic">X</em> and <em class="italic">y</em>, or when there are many irrelevant features in our <span class="No-Break">input data.</span></p>
<p>Linear regression is widely used to solve a wide range of real-world problems across different domains. For example, we can apply linear regression to predict the price of a house using factors such as the size, number of bedrooms, location, and proximity to social amenities. Similarly, in the <a id="_idIndexMarker110"/>field of <strong class="bold">human resource</strong> (<strong class="bold">HR</strong>), we can use linear regression to predict<a id="_idIndexMarker111"/> the<a id="_idIndexMarker112"/> salary of new hires, based on factors such as years of experience of the candidate and their level of education. These are a few examples of what is possible using linear regression. Next, let us see how we can evaluate linear <span class="No-Break">regression models.</span></p>
<h1 id="_idParaDest-64">Evaluating regression mod<a id="_idTextAnchor069"/>els</h1>
<p>In our <em class="italic">hello world</em> example from <a href="B18118_02.xhtml#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Introduction to TensorFlow</em>, we tried to predict a student’s test score when the student<a id="_idIndexMarker113"/> spent 38 hours studying during the term. Our study model arrived at 81.07 marks, while the true value was 81. So, we were close but not completely correct. When we subtract the difference between our model’s prediction and the ground truth, we get a residual of 0.07. The residual value could be either positive or negative, depending on whether our model overestimates or underestimates the predicted result. When we take the absolute value of the residual, we eliminate any negative signs; hence, the absolute error will always be a positive value, irrespective of whether the residual is positive <span class="No-Break">or negative.</span></p>
<p>The formula for absolute error is <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">r</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">u</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span></p>
<p>where <span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span> = the predicted value and <span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span> = the <span class="No-Break">ground truth.</span></p>
<p>The <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>) of a <a id="_idIndexMarker114"/>model is the average of all absolute errors of the data points under consideration. MAE measures the average of the residuals and can be represented using the <span class="No-Break">following equation:</span></p>
<p><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">r</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">u</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span></p>
<p><span class="No-Break">where:</span></p>
<ul>
<li><em class="italic">n</em> = the number of data points <span class="No-Break">under consideration</span></li>
<li>∑ = summation of the absolute errors of all <span class="No-Break">the observations</span></li>
<li><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base">|</span> = <span class="No-Break">Absolute value</span></li>
</ul>
<p>If the MAE = 0, it means that <span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span>. This means the model is 100 percent accurate; although<a id="_idIndexMarker115"/> this is an ideal scenario, it is highly unlikely. On the flip side, if MAE= ∞, this means the model is completely off, as it fails to capture any relationship between the input and output variables. The larger the error, the larger the value of the MAE. For performance evaluation, we aim for low values of MAE, but because MAE is a relative metric whose value depends on the scale of the data you work with, it is difficult to compare MAE results across <span class="No-Break">different datasets.</span></p>
<p>Another important evaluation<a id="_idIndexMarker116"/> metric is the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). MSE, in contrast to MAE, squares the residuals, thus removing any negative values in the residuals. MSE is represented using the <span class="No-Break">following equation:</span></p>
<p><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">r</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">u</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>Like MAE, when there are no residuals, we have a perfect model. So, the lower the MSE value, the better the performance of the model. Unlike MAE, where large or small errors have a proportional impact, MSE penalizes larger errors in comparison to smaller errors, and it has a higher order of units, since we square the residual in <span class="No-Break">this instant.</span></p>
<p>Another useful metric in regression <a id="_idIndexMarker117"/>modeling is the <strong class="bold">root mean square error</strong> (<strong class="bold">RMSE</strong>). As the name suggests, it is the square root of MSE, as shown in <span class="No-Break">the equation:</span></p>
<p><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">r</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">u</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">_______________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>Lastly, let us look<a id="_idIndexMarker118"/> at the <strong class="bold">coefficient of determination</strong> (<strong class="bold">R squared</strong>). R<span class="superscript">2</span> measures how well the<a id="_idIndexMarker119"/> dependent variable is explained by the independent variables in a regression modeling task. We can calculate R<span class="superscript">2</span> with <span class="No-Break">this equation:</span></p>
<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>where R<span class="subscript">res </span>is the sum of the square of residuals and R<span class="subscript">tot</span> is the total sum of squares. The closer the value of R<span class="superscript">2 </span>is to 1, the more accurate the model is, and the closer the R<span class="superscript">2  </span>value of a model is to 0, the worse the model is. Also, it is possible for R² to take on a negative value. This happens when the model does not follow the trend of the data – in this instance, R<span class="subscript">res</span> is greater than R<span class="subscript">tot</span>. A negative R<span class="superscript">2 </span>is a sign that our model requires significant improvement due to its <span class="No-Break">poor performance.</span></p>
<p>We have looked at some regression evaluation metrics. The good news is that we will not work them out by hand; we will leverage the <strong class="source-inline">tf.keras.metrics</strong> module from TensorFlow to help us do the heavy lifting. We have breezed quickly through the theory at a high level. Now, let us examine a multiple linear regression case study to enable us to understand all the moving parts required to build a model with TensorFlow, as well as understand how to evaluate, save, load, and use our trained model to make predictions on new data. Let’s proceed to our <span class="No-Break">case study.</span></p>
<h1 id="_idParaDest-65"><a id="_idTextAnchor070"/>Salary prediction with TensorFlow</h1>
<p>In this case study, you<a id="_idIndexMarker120"/> will assume the role of a new machine learning engineer at Tensor Limited, a rapidly growing start-up with over 200 employees. Now, the company wants to hire seven new employees, and the HR department is having a hard time coming up with the ideal salary based on varying qualifications, years of experience, the roles applied for, and the level of training of each of the potential new hires. Your job is to work with the HR unit to determine the optimal salary for each of these <span class="No-Break">potential hires.</span></p>
<p>Luckily, we went through the machine learning life cycle in <a href="B18118_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Machine Learning,</em> built our hello world case study in <a href="B18118_02.xhtml#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Introduction to TensorFlow,</em> and have already covered some key evaluation metrics required for regression modeling in this chapter. So, you are well equipped theoretically to carry out the task. You have had a productive discussion with the HR manager, and now you have a better understanding of the task and the requirements. You defined your task as a supervised learning task (regression). Also, the HR unit allowed you to download employee records and their corresponding salaries for this task. Now that you have the dataset, let us proceed to load the data into <span class="No-Break">our notebook.</span></p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor071"/>Loading the data</h2>
<p>Perform<a id="_idIndexMarker121"/> the following steps to load <span class="No-Break">the dataset:</span></p>
<ol>
<li>Open the notebook called <strong class="source-inline">Linear_Regression_with_TensorFlow.ipynb</strong>. We will start by importing all the necessary libraries for <span class="No-Break">this project:</span><pre class="source-code">
# import tensorflow</pre><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow import keras</pre><pre class="source-code">
from tensorflow.keras import Sequential</pre><pre class="source-code">
from tensorflow.keras.layers import Dense</pre><pre class="source-code">
print(tf.__version__)</pre></li>
</ol>
<p>We will run this code block. If everything goes well, we will get to see the version of TensorFlow we <span class="No-Break">are using:</span></p>
<pre class="source-code">
2.12.0</pre>
<ol>
<li value="2">Next, we will import some additional libraries that will help us simplify <span class="No-Break">our workflow:</span><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
import seaborn as sns</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import MinMaxScaler</pre></li>
</ol>
<p>We will run this cell, and everything should work perfectly. NumPy is a scientific computing<a id="_idIndexMarker122"/> library in Python that is used to perform mathematical operations on arrays, while pandas is a built-in Python library for data analysis and manipulation. Matplotlib and Seaborn are used to visualize data, and we will use sklearn for data preprocessing and splitting our data. We will apply these libraries in this case study, and you will get to understand what they do and also be able to apply them in your exam <span class="No-Break">and beyond.</span></p>
<ol>
<li value="3">Now, we will proceed to load the dataset, which we got from the HR team for <span class="No-Break">this project:</span><pre class="source-code">
#Loading from the course GitHub account</pre><pre class="source-code">
df=pd.read_csv('https://raw.githubusercontent.com/oluwole-packt/datasets/main/salary_dataset.csv')</pre><pre class="source-code">
df.head()</pre></li>
</ol>
<p>We will use pandas to generate a DataFrame that holds the record in a tabular format, and we will use <strong class="source-inline">df.head()</strong> to print the first five entries in <span class="No-Break">the data:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 3.2 – A DataFrame showing a snapshot of our dataset" height="236" src="image/B18118_03_002.jpg" width="1039"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – A DataFrame showing a snapshot of our dataset</p>
<p>We now have a sense of what data was collected, based on the details captured in each column. We will <a id="_idIndexMarker123"/>proceed to explore the data to see what we can learn and how we can effectively develop a solution to meet the business objective. Let us proceed by looking at <span class="No-Break">data pre-processing.</span></p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor072"/>Data preprocessing</h2>
<p>To be able to model <a id="_idIndexMarker124"/>our data, we need to ensure it is in the right form (i.e., numerical values). Also, we will need to deal with missing values and remove irrelevant features. In the real world, data preprocessing takes a long time. You will hear this repeatedly, and it is true. Without correctly shaping the data, we cannot model it. Let’s jump in and see how we can do this for our current task. From the DataFrame, we can immediately see that there are some irrelevant columns, and they hold personally identifiable information about employees. So, we will remove and also inform HR <span class="No-Break">about this:</span></p>
<pre class="source-code">
#drop irrelevant columns
df =df.drop(columns =['Name', 'Phone_Number',
    'Date_Of_Birth'])
df.head()</pre>
<p>We will use the <strong class="source-inline">drop</strong> function in pandas to drop the name, phone number, and date of birth columns. We will now display the DataFrame again using <strong class="source-inline">df.head()</strong> to show the first five rows of <span class="No-Break">the data:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<img alt="Figure 3.3 – The first five rows of the DataFrame after dropping the columns" height="233" src="image/B18118_03_003.jpg" width="594"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – The first five rows of the DataFrame after dropping the columns</p>
<p>We have successfully removed the irrelevant columns, so we can now proceed and check for missing values in our dataset using the <strong class="source-inline">isnull()</strong> function <span class="No-Break">in pandas:</span></p>
<pre class="source-code">
#check the data for any missing values
df.isnull().sum()</pre>
<p>When we run this<a id="_idIndexMarker125"/> code block, we can see that there are no missing values in the <strong class="source-inline">University</strong> and <strong class="source-inline">Salary</strong> columns. However, we have missing values for the <strong class="source-inline">Role</strong>, <strong class="source-inline">Cert</strong>, <strong class="source-inline">Qualification</strong>, and <span class="No-Break"><strong class="source-inline">Experience</strong></span><span class="No-Break"> columns:</span></p>
<pre class="source-code">
Experience       2
Qualification    1
University       0
Role             3
Cert             2
Salary           0
dtype: int64</pre>
<p>There are a number of ways to handle missing values – from simply asking HR to fix the omissions to simple imputations or replacements using mean, median, or mode. In this case study, we will drop the rows with missing values, since it’s a small subset of <span class="No-Break">our data:</span></p>
<pre class="source-code">
#drop the null values
df=df.dropna()</pre>
<p>We use the <strong class="source-inline">dropna</strong> function to drop all the missing values in the dataset, and then we save the new dataset <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If you want to learn more about how to handle missing values, check out this playlist by Data <span class="No-Break">Scholar: </span><a href="https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u"><span class="No-Break">https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u</span></a><span class="No-Break">.</span></p>
<p>Now, we need to check to ensure that there are no more missing values using the <span class="No-Break"><strong class="source-inline">isnull()</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
#check for null values
df.isnull().sum()</pre>
<p>Run the code, and <a id="_idIndexMarker126"/>let’s see whether there are any <span class="No-Break">missing values:</span></p>
<pre class="source-code">
Experience       0
Qualification    0
University       0
Role             0
Cert             0
Salary           0
dtype: int64</pre>
<p>We can see that there are no missing values in our dataset anymore. Our model requires us to pass in numerical values for it to be able to model our data and predict the target variable, so let us look at the <span class="No-Break">data types:</span></p>
<pre class="source-code">
df.dtypes</pre>
<p>When we run the code, we get an output showing the different columns and their <span class="No-Break">data types:</span></p>
<pre class="source-code">
Experience       float64
Qualification     object
University        object
Role              object
Cert              object
Salary             int64
dtype: object</pre>
<p>From the output, we can see that experience and salary are numeric values, since they are <strong class="source-inline">float</strong> and <strong class="source-inline">int</strong>, respectively, while <strong class="source-inline">Qualification</strong>, <strong class="source-inline">University</strong>, <strong class="source-inline">Role</strong>, and <strong class="source-inline">Cert</strong> are categorical values. This means we cannot train our model yet; we have to find a way to convert our categorical values to numerical values. Luckily, this is possible via a process called on<a id="_idTextAnchor073"/>e-hot encoding. <strong class="bold">One-hot encoding</strong> is a<a id="_idIndexMarker127"/> technique in which we convert the categorical variables in our data to individual columns representing each of the categories. We <a id="_idIndexMarker128"/>will use the <strong class="source-inline">get_dummies</strong> function in pandas to <span class="No-Break">achieve this:</span></p>
<pre class="source-code">
#Converting categorical variables to numeric values
df = pd.get_dummies(df, drop_first=True)
df.head()</pre>
<p>When we run the code, we will get a DataFrame like the one displayed in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em>. We use the <strong class="source-inline">drop_first</strong> argument to drop the <span class="No-Break">first category.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<img alt="Figure 3.4 – A DataFrame showing numerical values" height="230" src="image/B18118_03_004.jpg" width="1231"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – A DataFrame showing numerical values</p>
<p>If you are confused as to why we dropped one of the categorical columns, let’s look at the <strong class="source-inline">Cert</strong> column, which was made up of yes or no. values If we performed one hot encoding, without dropping any columns, we would have two <strong class="source-inline">Cert</strong> columns, as displayed in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em>. In the <strong class="source-inline">Cert_No</strong> column, if the employee has a relevant certification, the column gets a value of <strong class="source-inline">0</strong>, and when the employee does not have a relevant certification, the column gets a value of <strong class="source-inline">1</strong>. Looking at the <strong class="source-inline">Cert_Yes</strong> column, we can see that when an employee has a certificate, the column gets a value of <strong class="source-inline">1</strong>; otherwise, it <span class="No-Break">gets </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<img alt="Figure 3.5 – The dummy variables from the Cert column" height="233" src="image/B18118_03_005.jpg" width="177"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The dummy variables from the Cert column</p>
<p>From <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em>, we can see that both columns can be used to show whether an employee has a <a id="_idIndexMarker129"/>certificate or not. Using both dummy columns generated from our certificate column will lead to the <em class="italic">dummy variable trap</em>. This occurs when our one-hot encoded columns are strongly related or correlated, where one column can effectively explain the other column. Hence, we say both columns are multicollinear, and <em class="italic">multicollinearity</em> can lead to the overfitting of our model. We will talk more about overfitting in <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Image Classification with </em><span class="No-Break"><em class="italic">Neural Networks</em></span><span class="No-Break">.</span></p>
<p>For now, it is good enough to know that overfitting is a situation where our model performs very well on training data but poorly on test data. To avoid the dummy variable trap, we will drop one of the columns in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em>. If there are three categories, we only need two columns to capture all three categories; if we have four categories, we will only need three columns to capture four categories, and so on. Hence, we can drop the extra columns for all the other categorical columns <span class="No-Break">as well.</span></p>
<p>Now, we will use the <strong class="source-inline">corr()</strong> function to get the correlation of our <span class="No-Break">refined dataset:</span></p>
<pre class="source-code">
df.corr()</pre>
<p>We can see that there is a strong correlation between salary and years of experience. Also, there is a strong correlation between <strong class="source-inline">Role_Senior</strong> and <strong class="source-inline">Salary</strong>, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<img alt="Figure 3.6 – The correlation values for our data" height="395" src="image/B18118_03_006.jpg" width="1389"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – The correlation values for our data</p>
<p>We have completed the preprocessing phase of our task, or at least for now. We have removed all irrelevant columns; we also removed the missing values by dropping rows with missing values and, finally, used one-hot encoding to convert our categorical values to numeric values. It is important to note that we are skipping some <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) steps here, such <a id="_idIndexMarker130"/>as visualizing the data; although that’s an essential step, our core focus for the exam is building models with TensorFlow. In our Colab notebook, you will<a id="_idIndexMarker131"/> find some additional EDA steps; although they are not directly relevant to the exams, they will give you a better understanding of your data and help you <span class="No-Break">detect anomalies.</span></p>
<p>Now, let us move on to the <span class="No-Break">modeling phase.</span></p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor074"/>Model building</h2>
<p>To build a model, we <a id="_idIndexMarker132"/>will have to sort our data into features (<em class="italic">X</em>) and the target (<em class="italic">y</em>). To do this, we will run this <span class="No-Break">code block:</span></p>
<pre class="source-code">
# We split the attributes and labels into X and y variables
X = df.drop("Salary", axis=1)
y = df["Salary"]</pre>
<p>We will use the <strong class="source-inline">drop()</strong> function to drop the <strong class="source-inline">Salary</strong> column from the <strong class="source-inline">X</strong> variable, and we will make the <strong class="source-inline">y</strong> variable the <strong class="source-inline">Salary</strong> column alone, since this is <span class="No-Break">our target.</span></p>
<p>With our features and target variable well defined, we can proceed to split our data into training and test sets. This step is important, as it enables our model to learn patterns from our data to effectively predict employees’ salaries. To achieve this, we train our model using the training set and then evaluate the model’s efficacy on the hold-out test set. We discussed this at a high level in <a href="B18118_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><em class="italic">, Introduction to Machine Learning,</em> when we talked about the ML life cycle. It is a very important process, as we will use the test set to evaluate our model’s generalization capability before we deploy it for real-world use. To split our data into training and testing sets, we will use the <span class="No-Break"><strong class="source-inline">sklearn</strong></span><span class="No-Break"> library:</span></p>
<pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.2, random_state=10)</pre>
<p>Using the <strong class="source-inline">train_test_split</strong> function from the <strong class="source-inline">sklearn</strong> library, we split our data into training and testing datasets, with a test size of <strong class="source-inline">0.2</strong>. We set the <strong class="source-inline">random_state =10</strong> to ensure reproducibility so that every time you use the same <strong class="source-inline">random_state</strong> value, you’ll get the same split, even if you run the code multiple times. For instance, in our code, we set <strong class="source-inline">random_state</strong> to <strong class="source-inline">10</strong>, which means every time we run the code, we will get the same <a id="_idIndexMarker133"/>split. If we change this value from <strong class="source-inline">10</strong> to, say, <strong class="source-inline">50</strong>, we will get a different shuffled split for our training and test set. Setting the <strong class="source-inline">random_state</strong> argument when splitting our data into training and test sets is very useful, as it allows us to effectively compare different models, since our training set and test sets are the same across all the models we <span class="No-Break">experiment with.</span></p>
<p>When modeling our data in machine learning, we usually use 80 percent of the data to train the model and 20 percent of the data to test the model’s generalization capability. That’s why we set <strong class="source-inline">test_size</strong> to <strong class="source-inline">0.2</strong> for our dataset. Now that we have everything in place, we will start the modeling process in earnest. When it comes to building models with TensorFlow, there are three essential steps, as illustrated in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.7</em> – building the model, compiling the model, and fitting it to <span class="No-Break">our data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<img alt="Figure 3.7 – The three-step modeling process" height="201" src="image/B18118_03_007.jpg" width="1657"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – The three-step modeling process</p>
<p>Let us see how we can use this three-step approach to build our salary prediction model. We will begin by building <span class="No-Break">our model:</span></p>
<pre class="source-code">
#create a model using the Keras API
Model_1 = Sequential([Dense(units=1, activation='linear',
    input_shape=[len(X_<a id="_idTextAnchor075"/>train.columns)])])</pre>
<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em>, we<a id="_idIndexMarker134"/> can see the first line of code for our model. Here, we generated a single layer using the <strong class="source-inline">Sequential</strong> class as an array. The <strong class="source-inline">Sequential</strong> class is used for layer definition. The <strong class="source-inline">Dense</strong> function is used to generate a layer of fully connected neuron. In this case, we have just one unit. For our activation function here, we employ a linear activation function. Activation functions are used to determine the output of a neuron based on a given input or set of inputs. Here, the linear activation function simply outputs whatever the input is – that is, a direct relationship between the input and the output. Next, we pass in the input shape of our data, which in this case is 8, representing the features in data (columns) <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">X_train</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<img alt="Figure 3.8 – Building a model in TensorFlow" height="511" src="image/B18118_03_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Building a model in TensorFlow</p>
<p>In the first step of our three-step process, we designed the model structure. Now, we will move on to the model compilation step. This step is equally important as it determines how the model will learn. Here, we specify parameters such as the loss function, the optimizer, and the metrics we want to use to evaluate <span class="No-Break">our model.</span></p>
<p>The optimizer determines how our model will update its internal parameters, based on the information it gathers from the loss function and the data. The job of the loss function is to measure how well our model does on our training data. We then use our metrics to monitor the model’s performance on the training step and test steps. Here, we use <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) as our <a id="_idIndexMarker135"/>optimizer and MAE for our loss and <span class="No-Break">evaluation metric:</span></p>
<pre class="source-code">
#compile the model
Model_1.compile(loss=tf.keras.losses.mae,
    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])</pre>
<p>Now, all we have to<a id="_idIndexMarker136"/> do is feed our model with training data and the corresponding labels, with which our model can learn to intelligently predict the target numerical values, which in our case is the expected salary. Every time the model makes a prediction, the loss function compares the difference between the model’s prediction and the ground truth. This information is passed to the optimizer, which uses the information to make an improved prediction until the model can fashion the right mathematical equation to accurately predict our <span class="No-Break">employee’s salary.</span></p>
<p>Now, let’s fit our <span class="No-Break">training model:</span></p>
<pre class="source-code">
#Fit the model
model_1.fit(X_train, y_train, epochs =50)</pre>
<p>We use <strong class="source-inline">model_1.fit</strong> to fit our training data and labels and set the number of tries (epochs) to <strong class="source-inline">50</strong>. In just a few lines of code, we have generated a mini-brain that we can train over time to make sensible predictions. Let’s run the code and see what the output <span class="No-Break">looks like:</span></p>
<pre class="source-code">
Epoch 46/50
6/6 [==============================] - 0s 9ms/step - loss: 97378.0391 - mae: 97378.0391
Epoch 47/50
6/6 [==============================] - 0s 4ms/step - loss: 97377.2500 - mae: 97377.2500
Epoch 48/50
6/6 [==============================] - 0s 4ms/step - loss: 97376.4609 - mae: 97376.4609
Epoch 49/50
6/6 [==============================] - 0s 3ms/step - loss: 97375.6484 - mae: 97375.6484
Epoch 50/50
6/6 [==============================] - 0s 3ms/step - loss: 97374.8516 - mae: 97374.8516</pre>
<p>We have displayed the<a id="_idIndexMarker137"/> last five tries (epochs <strong class="source-inline">46</strong>–<strong class="source-inline">50</strong>). The error drops gradually; however, we end up with a very large error after <strong class="source-inline">50</strong> epochs. Perhaps we can train our model for more epochs, as we did in <a href="B18118_02.xhtml#_idTextAnchor045"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Introduction to TensorFlow</em>. <span class="No-Break">Why not?</span></p>
<pre class="source-code">
#create a model using the Keras API
model_2 = Sequential([Dense(units=1, activation='linear',
    input_shape=[len(X_train.columns)])])
#compile the model
model_2.compile(loss=tf.keras.losses.mae,
    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])
#Fit the model
history=model_2.fit(X_train, y_train, epochs =500)</pre>
<p>Now, we simply change the number of epochs to <strong class="source-inline">500</strong> using our single-layer model. The activation function, the loss, and optimizers are the same as our <span class="No-Break">initial model:</span></p>
<pre class="source-code">
Epoch 496/500
6/6 [==============================] - 0s 3ms/step - loss: 97014.8516 - mae: 97014.8516
Epoch 497/500
6/6 [==============================] - 0s 2ms/step - loss: 97014.0391 - mae: 97014.0391
Epoch 498/500
6/6 [==============================] - 0s 3ms/step - loss: 97013.2500 - mae: 97013.2500
Epoch 499/500
6/6 [==============================] - 0s 3ms/step - loss: 97012.4453 - mae: 97012.4453
Epoch 500/500
6/6 [==============================] - 0s 3ms/step - loss: 97011.6<a id="_idTextAnchor076"/>484 - mae: 97011.6484</pre>
<p>From the last <a id="_idIndexMarker138"/>five lines of our output, we can see that the loss is still quite high after <strong class="source-inline">500</strong> epochs. You may wish to experiment with the model for longer epochs to see how it will fare. It is also a good idea to visualize your model’s loss curve to see how it performs. A lower loss indicates a better-performing model. With this in mind, let us explore the loss curve <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">model_2</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
def visualize_model(history, ymin=None, ymax=None):
    # Lets visualize our model
    print(history.history.keys())
    # Lets plot the loss
    plt.plot(history.history['loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Number of epochs')
    plt.ylim([ymin,ymax]) # To zoom in on the y-axis
    plt.legend(['loss plot'], loc='upper right')
    plt.show()</pre>
<p>We will generate a utility plotting function, <strong class="source-inline">visualize_model</strong>, which we will use in our experiments to plot the model’s loss over time as it trains. In this code, we generate a figure to plot the loss values stored in the <strong class="source-inline">history</strong> object. The <strong class="source-inline">history</strong> object is the output of the <strong class="source-inline">fit</strong> function in our three-step modeling process, and it holds the loss and metrics values at the<a id="_idIndexMarker139"/> end of <span class="No-Break">each epoch.</span></p>
<p>To plot <strong class="source-inline">model_2</strong>, we simply call the function to visualize the plot and pass <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">history_2</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
visualize_model(history_2)</pre>
<p>When we run the code, we get the plot shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<img alt="Figure 3.9 – Model losses at 500" height="447" src="image/B18118_03_009.jpg" width="583"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Model losses at 500</p>
<p>From <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.9</em>, we can see the loss falling, and the rate at which it falls is too slow, as it takes <strong class="bold">500</strong> epochs to fall from around <strong class="bold">97400</strong> to <strong class="bold">97000</strong>. In your spare time, you can try to train the model for 2,000 or more epochs. It will not be able to generalize well, as the model is too <a id="_idIndexMarker140"/>simple to handle the complexity of our data. In machine learning lingo, we say the model <span class="No-Break">is </span><span class="No-Break"><em class="italic">underfitting</em></span><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">There are primarily two ways to build models with TensorFlow – the sequential API and the functional API. The sequential API is a simple way of building models by using a stack of layers, where data flows in a single direction, from the input layer to the output layer. Conversely, the functional API in TensorFlow allows us to build more complex models – this includes models with multiple inputs or outputs and models with shared layers. Here, we use the Sequential API. For more information about building models with the sequential API, check out the <span class="No-Break">documentation: </span><a href="https://www.tensorflow.org/guide/keras/sequential_model"><span class="No-Break">https://www.tensorflow.org/guide/keras/sequential_model</span></a><span class="No-Break">.</span></p>
<p>Hence, let us try to build a more complex model and see whether we can push the loss lower and quicker than our <span class="No-Break">initial model:</span></p>
<pre class="source-code">
#Set random set
tf.random.set_seed(10)
#create a model
model_3 =Sequential([
    Dense(units=64, activation='relu',
    input_shape=[len(X_train.columns)]),
    Dense(units=1)
    ])
#compile the model
model_3.compile(loss="mae", optimizer="SGD",
    metrics = ['mae'])
#Fit the model
history_3 =model_3.fit(X_train, y_train, epochs=500)</pre>
<p>Here, we have generated a new model. We stack a 64-neuron layer on top of our single-unit layer. We <a id="_idIndexMarker141"/>also use a <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>) activation function for this layer; its job is to help our model learn more <a id="_idIndexMarker142"/>complex patterns in our data and improve computational efficiency. The second layer is our output layer, made up of a single neuron because we have a regression task (predicting a continuous value). Let’s run it for 500 epochs and see whether this will make <span class="No-Break">any difference:</span></p>
<pre class="source-code">
Epoch 496/500
6/6 [==============================] - 0s 3ms/step - loss: 3651.6785 - mae: 3651.6785
Epoch 497/500
6/6 [==============================] - 0s 3ms/step - loss: 3647.4753 - mae: 3647.4753
Epoch 498/500
6/6 [==============================] - 0s 3ms/step - loss: 3722.4863 - mae: 3722.4863
Epoch 499/500
6/6 [==============================] - 0s 3ms/step - loss: 3570.9023 - mae: 3570.9023
Epoch 500/500
6/6 [==============================] - 0s 3ms/step - loss: 3686.0293 - mae: 3686.0293</pre>
<p>From the last five lines of our output, we can see that there is a significant drop in our loss to<a id="_idIndexMarker143"/> around <strong class="source-inline">3686</strong>. Let’s also plot the loss curve to get a visual understanding <span class="No-Break">as well.</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<img alt="Figure 3.10 – Model losses after 500 epochs" height="445" src="image/B18118_03_010.jpg" width="589"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Model losses after 500 epochs</p>
<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.10</em>, we can see that our model’s loss has fallen below our lowest recorded loss. This is a massive improvement in comparison to our previous model. However, this is not a desired result, nor does it look like the type of result we would like to present to the HR team. This is because, with this model, if an employee earns $50,000, the model could predict either around $46,300 as the employee’s salary, which would make them unhappy, or $53,700 as the employee’s salary, in which case the HR team will not be happy. So, we need to figure out how to improve <span class="No-Break">our result.</span></p>
<p>Let’s zoom into the plot to have a better understanding of what is happening with <span class="No-Break">our model:</span></p>
<pre class="source-code">
visualize_model(history_3, ymin=0, ymax=10000)</pre>
<p>When we <a id="_idIndexMarker144"/>run the code, it returns the plot shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 3.11 – Model losses after 500 epochs when we zoom into the plot" height="446" src="image/B18118_03_011.jpg" width="581"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Model losses after 500 epochs when we zoom into the plot</p>
<p>From the plot in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.11</em>, we can see that the loss falls sharply and settles at around the 100th epoch, and nothing significant seems to happen afterward. Hence, training the model for longer just as we did in our previous model may not be the optimal solution. What can we do to improve <span class="No-Break">our model?</span></p>
<p>Perhaps we can add another layer? Let’s do that and see what our results look like. As we initially pointed out, our job requires a lot of experimentation; only then can we learn how to do things better <span class="No-Break">and faster:</span></p>
<pre class="source-code">
#Set random set
tf.random.set_seed(10)
#create a model
model_4 =Sequential([
    Dense(units=64, activation='relu',
        input_shape=[len(X_train.columns)]),
    Dense(units=64, activation='relu'),
        Dense(units=1)
    ])
#compile the model
model_4.compile(loss="mae", optimizer="SGD",
    metrics = "mae")
#fit the model
history_4 =model_4.fit(X_train, y_train, epochs=500)</pre>
<p>Here, we added<a id="_idIndexMarker145"/> another dense layer of <strong class="source-inline">64</strong> neurons. Note that we also use ReLU as the activation <span class="No-Break">function here:</span></p>
<pre class="source-code">
Epoch 496/500
6/6 [==============================] - 0s 3ms/step - loss: 97384.4141 - mae: 97384.4141
Epoch 497/500
6/6 [==============================] - 0s 3ms/step - loss: 97384.3516 - mae: 97384.3516
Epoch 498/500
6/6 [==============================] - 0s 3ms/step - loss: 97384.3047 - mae: 97384.3047
Epoch 499/500
6/6 [==============================] - 0s 3ms/step - loss: 97384.2422 - mae: 97384.2422
Epoch 500/500
6/6 [==============================] - 0s 3ms/step - loss: 97384.1797 - mae: 97384.1797</pre>
<p>We display only the last five epochs, and we can see the loss is around <strong class="source-inline">97384</strong>, which is worse than the results achieved in <strong class="source-inline">model_3</strong>. So, how do we know how many layers to use in our modeling process? The answer is by experimenting. We use trial and error, backed by our understanding of what the results look like. We can decide whether we need to add more layers, as we did initially when the model was underfitting. And should the model get so complex that it masters the training data well but does not generalize well on our test (hold-out) data, it is said to be <em class="italic">overfitting</em> in machine <span class="No-Break">learning lingo.</span></p>
<p>Now that we have tried smaller and larger models, we cannot yet say we have achieved a suitable result, and <a id="_idIndexMarker146"/>the HR manager has checked in on us to find out how we are doing in terms of the prediction modeling task. So far, we did some research, as all ML engineers do, and we discovered a very important step that we can try out. What step? <span class="No-Break">Let’s see.</span></p>
<h3>Normalization</h3>
<p><strong class="bold">Normalization</strong> is a technique<a id="_idIndexMarker147"/> applied to<a id="_idIndexMarker148"/> input features to ensure they are of a consistent scale, usually between 0 and 1. This process helps our model to converge faster and more accurately. It is worth noting that we should apply normalization after completing other data preprocessing steps, such as handling <span class="No-Break">missing values.</span></p>
<p>It’s good practice to know that improving your model output can also rely strongly on your data preparation process. Hence, let us apply this here. We will take a step back from model building and look at our features after we converted all the columns to <span class="No-Break">numerical values:</span></p>
<pre class="source-code">
X.describe()</pre>
<p>We will use the <strong class="source-inline">describe</strong> function to get vital statistics of our data. This information shows us that most of the columns have a minimum value of 0 and a maximum value of 1, but the experience column is of a different scale, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 3.12 – A statistical summary of the dataset (before normalization)" height="350" src="image/B18118_03_012.jpg" width="1222"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – A statistical summary of the dataset (before normalization)</p>
<p>Why does this matter, you may ask? When the scale of our data is different, our model will arbitrarily attach more importance to columns with higher values, which could affect the model’s ability to predict our target correctly. To resolve this issue, we will use normalization to scale the data between 0 and 1 to bring all our features to the same scale, thereby giving <a id="_idIndexMarker149"/>every feature an equal <a id="_idIndexMarker150"/>chance when our model begins to learn how they relate to our <span class="No-Break">target (</span><span class="No-Break"><em class="italic">y</em></span><span class="No-Break">).</span></p>
<p>To normalize our data, we will use the following equation to <span class="No-Break">scale it:</span></p>
<p><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>where <em class="italic">X</em> is our data, <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span> is the minimum value of <em class="italic">X</em>, and <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">x</span> is the maximum value of <em class="italic">X</em>. In our case study, the minimum value of <em class="italic">X</em> in the <strong class="source-inline">Experience</strong> column is 1, and the maximum value of <em class="italic">X</em> in the <strong class="source-inline">Experience</strong> column is 7. The good news is that we can easily implement this step using the <strong class="source-inline">MinMaxScaler</strong> function from the <strong class="source-inline">sklearn</strong> library. Let’s see how to scale our <span class="No-Break">data next:</span></p>
<pre class="source-code">
# create a scaler object
scaler = MinMaxScaler()
# fit and transform the data
X_norm = pd.DataFrame(scaler.fit_transform(X),
    columns=X.columns)
X_norm.describe()</pre>
<p>Let’s use the <strong class="source-inline">describe()</strong> function to view the key statistics again, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 3.13 – A statistical summary of the dataset (after normalization)" height="351" src="image/B18118_03_013.jpg" width="1222"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – A statistical summary of the dataset (after normalization)</p>
<p>Now, all our data is of<a id="_idIndexMarker151"/> the same scale. So, we have<a id="_idIndexMarker152"/> successfully implemented normalization of our data in just a few lines <span class="No-Break">of code.</span></p>
<p>Now, we split our data into training and testing sets, but this time, we use our normalized <em class="italic">X</em> (<strong class="source-inline">X_norm</strong>) in <span class="No-Break">the code:</span></p>
<pre class="source-code">
# Create training and test sets with the normalized data (X_norm)
X_train, X_test, y_train, y_test = train_test_split(X_norm,
    y,  test_size=0.2, random_state=10)</pre>
<p>Now, we use our best-performing model (<strong class="source-inline">model_3</strong>) from the initial experiments we have done so far. Let’s see how our model performs <span class="No-Break">after normalization:</span></p>
<pre class="source-code">
#create a model
model_5 =Sequential([
    Dense(units=64, activation='relu',
        input_shape=[len(X_train.columns)]),
    Dense(units=64, activation ="relu"),
        Dense(units=1)
    ])
#compile the model
model_5.compile(loss="mae",
    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])
history_5 =model_5.fit(X_train, y_train, epochs=1000)</pre>
<p>The <a id="_idIndexMarker153"/>output is <a id="_idIndexMarker154"/><span class="No-Break">as follows:</span></p>
<pre class="source-code">
Epoch 996/1000
6/6 [==============================] - 0s 4ms/step - loss: 1459.2953 - mae: 1459.2953
Epoch 997/1000
6/6 [==============================] - 0s 4ms/step - loss: 1437.8248 - mae: 1437.8248
Epoch 998/1000
6/6 [==============================] - 0s 3ms/step - loss: 1469.3732 - mae: 1469.3732
Epoch 999/1000
6/6 [==============================] - 0s 4ms/step - loss: 1433.6071 - mae: 1433.6071
Epoch 1000/1000
6/6 [==============================] - 0s 3ms/step - loss: 1432.2891 - mae: 1432.2891</pre>
<p>From the results, we can see that MAE has reduced by more than half in comparison to the results we got without <span class="No-Break">applying normalization.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="Figure 3.14 – The zoomed-in loss curve for model_5" height="446" src="image/B18118_03_014.jpg" width="574"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – The zoomed-in loss curve for model_5</p>
<p>Also, if you look at the loss plot for <strong class="source-inline">model_5</strong> in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14</em>, you can see the loss fails to drop significantly after around the 100th epoch. Instead of guessing how many epochs are ideal to<a id="_idIndexMarker155"/> train the model, how <a id="_idIndexMarker156"/>about we set a rule to stop training when the model fails to improve its performance? Also, we can see that <strong class="source-inline">model_5</strong> doesn’t give us the result we want; perhaps now is a good time to try out a bigger model, in which we train it for longer and set a rule to stop training once it fails to improve its performance on the <span class="No-Break">training data:</span></p>
<pre class="source-code">
#create a model
model_6 =Sequential([
    Dense(units=64, activation='relu',
        input_shape=[len(X_train.columns)]),
        Dense(units=64, activation ="relu"), Dense(units=1)
    ])
#compile the model
model_6.compile(loss="mae",
    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])
#fit the model
early_stop=keras.callbacks.EarlyStopping(monitor='loss',
    patience=10)
history_6 =model_6.fit(
    X_train, y_train, epochs=1000, callbacks=[early_stop])</pre>
<p>Here, we use a three-layer model; the first two layers are made up of 64 neurons and the output layer has a single neuron. To set the rule to stop training, we use <em class="italic">early stopping</em>; this additional parameter is applied when we fit our model on the data to stop training when the model loss fails to improve after 10 epochs. This is achieved by specifying the metric <a id="_idIndexMarker157"/>to monitor loss<a id="_idIndexMarker158"/> and setting <strong class="source-inline">patience</strong> to <strong class="source-inline">10</strong>. Early stopping is also a great technique to prevent overfitting, as it stops training when the model fails to improve; we will discuss this further in <a href="B18118_06.xhtml#_idTextAnchor129"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Improving the Model</em>. Let’s look at the <span class="No-Break">result now:</span></p>
<pre class="source-code">
Epoch 25/1000
6/6 [==============================] - 0s 3ms/step - loss: 84910.6953 - mae: 84910.6953
Epoch 26/1000
6/6 [==============================] - 0s 3ms/step - loss: 81037.8516 - mae: 81037.8516
Epoch 27/1000
6/6 [==============================] - 0s 3ms/step - loss: 72761.0078 - mae: 72761.0078
Epoch 28/1000
6/6 [==============================] - 0s 3ms/step - loss: 81160.6562 - mae: 81160.6562
Epoch 29/1000
6/6 [==============================] - 0s 3ms/step - loss: 70687.3125 - mae: 70687.3125</pre>
<p>Although we set our training for <strong class="source-inline">1000</strong> epochs, our <strong class="source-inline">Earlystopping</strong> callback halted the training process on the 29th epoch because it observed no meaningful drop in the loss. Although the result here isn’t great, we have used <strong class="source-inline">EarlyStopping</strong> to save a considerable amount of computational resources and time. Perhaps now is a good time to try out a different optimizer. For this next exper<a id="_idTextAnchor077"/>iment, let’s use the Adam optimizer. Adam is another<a id="_idIndexMarker159"/> popular optimizer <a id="_idIndexMarker160"/>that is used in deep learning, due to its ability to adaptively control the learning rate for each parameter in a model, which accelerates the <span class="No-Break">model’s convergence:</span></p>
<pre class="source-code">
#create a model
model_7 =Sequential([
    Dense(units=64, activation='relu',
        input_shape=[len(X_train.columns)]),
    Dense(units=64, activation ="relu"),
    Dense(units=1)
    ])
#compile the model
model_7.compile(loss="mae", optimizer="Adam",
    metrics ="mae")
#fit the model
early_stop=keras.callbacks.EarlyStopping(monitor='loss',
    patience=10)
history_7 =model_7.fit(
    X_train, y_train, epochs=1000, callbacks=[early_stop])</pre>
<p>Note we only changed the optimizer to Adam in our compile step. Let’s see the result of this change in <span class="No-Break">the optimizer:</span></p>
<pre class="source-code">
Epoch 897/1000
6/6 [==============================] - 0s 4ms/step - loss: 30.4748 - mae: 30.4748
Epoch 898/1000
6/6 [==============================] - 0s 4ms/step - loss: 19.4643 - mae: 19.4643
Epoch 899/1000
6/6 [==============================] - 0s 3ms/step - loss: 17.0965 - mae: 17.0965
Epoch 900/1000
6/6 [==============================] - 0s 3ms/step - loss: 18.5009 - mae: 18.5009
Epoch 901/1000
6/6 [==============================] - 0s 3ms/step - loss: 15.5516 - mae: 15.5516</pre>
<p>By just changing <a id="_idIndexMarker161"/>the <a id="_idIndexMarker162"/>optimizer, we have recorded an incredible drop in loss. Also, note that we did not need the entire <strong class="source-inline">1000</strong> epochs, as training ended on <strong class="source-inline">901</strong> epochs. Let us add another layer; perhaps we will see an <span class="No-Break">improved performance:</span></p>
<pre class="source-code">
#create a model
model_8 =Sequential([
    Dense(units=64, activation='relu',
        input_shape=[len(X_train.columns)]),
    Dense(units=64, activation ="relu"),
    Dense(units=64, activation ="relu"),
    Dense(units=1)
    ])
#compile the model
model_8.compile(loss="mae", optimizer="Adam",
    metrics ="mae")
#fit the model
early_stop=keras.callbacks.EarlyStopping(monitor='loss',
    patience=10)
history_8 =model_8.fit(
    X_train, y_train, epochs=1000, callbacks=[early_stop])</pre>
<p>Here, we added <a id="_idIndexMarker163"/>an<a id="_idIndexMarker164"/> extra layer with <strong class="source-inline">64</strong> neurons, with ReLU as the activation function. Everything else is <span class="No-Break">the same:</span></p>
<pre class="source-code">
Epoch 266/1000
6/6 [==============================] - 0s 4ms/step - loss: 73.3237 - mae: 73.3237
Epoch 267/1000
6/6 [==============================] - 0s 4ms/step - loss: 113.9100 - mae: 113.9100
Epoch 268/1000
6/6 [==============================] - 0s 4ms/step - loss: 257.4851 - mae: 257.4851
Epoch 269/1000
6/6 [==============================] - 0s 4ms/step - loss: 149.9819 - mae: 149.9819
Epoch 270/1000
6/6 [==============================] - 0s 4ms/step - loss: 179.7796 - mae: 179.7796</pre>
<p>Training stops at 270 epochs; although our model is more complex, it doesn’t perform better than <strong class="source-inline">model_7</strong> on training. We <a id="_idIndexMarker165"/>have <a id="_idIndexMarker166"/>tried out different ideas while experimenting; now, let us try out all eight models on the test set and <span class="No-Break">evaluate them.</span></p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor078"/>Model evaluation</h2>
<p>To evaluate our <a id="_idIndexMarker167"/>models, we will write a function to apply the <strong class="source-inline">evaluate</strong> metrics to all <span class="No-Break">eight models:</span></p>
<pre class="source-code">
def eval_testing(model):
    return model.evaluate(X_test, y_test)
models = [model_1, model_2, model_3, model_4, model_5,
    model_6, model_7, model_8]
for x in models:
    eval_testing(x)</pre>
<p>We will generate an <strong class="source-inline">eval_testing(model)</strong> function that takes a model as an argument and uses the <strong class="source-inline">evaluate</strong> method to evaluate the performance of the model on our test dataset. Looping through the list of models, the code returns the loss and MAE values for all eight models for our <span class="No-Break">test data:</span></p>
<pre class="source-code">
2/2 [==============================] - 0s 8ms/step - loss: 100682.4609 - mae: 100682.4609
2/2 [==============================] - 0s 8ms/step - loss: 100567.9453 - mae: 100567.9453
2/2 [==============================] - 0s 10ms/step - loss: 17986.0801 - mae: 17986.0801
2/2 [==============================] - 0s 9ms/step - loss: 100664.0781 - mae: 100664.0781
2/2 [==============================] - 0s 6ms/step - loss: 1971.4187 - mae: 1971.4187
2/2 [==============================] - 0s 11ms/step - loss: 5831.1250 - mae: 5831.1250
2/2 [==============================] - 0s 7ms/step - loss: 5.0099 - mae: 5.0099
2/2 [==============================] - 0s 26ms/step - loss: 70.2970 - mae: 70.2970</pre>
<p>After we evaluate the models, we can that see <strong class="source-inline">model_7</strong> has the lowest loss. Let’s see how it does on <a id="_idIndexMarker168"/>our test set by using it to <span class="No-Break">make predictions.</span></p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor079"/>Making predictions</h2>
<p>Now that we are done <a id="_idIndexMarker169"/>with experimenting and have evaluated the models, let’s use <strong class="source-inline">model_7</strong> to predict our test set salaries and see how they compare with the ground truth. To do this, we will use the <span class="No-Break"><strong class="source-inline">predict()</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
#Let's make predictions on our test data
y_preds=model_7.predict(X_test).flatten()
y_preds</pre>
<p>After we run this code block, we get the output in an array, as <span class="No-Break">shown here:</span></p>
<pre class="source-code">
2/2 [==============================] - 0s 9ms/step
array([ 64498.64 , 131504.89 , 116491.73 ,  72500.13 , 102983.836,
        60504.645,  84503.36 , 119501.664, 112497.734,  63501.168,
        77994.87 ,  84497.16 , 112497.734,  90980.625,  87499.88 ,
       100502.234, 135498.88 , 112491.53 , 119501.664, 131504.89 ,
       108990.31 , 117506.63 ,  80503.16 , 123495.66 , 112497.734,
       117506.63 , 111994.03 ,  78985.125, 135498.88 , 129502.125,
       117506.64 , 119501.664, 100502.234, 113506.43 , 101987.38 ,
       113506.43 ,  93990.555,  65496.2  ,  61494.906, 107506.17 ,
       105993.77 , 106502.5  ,  72493.94 , 135498.88 ,  67501.37 ,
       107506.17 , 117506.63 ,  70505.1  ,  57500.906], dtype=float32)</pre>
<p>For clarity, let’s build a DataFrame with the model’s prediction and ground truth. This should be fun and somewhat magical when you see how good our model <span class="No-Break">has become:</span></p>
<pre class="source-code">
#Let's make a DataFrame to compare our prediction with the ground truth
df_predictions = pd.DataFrame({'Ground_Truth': y_test, 
    'Model_prediction': y_preds}, columns=['Ground_Truth', 
    'Model_prediction']) df_predictions[
    'Model_prediction']= df_predictions[
    'Model_prediction'].astype(int)</pre>
<p>Here, we generate<a id="_idIndexMarker170"/> two columns and convert the model’s prediction from <strong class="source-inline">float</strong> to <strong class="source-inline">int</strong>, just to keep it in scope with the ground truth. Ready for <span class="No-Break">the result?</span></p>
<p>We will use the <strong class="source-inline">head</strong> function to print out the first 10 values of the <span class="No-Break">test set:</span></p>
<pre class="source-code">
#Let's look at the top 10 data points in the test set
df_predictions.sample(10)</pre>
<p> We then see our results, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="Figure 3.15 – A DataFrame showing the actual values, predictions made by the model, and the resulting residuals" height="433" src="image/B18118_03_015.jpg" width="403"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – A DataFrame showing the actual values, predictions made by the model, and the resulting residuals</p>
<p>Our model has achieved something impressive; it is really close to the initial salaries in our test data. Now, you<a id="_idIndexMarker171"/> can show the HR manager your amazing result. We must save the model so that we can load it and make predictions anytime we want. Let’s learn <a id="_idTextAnchor080"/>how to do <span class="No-Break">this next.</span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor081"/>Saving and loading models</h2>
<p>The beauty of <a id="_idIndexMarker172"/>TensorFlow is the ease with which we can do complex stuff. To save a model, we just need one line <span class="No-Break">of code:</span></p>
<pre class="source-code">
#Saving the model in one line of code
Model7.save('salarypredictor.h5')
#Alternate method is
#model7.save('salarypredictor')</pre>
<p>You can save it as <strong class="source-inline">your_model.h5</strong> or <strong class="source-inline">your_model</strong>; either way works. TensorFlow recommends the <strong class="source-inline">SavedModel</strong> approach because it is language-agnostic, which makes it easy to deploy on various platforms. In this format, we can save the model and its individual components, such as the weights and variables. Conversely, the HDF5 format saves the complete model structure, its weights, and the training configurations as a single file. This approach gives us greater flexibility to share and distribute models; however, for deployment purposes, it’s not the preferred method. When we run the code, we can see the saved model on the left-hand panel in our Colab notebook, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">.</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<img alt="Figure 3.16 – A snapshot of our saved model" height="663" src="image/B18118_03_016.jpg" width="1537"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – A snapshot of our saved model</p>
<p>Now that we have <a id="_idIndexMarker173"/>saved the model, it is wise to test it out by reloading it and testing it. Let’s do that. Also, it’s just one line of code to load <span class="No-Break">the model:</span></p>
<pre class="source-code">
#loading the model
saved_model =tf.keras.models.load_model("/content/salarypredictor.h5")</pre>
<p>Let’s try out our <strong class="source-inline">saved_model</strong> and see whether it will work as well as <strong class="source-inline">model_7</strong>. We will generate <strong class="source-inline">y_pred</strong> again and generate a DataFrame, using <strong class="source-inline">y_test</strong> and <strong class="source-inline">y_pred</strong> as we did earlier checking first the 10 random samples from our <span class="No-Break">test data:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="Figure 3.17 – A DataFrame showing the actual values and predictions made by the saved model" height="430" src="image/B18118_03_017.jpg" width="348"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – A DataFrame showing the actual values and predictions made by the saved model</p>
<p>From the results in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.17</em>, we can see that our saved model performs at a high level. Now, you can deliver your result to the HR manager, and they should be excited about your <a id="_idIndexMarker174"/>results. Let’s imagine that the HR manager wants you to use your model to predict the salary of the new hires. Let’s do <span class="No-Break">that next:</span></p>
<pre class="source-code">
#Putting everything into a function for our big task
def salary_predictor(df):
    df_hires= df.drop(columns=['Name', 'Phone_Number',
        'Date_Of_Birth' ])
    df_hires = pd.get_dummies(df_hires, drop_first=True)
    X_norm = pd.DataFrame(scaler.fit_transform(df_hires),
        columns=df.columns)
    y_preds=saved_model.predict(X_norm).flatten()
    df_predictions = pd.DataFrame({ 'Model_prediction':
        y_preds}, columns=[ 'Model_prediction'])
    df_predictions['Model_prediction']= df_predictions[
        'Model_prediction'].astype(int)
    df['Salary']=df_predictions['Model_prediction']
    return df</pre>
<p>We generate a function using our saved model. We simply wrap all the steps we’ve covered so far into the function, and we return a DataFrame. Now, let’s read in the data of our <span class="No-Break">new hires:</span></p>
<pre class="source-code">
#Load the data
df_new=pd.read_csv('https://raw.githubusercontent.com/oluwole-packt/datasets/main/new_hires.csv')
df_new</pre>
<p>When we run the <a id="_idIndexMarker175"/>code block, we can see their data in a DataFrame, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.18</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="Figure 3.18 – A DataFrame showing the new hires" height="307" src="image/B18118_03_018.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – A DataFrame showing the new hires</p>
<p>Now, we pass the data into the function we generated to get the predicted salaries for our <span class="No-Break">new hires:</span></p>
<pre class="source-code">
#Lets see how much
salary_predictor(df_new)</pre>
<p>We pass <strong class="source-inline">df_new</strong> into the salary prediction function, and we get a new DataFrame, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.19</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<img alt="Figure 3.19 – A DataFrame showing the new hires with their predicted salaries" height="309" src="image/B18118_03_019.jpg" width="1019"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – A DataFrame showing the new hires with their predicted salaries</p>
<p>Finally, we have achieved our goal. HR is happy, the new hires are happy, and everyone in the company thinks you are a magician. Perhaps a pay raise could be on the table, but while you bask in<a id="_idIndexMarker176"/> the euphoria around your first success, your manager returns with another task. This time, it is a classification task, which we will look at this in the next chapter. For now, <span class="No-Break">good job!</span></p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor082"/>Summary</h1>
<p>In this chapter, we took a deeper dive into supervised learning, with a focus on regression modeling. Here, we discussed the difference between simple and multiple linear regression and looked at some important evaluation metrics for regression modeling. Then, we rolled up our sleeves on our case study, helping our company build a working regression model to predict the salaries of new employees. We carried out some data preprocessing steps and saw the importance of normalization in our <span class="No-Break">modeling process.</span></p>
<p>At the end of the case study, we successfully built a salary prediction model, evaluated the model on our test set, and mastered how to save and load models for use at a later stage. Now, you can confidently build a regression model <span class="No-Break">with TensorFlow.</span></p>
<p>In the next chapter, we’ll take a look at <span class="No-Break">classification modeling.</span></p>
<h1 id="_idParaDest-73"><a id="_idTextAnchor083"/>Questions</h1>
<p>Let’s test what we learned in <span class="No-Break">this chapter.</span></p>
<ol>
<li>What is <span class="No-Break">linear regression?</span></li>
<li>What is the difference between simple and multiple <span class="No-Break">linear regression?</span></li>
<li>What evaluation metric penalizes large errors in <span class="No-Break">regression modeling?</span></li>
<li>Use the salary dataset to <span class="No-Break">forecast salaries.</span></li>
</ol>
<h1 id="_idParaDest-74"><a id="_idTextAnchor084"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Amr, T., 2020. <em class="italic">Hands-On Machine Learning with scikit-learn and Scientific Python Toolkits</em>. [S.l.]: <span class="No-Break">Packt Publishing.</span></li>
<li>Raschka, S. and Mirjalili, V., 2019. <em class="italic">Python Machine Learning</em>. 3rd ed. <span class="No-Break">Packt Publishing.</span></li>
<li><em class="italic">TensorFlow </em><span class="No-Break"><em class="italic">documen</em></span><span class="No-Break"><em class="italic">t</em></span><span class="No-Break"><em class="italic">ation</em></span><span class="No-Break">: </span><a href="https://www.TensorFlow.org/guide"><span class="No-Break">https://www.TensorFlow.org/guide</span></a><span class="No-Break">.</span></li>
</ul>
</div>
</div></body></html>