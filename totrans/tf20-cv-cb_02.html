<html><head></head><body>
		<div id="_idContainer028">
			<h1 id="_idParaDest-51"><em class="italic"><a id="_idTextAnchor053"/>Chapter 2</em>: Performing Image Classification</h1>
			<p>Computer vision is a vast field that takes inspiration from many places. Of course, this means that its applications are wide and varied. However, the biggest breakthroughs over the past decade, especially in the context of deep learning applied to visual tasks, have occurred in a particular domain known as <strong class="bold">image classification</strong>.</p>
			<p>As the name suggests, image classification consists of the process of discerning what's in an image based on its visual content. Is there a dog or a cat in this image? What number is in this picture? Is the person in this photo smiling or not? </p>
			<p>Because image classification is such an important and pervasive task in deep learning applied to computer vision, the recipes in this chapter will focus on the ins and outs of classifying images using TensorFlow 2.x. </p>
			<p>We'll cover the following recipes:</p>
			<ul>
				<li>Creating a binary classifier to detect smiles</li>
				<li>Creating a multi-class classifier to play Rock Paper Scissors</li>
				<li>Creating a multi-label classifier to label watches</li>
				<li>Implementing ResNet from scratch</li>
				<li>Classifying images with a pre-trained network using the Keras API</li>
				<li>Classifying images with a pre-trained network using TensorFlow Hub</li>
				<li>Using data augmentation to improve performance with the Keras API</li>
				<li>Using data augmentation to improve performance with the tf.data and tf.image APIs</li>
			</ul>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor054"/>Technical requirements</h1>
			<p>Besides a working installation of TensorFlow 2.x, it's highly recommended to have access to a GPU, given that some of the recipes are very resource-intensive, making the use of a CPU an inviable option. In each recipe, you'll find the steps and dependencies needed to complete it in the <em class="italic">Getting ready</em> section. Finally, the code shown in this chapter is available in full here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3bOjqnU">https://bit.ly/3bOjqnU</a></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor055"/><a id="_idTextAnchor056"/>Creating a binary classifier to detect smiles</h1>
			<p>In its most basic <a id="_idIndexMarker066"/>form, image classification consists of discerning between two classes, or signaling the presence or absence of some trait. In this recipe, we'll implement a binary classifier that tells us whether a person in a photo is smiling. </p>
			<p>Let's begin, shall we?</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor057"/>Getting ready</h2>
			<p>You'll need to install <strong class="source-inline">Pillow</strong>, which is very easy with <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>We'll use the<a id="_idTextAnchor058"/><a id="_idTextAnchor059"/> <strong class="source-inline">SMILEs</strong> dataset, located here:<a id="_idTextAnchor060"/><a id="_idTextAnchor061"/> <a href="https://github.com/hromi/SMILEsmileD">https://github.com/hromi/SMILEsmileD</a>. Clone or download a zipped version of the repository to a location of your preference. In this recipe, we assume the data is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">SMILEsmileD-master</strong>:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B14768_02_001.jpg" alt="Figure 2.1 – Positive (left) and negative (right) examples"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Positive (left) and negative (right) examples</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor062"/>How to do it…</h2>
			<p>Follow these steps to train a smile classifier from scratch on the <strong class="source-inline">SMILEs</strong> dataset:</p>
			<ol>
				<li>Import all necessary packages:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define a <a id="_idIndexMarker067"/>function to load the images and labels from a list of file paths:<p class="source-code">def load_images_and_labels(image_paths):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, target_size=(32,32), </p><p class="source-code">                         color_mode='grayscale')</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        label = 'positive' in label</p><p class="source-code">        label = float(label)</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p><p>Notice that we are loading the images in grayscale, and we're encoding the labels by checking<a id="_idIndexMarker068"/> whether the word <em class="italic">positive</em> is in the file path of the image.</p></li>
				<li>Define a function to build the neural network. This model's structure is based on <strong class="bold">LeNet</strong> (you can find<a id="_idIndexMarker069"/> a link to LeNet's paper in the <em class="italic">See also</em> section):<p class="source-code">def build_network():</p><p class="source-code">    input_layer = Input(shape=(32, 32, 1))</p><p class="source-code">    x = Conv2D(filters=20,</p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(input_layer)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.4)(x)</p><p class="source-code">    x = Conv2D(filters=50,</p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.4)(x)</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=500)(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = Dropout(0.4)(x)</p><p class="source-code">    output = Dense(1, activation='sigmoid')(x)</p><p class="source-code">    model = Model(inputs=input_layer, outputs=output)</p><p class="source-code">    return model</p><p>Because this <a id="_idIndexMarker070"/>is a binary classification problem, a single Sigmoid-activated neuron is enough in the output layer.</p></li>
				<li>Load the image paths into a list:<p class="source-code">files_pattern = (pathlib.Path.home() / '.keras' / </p><p class="source-code">                 'datasets' /</p><p class="source-code">                 'SMILEsmileD-master' / 'SMILEs' / '*' </p><p class="source-code">                    / '*' / </p><p class="source-code">                 '*.jpg')</p><p class="source-code">files_pattern = str(files_pattern)</p><p class="source-code">dataset_paths = [*glob.glob(files_pattern)]</p></li>
				<li>Use the <strong class="source-inline">load_images_and_labels()</strong> function defined previously to load the dataset into memory:<p class="source-code">X, y = load_images_and_labels(dataset_paths)</p></li>
				<li>Normalize the images and compute the number of positive, negative, and total examples in <a id="_idIndexMarker071"/>the dataset:<p class="source-code">X /= 255.0</p><p class="source-code">total = len(y)</p><p class="source-code">total_positive = np.sum(y)</p><p class="source-code">total_negative = total - total_positive</p></li>
				<li>Create train, test, and validation subsets of the data:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                     stratify=y,</p><p class="source-code">                                     random_state=999)</p><p class="source-code">(X_train, X_val,</p><p class="source-code"> y_train, y_val) = train_test_split(X_train, y_train,</p><p class="source-code">                                    test_size=0.2,</p><p class="source-code">                                    stratify=y_train,</p><p class="source-code">                                    random_state=999)</p></li>
				<li>Instantiate the model and compile it:<p class="source-code">model = build_network()</p><p class="source-code">model.compile(loss='binary_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Train the model. Because the dataset is unbalanced, we are assigning weights to each class<a id="_idIndexMarker072"/> proportional to the number of positive and negative images in the dataset:<p class="source-code">BATCH_SIZE = 32</p><p class="source-code">EPOCHS = 20</p><p class="source-code">model.fit(X_train, y_train,</p><p class="source-code">          validation_data=(X_val, y_val),</p><p class="source-code">          epochs=EPOCHS,</p><p class="source-code">          batch_size=BATCH_SIZE,</p><p class="source-code">          class_weight={</p><p class="source-code">              1.0: total / total_positive,</p><p class="source-code">              0.0: total / total_negative</p><p class="source-code">          })</p></li>
				<li>Evaluate the model on the test set:<p class="source-code">test_loss, test_accuracy = model.evaluate(X_test, </p><p class="source-code">                                          y_test)</p></li>
			</ol>
			<p>After 20 epochs, the network should get around 90% accuracy on the test set. In the following section, we'll explain the previous steps.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor063"/>How it works…</h2>
			<p>We just trained a network to determine whether a person is smiling or not in a picture. Our first big task was to take the images in the dataset and load them into a format suitable for our neural network. Specifically, the <strong class="source-inline">load_image_and_labels()</strong> function is in charge of loading an image in grayscale, resizing it to 32x32x1, and then converting it into a <strong class="source-inline">numpy</strong> array. To extract the label, we looked at the containing folder of each image: if it contained the word positive, we encoded the label as 1; otherwise, we encoded it as 0 (a trick we used here was casting a Boolean as a float, like this: <strong class="source-inline">float(label)</strong>). </p>
			<p>Next, we built the neural network, which is inspired by the LeNet architecture. The biggest takeaway here is that because this is a binary classification problem, we can use a single Sigmoid-activated neuron to discern between the two classes.</p>
			<p>We then took 20% of the images to comprise our test set, and from the remaining 80% we took an <a id="_idIndexMarker073"/>additional 20% to create our validation set. With these three subsets in place, we proceeded to train the network over 20 epochs, using <strong class="source-inline">binary_crossentropy</strong> as our loss function and <strong class="source-inline">rmsprop</strong> as the optimizer. </p>
			<p>To account for the imbalance in the dataset (out of the 13,165 images, only 3,690 contain smiling people, while the remaining 9,475 do not), we passed a <strong class="source-inline">class_weight</strong> dictionary where we assigned a weight conversely proportional to the number of instances of each class in the dataset, effectively forcing the model to pay more attention to the 1.0 class, which corresponds to <em class="italic">smile</em>.</p>
			<p>Finally, we achieved around 90.5% accuracy on the test set.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor064"/>See also</h2>
			<p>For more information on the <strong class="source-inline">SMILEs</strong> dataset, you can visit the official GitHub repository <a id="_idIndexMarker074"/>here: <a href="https://github.com/hromi/SMILEsmileD">https://github.com/hromi/SMILEsmileD</a>. You can read the LeNet paper here (it's pretty long, though): <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a>.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor065"/>Creating a multi-class classifier to play rock paper scissors</h1>
			<p>More often <a id="_idIndexMarker075"/>than not, we are<a id="_idIndexMarker076"/> interested in categorizing an image into more than two classes. As we'll see in this recipe, implementing a neural network to differentiate between many categories is fairly straightforward, and what better way to demonstrate this than by training a model that can play the widely known Rock Paper Scissors game? </p>
			<p>Are you ready? Let's dive in!</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor066"/>Getting ready</h2>
			<p>We'll use the <strong class="source-inline">Rock-Paper-Scissors Images</strong> dataset, which is hosted on Kaggle at the following location: <a href="https://www.kaggle.com/drgfreeman/rockpaperscissors">https://www.kaggle.com/drgfreeman/rockpaperscissors</a>. To download it, you'll need a Kaggle account, so sign in or sign up accordingly. Then, unzip the dataset in a location of your preference. In this recipe, we assume the unzipped folder is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">rockpaperscissors</strong>. </p>
			<p>Here are some sample images:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B14768_02_002.jpg" alt="Figure 2.2 – Example images of rock (left), paper (center), and scissors (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Example images of rock (left), paper (center), and scissors (right)</p>
			<p>Let's begin implementing.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor067"/>How to do it…</h2>
			<p>The following steps <a id="_idIndexMarker077"/>explain how to train a multi-class <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) to distinguish between the three classes of the Rock Paper Scissors game:</p>
			<ol>
				<li value="1">Import the required packages:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">import glob</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import CategoricalCrossentropy</p></li>
				<li>Define<a id="_idIndexMarker078"/> a list with the<a id="_idIndexMarker079"/> three classes, and also an alias to <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong>, which we'll use later:<p class="source-code">CLASSES = ['rock', 'paper', 'scissors']</p><p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p>The values in <strong class="source-inline">CLASSES</strong> match the names of the directories that contain the images for each class.</p></li>
				<li>Define a function to load an image and its label, given its file path:<p class="source-code">def load_image_and_label(image_path, target_size=(32, 32)):</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_jpeg(image, channels=3)</p><p class="source-code">    image = tf.image.rgb_to_grayscale(image)</p><p class="source-code">    image = tf.image.convert_image_dtype(image, </p><p class="source-code">                                         np.float32)</p><p class="source-code">    image = tf.image.resize(image, target_size)</p><p class="source-code">    label = tf.strings.split(image_path,os.path.sep)[-2]</p><p class="source-code">    label = (label == CLASSES)  # One-hot encode.</p><p class="source-code">    label = tf.dtypes.cast(label, tf.float32)</p><p class="source-code">    return image, label</p><p>Notice that<a id="_idIndexMarker080"/> we are one-hot <a id="_idIndexMarker081"/>encoding by comparing the name of the folder that contains the image (extracted from <strong class="source-inline">image_path</strong>) with the <strong class="source-inline">CLASSES</strong> list.</p></li>
				<li>Define a function to build the network architecture. In this case, it's a very simple and shallow one, which is enough for the problem we are solving:<p class="source-code">def build_network():</p><p class="source-code">    input_layer = Input(shape=(32, 32, 1))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = Dropout(rate=0.5)(x)</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=3)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(inputs=input_layer, outputs=output)</p></li>
				<li>Define a function to, given a path to a dataset, return a <strong class="source-inline">tf.data.Dataset</strong> instance<a id="_idIndexMarker082"/> of images and<a id="_idIndexMarker083"/> labels, in batches and optionally shuffled:<p class="source-code">def prepare_dataset(dataset_path,</p><p class="source-code">                    buffer_size,</p><p class="source-code">                    batch_size,</p><p class="source-code">                    shuffle=True):</p><p class="source-code">    dataset = (tf.data.Dataset</p><p class="source-code">               .from_tensor_slices(dataset_path)</p><p class="source-code">               .map(load_image_and_label,</p><p class="source-code">                    num_parallel_calls=AUTOTUNE))</p><p class="source-code">    if shuffle:</p><p class="source-code">        dataset.shuffle(buffer_size=buffer_size)</p><p class="source-code">    dataset = (dataset</p><p class="source-code">               .batch(batch_size=batch_size)</p><p class="source-code">               .prefetch(buffer_size=buffer_size))</p><p class="source-code">    return dataset</p></li>
				<li>Load the image paths into a list:<p class="source-code">file_patten = (pathlib.Path.home() / '.keras' / </p><p class="source-code">               'datasets' /</p><p class="source-code">               'rockpaperscissors' / 'rps-cv-images' / </p><p class="source-code">                 '*' /</p><p class="source-code">               '*.png')</p><p class="source-code">file_pattern = str(file_patten)</p><p class="source-code">dataset_paths = [*glob.glob(file_pattern)]</p></li>
				<li>Create <a id="_idIndexMarker084"/>train, test, and<a id="_idIndexMarker085"/> validation subsets of image paths:<p class="source-code">train_paths, test_paths = train_test_split(dataset_paths,</p><p class="source-code">                                          test_size=0.2,</p><p class="source-code">                                        random_state=999)</p><p class="source-code">train_paths, val_paths = train_test_split(train_paths,</p><p class="source-code">                                      test_size=0.2,</p><p class="source-code">                                     random_state=999)</p></li>
				<li>Prepare the training, test, and validation datasets:<p class="source-code">BATCH_SIZE = 1024</p><p class="source-code">BUFFER_SIZE = 1024</p><p class="source-code">train_dataset = prepare_dataset(train_paths,</p><p class="source-code">                              buffer_size=BUFFER_SIZE,</p><p class="source-code">                                batch_size=BATCH_SIZE)</p><p class="source-code">validation_dataset = prepare_dataset(val_paths,</p><p class="source-code">                              buffer_size=BUFFER_SIZE,</p><p class="source-code">                               batch_size=BATCH_SIZE,</p><p class="source-code">                                shuffle=False)</p><p class="source-code">test_dataset = prepare_dataset(test_paths,</p><p class="source-code">                              buffer_size=BUFFER_SIZE,</p><p class="source-code">                               batch_size=BATCH_SIZE,</p><p class="source-code">                               shuffle=False)</p></li>
				<li>Instantiate<a id="_idIndexMarker086"/> and compile<a id="_idIndexMarker087"/> the model:<p class="source-code">model = build_network()</p><p class="source-code">model.compile(loss=CategoricalCrossentropy</p><p class="source-code">             (from_logits=True),</p><p class="source-code">              optimizer='adam',</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model for <strong class="source-inline">250</strong> epochs:<p class="source-code">EPOCHS = 250</p><p class="source-code">model.fit(train_dataset,</p><p class="source-code">          validation_data=validation_dataset,</p><p class="source-code">          epochs=EPOCHS)</p></li>
				<li>Evaluate the model on the test set:<p class="source-code">test_loss, test_accuracy = model.evaluate(test_dataset)</p></li>
			</ol>
			<p>After 250 epochs, our network achieves around 93.5% accuracy on the test set. Let's understand what we just did.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor068"/>How it works…</h2>
			<p>We started by defining the <strong class="source-inline">CLASSES</strong> list, which allowed us to quickly one-hot encode the labels of each image, based on the name of the directory where they were contained, as we observed in the body of the <strong class="source-inline">load_image_and_label()</strong> function. In this same function, we read an image from disk, decoded it from its JPEG format, converted it to <a id="_idIndexMarker088"/>grayscale (color information<a id="_idIndexMarker089"/> is not necessary in this problem), and then resized it to more manageable dimensions of 32x32x1.</p>
			<p><strong class="source-inline">build_network()</strong> creates a very simple and shallow CNN, comprising a single convolutional layer, activated with <strong class="source-inline">ReLU()</strong>, followed by an output, a fully connected layer of three neurons, corresponding to the number of categories in the dataset. Because this is a multi-class classification task, we use <strong class="source-inline">Softmax()</strong> to activate the outputs.</p>
			<p><strong class="source-inline">prepare_dataset()</strong> leverages the <strong class="source-inline">load_image_and_label()</strong> function defined previously to convert file paths into batches of image tensors and one-hot encoded labels. </p>
			<p>Using the three functions explained here, we prepared three subsets of data, with the purpose of training, validating, and testing the neural network. We trained the model for 250 epochs, using the <strong class="source-inline">adam</strong> optimizer and <strong class="source-inline">CategoricalCrossentropy(from_logits=True)</strong> as our loss function (<strong class="source-inline">from_logits=True</strong> produces a bit more numerical stability).</p>
			<p>Finally, we got around 93.5% accuracy on the test set. Based on these results, you could use this network as a component of a Rock Paper Scissors game to recognize the hand gestures of a player and react accordingly.</p>
			<p>See also</p>
			<p>For more information on the <strong class="source-inline">Rock-Paper-Scissors Images</strong> dataset, refer to the official Kaggle page where it's hosted: <a href="https://www.kaggle.com/drgfreeman/rockpaperscissors">https://www.kaggle.com/drgfreeman/rockpaperscissors</a>.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor069"/>Creating a multi-label classifier to label watches</h1>
			<p>A neural network<a id="_idIndexMarker090"/> is not limited to modeling the distribution of a single variable. In fact, it can easily handle instances where each image has multiple labels associated with it. In this recipe, we'll implement a CNN to classify the gender and style/usage of watches.</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor070"/>Getting ready</h2>
			<p>First, we must install <strong class="source-inline">Pillow</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>Next, we'll use the <strong class="source-inline">Fashion Product Images (Small)</strong> dataset hosted in Kaggle, which, after signing in, you can download here: <a href="https://www.kaggle.com/paramaggarwal/fashion-product-images-small">https://www.kaggle.com/paramaggarwal/fashion-product-images-small</a>. In this recipe, we assume the data is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">fashion-product-images-small</strong>. We'll only use a subset of the data, focused on watches, which we'll construct programmatically in the <em class="italic">How to do it…</em> section.</p>
			<p>Here are some sample images:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B14768_02_003.jpg" alt="Figure 2.3 – Example images"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Example images</p>
			<p>Let's begin the recipe.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor071"/>How to do it…</h2>
			<p>Let's review the steps to complete the recipe:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from csv import DictReader</p><p class="source-code">import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import MultiLabelBinarizer</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define a<a id="_idIndexMarker091"/> function to build the network architecture. First, implement the convolutional blocks:<p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p>Next, add<a id="_idIndexMarker092"/> the fully convolutional layers:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.5)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Activation('sigmoid')(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>Define a function to load all images and labels (gender and usage), given a list of image paths<a id="_idIndexMarker093"/> and a dictionary of metadata associated with each of them:<p class="source-code">def load_images_and_labels(image_paths, styles, </p><p class="source-code">                           target_size):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                         target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        image_id = image_path.split(os.path.sep)[-</p><p class="source-code">                                         1][:-4]</p><p class="source-code">        image_style = styles[image_id]</p><p class="source-code">        label = (image_style['gender'], </p><p class="source-code">                 image_style['usage'])</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Set the random seed to guarantee reproducibility:<p class="source-code">SEED = 999</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Define the paths to the images and the <strong class="source-inline">styles.csv</strong> metadata file:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">             'datasets' /</p><p class="source-code">             'fashion-product-images-small')</p><p class="source-code">styles_path = str(base_path / 'styles.csv')</p><p class="source-code">images_path_pattern = str(base_path / 'images/*.jpg')</p><p class="source-code">image_paths = glob.glob(images_path_pattern)</p></li>
				<li>Keep only <a id="_idIndexMarker094"/>the <strong class="source-inline">Watches</strong> images for <strong class="source-inline">Casual</strong>, <strong class="source-inline">Smart Casual</strong>, and <strong class="source-inline">Formal</strong> usage, suited to <strong class="source-inline">Men</strong> and <strong class="source-inline">Women</strong>:<p class="source-code">with open(styles_path, 'r') as f:</p><p class="source-code">    dict_reader = DictReader(f)</p><p class="source-code">    STYLES = [*dict_reader]</p><p class="source-code">    article_type = 'Watches'</p><p class="source-code">    genders = {'Men', 'Women'}</p><p class="source-code">    usages = {'Casual', 'Smart Casual', 'Formal'}</p><p class="source-code">    STYLES = {style['id']: style</p><p class="source-code">              for style in STYLES</p><p class="source-code">              if (style['articleType'] == article_type </p><p class="source-code">                                           and</p><p class="source-code">                  style['gender'] in genders and</p><p class="source-code">                  style['usage'] in usages)}</p><p class="source-code">image_paths = [*filter(lambda p: </p><p class="source-code">               p.split(os.path.sep)[-1][:-4]</p><p class="source-code">                                 in STYLES.keys(),</p><p class="source-code">                       image_paths)]</p></li>
				<li>Load the images and labels, resizing the images into a 64x64x3 shape:<p class="source-code">X, y = load_images_and_labels(image_paths, STYLES, </p><p class="source-code">                              (64, 64))</p></li>
				<li>Normalize <a id="_idIndexMarker095"/>the images and multi-hot encode the labels:<p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">mlb = MultiLabelBinarizer()</p><p class="source-code">y = mlb.fit_transform(y)</p></li>
				<li>Create the train, validation, and test splits:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     stratify=y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                     </p><p class="source-code">                                    random_state=SEED)</p><p class="source-code">(X_train, X_valid,</p><p class="source-code"> y_train, y_valid) = train_test_split(X_train, y_train,</p><p class="source-code">                                    stratify=y_train,</p><p class="source-code">                                      test_size=0.2,</p><p class="source-code">                                   random_state=SEED)</p></li>
				<li>Build and compile the network:<p class="source-code">model = build_network(width=64,</p><p class="source-code">                      height=64,</p><p class="source-code">                      depth=3,</p><p class="source-code">                      classes=len(mlb.classes_))</p><p class="source-code">model.compile(loss='binary_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Train the <a id="_idIndexMarker096"/>model for <strong class="source-inline">20</strong> epochs, in batches of <strong class="source-inline">64</strong> images at a time:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">EPOCHS = 20</p><p class="source-code">model.fit(X_train, y_train,</p><p class="source-code">          validation_data=(X_valid, y_valid),</p><p class="source-code">          batch_size=BATCH_SIZE,</p><p class="source-code">          epochs=EPOCHS)</p></li>
				<li>Evaluate the model on the test set:<p class="source-code">result = model.evaluate(X_test, y_test, </p><p class="source-code">                       batch_size=BATCH_SIZE)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p><p>This block prints as follows:</p><p class="source-code">Test accuracy: 0.90233546</p></li>
				<li>Use the model to make predictions on a test image, displaying the probability of each label:<p class="source-code">test_image = np.expand_dims(X_test[0], axis=0)</p><p class="source-code">probabilities = model.predict(test_image)[0]</p><p class="source-code">for label, p in zip(mlb.classes_, probabilities):</p><p class="source-code">    print(f'{label}: {p * 100:.2f}%')</p><p>That prints this:</p><p class="source-code">Casual: 100.00%</p><p class="source-code">Formal: 0.00%</p><p class="source-code">Men: 1.08%</p><p class="source-code">Smart Casual: 0.01%</p><p class="source-code">Women: 99.16%</p></li>
				<li>Compare the <a id="_idIndexMarker097"/>ground truth labels with the network's prediction:<p class="source-code">ground_truth_labels = np.expand_dims(y_test[0], </p><p class="source-code">                                     axis=0)</p><p class="source-code">ground_truth_labels = mlb.inverse_transform(ground_truth_labels)</p><p class="source-code">print(f'Ground truth labels: {ground_truth_labels}')</p><p>The output is as follows:</p><p class="source-code">Ground truth labels: [('Casual', 'Women')]</p></li>
			</ol>
			<p>Let's see how it all works in the next section.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor072"/>How it works…</h2>
			<p>We implemented a smaller<a id="_idIndexMarker098"/> version of a <strong class="bold">VGG</strong> network, which is capable of performing multi-label, multi-class classification, by modeling independent distributions for the <strong class="source-inline">gender</strong> and <strong class="source-inline">usage</strong> metadata associated with each watch. In other words, we modeled two binary classification problems at the same time: one for <strong class="source-inline">gender</strong>, and one for <strong class="source-inline">usage</strong>. This is the reason we activated the outputs of the network with Sigmoid, instead of Softmax, and also why the loss function used is <strong class="source-inline">binary_crossentropy</strong> and not <strong class="source-inline">categorical_crossentropy</strong>. </p>
			<p>We trained the aforementioned network over 20 epochs, on batches of 64 images at a time, obtaining a respectable 90% accuracy on the test set. Finally, we made a prediction on an unseen image from the test set and verified that the labels produced with great certainty by the network (100% certainty for <strong class="source-inline">Casual</strong>, and 99.16% for <strong class="source-inline">Women</strong>) correspond<a id="_idIndexMarker099"/> to the ground truth categories <strong class="source-inline">Casual</strong> and <strong class="source-inline">Women</strong>. </p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor073"/>See also</h2>
			<p>For more information on the <strong class="source-inline">Fashion Product Images (Small)</strong> dataset, refer to the official Kaggle page where it is hosted: <a href="https://www.kaggle.com/paramaggarwal/fashion-product-images-small">https://www.kaggle.com/paramaggarwal/fashion-product-images-small</a>. I recommend you read the paper where the seminal <strong class="bold">VGG</strong> architecture was introduced: <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a>.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor074"/>Implementing ResNet from scratch</h1>
			<p><strong class="bold">Residual Network</strong>, or <strong class="bold">ResNet</strong> for short, constitutes one of the most groundbreaking advancements in <a id="_idIndexMarker100"/>deep learning. This architecture relies on a component called the residual module, which allows us to ensemble networks with depths that were unthinkable a couple of years ago. There are variants of <strong class="bold">ResNet</strong> that have more than 100 layers, without any loss of performance!</p>
			<p>In this recipe, we'll implement <strong class="bold">ResNet</strong> from scratch and train it on the challenging drop-in replacement to <strong class="source-inline">CIFAR-10</strong>, <strong class="source-inline">CINIC-10</strong>.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor075"/>Getting ready</h2>
			<p>We won't explain <strong class="bold">ResNet</strong> in depth, so it is a good idea to familiarize yourself with the architecture if you are interested in the details. You can read the original paper here: <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor076"/>How to do it…</h2>
			<p>Follow these steps to implement <strong class="bold">ResNet</strong> from the ground up:</p>
			<ol>
				<li value="1">Import all necessary modules:<p class="source-code">import os</p><p class="source-code">import numpy as np</p><p class="source-code">import tarfile</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.callbacks import ModelCheckpoint</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.regularizers import l2</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Define an alias <a id="_idIndexMarker101"/>to the <strong class="source-inline">tf.data.expertimental.AUTOTUNE</strong> option, which we'll use later:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p></li>
				<li>Define a function to create a residual module in the <strong class="bold">ResNet</strong> architecture. Let's start by specifying the function signature and implementing the first block:<p class="source-code">def residual_module(data,</p><p class="source-code">                    filters,</p><p class="source-code">                    stride,</p><p class="source-code">                    reduce=False,</p><p class="source-code">                    reg=0.0001,</p><p class="source-code">                    bn_eps=2e-5,</p><p class="source-code">                    bn_momentum=0.9):</p><p class="source-code">    bn_1 = BatchNormalization(axis=-1,</p><p class="source-code">                              epsilon=bn_eps,</p><p class="source-code">                           momentum=bn_momentum)(data)</p><p class="source-code">    act_1 = ReLU()(bn_1)</p><p class="source-code">    conv_1 = Conv2D(filters=int(filters / 4.),</p><p class="source-code">                    kernel_size=(1, 1),</p><p class="source-code">                    use_bias=False,</p><p class="source-code">                    kernel_regularizer=l2(reg))(act_1)</p><p>Let's now <a id="_idIndexMarker102"/>implement the second and third blocks:</p><p class="source-code">    bn_2 = BatchNormalization(axis=-1,</p><p class="source-code">                              epsilon=bn_eps,</p><p class="source-code">                         momentum=bn_momentum)(conv_1)</p><p class="source-code">    act_2 = ReLU()(bn_2)</p><p class="source-code">    conv_2 = Conv2D(filters=int(filters / 4.),</p><p class="source-code">                    kernel_size=(3, 3),</p><p class="source-code">                    strides=stride,</p><p class="source-code">                    padding='same',</p><p class="source-code">                    use_bias=False,</p><p class="source-code">                    kernel_regularizer=l2(reg))(act_2)</p><p class="source-code">    bn_3 = BatchNormalization(axis=-1,</p><p class="source-code">                              epsilon=bn_eps,</p><p class="source-code">                              momentum=bn_momentum)(conv_2)</p><p class="source-code">    act_3 = ReLU()(bn_3)</p><p class="source-code">    conv_3 = Conv2D(filters=filters,</p><p class="source-code">                    kernel_size=(1, 1),</p><p class="source-code">                    use_bias=False,</p><p class="source-code">                    kernel_regularizer=l2(reg))(act_3)</p><p>If <strong class="source-inline">reduce=True</strong>, we apply a 1x1 convolution:</p><p class="source-code">    if reduce:</p><p class="source-code">        shortcut = Conv2D(filters=filters,</p><p class="source-code">                          kernel_size=(1, 1),</p><p class="source-code">                          strides=stride,</p><p class="source-code">                          use_bias=False,</p><p class="source-code">                    kernel_regularizer=l2(reg))(act_1)</p><p>Finally, we <a id="_idIndexMarker103"/>combine the shortcut and the third block into a single layer and return that as our output:</p><p class="source-code">    x = Add()([conv_3, shortcut])</p><p class="source-code">    return x</p></li>
				<li>Define a function to build a custom <strong class="bold">ResNet</strong> network:<p class="source-code">def build_resnet(input_shape,</p><p class="source-code">                 classes,</p><p class="source-code">                 stages,</p><p class="source-code">                 filters,</p><p class="source-code">                 reg=1e-3,</p><p class="source-code">                 bn_eps=2e-5,</p><p class="source-code">                 bn_momentum=0.9):</p><p class="source-code">    inputs = Input(shape=input_shape)</p><p class="source-code">    x = BatchNormalization(axis=-1,</p><p class="source-code">                           epsilon=bn_eps,</p><p class="source-code">                           </p><p class="source-code">                         momentum=bn_momentum)(inputs)</p><p class="source-code">    x = Conv2D(filters[0], (3, 3),</p><p class="source-code">               use_bias=False,</p><p class="source-code">               padding='same',</p><p class="source-code">               kernel_regularizer=l2(reg))(x)</p><p class="source-code">    for i in range(len(stages)):</p><p class="source-code">        stride = (1, 1) if i == 0 else (2, 2)</p><p class="source-code">        x = residual_module(data=x,</p><p class="source-code">                            filters=filters[i + 1],</p><p class="source-code">                            stride=stride,</p><p class="source-code">                            reduce=True,</p><p class="source-code">                            bn_eps=bn_eps,</p><p class="source-code">                            bn_momentum=bn_momentum)</p><p class="source-code">        for j in range(stages[i] - 1):</p><p class="source-code">            x = residual_module(data=x,</p><p class="source-code">                                filters=filters[i + </p><p class="source-code">                                               1],</p><p class="source-code">                                stride=(1, 1),</p><p class="source-code">                                bn_eps=bn_eps,</p><p class="source-code">                                </p><p class="source-code">                            bn_momentum=bn_momentum)</p><p class="source-code">    x = BatchNormalization(axis=-1,</p><p class="source-code">                           epsilon=bn_eps,</p><p class="source-code">                           momentum=bn_momentum)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = AveragePooling2D((8, 8))(x)</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(classes, kernel_regularizer=l2(reg))(x)</p><p class="source-code">    x = Softmax()(x)</p><p class="source-code">    return Model(inputs, x, name='resnet')</p></li>
				<li>Define a <a id="_idIndexMarker104"/>function to load an image and its one-hot encoded labels, based on its file path:<p class="source-code">def load_image_and_label(image_path, target_size=(32, 32)):</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_png(image, channels=3)</p><p class="source-code">    image = tf.image.convert_image_dtype(image, </p><p class="source-code">                                        np.float32)</p><p class="source-code">    image -= CINIC_MEAN_RGB  # Mean normalize</p><p class="source-code">    image = tf.image.resize(image, target_size)</p><p class="source-code">    label = tf.strings.split(image_path, os.path.sep)[-2]</p><p class="source-code">    label = (label == CINIC_10_CLASSES)  # One-hot encode.</p><p class="source-code">    label = tf.dtypes.cast(label, tf.float32)</p><p class="source-code">    return image, label</p></li>
				<li>Define a function to create a <strong class="source-inline">tf.data.Dataset</strong> instance of images and labels from a<a id="_idIndexMarker105"/> glob-like pattern that refers to the folder where the images are:<p class="source-code">def prepare_dataset(data_pattern, shuffle=False):</p><p class="source-code">    dataset = (tf.data.Dataset</p><p class="source-code">               .list_files(data_pattern)</p><p class="source-code">               .map(load_image_and_label,</p><p class="source-code">                    num_parallel_calls=AUTOTUNE)</p><p class="source-code">               .batch(BATCH_SIZE))</p><p class="source-code">    </p><p class="source-code">    if shuffle:</p><p class="source-code">        dataset = dataset.shuffle(BUFFER_SIZE)</p><p class="source-code">        </p><p class="source-code">    return dataset.prefetch(BATCH_SIZE)</p></li>
				<li>Define the mean RGB values of the <strong class="source-inline">CINIC-10</strong> dataset, which is used in the <strong class="source-inline">load_image_and_label()</strong> function to mean normalize the images (this information is available on the official <strong class="source-inline">CINIC-10</strong> site):<p class="source-code">CINIC_MEAN_RGB = np.array([0.47889522, 0.47227842, 0.43047404])</p></li>
				<li>Define the classes of the <strong class="source-inline">CINIC-10</strong> dataset:<p class="source-code">CINIC_10_CLASSES = ['airplane', 'automobile', 'bird', 'cat',</p><p class="source-code">                    'deer', 'dog', 'frog', 'horse',    'ship',</p><p class="source-code">                    'truck']</p></li>
				<li>Download and <a id="_idIndexMarker106"/>extract the <strong class="source-inline">CINIC-10</strong> dataset to the <strong class="source-inline">~/.keras/datasets</strong> directory:<p class="source-code">DATASET_URL = ('https://datashare.is.ed.ac.uk/bitstream/handle/'</p><p class="source-code">               '10283/3192/CINIC-10.tar.gz?'</p><p class="source-code">               'sequence=4&amp;isAllowed=y')</p><p class="source-code">DATA_NAME = 'cinic10'</p><p class="source-code">FILE_EXTENSION = 'tar.gz'</p><p class="source-code">FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])</p><p class="source-code">downloaded_file_location = get_file(origin=DATASET_URL,</p><p class="source-code">                                    fname=FILE_NAME,</p><p class="source-code">                                    extract=False)</p><p class="source-code">data_directory, _ = (downloaded_file_location</p><p class="source-code">                     .rsplit(os.path.sep, maxsplit=1))</p><p class="source-code">data_directory = os.path.sep.join([data_directory, </p><p class="source-code">                                  DATA_NAME])</p><p class="source-code">tar = tarfile.open(downloaded_file_location)</p><p class="source-code">if not os.path.exists(data_directory):</p><p class="source-code">    tar.extractall(data_directory)</p></li>
				<li>Define the glob-like patterns to the train, test, and validation subsets:<p class="source-code">train_pattern = os.path.sep.join(</p><p class="source-code">    [data_directory, 'train/*/*.png'])</p><p class="source-code">test_pattern = os.path.sep.join(</p><p class="source-code">    [data_directory, 'test/*/*.png'])</p><p class="source-code">valid_pattern = os.path.sep.join(</p><p class="source-code">    [data_directory, 'valid/*/*.png'])</p></li>
				<li>Prepare the datasets:<p class="source-code">BATCH_SIZE = 128</p><p class="source-code">BUFFER_SIZE = 1024</p><p class="source-code">train_dataset = prepare_dataset(train_pattern, </p><p class="source-code">                                shuffle=True)</p><p class="source-code">test_dataset = prepare_dataset(test_pattern)</p><p class="source-code">valid_dataset = prepare_dataset(valid_pattern)</p></li>
				<li>Build, compile, and train a <strong class="bold">ResNet</strong> model. Because this is a time-consuming process, we'll<a id="_idIndexMarker107"/> save a version of the model after each epoch, using the <strong class="source-inline">ModelCheckpoint()</strong> callback:<p class="source-code">model = build_resnet(input_shape=(32, 32, 3),</p><p class="source-code">                     classes=10,</p><p class="source-code">                     stages=(9, 9, 9),</p><p class="source-code">                     filters=(64, 64, 128, 256),</p><p class="source-code">                     reg=5e-3)</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">model_checkpoint_callback = ModelCheckpoint(</p><p class="source-code">    filepath='./model.{epoch:02d}-{val_accuracy:.2f}.hdf5',</p><p class="source-code">    save_weights_only=False,</p><p class="source-code">    monitor='val_accuracy')</p><p class="source-code">EPOCHS = 100</p><p class="source-code">model.fit(train_dataset,</p><p class="source-code">          validation_data=valid_dataset,</p><p class="source-code">          epochs=EPOCHS,</p><p class="source-code">          callbacks=[model_checkpoint_callback])</p></li>
				<li>Load the best <a id="_idIndexMarker108"/>model (in this case, <strong class="source-inline">model.38-0.72.hdf5</strong>) and evaluate it on the test set:<p class="source-code">model = load_model('model.38-0.72.hdf5')</p><p class="source-code">result = model.evaluate(test_dataset)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p><p>This prints the following:</p><p class="source-code">Test accuracy: 0.71956664</p></li>
			</ol>
			<p>Let's learn how it all works in the next section.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor077"/>How it works…</h2>
			<p>The key to <strong class="bold">ResNet</strong> is the <a id="_idIndexMarker109"/>residual module, which we implemented in <em class="italic">Step 3</em>. A residual module is a micro-architecture that can be reused many times to create a macro-architecture, thus achieving great depths. The <strong class="source-inline">residual_module()</strong> function receives the input data (<strong class="source-inline">data</strong>), the number of filters (<strong class="source-inline">filters</strong>), the stride (<strong class="source-inline">stride</strong>) of the convolutional blocks, a <strong class="source-inline">reduce</strong> flag to indicate whether we want to reduce the spatial size of the shortcut branch by applying a 1x1 convolution (a technique used to reduce the dimensionality of the output volumes of the filters), and parameters to adjust the amount of regularization (<strong class="source-inline">reg</strong>) and batch normalization applied to the different layers (<strong class="source-inline">bn_eps</strong> and <strong class="source-inline">bn_momentum</strong>).</p>
			<p>A residual module comprises two branches: the first one is the skip connection, also known as the shortcut branch, which is basically the same as the input. The second or main branch is composed of three convolution blocks: a 1x1 with a quarter of the filters, a 3x3 one, also with a quarter of the filters, and finally another 1x1, which uses all the filters. The shortcut and main branches are concatenated in the end using the <strong class="source-inline">Add()</strong> layer.</p>
			<p><strong class="source-inline">build_network()</strong> allows us to specify the number of stages to use, and also the number of filters per stage. We start by applying a 3x3 convolution to the input (after being batch normalized). Then we proceed to create the stages. A stage is a series of residual modules connected to each other. The length of the <strong class="source-inline">stages</strong> list controls the number of stages to create, and each element in this list controls the number of layers in that particular<a id="_idIndexMarker110"/> stage. The <strong class="source-inline">filters</strong> parameter contains the number of filters to use in each residual block within a stage. Finally, we built a fully connected network, Softmax-activated, on top of the stages with as many units as there are classes in the dataset (in this case, 10). </p>
			<p>Because <strong class="bold">ResNet</strong> is a very deep, heavy, and slow-to-train architecture, we checkpointed the model after each epoch. In this recipe, we obtained the best model in epoch 38, which produced 72% accuracy on the test set, a respectable performance considering that <strong class="source-inline">CINIC-10</strong> is not an easy dataset and that we did not apply any data augmentation or transfer learning.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor078"/>See also</h2>
			<p>For more information on the <strong class="source-inline">CINIC-10</strong> dataset, visit this link: <a href="https://datashare.is.ed.ac.uk/handle/10283/3192">https://datashare.is.ed.ac.uk/handle/10283/3192</a>.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor079"/>Classifying images with a pre-trained network using the Keras API</h1>
			<p>We do<a id="_idIndexMarker111"/> not <a id="_idIndexMarker112"/>always need to <a id="_idIndexMarker113"/>train a classifier from scratch, especially when the images we want to categorize resemble ones that another network trained on. In these instances, we can simply reuse the model, saving ourselves lots of time. In this recipe, we'll use a pre-trained network on ImageNet to classify a custom image.</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor080"/>Getting ready</h2>
			<p>We will need <strong class="source-inline">Pillow</strong>. We can install it as follows:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>You're <a id="_idIndexMarker114"/>free to use your own<a id="_idIndexMarker115"/> images in the<a id="_idIndexMarker116"/> recipe. Alternatively, you can download the one at this link: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe5/dog.jpg">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe5/dog.jpg</a>.</p>
			<p>Here's the image we'll pass to the classifier:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B14768_02_004.jpg" alt="Figure 2.4 – Image passed to the pre-trained classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Image passed to the pre-trained classifier</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor081"/>How to do it…</h2>
			<p>As we'll see in this section, re-using a pre-trained classifier is very easy!</p>
			<ol>
				<li value="1">Import the required packages. These include the pre-trained network used for classification, as well as some helper functions to pre process the images:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras.applications import imagenet_utils</p><p class="source-code">from tensorflow.keras.applications.inception_v3 import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Instantiate<a id="_idIndexMarker117"/> an <strong class="source-inline">InceptionV3</strong> network <a id="_idIndexMarker118"/>pre-trained <a id="_idIndexMarker119"/>on ImageNet:<p class="source-code">model = InceptionV3(weights='imagenet')</p></li>
				<li>Load the image to classify. <strong class="source-inline">InceptionV3</strong> takes a 299x299x3 image, so we must resize it accordingly:<p class="source-code">image = load_img('dog.jpg', target_size=(299, 299))</p></li>
				<li>Convert the image to a <strong class="source-inline">numpy</strong> array, and wrap it into a singleton batch:<p class="source-code">image = img_to_array(image)</p><p class="source-code">image = np.expand_dims(image, axis=0)</p></li>
				<li>Pre process the image the same way <strong class="source-inline">InceptionV3</strong> does:<p class="source-code">image = preprocess_input(image)</p></li>
				<li>Use the model to make predictions on the image, and then decode the predictions to a matrix:<p class="source-code">predictions = model.predict(image)</p><p class="source-code">prediction_matrix = (imagenet_utils</p><p class="source-code">                     .decode_predictions(predictions))</p></li>
				<li>Examine the top <strong class="source-inline">5</strong> predictions along with their probability:<p class="source-code">for i in range(5):</p><p class="source-code">    _, label, probability = prediction_matrix[0][i]</p><p class="source-code">    print(f'{i + 1}. {label}: {probability * 100:.3f}%')</p><p>This <a id="_idIndexMarker120"/>produces<a id="_idIndexMarker121"/> the <a id="_idIndexMarker122"/>following output:</p><p class="source-code">1. pug: 85.538%</p><p class="source-code">2. French_bulldog: 0.585%</p><p class="source-code">3. Brabancon_griffon: 0.543%</p><p class="source-code">4. Boston_bull: 0.218%</p><p class="source-code">5. bull_mastiff: 0.125%</p></li>
				<li>Plot the original image with its most probable label:<p class="source-code">_, label, _ = prediction_matrix[0][0]</p><p class="source-code">plt.figure()</p><p class="source-code">plt.title(f'Label: {label}.')</p><p class="source-code">original = load_img('dog.jpg')</p><p class="source-code">original = img_to_array(original)</p><p class="source-code">plt.imshow(original / 255.0)</p><p class="source-code">plt.show()</p><p>This block generates the following image:</p></li>
			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B14768_02_005.jpg" alt="Figure 2.5 – Correctly classified image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Correctly classified image</p>
			<p>Let's see how it all works in the next section.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor082"/>How it works…</h2>
			<p>As evidenced <a id="_idIndexMarker123"/>here, in order to<a id="_idIndexMarker124"/> classify<a id="_idIndexMarker125"/> images effortlessly, using a pre-trained network on ImageNet, we just need to instantiate the proper model with the right weights, like this: <strong class="source-inline">InceptionV3(weights='imagenet')</strong>. This will download the architecture and the weights if it is the first time we are using them; otherwise, a version of these files will be cached in our system. </p>
			<p>Then, we loaded the image we wanted to classify, resized it to dimensions compatible with <strong class="source-inline">InceptionV3</strong> (299x299x3), converted it into a singleton batch with <strong class="source-inline">np.expand_dims(image, axis=0)</strong>, and pre processed it the same way <strong class="source-inline">InceptionV3</strong> did when it was trained, with <strong class="source-inline">preprocess_input(image)</strong>.</p>
			<p>Next, we got the predictions from the model, which we need to transform to a prediction matrix with the help of <strong class="source-inline">imagenet_utils.decode_predictions(predictions)</strong>. This matrix contains the <a id="_idIndexMarker126"/>label and <a id="_idIndexMarker127"/>probabilities in the 0th row, which <a id="_idIndexMarker128"/>we inspected to get the five most probable classes.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor083"/>See also</h2>
			<p>You can read <a id="_idIndexMarker129"/>more about Keras pre-trained models here: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications">https://www.tensorflow.org/api_docs/python/tf/keras/applications</a>.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor084"/>Classifying images with a pre-trained network using TensorFlow Hub</h1>
			<p><strong class="bold">TensorFlow Hub</strong> (<strong class="bold">TFHub</strong>) is a <a id="_idIndexMarker130"/>repository<a id="_idIndexMarker131"/> of hundreds of machine learning models contributed to by the <a id="_idIndexMarker132"/>big and rich community that surrounds TensorFlow. Here we can find models for a myriad of different tasks, not only for computer vision but for applications in many different domains, such as <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) and<a id="_idIndexMarker133"/> reinforcement learning.</p>
			<p>In this recipe, we'll use a model trained on ImageNet, hosted on TFHub, to make predictions on a custom image. Let's begin!</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor085"/>Getting ready</h2>
			<p>We'll need the <strong class="source-inline">tensorflow-hub</strong> and <strong class="source-inline">Pillow</strong> packages, which can be easily installed using <strong class="source-inline">pip</strong>, as follows:</p>
			<p class="source-code">$&gt; pip install tensorflow-hub Pillow</p>
			<p>If you want to use the same image we use in this recipe, you can download it here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe6/beetle.jpg">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe6/beetle.jpg</a>.</p>
			<p>Here's <a id="_idIndexMarker134"/>the image<a id="_idIndexMarker135"/> we'll <a id="_idIndexMarker136"/>classify:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B14768_02_006.jpg" alt="Figure 2.6 – Image to be classified&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Image to be classified</p>
			<p>Let's head to the next section.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor086"/>How to do it…</h2>
			<p>Let's proceed with the recipe steps:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">from tensorflow.keras import Sequential</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Define the URL of the pre-trained <strong class="source-inline">ResNetV2152</strong> classifier in <strong class="bold">TFHub</strong>:<p class="source-code">classifier_url = ('https://tfhub.dev/google/imagenet/'</p><p class="source-code">                  'resnet_v2_152/classification/4')</p></li>
				<li>Download and instantiate the classifier hosted on TFHub:<p class="source-code">model = Sequential([</p><p class="source-code">    hub.KerasLayer(classifier_url, input_shape=(224, </p><p class="source-code">                                              224, 3))])</p></li>
				<li>Load<a id="_idIndexMarker137"/> the<a id="_idIndexMarker138"/> image <a id="_idIndexMarker139"/>we'll classify, convert it to a <strong class="source-inline">numpy</strong> array, normalize it, and wrap it into a singleton batch:<p class="source-code">image = load_img('beetle.jpg', target_size=(224, 224))</p><p class="source-code">image = img_to_array(image)</p><p class="source-code">image = image / 255.0</p><p class="source-code">image = np.expand_dims(image, axis=0)</p></li>
				<li>Use the pre-trained model to classify the image:<p class="source-code">predictions = model.predict(image)</p></li>
				<li>Extract the index of the most probable prediction:<p class="source-code">predicted_index = np.argmax(predictions[0], axis=-1)</p></li>
				<li>Download the ImageNet labels into a file named <strong class="source-inline">ImageNetLabels.txt</strong>:<p class="source-code">file_name = 'ImageNetLabels.txt'</p><p class="source-code">file_url = ('https://storage.googleapis.com/'</p><p class="source-code">    'download.tensorflow.org/data/ImageNetLabels.txt')</p><p class="source-code">         labels_path = get_file(file_name, file_url)</p></li>
				<li>Read the labels into a <strong class="source-inline">numpy</strong> array:<p class="source-code">with open(labels_path) as f:</p><p class="source-code">    imagenet_labels = np.array(f.read().splitlines())</p></li>
				<li>Extract <a id="_idIndexMarker140"/>the <a id="_idIndexMarker141"/>name<a id="_idIndexMarker142"/> of the class corresponding to the index of the most probable prediction:<p class="source-code">predicted_class = imagenet_labels[predicted_index]</p></li>
				<li>Plot the original image with its most probable label:<p class="source-code">plt.figure()</p><p class="source-code">plt.title(f'Label: {predicted_class}.')</p><p class="source-code">original = load_img('beetle.jpg')</p><p class="source-code">original = img_to_array(original)</p><p class="source-code">plt.imshow(original / 255.0)</p><p class="source-code">plt.show()</p><p>This produces the following:</p></li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B14768_02_007.jpg" alt="Figure 2.7 – Correctly classified image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Correctly classified image</p>
			<p>Let's see how it all works.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor087"/>How it works…</h2>
			<p>After <a id="_idIndexMarker143"/>importing<a id="_idIndexMarker144"/> the<a id="_idIndexMarker145"/> relevant packages, we proceeded to define the URL of the model we wanted to use to classify our input image. To download and convert such a network into a Keras model, we used the convenient <strong class="source-inline">hub.KerasLayer</strong> class in <em class="italic">Step 3</em>. Then, in <em class="italic">Step 4</em>, we loaded the image we wanted to classify into memory, making sure its dimensions match the ones the network expects: 224x224x3.</p>
			<p><em class="italic">Steps 5</em> and <em class="italic">6</em> perform the classification and extract the most probable category, respectively. However, to make this prediction human-readable, we downloaded a plain text file with all ImageNet labels in <em class="italic">Step 7</em>, which we then parsed using <strong class="source-inline">numpy</strong>, allowing us to use the index of the most probable category to obtain the corresponding label, finally displayed in <em class="italic">Step 10</em> along with the input image.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor088"/>See also</h2>
			<p>You can learn more about the pre-trained model we used here: <a href="https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4">https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4</a>.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor089"/>Using data augmentation to improve performance with the Keras API</h1>
			<p>More<a id="_idIndexMarker146"/> often than <a id="_idIndexMarker147"/>not, we can benefit from providing more data to our model. But data is expensive and scarce. Is there a way to circumvent this limitation? Yes, there is! We can synthesize new training examples by performing little modifications on the ones we already have, such as random rotations, random cropping, and horizontal flipping, among others. In this recipe, we'll learn how to use data augmentation with the Keras API to improve performance.</p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor090"/>Getting ready</h2>
			<p>We must install <strong class="source-inline">Pillow</strong> and <strong class="source-inline">tensorflow_docs</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow git+https://github.com/tensorflow/docs</p>
			<p>In this recipe, we'll use the <strong class="source-inline">Caltech 101</strong> dataset, which is available here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>. Download and decompress <strong class="source-inline">101_ObjectCategories.tar.gz</strong> to your preferred location. From now on, we assume the data is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">101_ObjectCategories</strong>.</p>
			<p>Here are sample images from <strong class="source-inline">Caltech 101</strong>:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B14768_02_008.jpg" alt="Figure 2.8 – Caltech 101 sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Caltech 101 sample images</p>
			<p>Let's implement!</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor091"/>How to do it…</h2>
			<p>The steps<a id="_idIndexMarker148"/> listed here <a id="_idIndexMarker149"/>are necessary to complete the recipe. Let's get started!</p>
			<ol>
				<li value="1">Import the required modules:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow_docs as tfdocs</p><p class="source-code">import tensorflow_docs.plots</p><p class="source-code">from glob import glob</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define a<a id="_idIndexMarker150"/> function <a id="_idIndexMarker151"/>to load all images in the dataset, along with their labels, based on their file paths:<p class="source-code">def load_images_and_labels(image_paths, target_size=(64, 64)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                         target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Define a function to build a smaller version of <strong class="bold">VGG</strong>:<p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>Define a <a id="_idIndexMarker152"/>function<a id="_idIndexMarker153"/> to plot and save a model's training curve:<p class="source-code">def plot_model_history(model_history, metric, </p><p class="source-code">                       plot_name):</p><p class="source-code">    plt.style.use('seaborn-darkgrid')</p><p class="source-code">    plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">    plotter.plot({'Model': model_history}, </p><p class="source-code">                  metric=metric)</p><p class="source-code">    plt.title(f'{metric.upper()}')</p><p class="source-code">    plt.ylim([0, 1])</p><p class="source-code">    plt.savefig(f'{plot_name}.png')</p><p class="source-code">    plt.close()</p></li>
				<li>Set the random seed:<p class="source-code">SEED = 999</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Load the paths to all images in the dataset, excepting the ones of the <strong class="source-inline">BACKGROUND_Google</strong> class:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">             'datasets' /</p><p class="source-code">             '101_ObjectCategories')</p><p class="source-code">images_pattern = str(base_path / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(images_pattern)]</p><p class="source-code">image_paths = [p for p in image_paths if</p><p class="source-code">               p.split(os.path.sep)[-2] !=</p><p class="source-code">              'BACKGROUND_Google']</p></li>
				<li>Compute <a id="_idIndexMarker154"/>the <a id="_idIndexMarker155"/>set of classes in the dataset:<p class="source-code">classes = {p.split(os.path.sep)[-2] for p in </p><p class="source-code">          image_paths}</p></li>
				<li>Load the dataset into memory, normalizing the images and one-hot encoding the labels:<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p></li>
				<li>Create the training and testing subsets:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                    random_state=SEED)</p></li>
				<li>Build, compile, train, and evaluate a neural network without data augmentation:<p class="source-code">EPOCHS = 40</p><p class="source-code">BATCH_SIZE = 64</p><p class="source-code">model = build_network(64, 64, 3, len(classes))</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">history = model.fit(X_train, y_train,</p><p class="source-code">                    validation_data=(X_test, y_test),</p><p class="source-code">                    epochs=EPOCHS,</p><p class="source-code">                    batch_size=BATCH_SIZE)</p><p class="source-code">result = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p><p class="source-code">plot_model_history(history, 'accuracy', 'normal')</p><p>The <a id="_idIndexMarker156"/>accuracy <a id="_idIndexMarker157"/>on the test set is as follows:</p><p class="source-code">Test accuracy: 0.61347926</p><p>And here's the accuracy curve:</p><div id="_idContainer023" class="IMG---Figure"><img src="image/B14768_02_009.jpg" alt="Figure 2.9 – Training and validation accuracy for a network without data augmentation&#13;&#10;"/></div><p class="figure-caption">Figure 2.9 – Training and validation accuracy for a network without data augmentation</p></li>
				<li>Build, compile, train, and <a id="_idIndexMarker158"/>evaluate <a id="_idIndexMarker159"/>the same network, this time with data augmentation:<p class="source-code">model = build_network(64, 64, 3, len(classes))</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">augmenter = ImageDataGenerator(horizontal_flip=True,</p><p class="source-code">                               rotation_range=30,</p><p class="source-code">                               width_shift_range=0.1,</p><p class="source-code">                               height_shift_range=0.1,</p><p class="source-code">                               shear_range=0.2,</p><p class="source-code">                               zoom_range=0.2,</p><p class="source-code">                               fill_mode='nearest')</p><p class="source-code">train_generator = augmenter.flow(X_train, y_train, </p><p class="source-code">                                  BATCH_SIZE)</p><p class="source-code">hist = model.fit(train_generator,</p><p class="source-code">                 steps_per_epoch=len(X_train) // </p><p class="source-code">                 BATCH_SIZE,</p><p class="source-code">                 validation_data=(X_test, y_test),</p><p class="source-code">                 epochs=EPOCHS)</p><p class="source-code">result = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p><p class="source-code">plot_model_history(hist, 'accuracy', 'augmented')</p><p>The accuracy <a id="_idIndexMarker160"/>on<a id="_idIndexMarker161"/> the test set when we use data augmentation is as follows:</p><p class="source-code">Test accuracy: 0.65207374</p><p>And the accuracy curve looks like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B14768_02_010.jpg" alt="Figure 2.10 – Training and validation accuracy for a network with data augmentation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Training and validation accuracy for a network with data augmentation</p>
			<p>Comparing <em class="italic">Steps 10</em> and <em class="italic">11</em>, we observe a noticeable gain in performance by using data augmentation. Let's understand better what we did in the next section.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor092"/>How it works…</h2>
			<p>In this recipe, we implemented a scaled-down version of <strong class="bold">VGG</strong> on the challenging <strong class="source-inline">Caltech 101</strong> dataset. First, we trained a network only on the original data, and then using data augmentation. The first network (see <em class="italic">Step 10</em>) obtained an accuracy level on the test set of 61.3% and clearly shows signs of overfitting, because the gap that separates the<a id="_idIndexMarker162"/> training and<a id="_idIndexMarker163"/> validation accuracy curves is very wide. On the other hand, by applying a series of random perturbations, through <strong class="source-inline">ImageDataGenerator()</strong>, such as horizontal flips, rotations, width, and height shifting, among others (see <em class="italic">Step 11</em>), we increased the accuracy on the test set to 65.2%. Also, the gap between the training and validation accuracy curves is much smaller this time, which suggests a regularization effect resulting from the application of data augmentation.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor093"/>See also</h2>
			<p>You can learn more about <strong class="source-inline">Caltech 101</strong> here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor094"/>Using data augmentation to improve performance with the tf.data and tf.image APIs</h1>
			<p>Data augmentation<a id="_idIndexMarker164"/> is <a id="_idIndexMarker165"/>a powerful technique we can apply to artificially increment the size of our dataset, by creating slightly modified copies of the images at our disposal. In this recipe, we'll leverage the <strong class="source-inline">tf.data</strong> and <strong class="source-inline">tf.image</strong> APIs to increase the performance of a CNN trained on the challenging <strong class="source-inline">Caltech 101</strong> dataset. </p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor095"/>Getting ready</h2>
			<p>We must install <strong class="source-inline">tensorflow_docs</strong>:</p>
			<p class="source-code">$&gt; pip install git+https://github.com/tensorflow/docs</p>
			<p>In this recipe, we'll use the <strong class="source-inline">Caltech 101</strong> dataset, which is available here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>. Download and decompress <strong class="source-inline">101_ObjectCategories.tar.gz</strong> to your preferred location. From now on, we assume the data is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, in a folder named <strong class="source-inline">101_ObjectCategories</strong>.</p>
			<p>Here <a id="_idIndexMarker166"/>are <a id="_idIndexMarker167"/>some sample images from <strong class="source-inline">Caltech 101</strong>:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B14768_02_011.jpg" alt="Figure 2.11 – Caltech 101 sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Caltech 101 sample images</p>
			<p>Let's go to the next section.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor096"/>How to do it…</h2>
			<p>Let's go over the steps required to complete this recipe.</p>
			<ol>
				<li value="1">Import the necessary dependencies:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_docs as tfdocs</p><p class="source-code">import tensorflow_docs.plots</p><p class="source-code">from glob import glob</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import Model</p></li>
				<li>Create <a id="_idIndexMarker168"/>an <a id="_idIndexMarker169"/>alias for the <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong> flag, which we'll use later on:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p></li>
				<li>Define a function to create a smaller version of <strong class="bold">VGG</strong>. Start by creating the input layer and the first block of two convolutions with 32 filters each:<p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p></li>
				<li>Continue <a id="_idIndexMarker170"/>with<a id="_idIndexMarker171"/> the second block of two convolutions, this time each with 64 kernels:<p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p></li>
				<li>Define the last part of the architecture, which consists of a series of fully connected layers:<p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.5)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>Define a<a id="_idIndexMarker172"/> function<a id="_idIndexMarker173"/> to plot and save the training curves of a model, given its training history:<p class="source-code">def plot_model_history(model_history, metric, </p><p class="source-code">                       plot_name):</p><p class="source-code">    plt.style.use('seaborn-darkgrid')</p><p class="source-code">    plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">    plotter.plot({'Model': model_history}, </p><p class="source-code">                  metric=metric)</p><p class="source-code">    plt.title(f'{metric.upper()}')</p><p class="source-code">    plt.ylim([0, 1])</p><p class="source-code">    plt.savefig(f'{plot_name}.png')</p><p class="source-code">    plt.close()</p></li>
				<li>Define a function to load an image and one-hot encode its label, based on the image's file path:<p class="source-code">def load_image_and_label(image_path, target_size=(64, </p><p class="source-code">                                                 64)):</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_jpeg(image, channels=3)</p><p class="source-code">    image = tf.image.convert_image_dtype(image, </p><p class="source-code">                                         np.float32)</p><p class="source-code">    image = tf.image.resize(image, target_size)</p><p class="source-code">    label = tf.strings.split(image_path, os.path.sep)[-2]</p><p class="source-code">    label = (label == CLASSES)  # One-hot encode.</p><p class="source-code">    label = tf.dtypes.cast(label, tf.float32)</p><p class="source-code">    return image, label</p></li>
				<li>Define a<a id="_idIndexMarker174"/> function<a id="_idIndexMarker175"/> to augment an image by performing random transformations on it:<p class="source-code">def augment(image, label):</p><p class="source-code">    image = tf.image.resize_with_crop_or_pad(image, </p><p class="source-code">                                             74, 74)</p><p class="source-code">    image = tf.image.random_crop(image, size=(64, 64, 3))</p><p class="source-code">    image = tf.image.random_flip_left_right(image)</p><p class="source-code">    image = tf.image.random_brightness(image, 0.2)</p><p class="source-code">    return image, label</p></li>
				<li>Define a function to prepare a <strong class="source-inline">tf.data.Dataset</strong> of images, based on a glob-like pattern that refers to the folder where they live:<p class="source-code">def prepare_dataset(data_pattern):</p><p class="source-code">    return (tf.data.Dataset</p><p class="source-code">            .from_tensor_slices(data_pattern)</p><p class="source-code">            .map(load_image_and_label,</p><p class="source-code">                 num_parallel_calls=AUTOTUNE))</p></li>
				<li>Set the random seed:<p class="source-code">SEED = 999</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Load the<a id="_idIndexMarker176"/> paths<a id="_idIndexMarker177"/> to all images in the dataset, excepting the ones of the <strong class="source-inline">BACKGROUND_Google</strong> class: <p class="source-code">base_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">             'datasets' /</p><p class="source-code">             '101_ObjectCategories')</p><p class="source-code">images_pattern = str(base_path / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(images_pattern)]</p><p class="source-code">image_paths = [p for p in image_paths if</p><p class="source-code">               p.split(os.path.sep)[-2] !=</p><p class="source-code">              'BACKGROUND_Google']</p></li>
				<li>Compute the unique categories in the dataset:<p class="source-code">CLASSES = np.unique([p.split(os.path.sep)[-2]</p><p class="source-code">                     for p in image_paths])</p></li>
				<li>Split the image paths into training and testing subsets:<p class="source-code">train_paths, test_paths = train_test_split(image_paths,</p><p class="source-code">                                          test_size=0.2,</p><p class="source-code">                                      random_state=SEED)</p></li>
				<li>Prepare<a id="_idIndexMarker178"/> the<a id="_idIndexMarker179"/> training and testing datasets, without augmentation:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">BUFFER_SIZE = 1024</p><p class="source-code">train_dataset = (prepare_dataset(train_paths)</p><p class="source-code">                 .batch(BATCH_SIZE)</p><p class="source-code">                 .shuffle(buffer_size=BUFFER_SIZE)</p><p class="source-code">                 .prefetch(buffer_size=BUFFER_SIZE))</p><p class="source-code">test_dataset = (prepare_dataset(test_paths)</p><p class="source-code">                .batch(BATCH_SIZE)</p><p class="source-code">                .prefetch(buffer_size=BUFFER_SIZE))</p></li>
				<li>Instantiate, compile, train and evaluate the network:<p class="source-code">EPOCHS = 40</p><p class="source-code">model = build_network(64, 64, 3, len(CLASSES))</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">history = model.fit(train_dataset,</p><p class="source-code">                    validation_data=test_dataset,</p><p class="source-code">                    epochs=EPOCHS)</p><p class="source-code">result = model.evaluate(test_dataset)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p><p class="source-code">plot_model_history(history, 'accuracy', 'normal')</p><p>The accuracy on the test set is:</p><p class="source-code">Test accuracy: 0.6532258</p><p>And here's the accuracy curve:</p><div id="_idContainer026" class="IMG---Figure"><img src="image/B14768_02_012.jpg" alt="Figure 2.12 – Training and validation accuracy for a network without data augmentation&#13;&#10;"/></div><p class="figure-caption">Figure 2.12 – Training and validation accuracy for a network without data augmentation</p></li>
				<li>Prepare<a id="_idIndexMarker180"/> the<a id="_idIndexMarker181"/> training and testing sets, this time applying data augmentation to the training set:<p class="source-code">train_dataset = (prepare_dataset(train_paths)</p><p class="source-code">                 .map(augment, </p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                 .batch(BATCH_SIZE)</p><p class="source-code">                 .shuffle(buffer_size=BUFFER_SIZE)</p><p class="source-code">                 .prefetch(buffer_size=BUFFER_SIZE))</p><p class="source-code">test_dataset = (prepare_dataset(test_paths)</p><p class="source-code">                .batch(BATCH_SIZE)</p><p class="source-code">                .prefetch(buffer_size=BUFFER_SIZE))</p></li>
				<li>Instantiate, compile, train, and <a id="_idIndexMarker182"/>evaluate the network on the augmented<a id="_idIndexMarker183"/> data:<p class="source-code">model = build_network(64, 64, 3, len(CLASSES))</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='rmsprop',</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">history = model.fit(train_dataset,</p><p class="source-code">                    validation_data=test_dataset,</p><p class="source-code">                    epochs=EPOCHS)</p><p class="source-code">result = model.evaluate(test_dataset)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p><p class="source-code">plot_model_history(history, 'accuracy', 'augmented')</p><p>The accuracy on the test set when we use data augmentation is as follows:</p><p class="source-code">Test accuracy: 0.74711984</p><p>And the accuracy curve looks like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B14768_02_013.jpg" alt="Figure 2.13 – Training and validation accuracy for a network with data augmentation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Training and validation accuracy for a network with data augmentation</p>
			<p>Let's understand what we just did in the next section.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor097"/>How it works…</h2>
			<p>We just<a id="_idIndexMarker184"/> implemented a<a id="_idIndexMarker185"/> trimmed down version of the famous <strong class="bold">VGG</strong> architecture, trained on the <strong class="source-inline">Caltech 101</strong> dataset. To better understand the advantages of data augmentation, we fitted a first version on the original data, without any modification, obtaining an accuracy level of 65.32% on the test set. This first model displays signs of overfitting, because the gap that separates the training and validation accuracy curves widens early in the training process. </p>
			<p>Next, we trained the same network on an augmented dataset (see <em class="italic">Step 15</em>), using the <strong class="source-inline">augment()</strong> function defined earlier. This greatly improved the model's performance, reaching a respectable accuracy of 74.19% on the test set. Also, the gap between the training and validation accuracy curves is noticeably smaller, which suggests a regularization effect coming out from the application of data augmentation.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor098"/>See also</h2>
			<p>You can learn more about <strong class="source-inline">Caltech 101</strong> here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>.</p>
		</div>
	</body></html>