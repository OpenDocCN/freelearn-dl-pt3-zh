<html><head></head><body>
  <div id="_idContainer044">
    <h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-14" class="chapterTitle">Essentials of NLP</h1>
    <p class="normal">Language has been a part of human evolution. The development of language allowed better communication between people and tribes. The evolution of written language, initially as cave paintings and later as characters, allowed information to be distilled, stored, and passed on from generation to generation. Some would even say that the hockey stick curve of advancement is because of the ever-accumulating cache of stored information. As this stored information trove becomes larger and larger, the need for computational methods to process and distill the data becomes more acute. In the past decade, a lot of advances were made in the areas of image and speech recognition. Advances in <strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>) are more recent, though computational methods for NLP have been an area of research for decades. Processing textual data requires many different building blocks upon which advanced models can be built. Some of these building blocks themselves can be quite challenging and advanced. This chapter and the next focus on these building blocks and the problems that can be solved with them through simple models.</p>
    <p class="normal">In this chapter, we will focus on the basics of pre-processing text and build a simple spam detector. Specifically, we will learn about the following:</p>
    <ul>
      <li class="bullet">The typical text processing workflow</li>
      <li class="bullet">Data collection and labeling</li>
      <li class="bullet">Text normalization, including case normalization, text tokenization, stemming, and lemmatization<ul>
          <li class="bullet-l2">Modeling datasets that have been text normalized</li>
          <li class="bullet-l2">Vectorizing text</li>
          <li class="bullet-l2">Modeling datasets with vectorized text</li>
        </ul>
      </li>
    </ul>
    <p class="normal">Let's start by getting to grips with the text processing workflow most NLP modelsÂ use.</p>
    <h1 id="_idParaDest-15" class="title">A typical text processing workflow</h1>
    <p class="normal">To understand how to <a id="_idIndexMarker000"/>process text, it is important to understand the general workflow for NLP. The following diagram illustrates the basic steps:</p>
    <figure class="mediaobject"><img src="image/B16252_01_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.1: Typical stages of a text processing workflow</p>
    <p class="normal">The first two steps of the process in the preceding diagram involve collecting labeled data. A supervised model or even a semi-supervised model needs data to operate. The next step is usually normalizing and<a id="_idIndexMarker001"/> featurizing the data. Models have a hard time processing text data as is. There is a lot of hidden structure in a given text that needs to be processed and exposed. These two steps focus on that. The last step is building a model with the processed inputs. While NLP has some unique models, this chapter will use only a simple deep neural network and focus more on the normalization and vectorization/featurization. Often, the last three stages operate in a cycle, even though the diagram may give the impression of linearity. In industry, additional features require more effort to develop and more resources to keep running. Hence, it is important that features add value. Taking this approach, we will use a simple model to validate different normalization/vectorization/featurization steps. Now, let's look at each of these stages in detail.</p>
    <h1 id="_idParaDest-16" class="title">Data collection and labeling</h1>
    <p class="normal">The first step of any <strong class="keyword">Machine Learning</strong> (<strong class="keyword">ML</strong>) project is<a id="_idIndexMarker002"/> to obtain a dataset. Fortunately, in the text domain, there is<a id="_idIndexMarker003"/> plenty of data to be found. A common approach is to use libraries such<a id="_idIndexMarker004"/> as <code class="Code-In-Text--PACKT-">scrapy</code> or Beautiful Soup to scrape data from the web. However, data is usually unlabeled, and as such can't be used in supervised models directly. This data is quite useful though. Through the use of transfer learning, a language model can be trained using unsupervised or semi-supervised methods and can be further used with a small training dataset specific to the task at hand. We will cover transfer learning in more depth in <em class="chapterRef">Chapter 3</em>, <em class="italic">Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding</em>, when we look at transfer learning using BERT embeddings.</p>
    <p class="normal">In the labeling step, textual data <a id="_idIndexMarker005"/>sourced in the data collection step is labeled with the right classes. Let's take some examples. If the task is to build a<a id="_idIndexMarker006"/> spam classifier for emails, then the previous step would involve collecting lots of emails. This labeling step would be to attach a <em class="italic">spam</em> or <em class="italic">not spam</em> label to each email. Another example could be sentiment detection on tweets. The data collection step would involve gathering a number of tweets. This step would label each tweet with a label that acts as a ground truth. A more involved example would involve collecting news articles, where the labels would be summaries of the articles. Yet another example of such a case would be an email auto-reply functionality. Like the spam case, a number of emails with their replies would need to be collected. The labels in this case would be short pieces of text that would approximate replies. If you are working on a specific domain without much public data, you may have to do these steps yourself.</p>
    <p class="normal">Given that text data is generally available (outside of specific domains like health), labeling is usually the biggest challenge. It can be quite time consuming or resource intensive to label data. There has been a lot of recent focus on using semi-supervised approaches to labeling data. We will cover some methods for labeling data at scale using semi-supervised methods and the <strong class="keyword">snorkel</strong> library in <em class="chapterRef">Chapter 7</em>, <em class="italic">Multi-modal Networks and Image Captioning with ResNets and Transformer</em>, when we look at weakly supervised learning for classification using Snorkel.</p>
    <p class="normal">There is a number of commonly used datasets that are available on the web for use in training models. Using transfer learning, these generic datasets can be used to prime ML models and then you can use a small amount of domain-specific data to fine-tune the model. Using these publicly available datasets gives us a few advantages. First, all the data collection has been already performed. Second, labeling has already been done. Lastly, using such a dataset allows the comparison of results with the state of the art; most papers use specific datasets in their<a id="_idIndexMarker007"/> area of research and publish benchmarks. For example, the <strong class="keyword">Stanford Question Answering Dataset</strong> (or <strong class="keyword">SQuAD</strong> for short) is often used as a benchmark for question-answering models. It is a good source to train on as well.</p>
    <h2 id="_idParaDest-17" class="title">Collecting labeled data</h2>
    <p class="normal">In this book, we will<a id="_idIndexMarker008"/> rely on publicly available datasets. The appropriate datasets will be called out in their respective chapters along with instructions on downloading them. To build a spam detection system on an email dataset, we will be using the SMS Spam Collection dataset made available by University of California, Irvine. This dataset can be downloaded using instructions available in the tip box below. Each SMS is tagged as "SPAM" or "HAM," with the latter indicating it is not a spam message.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">University of California, Irvine, is a great source of machine learning datasets. You can see all the datasets they provide by visiting <a href="http://archive.ics.uci.edu/ml/datasets.php"><span class="url">http://archive.ics.uci.edu/ml/datasets.php</span></a>. Specifically for NLP, you can see some publicly available datasets on <a href="https://github.com/niderhoff/nlp-datasets"><span class="url">https://github.com/niderhoff/nlp-datasets</span></a>.</p>
    </div>
    <p class="normal">Before we start working with the data, the development environment needs to be set up. Let's take a quick moment to set up the development environment.</p>
    <h3 id="_idParaDest-18" class="title">Development environment setup</h3>
    <p class="normal">In this chapter, we will <a id="_idIndexMarker009"/>be using Google Colaboratory, or Colab for short, to write code. You can use your Google account, or register a new account. Google Colab is free to use, requires no configuration, and also provides access to GPUs. The user interface is very similar to a Jupyter notebook, so it should seem familiar. To get started, please navigate to <a href="http://colab.research.google.com"><span class="url">colab.research.google.com</span></a> using a supported web browser. A web page similar to the screenshot below should appear:</p>
    <figure class="mediaobject"><img src="image/B16252_01_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.2: Google Colab website</p>
    <p class="normal">The next step is to create a new notebook. There are a couple of options. The first option is to create a new notebook in Colab and type in the code as you go along in the chapter. The second option is to upload a notebook from the local drive into Colab. It is also possible to pull in <a id="_idIndexMarker010"/>notebooks from GitHub into Colab, the process for which is detailed on the Colab website. For the purposes of this chapter, a complete notebook named <code class="Code-In-Text--PACKT-">SMS_Spam_Detection.ipynb</code> is available in the GitHub repository of the book in the <code class="Code-In-Text--PACKT-">chapter1-nlp-essentials</code> folder. Please upload this notebook into Google Colab by clicking <strong class="scree Text">File | Upload Notebook</strong>. Specific sections of this notebook will be referred to at the appropriate points in the chapter in tip boxes. TheÂ instructions for creating the notebook from scratch are in the main description.</p>
    <p class="normal">Click on the <strong class="scree Text">File</strong> menu option at the top left and click on <strong class="scree Text">New Notebook</strong>. A new notebook will open in a new browser tab. Click on the notebook name at the top left, just above the <strong class="scree Text">File</strong> menu option, and edit it to read <code class="Code-In-Text--PACKT-">SMS_Spam_Detection</code>. Now the development environment is set up. It is time to begin loading in data.</p>
    <p class="normal">First, let us edit the first line of the notebook and import TensorFlow 2. Enter the following code in the first cell and execute it:</p>
    <pre class="programlisting code"><code class="hljs-code">%tensorflow_version <span class="hljs-number">2.</span>x
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> io
tf.__version__
</code></pre>
    <p class="normal">The output of running this cell should look like this:</p>
    <pre class="programlisting con"><code class="hljs-con">TensorFlow 2.x is selected.
'2.4.0'
</code></pre>
    <p class="normal">This confirms that <a id="_idIndexMarker011"/>version 2.4.0 of the TensorFlow library was loaded. The highlighted line in the preceding code block is a magic command for Google Colab, instructing it to use TensorFlow version 2+. The next step is to download the data file and unzip to a location in the Colab notebook on the cloud.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The code for loading the data is in the <em class="italic">Download Data</em> section of the notebook. Also note that as of writing, the release version of TensorFlow was 2.4.</p>
    </div>
    <p class="normal">This can be done with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Download the zip file</span>
path_to_zip = tf.keras.utils.get_file(<span class="hljs-string">"smsspamcollection.zip"</span>,
origin=<span class="hljs-string">"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"</span>,
                  extract=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Unzip the file into a folder</span>
!unzip $path_to_zip -d data
</code></pre>
    <p class="normal">The following output confirms that the data was downloaded and extracted:</p>
    <pre class="programlisting con"><code class="hljs-con">Archive:  /root/.keras/datasets/smsspamcollection.zip
  inflating: data/SMSSpamCollection  
  inflating: data/readme
</code></pre>
    <p class="normal">Reading the data file is trivial:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Let's see if we read the data correctly</span>
lines = io.<span class="hljs-built_in">open</span>(<span class="hljs-string">'data/SMSSpamCollection'</span>).read().strip().split(<span class="hljs-string">'\n'</span>)
lines[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">The last line of code shows a sample line of data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">'ham\tGo until jurong point, crazy.. Available only in bugis n great world'</span>
</code></pre>
    <p class="normal">This example is labeled <a id="_idIndexMarker012"/>as not spam. The next step is to split each line into two columns â one with the text of the message and the other as the label. While we are separating these labels, we will also convert the labels to numeric values. Since we are interested in predicting spam messages, we can assign a value of <code class="Code-In-Text--PACKT-">1</code> to the spam messages. A value of <code class="Code-In-Text--PACKT-">0</code> will be assigned to legitimate messages.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The code for this part is in the <em class="italic">Pre-Process Data</em> section of the notebook.</p>
    </div>
    <p class="normal">Please note that the following code is verbose for clarity:</p>
    <pre class="programlisting code"><code class="hljs-code">spam_dataset = []
<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines:
  label, text = line.split(<span class="hljs-string">'\t'</span>)
  <span class="hljs-keyword">if</span> label.strip() == <span class="hljs-string">'spam'</span>:
    spam_dataset.append((<span class="hljs-number">1</span>, text.strip()))
  <span class="hljs-keyword">else</span>:
    spam_dataset.append(((<span class="hljs-number">0</span>, text.strip())))
print(spam_dataset[<span class="hljs-number">0</span>]) 
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">(0, 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')
</code></pre>
    <p class="normal">Now the dataset is ready for further processing in the pipeline. However, let's take aÂ short detour to see how to configure GPU access in Google Colab.</p>
    <h2 id="_idParaDest-19" class="title">Enabling GPUs on Google Colab</h2>
    <p class="normal">One of the advantages of using<a id="_idIndexMarker013"/> Google Colab is access to free GPUs for small tasks. GPUs<a id="_idIndexMarker014"/> make a big difference in the training time of NLP models, especially <a id="_idIndexMarker015"/>ones that use <strong class="keyword">Recurrent Neural Networks</strong> (<strong class="keyword">RNNs</strong>). The first step in enabling GPU access is to start a runtime, which can be done by executing a command in the notebook. Then, click on the <strong class="scree Text">Runtime</strong> menu option and select the <strong class="scree Text">Change Runtime</strong> option, as shown in the following screenshot:</p>
    <figure class="mediaobject"><img src="image/B16252_01_03.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.3: Colab runtime settings menu option</p>
    <p class="normal">Next, a dialog box will show<a id="_idIndexMarker016"/> up, as shown in the following screenshot. Expand the<a id="_idIndexMarker017"/> <strong class="scree Text">Hardware Accelerator</strong> option and select <strong class="scree Text">GPU</strong>:</p>
    <figure class="mediaobject"><img src="image/B16252_01_04.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.4: Enabling GPUs on Colab</p>
    <p class="normal">Now you should<a id="_idIndexMarker018"/> have access to a GPU in your Colab notebook! In NLP <a id="_idIndexMarker019"/>models, especially when using RNNs, GPUs can shave a lot of minutes or hours off the training time.</p>
    <p class="normal">For now, let's turn our attention back to the data that has been loaded and is ready toÂ be processed further for use in models.</p>
    <h1 id="_idParaDest-20" class="title">Text normalization</h1>
    <p class="normal">Text normalization is a <a id="_idIndexMarker020"/>pre-processing step aimed at improving the quality of the textÂ and making it suitable for machines to process. Four<a id="_idIndexMarker021"/> main steps in textÂ normalization are case normalization, tokenization and stop word removal, <strong class="keyword">Parts-of-Speech</strong> (<strong class="keyword">POS</strong>) tagging, and stemming.</p>
    <p class="normal">Case normalization applies to languages that use uppercase and lowercase letters. All languages based on the Latin alphabet or the Cyrillic alphabet (Russian, Mongolian, and so on) use upper- and lowercase letters. Other languages that sometimes use this are Greek, Armenian, Cherokee, and Coptic. In case normalization, all letters are converted to the same case. It is quite helpful in semantic use cases. However, in other cases, this may hinder performance. In the spam example, spam messages may have more words in all-caps compared to regular messages.</p>
    <p class="normal">Another common normalization step removes punctuation in the text. Again, this may or may not be useful given the problem at hand. In most cases, this should give good results. However, in some cases, such as spam or grammar models, it may hinder performance. It is more likely for spam messages to use more exclamation marks or other punctuation for emphasis.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The code for this part is in the <em class="italic">Data Normalization</em> section of the notebook.</p>
    </div>
    <p class="normal">Let's build a baseline model <a id="_idIndexMarker022"/>with three simple features:</p>
    <ul>
      <li class="bullet">Number of characters in the message</li>
      <li class="bullet">Number of capital letters in the message</li>
      <li class="bullet">Number of punctuation symbols in the message</li>
    </ul>
    <p class="normal">To do so, first, we will convert the data into a <code class="Code-In-Text--PACKT-">pandas</code> DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.DataFrame(spam_dataset, columns=[<span class="hljs-string">'Spam'</span>, <span class="hljs-string">'Message'</span>])
</code></pre>
    <p class="normal">Next, let's build some simple functions that can count the length of the message, andÂ the numbers of capital letters and punctuation symbols. Python's regular expression package, <code class="Code-In-Text--PACKT-">re</code>, will be used to implement these:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> re
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">message_length</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
  <span class="hljs-comment"># returns total number of characters</span>
  <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(x)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">num_capitals</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
<span class="code-highlight"><strong class="hljs-slc">  _, count = re.subn(</strong><strong class="hljs-string-slc">r'[A-Z]'</strong><strong class="hljs-slc">, </strong><strong class="hljs-string-slc">''</strong><strong class="hljs-slc">, x) </strong><strong class="hljs-comment-slc"># only works in english</strong></span>
  <span class="hljs-keyword">return</span> count
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">num_punctuation</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
  _, count = re.subn(<span class="hljs-string">r'\W'</span>, <span class="hljs-string">''</span>, x)
  <span class="hljs-keyword">return</span> count
</code></pre>
    <p class="normal">In the <code class="Code-In-Text--PACKT-">num_capitals()</code> function, substitutions are performed for the capital letters in English. The <code class="Code-In-Text--PACKT-">count</code> of these substitutions provides the count of capital letters. The same technique is used to count the number of punctuation symbols. Please note that the method used to count capital letters is specific to English.</p>
    <p class="normal">Additional feature columns will be added to the DataFrame, and then the set will beÂ split into test and train sets:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'Capitals'</span>] = df[<span class="hljs-string">'Message'</span>].apply(num_capitals)
df[<span class="hljs-string">'Punctuation'</span>] = df[<span class="hljs-string">'Message'</span>].apply(num_punctuation)
df[<span class="hljs-string">'Length'</span>] = df[<span class="hljs-string">'Message'</span>].apply(message_length)
df.describe()
</code></pre>
    <p class="normal">This should generate the<a id="_idIndexMarker023"/> following output:</p>
    <figure class="mediaobject"><img src="image/B16252_01_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.5: Base dataset for initial spam model</p>
    <p class="normal">The following code can be used to split the dataset into training and test sets, with 80% of the records in the training set and the rest in the test set. Further more, labels will be removed from both the training and test sets:</p>
    <pre class="programlisting code"><code class="hljs-code">train=df.sample(frac=<span class="hljs-number">0.8</span>,random_state=<span class="hljs-number">42</span>)
test=df.drop(train.index)
x_train = train[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Capitals'</span>, <span class="hljs-string">'Punctuation'</span>]]
y_train = train[[<span class="hljs-string">'Spam'</span>]]
x_test = test[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Capitals'</span>, <span class="hljs-string">'Punctuation'</span>]]
y_test = test[[<span class="hljs-string">'Spam'</span>]]
</code></pre>
    <p class="normal">Now we are ready to build a simple classifier to use this data. </p>
    <h2 id="_idParaDest-21" class="title">Modeling normalized data</h2>
    <p class="normal">Recall that modeling was the<a id="_idIndexMarker024"/> last part of the text processing pipeline described earlier. In this chapter, we will use a very simple model, as the objective is to show different basic NLP data processing techniques more than modeling. Here, we want to see if three simple features can aid in the classification of spam. As more features are added, passing them through the same model will help in seeing if the featurization aids or hampers the accuracy of the classification.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The <em class="italic">Model Building</em> section of the workbook has the code shown in this section.</p>
    </div>
    <p class="normal">A function is defined that allows the construction of models with different numbers of inputs and hidden units:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Basic 1-layer neural network model for evaluation</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">make_model</span><span class="hljs-functio">(</span><span class="hljs-params">input_dims=</span><span class="hljs-number">3</span><span class="hljs-params">, num_units=</span><span class="hljs-number">12</span><span class="hljs-functio">):</span>
  model = tf.keras.Sequential()
  <span class="hljs-comment"># Adds a densely-connected layer with 12 units to the model:</span>
  model.add(tf.keras.layers.Dense(num_units, 
                                  input_dim=input_dims,
                                  activation=<span class="hljs-string">'relu'</span>))
  <span class="hljs-comment"># Add a sigmoid layer with a binary output unit:</span>
  model.add(tf.keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
  model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>, 
                metrics=[<span class="hljs-string">'accuracy'</span>])
  <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">This model uses binary cross-entropy for computing loss and the Adam optimizer for training. The key metric, given that this is a binary classification problem, is accuracy. The default parameters passed to the function are sufficient as only three features are being passed in. </p>
    <p class="normal">We can train our simple baseline model with only three features like so:</p>
    <pre class="programlisting code"><code class="hljs-code">model = make_model()
model.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Train on 4459 samples
Epoch 1/10
4459/4459 [==============================] - 1s 281us/sample - loss: 0.6062 - accuracy: 0.8141
Epoch 2/10
â¦
Epoch 10/10
4459/4459 [==============================] - 1s 145us/sample - loss: 0.1976 - accuracy: 0.9305
</code></pre>
    <p class="normal">This is not bad as our three simple features help us get to 93% accuracy. A quick check shows that there are 592 spam messages in the test set, out of a total of 4,459. So, this model is doing better than a <a id="_idIndexMarker025"/>very simple model that guesses everything as not spam. That model would have an accuracy of 87%. This number may be surprising but is fairly common in classification problems where there is a severe class imbalance in the data. Evaluating it on the training set gives an accuracy of around 93.4%:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(x_test, y_test)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1115/1115 [==============================] - 0s 94us/sample - loss: 0.1949 - accuracy: 0.9336
[0.19485870356516988, 0.9336323]
</code></pre>
    <p class="normal">Please note that the actual performance you see may be slightly different due toÂ the data splits and computational vagaries. A quick verification can be performed by plotting the confusion matrix to see the performance:</p>
    <pre class="programlisting code"><code class="hljs-code">y_train_pred = model.predict_classes(x_train)
<span class="hljs-comment"># confusion matrix</span>
tf.math.confusion_matrix(tf.constant(y_train.Spam), 
                         y_train_pred)
&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int32, numpy=
array([[<span class="hljs-number">3771</span>,   <span class="hljs-number">96</span>],
      [ <span class="hljs-number">186</span>,  <span class="hljs-number">406</span>]], dtype=int32)&gt;
</code></pre>
    <table id="table001">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Predicted Not Spam</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Predicted Spam</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Actual Not Spam</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">3,771</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">96</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Actual Spam</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">186</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">406</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">This shows that 3,771 out of 3,867 regular messages were classified correctly, while 406 out of 592 spam messages were classified correctly. Again, you may get a slightlyÂ different result.</p>
    <p class="normal">To test the value of the <a id="_idIndexMarker026"/>features, try re-running the model by removing one of the features, such as punctuation or a number of capital letters, to get a sense of their contribution to the model. This is left as an exercise for the reader.</p>
    <h2 id="_idParaDest-22" class="title">Tokenization</h2>
    <p class="normal">This step takes a piece of text and <a id="_idIndexMarker027"/>converts it into a list of tokens. If the input is a sentence, then separating the words would be an example of tokenization. Depending on the model, different granularities can be chosen. At the lowest level, each character could become a token. In some cases, entire sentences of paragraphs can beÂ considered as a token:</p>
    <figure class="mediaobject"><img src="image/B16252_01_06.png" alt="A close up of a logo  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.6: Tokenizing a sentence</p>
    <p class="normal">The preceding diagram shows two ways a sentence can be tokenized. One way to tokenize is to chop a sentence into words. Another way is to chop into individual characters. However, this can be a complex proposition in some languages such as Japanese and Mandarin.</p>
    <h3 id="_idParaDest-23" class="title">Segmentation in Japanese</h3>
    <p class="normal">Many languages use a word separator, a <a id="_idIndexMarker028"/>space, to separate words. This makes the task of tokenizing on words trivial. However, there are other languages that do not use any markers or separators between words. Some examples of such languages are Japanese <a id="_idIndexMarker029"/>and Chinese. In such languages, the task is referred to as <em class="italic">segmentation</em>. </p>
    <p class="normal">Specifically, in Japanese, there are mainly three different types of characters that are used: <em class="italic">Hiragana</em>, <em class="italic">Kanji</em>, and <em class="italic">Katakana</em>. Kanji is adapted from Chinese characters, and similar to Chinese, there are thousands of characters. Hiragana is used for grammatical elements and native Japanese words. Katakana is mostly used for foreign words and names. Depending on the preceding characters, a character may be part of an existing word or the start of a new word. This makes Japanese one of the most complicated writing systems in the world. Compound words are especially hard. Consider the following compound word that reads <em class="italic">Election Administration Committee</em>:</p>
    <p class="normal"><img src="image/B16252_01_003.png" alt=""/></p>
    <p class="normal">This can be tokenized in two different ways, outside of the entire phrase being considered one word. Here are two examples of tokenizing (from the Sudachi library):</p>
    <p class="normal"><img src="image/B16252_01_004.png" alt=""/> (Election / Administration / Committee)</p>
    <p class="normal"><img src="image/B16252_01_005.png" alt=""/> (Election / Administration / Committee / Meeting)</p>
    <p class="normal">Common libraries that are used specifically for Japanese segmentation or tokenization are MeCab, Juman, Sudachi, and Kuromoji. MeCab is used in Hugging Face, spaCy, and other libraries.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The code shown in this section is in the <em class="italic">Tokenization and Stop Word Removal</em> section of the notebook.</p>
    </div>
    <p class="normal">Fortunately, most languages are not as complex as Japanese and use spaces to separate words. In Python, splitting by spaces is trivial. Let's take an example:</p>
    <pre class="programlisting code"><code class="hljs-code">Sentence = <span class="hljs-string">'Go until Jurong point, crazy.. Available only in bugis n great world'</span>
sentence.split()
</code></pre>
    <p class="normal">The output of the preceding split operation results in the following:</p>
    <pre class="programlisting con"><code class="hljs-con">['Go',
 'until',
 'jurong',
<span class="code-highlight"><strong class="hljs-con-slc"> 'point,',
 'crazy..',</strong></span>
 'Available',
 'only',
 'in',
 'bugis',
 'n',
 'great',
 'world']
</code></pre>
    <p class="normal">The two highlighted lines in the preceding output show that the naÃ¯ve approach in Python will result in punctuation being included in the words, among other issues. Consequently, this step is done through a library like StanfordNLP. Using <code class="Code-In-Text--PACKT-">pip</code>, let's install this package in our Colab notebook:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install stanfordnlp
</code></pre>
    <p class="normal">The StanfordNLP package uses <a id="_idIndexMarker030"/>PyTorch under the hood as well as a number of other packages. These and other <a id="_idIndexMarker031"/>dependencies will be installed. By default, the package does not install language files. These have to be downloaded. This is shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">Import stanfordnlp <span class="hljs-keyword">as</span> snlp
en = snlp.download(<span class="hljs-string">'en'</span>)
</code></pre>
    <p class="normal">The English file is approximately 235 MB. A prompt will be displayed to confirm the download and the location to store it in:</p>
    <figure class="mediaobject"><img src="image/B16252_01_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.7: Prompt for downloading English models</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Google Colab recycles the runtimes upon inactivity. This means that if you perform commands in the book at different times, you may have to re-execute every command again from the start, including downloading and processing the dataset, downloading the StanfordNLP English files, and so on. A local notebook server would usually maintain the state of the runtime but may have limited processing power. For simpler examples as in this chapter, Google Colab is a decent solution. For the more advanced examples later in the book, where training may run for hours or days, a local runtime or one running on a cloud <strong class="keyword">Virtual Machine</strong> (<strong class="keyword">VM</strong>) would be preferred.</p>
    </div>
    <p class="normal">This package provides<a id="_idIndexMarker032"/> capabilities for tokenization, POS tagging, and lemmatization out of the box. To start with tokenization, we instantiate a pipeline and tokenize a sample text to see how this works:</p>
    <pre class="programlisting code"><code class="hljs-code">en = snlp.Pipeline(lang=<span class="hljs-string">'en'</span>, processors=<span class="hljs-string">'tokenize'</span>)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">lang</code> parameter is used to indicate that an English pipeline is desired. The second parameter, <code class="Code-In-Text--PACKT-">processors</code>, indicates the type of processing that is desired in the pipeline. This library can also perform the following processing steps in the pipeline:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">pos</code> labels each token with a POS token. The next section provides more details on POS tags.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">lemma</code>, which can convert different forms of verbs, for example, to the base form. This will be covered in detail in the <em class="italic">Stemming and lemmatization</em> section later in this chapter.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">depparse</code> performs dependency parsing between words in a sentence. Consider the following example sentence, "Hari went to school." <em class="italic">Hari</em> is interpreted as a noun by the POS tagger, and becomes the governor of the word <em class="italic">went</em>. The word <em class="italic">school</em> is dependent on <em class="italic">went</em> as it describes the object ofÂ the verb.</li>
    </ul>
    <p class="normal">For now, only tokenization of text is desired, so only the tokenizer is used:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenized = en(sentence)
<span class="hljs-built_in">len</span>(tokenized.sentences)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">2
</code></pre>
    <p class="normal">This shows that the tokenizer correctly divided the text into two sentences. To investigate what words were removed, the following code can be used:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> snt <span class="hljs-keyword">in</span> tokenized.sentences:
  <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> snt.tokens:
    print(word.text)
  print(<span class="hljs-string">"&lt;End of Sentence&gt;"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Go
until
jurong
<span class="code-highlight"><strong class="hljs-con-slc">point
,
crazy</strong></span>
..
&lt;End of Sentence&gt;
Available
only
in
bugis
n
great
world
&lt;End of Sentence&gt;
</code></pre>
    <p class="normal">Note the highlighted words in the preceding output. Punctuation marks were separated out into their own words. Text was split into multiple sentences. This is an improvement over only using spaces to split. In some<a id="_idIndexMarker033"/> applications, removal of punctuation may be required. This will be covered in the next section.</p>
    <p class="normal">Consider the preceding example of Japanese. To see the performance of StanfordNLP on Japanese tokenization, the following piece of code can be used:</p>
    <pre class="programlisting code"><code class="hljs-code">jp = snlp.download(<span class="hljs-string">'ja'</span>)
</code></pre>
    <p class="normal">This is the first step, which involves downloading the Japanese language model, similar to the English model that was downloaded and installed previously. Next, a Japanese pipeline will be instantiated and the words will be processed:</p>
    <pre class="programlisting code"><code class="hljs-code">jp = snlp.download(<span class="hljs-string">'ja'</span>)
jp_line = jp(<span class="hljs-string">"<img src="image/B16252_01_001.png" alt="" style="max-height:15px;"/></span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">You may recall that the Japanese text reads Election Administration Committee. Correct tokenization should produce three words, where first two should be two characters each, and the last word is three characters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> snt <span class="hljs-keyword">in</span> jp_line.sentences:
  <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> snt.tokens:
    print(word.text)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"><img src="image/B16252_01_002.png" alt=""/>
</code></pre>
    <p class="normal">This matches the expected output. StanfordNLP supports 53 languages, so the same code can be used for tokenizing any language that is supported.</p>
    <p class="normal">Coming back to the spam detection example, a new feature can be implemented that counts the number of words in the message using this tokenization functionality.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">This word count feature is implemented in the <em class="italic">Adding Word Count Feature</em> section of the notebook.</p>
    </div>
    <p class="normal">It is possible that spam messages <a id="_idIndexMarker034"/>have different numbers of words than regular messages. The first step is to define a method to compute the number of words:</p>
    <pre class="programlisting code"><code class="hljs-code">en = snlp.Pipeline(lang=<span class="hljs-string">'en'</span>)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">word_counts</span><span class="hljs-functio">(</span><span class="hljs-params">x, pipeline=en</span><span class="hljs-functio">):</span>
  doc = pipeline(x)
  count = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(sentence.tokens) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> doc.sentences])
  <span class="hljs-keyword">return</span> count
</code></pre>
    <p class="normal">Next, using the train and test splits, add a column for the word count feature:</p>
    <pre class="programlisting code"><code class="hljs-code">train[<span class="hljs-string">'Words'</span>] = train[<span class="hljs-string">'Message'</span>].apply(word_counts)
test[<span class="hljs-string">'Words'</span>] = test[<span class="hljs-string">'Message'</span>].apply(word_counts)
x_train = train[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Punctuation'</span>, <span class="hljs-string">'Capitals'</span>, <span class="hljs-string">'Words'</span>]]
y_train = train[[<span class="hljs-string">'Spam'</span>]]
x_test = test[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Punctuation'</span>, <span class="hljs-string">'Capitals'</span> , <span class="hljs-string">'Words'</span>]]
y_test = test[[<span class="hljs-string">'Spam'</span>]]
model = make_model(input_dims=<span class="hljs-number">4</span>)
</code></pre>
    <p class="normal">The last line in the preceding code block creates a new model with four input features.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-"><strong class="scree Text">PyTorch warning</strong></p>
      <p class="Tip--PACKT-">When you execute functions in the StanfordNLP library, you may see a warning like this:</p>
      <pre class="programlisting gen"><code class="hljs"><code class="Code-In-Text--PACKT-">/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.</code>
</code></pre>
      <p class="Tip--PACKT-">Internally, StanfordNLP uses the PyTorch library. This warning is due to StanfordNLP using an older version of a function that is now deprecated. For all intents and purposes, this warning can be ignored. It is expected that maintainers of StanfordNLP will <a id="_idIndexMarker035"/>update their code.</p>
    </div>
    <h3 id="_idParaDest-24" class="title">Modeling tokenized data</h3>
    <p class="normal">This model can be trained like<a id="_idIndexMarker036"/> so:</p>
    <pre class="programlisting con"><code class="hljs-con">model.fit(x_train, y_train, epochs=10, batch_size=10)
Train on 4459 samples
Epoch 1/10
4459/4459 [==============================] - 1s 202us/sample - loss: 2.4261 - accuracy: 0.6961
<span class="hljs-co -meta">...</span>
Epoch 10/10
4459/4459 [==============================] - 1s 142us/sample - loss: 0.2061 - accuracy: 0.9312
</code></pre>
    <p class="normal">There is only a marginal improvement in accuracy. One hypothesis is that the number of words is not useful. It would be useful if the average number of words in spam messages were smaller or larger than regular messages. Using pandas, this can be quickly verified:</p>
    <pre class="programlisting code"><code class="hljs-code">train.loc[train.Spam == <span class="hljs-number">1</span>].describe()
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_01_08.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.8: Statistics for spam message features</p>
    <p class="normal">Let's compare the preceding results to<a id="_idIndexMarker037"/> the statistics for regular messages:</p>
    <pre class="programlisting code"><code class="hljs-code">train.loc[train.Spam == <span class="hljs-number">0</span>].describe()
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_01_09.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.9: Statistics for regular message features</p>
    <p class="normal">Some interesting patterns can quickly be seen. Spam messages usually have much less deviation from the mean. Focus on the <strong class="keyword">Capitals</strong> feature column. It shows that regular messages use far fewer capitals than spam messages. At the 75<sup class="Superscript--PACKT-">th</sup> percentile, there are 3 capitals in a regular message versus 21 for spam messages. On average, regular messages have 4 capital letters while spam messages have 15. This variation is much less pronounced in the number of words category. Regular messages have 17 words on average, while spam has 29. At the 75<sup class="Superscript--PACKT-">th</sup> percentile, regular messages have 22 words while spam messages have 35. This quick check yields an indication as to why adding the word features wasn't that useful. However, there are a couple of things to consider still. First, the tokenization model split out punctuation marks as words. Ideally, these words should be removed <a id="_idIndexMarker038"/>from the word counts as the punctuation feature is showing that spam messages use a lot more punctuation characters. This will be covered in the <em class="italic">Parts-of-speech tagging</em> section. Secondly, languages have some common words that are usually excluded. This is called stop word removal and is the focus of the next section.</p>
    <h2 id="_idParaDest-25" class="title">Stop word removal</h2>
    <p class="normal">Stop word removal involves removing common <a id="_idIndexMarker039"/>words such as articles (the, an) andÂ conjunctions (and, but), among others. In the context of information retrieval or search, these words would not be helpful in identifying documents or web pages that <a id="_idIndexMarker040"/>would match the query. As an example, consider the query "Where is Google based?". In this query, <em class="italic">is</em> is a stop word. The query would produce similar results irrespective of the inclusion of <em class="italic">is</em>. To determine the stop words, a simple approach is to use grammar clues. </p>
    <p class="normal">In English, articles and conjunctions are examples of classes of words that can usually be removed. A more robust way is to consider the frequency of occurrence of words in a corpus, set of documents, or text. The most frequent terms can be selected as candidates for the stop word list. It is recommended that this list be reviewed manually. There can be cases where words may be frequent in a collection of documents but are still meaningful. This can happen if all the documents in the collection are from a specific domain or on a specific topic. Consider a set of documents from the Federal Reserve. The word <em class="italic">economy</em> may appear quite frequently in this case; however, it is unlikely to be a candidate for removal as a stop word.</p>
    <p class="normal">In some cases, stop words may actually contain information. This may be applicable to phrases. Consider the fragment "flights to Paris." In this case, <em class="italic">to</em> provides valuable information, and its removal may change the meaning of the fragment.</p>
    <p class="normal">Recall the stages of the text processing workflow. The step after text normalization is vectorization. This step is discussed in detail later in the <em class="italic">Vectorizing text</em> section of this chapter, but the key step in vectorization is to build a vocabulary or dictionary of all the tokens. The size of this vocabulary can be reduced by removing stop words. While training and evaluating models, removing stop words reduces the number of computation steps that need to be performed. Hence, the removal of stop words can yield benefits in terms of computation<a id="_idIndexMarker041"/> speed and storage space. Modern advances in NLP see smaller and smaller stop words lists as more efficient encoding schemes and computation methods evolve. Let's try and see the impact of stop words on the spam problem to develop some intuition about its usefulness.</p>
    <p class="normal">Many NLP packages<a id="_idIndexMarker042"/> provide lists of stop words. These can be removed from the text after tokenization. Tokenization was done through the StanfordNLP library previously. However, this library does not come with a list of stop words. NLTK andÂ spaCy supply stop words for a set of languages. For this example, we will use anÂ open source package called <code class="Code-In-Text--PACKT-">stopwordsiso</code>.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The <em class="italic">Stop Word Removal</em> section of the notebook contains the code for this section.</p>
    </div>
    <p class="normal">This Python package takes the list of stop words from the stopwords-iso GitHub project at <a href="https://github.com/stopwords-iso/stopwords-iso"><span class="url">https://github.com/stopwords-iso/stopwords-iso</span></a>. This package provides stop words in 57 languages. The first step is to install the Python package that provides access to the stop words lists. </p>
    <p class="normal">The following command will install the package through the notebook:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install stopwordsiso
</code></pre>
    <p class="normal">Supported languages can be checked with the following commands:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> stopwordsiso <span class="hljs-keyword">as</span> stopwords
stopwords.langs()
</code></pre>
    <p class="normal">English language stop words can be checked as well to get an idea of some of the words:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">sorted</span>(stopwords.stopwords(<span class="hljs-string">'en'</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">["'ll",
 "'tis",
 "'twas",
 "'ve",
 '10',
 '39',
 'a',
 "a's",
 'able',
 'ableabout',
 'about',
 'above',
 'abroad',
 'abst',
 'accordance',
 'according',
 'accordingly',
 'across',
 'act',
 'actually',
 'ad',
 'added',
<span class="hljs-co -meta">...</span>
</code></pre>
    <p class="normal">Given that tokenization<a id="_idIndexMarker043"/> was already implemented in the preceding <code class="Code-In-Text--PACKT-">word_counts()</code> method, the implementation of that method can be updated to include removing stop words. However, all the stop words are in lowercase. Case normalization was discussed earlier, and capital letters were a useful feature for spam detection. In this case, tokens need to be converted to lowercase to effectively remove them:</p>
    <pre class="programlisting code"><code class="hljs-code">en_sw = stopwords.stopwords(<span class="hljs-string">'en'</span>)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">word_counts</span><span class="hljs-functio">(</span><span class="hljs-params">x, pipeline=en</span><span class="hljs-functio">):</span>
  doc = pipeline(x)
  count = <span class="hljs-number">0</span>
  <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> doc.sentences:
    <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> sentence.tokens:
        <span class="hljs-keyword">if</span> token.text.lower() <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> en_sw:
          count += <span class="hljs-number">1</span>
  <span class="hljs-keyword">return</span> count
</code></pre>
    <p class="normal">A consequence of using stop words is that a message such as "When are you going to ride your bike?" counts as only 3 words. When we see if this has had any effect on the statistics for word length, the following picture emerges:</p>
    <figure class="mediaobject"><img src="image/B16252_01_10.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.10: Word counts for spam messages after removing stop words</p>
    <p class="normal">Compared to the word<a id="_idIndexMarker044"/> counts prior to stop word removal, the average number of words has been reduced from 29 to 18, almost a 30% decrease. The 25<sup class="Superscript--PACKT-">th</sup> percentile<a id="_idIndexMarker045"/> changed from 26 to 14. The maximum has also reduced from 49 to 33. </p>
    <p class="normal">The impact on regular messages is even more dramatic:</p>
    <figure class="mediaobject"><img src="image/B16252_01_11.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.11: Word counts for regular messages after removing stop words</p>
    <p class="normal">Comparing these statistics to<a id="_idIndexMarker046"/> those from before stop word removal, the average number of words has more than halved to almost 8. The maximum number<a id="_idIndexMarker047"/> of words has also reduced from 209 to 147. The standard deviation of regular messages is about the same as its mean, indicating that there is a lot of variation in the number ofÂ words in regular messages. Now, let's see if this helps us train a model and improve its accuracy.</p>
    <h3 id="_idParaDest-26" class="title">Modeling data with stop words removed</h3>
    <p class="normal">Now that the<a id="_idIndexMarker048"/> feature without stop words is computed, it can be added to the model to see its impact:</p>
    <pre class="programlisting code"><code class="hljs-code">train[<span class="hljs-string">'Words'</span>] = train[<span class="hljs-string">'Message'</span>].apply(word_counts)
test[<span class="hljs-string">'Words'</span>] = test[<span class="hljs-string">'Message'</span>].apply(word_counts)
x_train = train[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Punctuation'</span>, <span class="hljs-string">'Capitals'</span>, <span class="hljs-string">'Words'</span>]]
y_train = train[[<span class="hljs-string">'Spam'</span>]]
x_test = test[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Punctuation'</span>, <span class="hljs-string">'Capitals'</span>, <span class="hljs-string">'Words'</span>]]
y_test = test[[<span class="hljs-string">'Spam'</span>]]
model = make_model(input_dims=<span class="hljs-number">4</span>)
model.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Epoch 1/10
4459/4459 [==============================] - 2s 361us/sample - loss: 0.5186 - accuracy: 0.8652
Epoch 2/10
<span class="hljs-co -meta">...</span>
Epoch 9/10
4459/4459 [==============================] - 2s 355us/sample - loss: 0.1790 - accuracy: 0.9417
Epoch 10/10
4459/4459 [==============================] - 2s 361us/sample - loss: 0.1802 - accuracy: 0.9421
</code></pre>
    <p class="normal">This accuracy reflects a<a id="_idIndexMarker049"/> slight improvement over the previous model:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(x_test, y_test)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1115/1115 [==============================] - 0s 74us/sample - loss: 0.1954 - accuracy: 0.9372
 [0.19537461110027382, 0.93721974]
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">In NLP, stop word removal used to be standard practice. In more modern applications, stop words may actually end up hindering performance in some use cases, rather than helping. It is becoming more common not to exclude stop words. Depending on the problem you are solving, stop word removal may or may not help.</p>
    </div>
    <p class="normal">Note that StanfordNLP will <a id="_idIndexMarker050"/>separate words like <em class="italic">can't</em> into <em class="italic">ca</em> and <em class="italic">n't</em>. This represents the expansion of the short form into its constituents, <em class="italic">can</em> and <em class="italic">not</em>. These contractions may or may not appear in the stop word list. Implementing a more robust stop word detector is left to the reader as an exercise.</p>
    <p class="normal">StanfordNLP uses a supervised RNN<a id="_idIndexMarker051"/> with <strong class="keyword">Bi-directional Long Short-Term Memory</strong> (<strong class="keyword">BiLSTM</strong>) units. This architecture uses a vocabulary to generate embeddings through the vectorization of the vocabulary. The vectorization and generation of embeddings is covered later in the chapter, in the <em class="italic">Vectorizing text</em> section. This architecture of BiLSTMs with embeddings is often a common starting point in NLP tasks. This will be covered and used in successive chapters in detail. This particular architecture for tokenization is considered the state of the art as of the time of writing this book. Prior to this, <strong class="keyword">Hidden Markov Model</strong> (<strong class="keyword">HMM</strong>)-based models were<a id="_idIndexMarker052"/> popular. </p>
    <p class="normal">Depending on the languages in question, regular expression-based tokenization is also another approach. The NLTK library provides the Penn Treebank tokenizer based on regular expressions in a <code class="Code-In-Text--PACKT-">sed</code> script. In future chapters, other<a id="_idIndexMarker053"/> tokenization or segmentation schemes such as <strong class="keyword">Byte Pair Encoding</strong> (<strong class="keyword">BPE</strong>) and WordPiece will be <a id="_idIndexMarker054"/>explained.</p>
    <p class="normal">The next task in text normalization is to understand the structure of a text through POS tagging.</p>
    <h2 id="_idParaDest-27" class="title">Part-of-speech tagging </h2>
    <p class="normal">Languages have a grammatical structure. In most languages, words can be categorized primarily into verbs, adverbs, nouns, and adjectives. The objective of this part of the processing step is to take a piece of text and<a id="_idIndexMarker055"/> tag each word token with a POS identifier. Note that this makes sense only in the case of word-level tokens. Commonly, the Penn Treebank POS tagger is used by libraries including StanfordNLP to tag words. By convention, POS tags are added by using a code after the word, separated by a slash. As an example, <code class="Code-In-Text--PACKT-">NNS</code> is the tag for a plural noun. If the words <code class="Code-In-Text--PACKT-">goats</code> was encountered, it would be represented as <code class="Code-In-Text--PACKT-">goats/NNS</code>. In the StandfordNLP library, <strong class="keyword">Universal POS</strong> (<strong class="keyword">UPOS</strong>) tags are<a id="_idIndexMarker056"/> used. The following tags are part of the UPOS tag set. More details on mapping of standard POS tags to UPOS tags can be seen at <a href="https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html"><span class="url">https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html</span></a>. The following is a table of the most common tags:</p>
    <table id="table002">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Tag</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Class</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Examples</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">ADJ</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Adjective</strong>: Usually describes a noun. Separate tags are used for comparatives and superlatives.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Great, pretty</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">ADP</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Adposition</strong>: Used to modify an object such as a noun, pronoun, or phrase; for example, "Walk <strong class="keyword">up</strong> the stairs." Some languages like English use prepositions while others such as Hindi and Japanese use postpositions.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Up, inside</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">ADV</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Adverb</strong>: A word or phrase that modifies or qualifies an adjective, verb, or another adverb.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Loudly, often</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">AUX</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Auxiliary verb</strong>: Used in forming mood, voice, or tenses of other verbs.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Will, can, may</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">CCONJ</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Co-ordinating conjunction</strong>: Joins two phrases, clauses, or sentences.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">And, but, that</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">INTJ</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Interjection</strong>: An exclamation, interruption, or sudden remark.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Oh, uh, lol</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">NOUN</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Noun</strong>: Identifies people, places, or things.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Office, book</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">NUM</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Numeral</strong>: Represents a quantity.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Six, nine</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">DET</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Determiner</strong>: Identifies a specific noun, usually as a singular.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">A, an, the</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PART</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Particle</strong>: Parts of speech outside of the main types.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">To, n't</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PRON</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Pronoun</strong>: Substitutes for other nouns, especially proper nouns.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">She, her</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PROPN</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Proper noun</strong>: A name for a specific person, place, or thing.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Gandhi, US</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PUNCT</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Different punctuation symbols.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">, ? /</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">SCONJ</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Subordinating conjunction</strong>: Connects independent clause to a dependent clause.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Because, while</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">SYM</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Symbols including currency signs, emojis, and so on.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">$, #, % :)</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">VERB</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Verb</strong>: Denotes action or occurrence.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Go, do</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">X</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Other</strong>: That which cannot be classified elsewhere.</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Etc, 4. (a numbered list bullet)</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The best way to understand how POS tagging works is to try it out:</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The code for this section is in the <em class="italic">POS Based Features</em> section of the notebook.</p>
    </div>
    <pre class="programlisting code"><code class="hljs-code">en = snlp.Pipeline(lang=<span class="hljs-string">'en'</span>)
txt = <span class="hljs-string">"Yo you around? A friend of mine's lookin."</span>
pos = en(txt)
</code></pre>
    <p class="normal">The preceding code instantiates <a id="_idIndexMarker057"/>an English pipeline and processes a sample piece of text. The next piece of code is a reusable function to print back the sentence tokens with the POS tags:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">print_pos</span><span class="hljs-functio">(</span><span class="hljs-params">doc</span><span class="hljs-functio">):</span>
    text = <span class="hljs-string">""</span>
    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> doc.sentences:
         <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> sentence.tokens:
            text += token.words[<span class="hljs-number">0</span>].text + <span class="hljs-string">"/"</span> + \
                    token.words[<span class="hljs-number">0</span>].upos + <span class="hljs-string">" "</span>
         text += <span class="hljs-string">"\n"</span>
    <span class="hljs-keyword">return</span> text
</code></pre>
    <p class="normal">This method can be used to investigate the tagging for the preceding example sentence:</p>
    <pre class="programlisting code"><code class="hljs-code">print(print_pos(pos))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Yo/PRON you/PRON around/ADV ?/PUNCT 
A/DET friend/NOUN of/ADP mine/PRON 's/PART lookin/NOUN ./PUNCT 
</code></pre>
    <p class="normal">Most of these tags would make sense, though there may be some inaccuracies. For example, the word <em class="italic">lookin</em> is miscategorized as a noun. Neither StanfordNLP, nor a model from another package, will be perfect. This is something that we have to account for in building models using such features. There are a couple of different features that can be built using these POS. First, we can update the <code class="Code-In-Text--PACKT-">word_counts()</code> method to exclude the punctuation from the count of words. The current method is unaware of the punctuation when it counts the words. Additional features can be created that look at the proportion of different types of grammatical <a id="_idIndexMarker058"/>elements in the messages. Note that so far, all features are based on the structure of the text, and not on the content itself. Working with content features will be covered in more detail as this book continues.</p>
    <p class="normal">As a next step, let's update the <code class="Code-In-Text--PACKT-">word_counts()</code> method and add a feature to show the percentages of symbols and punctuation in a message â with the hypothesis that maybe spam messages use more punctuation and symbols. Other features around types of different grammatical elements can also be built. These are left to you to implement. Our <code class="Code-In-Text--PACKT-">word_counts()</code> method is updated as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">en_sw = stopwords.stopwords(<span class="hljs-string">'en'</span>)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">word_counts_v3</span><span class="hljs-functio">(</span><span class="hljs-params">x, pipeline=en</span><span class="hljs-functio">):</span>
  doc = pipeline(x)
  totals = <span class="hljs-number">0.</span>
  count = <span class="hljs-number">0.</span>
  non_word = <span class="hljs-number">0.</span>
  <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> doc.sentences:
    totals += <span class="hljs-built_in">len</span>(sentence.tokens)  <span class="hljs-comment"># (1)</span>
    <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> sentence.tokens:
        <span class="hljs-keyword">if</span> token.text.lower() <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> en_sw:
          <span class="hljs-keyword">if</span> token.words[<span class="hljs-number">0</span>].upos <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">'PUNCT'</span>, <span class="hljs-string">'SYM'</span>]:
            count += <span class="hljs-number">1.</span>
          <span class="hljs-keyword">else</span>:
            non_word += <span class="hljs-number">1.</span>
  non_word = non_word / totals
  <span class="hljs-keyword">return</span> pd.Series([count, non_word], index=[<span class="hljs-string">'Words_NoPunct'</span>, <span class="hljs-string">'Punct'</span>])
</code></pre>
    <p class="normal">This function is a little different compared to the previous one. Since there are multiple computations that need to be performed on the message in each row, these operations are combined and a <code class="Code-In-Text--PACKT-">Series</code> object with column labels is returned. This can be merged with the main DataFrame like so:</p>
    <pre class="programlisting code"><code class="hljs-code">train_tmp = train[<span class="hljs-string">'Message'</span>].apply(word_counts_v3)
train = pd.concat([train, train_tmp], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">A similar process can be performed on the test set:</p>
    <pre class="programlisting code"><code class="hljs-code">test_tmp = test[<span class="hljs-string">'Message'</span>].apply(word_counts_v3)
test = pd.concat([test, test_tmp], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">A quick check of the statistics for <a id="_idIndexMarker059"/>spam and non-spam messages in the training set shows the following, first for non-spam messages:</p>
    <pre class="programlisting code"><code class="hljs-code">train.loc[train[<span class="hljs-string">'Spam'</span>]==<span class="hljs-number">0</span>].describe()
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_01_12.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.12: Statistics for regular messages after using POS tags</p>
    <p class="normal">And then for spam messages:</p>
    <pre class="programlisting code"><code class="hljs-code">train.loc[train[<span class="hljs-string">'Spam'</span>]==<span class="hljs-number">1</span>].describe()
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_01_13.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.13: Statistics for spam messages after using POS tags</p>
    <p class="normal">In general, word counts have been reduced even further after stop word removal. Further more, the new <code class="Code-In-Text--PACKT-">Punct</code> feature<a id="_idIndexMarker060"/> computes the ratio of punctuation tokens in a message relative to the total tokens. Now we can build a model with this data.</p>
    <h3 id="_idParaDest-28" class="title">Modeling data with POS tagging</h3>
    <p class="normal">Plugging these<a id="_idIndexMarker061"/> features into the model, the following<a id="_idIndexMarker062"/> results are obtained:</p>
    <pre class="programlisting code"><code class="hljs-code">x_train = train[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Punctuation'</span>, <span class="hljs-string">'Capitals'</span>, <span class="hljs-string">'Words_NoPunct'</span>, <span class="hljs-string">'Punct'</span>]]
y_train = train[[<span class="hljs-string">'Spam'</span>]]
x_test = test[[<span class="hljs-string">'Length'</span>, <span class="hljs-string">'Punctuation'</span>, <span class="hljs-string">'Capitals'</span> , <span class="hljs-string">'Words_NoPunct'</span>, <span class="hljs-string">'Punct'</span>]]
y_test = test[[<span class="hljs-string">'Spam'</span>]]
model = make_model(input_dims=<span class="hljs-number">5</span>)
<span class="hljs-comment"># model = make_model(input_dims=3)</span>
model.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Train on 4459 samples
Epoch 1/10
4459/4459 [==============================] - 1s 236us/sample - loss: 3.1958 - accuracy: 0.6028
Epoch 2/10
<span class="hljs-co -meta">...</span>
Epoch 10/10
4459/4459 [==============================] - 1s 139us/sample - loss: 0.1788 - <span class="code-highlight"><strong class="hljs-con-slc">accuracy: 0.9466</strong></span>
</code></pre>
    <p class="normal">The accuracy shows a slight increase and is now up to 94.66%. Upon testing, it seemsÂ to hold:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(x_test, y_test)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1115/1115 [==============================] - 0s 91us/sample - loss: 0.2076 - accuracy: <span class="code-highlight"><strong class="hljs-con-slc">0.9426</strong></span>
[0.20764057086989485, 0.9426009]
</code></pre>
    <p class="normal">The final part of text <a id="_idIndexMarker063"/>normalization is stemming and<a id="_idIndexMarker064"/> lemmatization. Though we will not be building any features for the spam model using this, it can be quite useful in other cases.</p>
    <h2 id="_idParaDest-29" class="title">Stemming and lemmatization</h2>
    <p class="normal">In certain languages, the<a id="_idIndexMarker065"/> same word can take <a id="_idIndexMarker066"/>a slightly different form depending on its usage. Consider the word <em class="italic">depend</em> itself. The following are all valid forms of the word <em class="italic">depend</em>: <em class="italic">depends</em>, <em class="italic">depending</em>, <em class="italic">depended</em>, <em class="italic">dependent</em>. Often, these variations are due to tenses. In some languages like Hindi, verbs may have different forms for different genders. Another case is derivatives of the same word such as <em class="italic">sympathy</em>, <em class="italic">sympathetic</em>, <em class="italic">sympathize</em>, and <em class="italic">sympathizer</em>. These variations can take different forms in other languages. In Russian, proper nouns take different forms based on usage. Suppose there is a document talking about London (ÐÐ¾Ð½Ð´Ð¾Ð½). The phrase <em class="italic">in London</em> (Ð² ÐÐ¾Ð½Ð´Ð¾Ð½Ðµ) spells <em class="italic">London</em> differently than <em class="italic">from London</em> (Ð¸Ð· ÐÐ¾Ð½Ð´Ð¾Ð½Ð°). These variations in the spelling of <em class="italic">London</em> can cause issues when matching some input to sections or words in a document.</p>
    <p class="normal">When processing and tokenizing text to construct a vocabulary of words appearing in the corpora, the ability to identify the root word can reduce the size of the vocabulary while expanding the accuracy of matches. In the preceding Russian example, any form of the word London can be matched to any other form if all the forms are normalized to a common representation post-tokenization. This process ofÂ normalization is called stemming or lemmatization.</p>
    <p class="normal">Stemming and lemmatization differ in their approach and sophistication but serve the same objective. Stemming is a simpler, heuristic rule-based approach that chops off the affixes of words. The most famous stemmer is called the <a id="_idIndexMarker067"/>Porter stemmer, published by Martin Porter in 1980. The official website is <a href="https://tartarus.org/martin/PorterStemmer/"><span class="url">https://tartarus.org/martin/PorterStemmer/</span></a>, where various versions of the algorithm implemented in various languages are linked. </p>
    <p class="normal">This stemmer only works for English and has rules including removing <em class="italic">s</em> at the end of the words for plurals, and removing endings such as <em class="italic">-ed</em> or <em class="italic">-ing</em>. Consider the following sentence:</p>
    <p class="normal">"Stemming is aimed at<a id="_idIndexMarker068"/> reducing vocabulary and aid understanding of morphological processes. This helps people understand the morphology of words and reduce size ofÂ corpus."</p>
    <p class="normal">After stemming using Porter's algorithm, this sentence will be reduced to the following:</p>
    <p class="normal">"Stem is aim at reduce vocabulari and aid<a id="_idIndexMarker069"/> understand of morpholog process . ThiÂ help peopl understand the morpholog of word and reduc size of corpu ."</p>
    <p class="normal">Note how different forms of <em class="italic">morphology</em>, <em class="italic">understand</em>, and <em class="italic">reduce</em> are all tokenized to the same form.</p>
    <p class="normal">Lemmatization approaches this task in a more sophisticated manner, using vocabularies and morphological analysis of words. In the study of linguistics, a morpheme is a unit smaller than or equal to a word. When a morpheme is a word in itself, it is called a root or a free morpheme. Conversely, every<a id="_idIndexMarker070"/> word can be decomposed into one or more morphemes. The study of morphemes is called <a id="_idIndexMarker071"/>morphology. Using this morphological information, a word's root form can be returned post-tokenization. This base or dictionary form of the word is called a <em class="italic">lemma</em>, hence the <a id="_idIndexMarker072"/>process is called lemmatization. StanfordNLP includes lemmatization as part of processing.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The <em class="italic">Lemmatization</em> section of the notebook has the code shown here.</p>
    </div>
    <p class="normal">Here is a simple piece of code to take the preceding sentences and parse them:</p>
    <pre class="programlisting code"><code class="hljs-code">text = <span class="hljs-string">"Stemming is aimed at reducing vocabulary and aid understanding of morphological processes. This helps people understand the morphology of words and reduce size of corpus."</span>
lemma = en(text)
</code></pre>
    <p class="normal">After processing, we can iterate through the tokens to get the lemma of each word. This is shown in the following code fragment. The lemma of a word is exposed as the <code class="Code-In-Text--PACKT-">.lemma</code> property of each word inside a token. For the sake of brevity of code, a simplifying assumption is made here that each token has only one word. </p>
    <p class="normal">The POS for each word is also printed out to <a id="_idIndexMarker073"/>help us understand how the process was performed. Some key words in<a id="_idIndexMarker074"/> the following output are highlighted:</p>
    <pre class="programlisting code"><code class="hljs-code">lemmas = <span class="hljs-string">""</span>
<span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> lemma.sentences:
        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> sentence.tokens:
            lemmas += token.words[<span class="hljs-number">0</span>].lemma +<span class="hljs-string">"/"</span> + \
                    token.words[<span class="hljs-number">0</span>].upos + <span class="hljs-string">" "</span>
        lemmas += <span class="hljs-string">"\n"</span>
print(lemmas)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">stem/NOUN be/AUX aim/VERB at/SCONJ <span class="code-highlight"><strong class="hljs-con-slc">reduce/VERB</strong></span> vocabulary/NOUN and/CCONJ aid/NOUN <span class="code-highlight"><strong class="hljs-con-slc">understanding/NOUN</strong></span> of/ADP <span class="code-highlight"><strong class="hljs-con-slc">morphological/ADJ</strong></span> process/NOUN ./PUNCT 
this/PRON help/VERB people/NOUN <span class="code-highlight"><strong class="hljs-con-slc">understand/VERB</strong></span> the/DET <span class="code-highlight"><strong class="hljs-con-slc">morphology/NOUN</strong></span> of/ADP word/NOUN and/CCONJ <span class="code-highlight"><strong class="hljs-con-slc">reduce/VERB</strong></span> size/NOUN of/ADP corpus/ADJ ./PUNCT
</code></pre>
    <p class="normal">Compare this output to the output of the Porter stemmer earlier. One immediate thing to notice is that lemmas are actual words as opposed to fragments, as was the case with the Porter stemmer. In the case of <em class="italic">reduce</em>, the usage in both sentences is in the form of a verb, so the choice of lemma is consistent. Focus on the words <em class="italic">understand</em> and <em class="italic">understanding</em> in the preceding output. As the POS tag shows, it is used in two different forms. Consequently, it is not reduced to the same lemma. This is different from the Porter stemmer. The same behavior can be observed for <em class="italic">morphology</em> and <em class="italic">morphological</em>. This is a quite sophisticated behavior.</p>
    <p class="normal">Now that text normalization is completed, we can begin the vectorization of text.</p>
    <h1 id="_idParaDest-30" class="title">Vectorizing text</h1>
    <p class="normal">While building<a id="_idIndexMarker075"/> models for the SMS message spam detection thus far, only <a id="_idIndexMarker076"/>aggregate features based on counts or distributions of lexical or grammatical features have been considered. The actual words in the messages have not been used thus far. There are a couple of challenges in using the text content of messages. The first is that text can be of arbitrary lengths. Comparing this to image data, we know that each image has a fixed width and height. Even if the corpus of images has a mixture of sizes, images can be resized to a common size with minimal loss of information by using a variety of compression mechanisms. In NLP, this is a bigger problem compared to computer vision. A common approach to handle this is to truncate the text. We will see various ways to handle variable-length texts in various examples throughout the book.</p>
    <p class="normal">The second issue is that of the<a id="_idIndexMarker077"/> representation of words with a numerical quantity or feature. In computer vision, the smallest unit is a pixel. Each pixel has a set of numerical values indicating color or intensity. In a text, the smallest unit could be a word. Aggregating the Unicode values of the characters does not convey or embody the meaning of the word. In fact, these character codes embody no information at all about the character, such as its prevalence, whether it is a consonant or a vowel, and so on. However, averaging the pixels in a section of an image could be a reasonable approximation of that region of the image. It may represent how that region would look if seen from a large distance. A core problem then is to construct a numerical representation of words. Vectorization is the process of converting a word to a vector of numbers that embodies the information contained in the word. Depending on the vectorization technique, this vector may have additional properties that may allow comparison with other words, as will be shown in the <em class="italic">Word vectors</em> section later in this chapter.</p>
    <p class="normal">The simplest approach for vectorizing is to use counts of words. The second approach is more sophisticated, with its origins in information retrieval, and is called TF-IDF. The third approach is relatively new, having been published in 2013, and uses RNNs to generate embeddings or word vectors. This method is called Word2Vec. The newest method in this area as of the time of writing was BERT, which came out in the last quarter of 2018. The first three methods will be discussed in this chapter. BERT will be discussed in detail in <em class="chapterRef">Chapter 3</em>, <em class="italic">Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding.</em></p>
    <h2 id="_idParaDest-31" class="title">Count-based vectorization</h2>
    <p class="normal">The idea behind count-based vectorization is <a id="_idIndexMarker078"/>really simple. Each unique word appearing in the corpus is assigned a column in the vocabulary. Each document, which would correspond to individual messages in the spam example, is <a id="_idIndexMarker079"/>assigned aÂ row. The counts of the words appearing in that document are entered in the relevant cell corresponding to the document and the word. With <code class="Code-In-Text--PACKT-">n</code> unique documentsÂ containing <code class="Code-In-Text--PACKT-">m</code> unique words, this results in a matrix of <code class="Code-In-Text--PACKT-">n</code> rows by <code class="Code-In-Text--PACKT-">m</code> columns. Consider a corpus like so:</p>
    <pre class="programlisting code"><code class="hljs-code">corpus = [
          <span class="hljs-string">"I like fruits. Fruits like bananas"</span>,
          <span class="hljs-string">"I love bananas but eat an apple"</span>,
          <span class="hljs-string">"An apple a day keeps the doctor away"</span>
]
</code></pre>
    <p class="normal">There are three documents in this corpus of text. The <code class="Code-In-Text--PACKT-">scikit-learn</code> (<code class="Code-In-Text--PACKT-">sklearn</code>) libraryÂ provides methods for<a id="_idIndexMarker080"/> undertaking count-based vectorization.</p>
    <h3 id="_idParaDest-32" class="title">Modeling after count-based vectorization</h3>
    <p class="normal">In Google Colab, this library<a id="_idIndexMarker081"/> should already be installed. If it is not installed in your Python environment, it can be installed via the notebook like so:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install sklearn
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">CountVectorizer</code> class provides a built-in tokenizer that separates the tokens of two or more characters in length. This class takes a variety of options including a custom tokenizer, a stop word list, the option to convert characters to lowercase prior to tokenization, and a binary mode that converts every positive count to 1. TheÂ defaults provide a reasonable choice for an English language corpus:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
vectorizer.get_feature_names()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">['an',
 'apple',
 'away',
 'bananas',
 'but',
 'day',
 'doctor',
 'eat',
 'fruits',
 'keeps',
 'like',
 'love',
 'the']
</code></pre>
    <p class="normal">In the preceding code, a model is fit to the corpus. The last line prints out the tokens that are used as columns. The full matrix can be seen as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">X.toarray()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">array([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],
       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],
       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])
</code></pre>
    <p class="normal">This process has now converted a sentence such as "I like fruits. Fruits like bananas" into a vector (<code class="Code-In-Text--PACKT-">0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0</code>). This is an example of <strong class="keyword">context-free</strong> vectorization. Context-free refers <a id="_idIndexMarker082"/>to the fact that the order of the words in the document did not make any difference in the generation of the vector. This is merely counting the instances of the words in a document. Consequently, words with multiple meanings may be grouped into one, for example, <em class="italic">bank</em>. This may refer to a place near the river or a place to keep<a id="_idIndexMarker083"/> money. However, it does provide a method to compare documents and derive similarity. The cosine similarity or distance can be computed between two documents, to see which documents are similar to which other documents:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity
cosine_similarity(X.toarray())
array([[<span class="hljs-number">1.</span>        , <span class="hljs-number">0.13608276</span>, <span class="hljs-number">0.</span>        ],
       [<span class="hljs-number">0.13608276</span>, <span class="hljs-number">1.</span>        , <span class="hljs-number">0.3086067</span> ],
       [<span class="hljs-number">0.</span>        , <span class="hljs-number">0.3086067</span> , <span class="hljs-number">1.</span>        ]])
</code></pre>
    <p class="normal">This shows that the first sentence and the second sentence have a 0.136 similarity score (on a scale of 0 to 1). The first and third sentence have nothing in common. The second and third sentence have a similarity score of 0.308 â the highest in this set. Another use case of this technique is to check the similarity of the documents with given keywords. Let's say that the query is <em class="italic">apple and bananas</em>. This first step is to compute the vector of this query, and then compute the cosine similarity scores against the documents in the corpus:</p>
    <pre class="programlisting code"><code class="hljs-code">query = vectorizer.transform([<span class="hljs-string">"apple and bananas"</span>])
cosine_similarity(X, query)
array([[<span class="hljs-number">0.23570226</span>],
       [<span class="hljs-number">0.57735027</span>],
       [<span class="hljs-number">0.26726124</span>]])
</code></pre>
    <p class="normal">This shows that this query matches the second sentence in the corpus the best. The third sentence would rank second, and the first sentence would rank lowest. In a few lines, a basic search engine has been implemented, along with logic to serve queries! At scale, this is a very difficult problem, as the number of words or columns in a web crawler would top 3 billion. Every web page would be represented as a row, so that would also require billions<a id="_idIndexMarker084"/> of rows. Computing a cosine similarity in milliseconds to serve an online query and keeping the content of this matrix updated is a massive undertaking.</p>
    <p class="normal">The next step from this rather simple vectorization scheme is to consider the information content of each word in constructing this matrix.</p>
    <h2 id="_idParaDest-33" class="title">Term Frequency-Inverse Document Frequency (TF-IDF)</h2>
    <p class="normal">In creating a vector representation of the<a id="_idIndexMarker085"/> document, only the presence of words was included â it does not factor in the importance of a word. If the corpus of documents being processed is about a set of<a id="_idIndexMarker086"/> recipes with fruits, then one may expect words like <em class="italic">apples</em>, <em class="italic">raspberries</em>, and <em class="italic">washing</em> to appear frequently. <strong class="keyword">Term Frequency </strong>(<strong class="keyword">TF</strong>) represents <a id="_idIndexMarker087"/>how often a word or token occurs in a given document. This is exactly what we did in the previous section. In a set of documents about fruits and cooking, a word like <em class="italic">apple</em> may not be terribly specific to help identify a recipe. However, a word like <em class="italic">tuile</em> may be uncommon in that context. Therefore, it may help to narrow the search for recipes much faster than a word like <em class="italic">raspberry</em>. On a side note, feel free to search the web for raspberry tuile recipes. If a word is rare, we want to give it a higher weight, as it may contain more information than a common word. A term can be upweighted by the inverse of the number of documents it appears in. Consequently, words that occur in a lot of documents will get a smaller score compared to<a id="_idIndexMarker088"/> terms that appear in fewer documents. This is called the <strong class="keyword">Inverse Document Frequency </strong>(<strong class="keyword">IDF</strong>).</p>
    <p class="normal">Mathematically, the score of each term in a document can be computed as follows:</p>
    <figure class="mediaobject"><img src="image/B16252_01_006.png" alt="" style="max-height:20px;"/></figure>
    <p class="normal">Here, <em class="italic">t</em> represents the word or term, and <em class="italic">d</em> represents a specific document. </p>
    <p class="normal">It is common to normalize the TF of a term in a document by the total number of tokens in that document.</p>
    <p class="normal">The IDF is defined as follows:</p>
    <figure class="mediaobject"><img src="image/B16252_01_007.png" alt="" style="max-height:42px;"/></figure>
    <p class="normal">Here, <em class="italic">N</em> represents the total number of documents in the corpus, and <em class="italic">n</em><sub class="" style="font-style: italic;">t</sub> represents the number of documents where the term is present. The addition of 1 in the denominator avoids the divide-by-zero error. Fortunately, <code class="Code-In-Text--PACKT-">sklearn</code> provides methodsÂ to compute TF-IDF.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The <em class="italic">TF-IDF Vectorization</em> section of the notebook contains the code for this section.</p>
    </div>
    <p class="normal">Let's convert the counts from the <a id="_idIndexMarker089"/>previous section into their TF-IDF equivalents:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfTransformer
transformer = TfidfTransformer(smooth_idf=<span class="hljs-literal">False</span>)
tfidf = transformer.fit_transform(X.toarray())
pd.DataFrame(tfidf.toarray(), 
             columns=vectorizer.get_feature_names())
</code></pre>
    <p class="normal">This produces the following output:</p>
    <figure class="mediaobject"><img src="image/B16252_01_14.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="normal">This should give some intuition on how TF-IDF is computed. Even with three toy sentences and a very limited vocabulary, many<a id="_idIndexMarker090"/> of the columns in each row are 0. This vectorization produces <strong class="keyword">sparse representations</strong>.</p>
    <p class="normal">Now, this can be applied to the problem of detecting spam messages. Thus far, the features for each message have been computed based on some aggregate statistics and added to the <code class="Code-In-Text--PACKT-">pandas</code> DataFrame. Now, the content of the message will be tokenized and converted into a set of <a id="_idIndexMarker091"/>columns. The TF-IDF score for each word or token will be computed for each message in the array. This is surprisingly easy to do with <code class="Code-In-Text--PACKT-">sklearn</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-keyword">from</span> sklearn. pre-processing <span class="hljs-keyword">import</span> LabelEncoder
tfidf = TfidfVectorizer(binary=<span class="hljs-literal">True</span>)
X = tfidf.fit_transform(train[<span class="hljs-string">'Message'</span>]).astype(<span class="hljs-string">'float32'</span>)
X_test = tfidf.transform(test[<span class="hljs-string">'Message'</span>]).astype(<span class="hljs-string">'float32'</span>)
X.shape
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">(4459, 7741)
</code></pre>
    <p class="normal">The second parameter shows that 7,741 tokens were uniquely identified. These are the columns of features that will be used in the model later. Note that the vectorizer was created with the binary flag. This implies that even if a token appears multiple times in a message, it is counted as one. The next line trains the TF-IDF model on the training dataset. Then, it converts the words in the test set according to the TF-IDF scores learned from the training set. Let's train a model on just these TF-IDF features.</p>
    <h3 id="_idParaDest-34" class="title">Modeling using TF-IDF features</h3>
    <p class="normal">With these TF-IDF features, let's <a id="_idIndexMarker092"/>train a model and see how it does:</p>
    <pre class="programlisting code"><code class="hljs-code">_, cols = X.shape
model2 = make_model(cols)  <span class="hljs-comment"># to match tf-idf dimensions</span>
y_train = train[[<span class="hljs-string">'Spam'</span>]]
y_test = test[[<span class="hljs-string">'Spam'</span>]]
model2.fit(X.toarray(), y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Train on 4459 samples
Epoch 1/10
4459/4459 [==============================] - 2s 380us/sample - loss: 0.3505 - accuracy: 0.8903
<span class="hljs-co -meta">...</span>
Epoch 10/10
4459/4459 [==============================] - 1s 323us/sample - loss: 0.0027 - accuracy: 1.0000
</code></pre>
    <p class="normal">Whoa â we are able to<a id="_idIndexMarker093"/> classify every one correctly! In all honesty, the model is probably overfitting, so some regularization should be applied. The test set gives this result:</p>
    <pre class="programlisting code"><code class="hljs-code">model2.evaluate(X_test.toarray(), y_test)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1115/1115 [==============================] - 0s 134us/sample - loss: 0.0581 - accuracy: 0.9839
[0.05813191874545786, 0.9838565]
</code></pre>
    <p class="normal">An accuracy rate of 98.39% is by far the best we have gotten in any model so far. Checking the confusion matrix, it is evident that this model is indeed doing very well:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_pred = model2.predict_classes(X_test.toarray())
tf.math.confusion_matrix(tf.constant(y_test.Spam), 
                         y_test_pred)
&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int32, numpy=
array([[<span class="hljs-number">958</span>,   <span class="hljs-number">2</span>],
       [ <span class="hljs-number">16</span>, <span class="hljs-number">139</span>]], dtype=int32)&gt;
</code></pre>
    <p class="normal">Only 2 regular messages were classified as spam, while only 16 spam messages were classified as being not spam. This is indeed a very good model. Note that this dataset has Indonesian (or Bahasa) words as well as English words in it. Bahasa uses the Latin alphabet. This model, without using a lot of pretraining and knowledge of language, vocabulary, and grammar, was able to do a very reasonable job with the task at hand.</p>
    <p class="normal">However, this model ignores the relationships between words completely. It treats the words in a document as unordered items in a set. There are better models that vectorize the tokens in a way that preserves some of the relationships between the tokens. This is explored in the next section.</p>
    <h2 id="_idParaDest-35" class="title">Word vectors</h2>
    <p class="normal">In the previous example, a row vector<a id="_idIndexMarker094"/> was used to represent a document. This was used as a feature for the classification model to predict spam labels. However, no<a id="_idIndexMarker095"/> information can be gleaned reliably from the relationships between words. In NLP, a lot of research has been focused on learning the words or representations in an unsupervised way. This is called representation learning. The <a id="_idIndexMarker096"/>output of this approach is a representation of a word in some vector space, and the word can be considered <strong class="keyword">embedded</strong> in that space. Consequently, these word vectors<a id="_idIndexMarker097"/> are also called embeddings.</p>
    <p class="normal">The core hypothesis behind word vector algorithms is that words that occur near each other are related to each other. To see the intuition behind this, consider two words, <em class="italic">bake</em> and <em class="italic">oven</em>. Given a sentence fragment of five words, where one of these words is present, what would be the probability of the other being present as well? You would be right in guessing that the probability is likely quite high. Suppose nowÂ that words are being mapped into some two-dimensional space. In that space, these two words should be closer to each other, and probably further away from words like <em class="italic">astronomy</em> and <em class="italic">tractor</em>. </p>
    <p class="normal">The task of learning these embeddings for the words can be then thought of as adjusting words in a giant multidimensional space where similar words are closer to each other and dissimilar words are further apart from each other.</p>
    <p class="normal">A revolutionary approach to do this is called Word2Vec. This algorithm was published by Tomas Mikolov and collaborators from Google in 2013. This approach produces dense vectors of the order of 50-300 dimensions generally (though larger are known), where most of the values are non-zero. In contrast, in our previous trivial spam example, the TF-IDF model <a id="_idIndexMarker098"/>had 7,741 dimensions. The original paper had two algorithms proposed in it: <strong class="keyword">continuous bag-of-words</strong> and <strong class="keyword">continuous skip-gram</strong>. On semantic tasks and overall, the performance of <a id="_idIndexMarker099"/>skip-gram was state of the art at the time of its publication. Consequently, the continuous skip-gram model with negative sampling has become synonymous with Word2Vec. The intuition behind this model is fairly straightforward.</p>
    <p class="normal">Consider this sentence fragment from a recipe: "Bake until the cookie is golden brown all over." Under the assumption that a word is related to the words that appear near it, a word from this fragment can be picked and a classifier can be trained to predict the words around it:</p>
    <figure class="mediaobject"><img src="image/B16252_01_15.png" alt="A close up of a logo  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.14: A window of 5 centered on cookie</p>
    <p class="normal">Taking an example of a window of five words, the word in the center is used to predict two words before and two words after it. In the preceding figure, the fragment is <em class="italic">until the cookie is golden</em>, with the focus on the word <em class="italic">cookie</em>. Assuming that there are 10,000 words in the vocabulary, a network can be trained to predict binary decisions given a pair of words. The training objective is that the network predicts <code class="Code-In-Text--PACKT-">true</code> for pairs like (<em class="italic">cookie</em>, <em class="italic">golden</em>) while predicting <code class="Code-In-Text--PACKT-">false</code> for (<em class="italic">cookie</em>, <em class="italic">kangaroo</em>). This particular approach is called <strong class="keyword">Skip-Gram Negative Sampling</strong> (<strong class="keyword">SGNS</strong>) and it considerably reduces the training time required for large vocabularies. Very<a id="_idIndexMarker100"/> similar to the single-layer neural model in the previous section, a model can be trained with a one-to-many as the output layer. The sigmoid activation would be changed to a <code class="Code-In-Text--PACKT-">softmax</code> function. If the hidden layer has 300 units, then its dimensions would be 10,000 x 300, that is, for each of the words, there will be a set of weights. The <a id="_idIndexMarker101"/>objective of the training is to learn these weights. In fact, these weights become the embedding for that word once training is complete. </p>
    <p class="normal">The choice of units in the hidden layer is a hyperparameter that can be adapted for specific applications. 300 is commonly found as it is available through pretrained embeddings on the Google News dataset. Finally, the error is computed as the sum of the categorical cross-entropy of all the word pairs in negative and positive examples.</p>
    <p class="normal">The beauty of this model is that it does not require any supervised training data. Running sentences can be used to provide positive examples. For the model to learn effectively, it is important to provide negative samples as well. Words are randomly sampled using their probability of occurrence in the training corpus and fed as negative examples.</p>
    <p class="normal">To understand how the Word2Vec embeddings work, let's download a set of pretrained embeddings.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The code shown in the following section can be found in the <em class="italic">Word Vectors</em> section of the notebook.</p>
    </div>
    <h3 id="_idParaDest-36" class="title">Pretrained models using Word2Vec embeddings</h3>
    <p class="normal">Since we are only interested in<a id="_idIndexMarker102"/> experimenting with a pretrained model, we can use the Gensim library and its pretrained embeddings. Gensim should already be installed in Google Colab. It can be installed like so:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install gensim
</code></pre>
    <p class="normal">After the requisite imports, pretrained embeddings can be downloaded and loaded. Note that these particular embeddings are approximately 1.6 GB in size, so may take a very long time to load (you may encounter some memory issues as well):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> gensim.models.word2vec <span class="hljs-keyword">import</span> Word2Vec
<span class="hljs-keyword">import</span> gensim.downloader <span class="hljs-keyword">as</span> api
model_w2v = api.load(<span class="hljs-string">"word2vec-google-news-300"</span>)
</code></pre>
    <p class="normal">Another issue that you may run into is the Colab session expiring if left alone for too long while waiting for the download to finish. This may be a good time to switch to a local notebook, which will also be helpful in future chapters. Now, we are ready to inspect the similar words:</p>
    <pre class="programlisting code"><code class="hljs-code">model_w2v.most_similar(<span class="hljs-string">"cookies"</span>,topn=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[('cookie', 0.745154082775116),
 ('oatmeal_raisin_cookies', 0.6887780427932739),
 ('oatmeal_cookies', 0.662139892578125),
 ('cookie_dough_ice_cream', 0.6520504951477051),
 ('brownies', 0.6479344964027405),
 ('homemade_cookies', 0.6476464867591858),
 ('gingerbread_cookies', 0.6461867690086365),
 ('Cookies', 0.6341644525527954),
 ('cookies_cupcakes', 0.6275068521499634),
 ('cupcakes', 0.6258294582366943)]
</code></pre>
    <p class="normal">This is pretty good. Let's see how this model does at a word analogy task:</p>
    <pre class="programlisting code"><code class="hljs-code">model_w2v.doesnt_match([<span class="hljs-string">"USA"</span>,<span class="hljs-string">"Canada"</span>,<span class="hljs-string">"India"</span>,<span class="hljs-string">"Tokyo"</span>])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'Tokyo'
</code></pre>
    <p class="normal">The model is able to guess<a id="_idIndexMarker103"/> that compared to the other words, which are all countries, Tokyo is the odd one out, as it is a city. Now, let's try a very famous example of mathematics on these word vectors:</p>
    <pre class="programlisting code"><code class="hljs-code">king = model_w2v[<span class="hljs-string">'king'</span>]
man = model_w2v[<span class="hljs-string">'man'</span>]
woman = model_w2v[<span class="hljs-string">'woman'</span>]
queen = king - man + woman  
model_w2v.similar_by_vector(queen)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[('king', 0.8449392318725586),
 ('queen', 0.7300517559051514),
 ('monarch', 0.6454660892486572),
 ('princess', 0.6156251430511475),
 ('crown_prince', 0.5818676948547363),
 ('prince', 0.5777117609977722),
 ('kings', 0.5613663792610168),
 ('sultan', 0.5376776456832886),
 ('Queen_Consort', 0.5344247817993164),
 ('queens', 0.5289887189865112)]
</code></pre>
    <p class="normal">Given that <em class="italic">King</em> was provided as an input to the equation, it is simple to filter the inputs from the outputs and <em class="italic">Queen</em> would be the top result. SMS spam classification could be attempted using these embeddings. However, future chapters will cover the use of GloVe embeddings and BERT embeddings for sentiment analysis.</p>
    <p class="normal">A pretrained model like the preceding can be used to vectorize a document. Using these embeddings, models can be trained for specific purposes. In later chapters, newer methods of generating contextual embeddings, such as BERT, will be discussed in detail.</p>
    <h1 id="_idParaDest-37" class="title">Summary</h1>
    <p class="normal">In this chapter, we worked through the basics of NLP, including collecting and labeling training data, tokenization, stop word removal, case normalization, POS tagging, stemming, and lemmatization. Some vagaries of these in languages such asÂ Japanese and Russian were also covered. Using a variety of features derived fromÂ these approaches, we trained a model to classify spam messages, where the messages had a combination of English and Bahasa Indonesian words. This got us toÂ a model with 94% accuracy.</p>
    <p class="normal">However, the major challenge in using the content of the messages was in defining a way to represent words as vectors such that computations could be performed on them. We started with a simple count-based vectorization scheme and then graduated to a more sophisticated TF-IDF approach, both of which produced sparse vectors. This TF-IDF approach gave a model with 98%+ accuracy in the spam detection task. </p>
    <p class="normal">Finally, we saw a contemporary method of generating dense word embeddings, called Word2Vec. This method, though a few years old, is still very relevant in many production applications. Once the word embeddings are generated, they can be cached for inference and that makes an ML model using these embeddings run with relatively low latency.</p>
    <p class="normal">We used a very basic deep learning model for solving the SMS spam classification task. Like how <strong class="keyword">Convolutional Neural Networks</strong> (<strong class="keyword">CNNs</strong>) are the predominant architecture in computer vision, <strong class="keyword">Recurrent Neural Networks</strong> (<strong class="keyword">RNNs</strong>), especially those based on <strong class="keyword">Long Short-Term Memory</strong> (<strong class="keyword">LSTM</strong>) and <strong class="keyword">Bi-directional LSTMs</strong> (<strong class="keyword">BiLSTMs</strong>), are most commonly used to build NLP models. In the next chapter, we cover the structure of LSTMs and build a sentiment analysis model using BiLSTMs. These models will be used extensively in creative ways to solve different NLP problems in future chapters.</p>
  </div>
</body></html>