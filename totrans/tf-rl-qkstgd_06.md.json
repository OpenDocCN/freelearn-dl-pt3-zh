["```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport gym\nimport os\nimport threading\nimport multiprocessing\n\nfrom random import choice\nfrom time import sleep\nfrom time import time\n\nfrom a3c import *\nfrom utils import *\n```", "```\nmax_episode_steps = 200\ngamma = 0.99\ns_size = 4 \na_size = 2 \nload_model = False\nmodel_path = './model'\n```", "```\ntf.reset_default_graph()\n\nif not os.path.exists(model_path):\n    os.makedirs(model_path)\n\nwith tf.device(\"/cpu:0\"): \n\n    # keep count of global episodes\n    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n\n    # number of worker threads\n    num_workers = multiprocessing.cpu_count() \n\n    # Adam optimizer\n    trainer = tf.train.AdamOptimizer(learning_rate=2e-4, use_locking=True) \n\n    # global network\n    master_network = AC(s_size,a_size,'global',None) \n\n    workers = []\n    for i in range(num_workers):\n        env = gym.make('CartPole-v0')\n        workers.append(Worker(env,i,s_size,a_size,trainer,model_path,global_episodes))\n\n    # tf saver\n    saver = tf.train.Saver(max_to_keep=5)\n```", "```\nwith tf.Session() as sess:\n\n    # tf coordinator for threads\n    coord = tf.train.Coordinator()\n\n    if load_model == True:\n        print ('Loading Model...')\n        ckpt = tf.train.get_checkpoint_state(model_path)\n        saver.restore(sess,ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n```", "```\n    # start the worker threads\n    worker_threads = []\n    for worker in workers:\n        worker_work = lambda: worker.work(max_episode_steps, gamma, sess, coord,saver)\n        t = threading.Thread(target=(worker_work))\n        t.start()\n        worker_threads.append(t)\n    coord.join(worker_threads)\n```", "```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport gym\nimport threading\nimport multiprocessing\n\nfrom random import choice\nfrom time import sleep\nfrom time import time\nfrom threading import Lock\n\nfrom utils import *\n```", "```\nxavier = tf.contrib.layers.xavier_initializer()\nbias_const = tf.constant_initializer(0.05)\nrand_unif = tf.keras.initializers.RandomUniform(minval=-3e-3,maxval=3e-3)\nregularizer = tf.contrib.layers.l2_regularizer(scale=5e-4)\n```", "```\nclass AC():\n    def __init__(self,s_size,a_size,scope,trainer):\n        with tf.variable_scope(scope):\n\n            self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n\n            # 2 FC layers \n            net = tf.layers.dense(self.inputs, 256, activation=tf.nn.elu, kernel_initializer=xavier, bias_initializer=bias_const, kernel_regularizer=regularizer)\n            net = tf.layers.dense(net, 128, activation=tf.nn.elu, kernel_initializer=xavier, bias_initializer=bias_const, kernel_regularizer=regularizer)\n\n            # policy\n            self.policy = tf.layers.dense(net, a_size, activation=tf.nn.softmax, kernel_initializer=xavier, bias_initializer=bias_const)\n\n            # value\n            self.value = tf.layers.dense(net, 1, activation=None, kernel_initializer=rand_unif, bias_initializer=bias_const)\n```", "```\n# only workers need tf operations for loss functions and gradient updating\n            if scope != 'global':\n                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n\n                self.policy_times_a = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n\n                # loss \n                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy + 1.0e-8))\n                self.policy_loss = -tf.reduce_sum(tf.log(self.policy_times_a + 1.0e-8) * self.advantages)\n                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.005\n```", "```\n# get gradients from local networks using local losses; clip them to avoid exploding gradients\nlocal_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\nself.gradients = tf.gradients(self.loss,local_vars)\nself.var_norms = tf.global_norm(local_vars)\ngrads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n\n# apply local gradients to global network using tf.apply_gradients()\nglobal_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\nself.apply_grads = trainer.apply_gradients(zip(grads,global_vars))\n```", "```\nclass Worker():\n    def __init__(self,env,name,s_size,a_size,trainer,model_path,global_episodes):\n        self.name = \"worker_\" + str(name)\n        self.number = name \n        self.model_path = model_path\n        self.trainer = trainer\n        self.global_episodes = global_episodes\n        self.increment = self.global_episodes.assign_add(1)\n```", "```\n# local copy of the AC network \nself.local_AC = AC(s_size,a_size,self.name,trainer)\n\n# tensorflow op to copy global params to local network\nself.update_local_ops = update_target_graph('global',self.name) \n\nself.actions = np.identity(a_size,dtype=bool).tolist()\nself.env = env\n```", "```\n# train function\n    def train(self,experience,sess,gamma,bootstrap_value):\n        experience = np.array(experience)\n        observations = experience[:,0]\n        actions = experience[:,1]\n        rewards = experience[:,2]\n        next_observations = experience[:,3]\n        values = experience[:,5]\n\n        # discounted rewards\n        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n\n        # value \n        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n\n        # advantage function \n        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n        advantages = discount(advantages,gamma)\n```", "```\n# lock for updating global params\nlock = Lock()\nlock.acquire() \n\n# update global network params\nfd = {self.local_AC.target_v:discounted_rewards, self.local_AC.inputs:np.vstack(observations), self.local_AC.actions:actions, self.local_AC.advantages:advantages}\nvalue_loss, policy_loss, entropy, _, _, _ = sess.run([self.local_AC.value_loss, self.local_AC.policy_loss, self.local_AC.entropy, self.local_AC.grad_norms, self.local_AC.var_norms, self.local_AC.apply_grads], feed_dict=fd)\n\n# release lock\nlock.release() \n\nreturn value_loss / len(experience), policy_loss / len(experience), entropy / len(experience)\n```", "```\n# worker's work function\ndef work(self,max_episode_steps, gamma, sess, coord, saver):\n    episode_count = sess.run(self.global_episodes)\n    total_steps = 0\n    print (\"Starting worker \" + str(self.number))\n\n        with sess.as_default(), sess.graph.as_default(): \n            while not coord.should_stop():\n\n                # copy global params to local network \n                sess.run(self.update_local_ops)\n\n                # lists for book keeping\n                episode_buffer = []\n                episode_values = []\n                episode_frames = []\n\n                episode_reward = 0\n                episode_step_count = 0\n                d = False\n\n                s = self.env.reset()\n                episode_frames.append(s)\n\n                while not d:\n\n                    # action and value\n                    a_dist, v = sess.run([self.local_AC.policy,self.local_AC.value], feed_dict={self.local_AC.inputs:[s]})\n                    a = np.random.choice(np.arange(len(a_dist[0])), p=a_dist[0])\n\n                    if (self.name == 'worker_0'):\n                       self.env.render()\n\n                    # step\n                    s1, r, d, info = self.env.step(a)\n\n                    # normalize reward\n                    r = r/100.0\n\n                    if d == False:\n                        episode_frames.append(s1)\n                    else:\n                        s1 = s\n\n                    # collect experience in buffer \n                    episode_buffer.append([s,a,r,s1,d,v[0,0]])\n\n                    episode_values.append(v[0,0])\n\n                    episode_reward += r\n                    s = s1 \n                    total_steps += 1\n                    episode_step_count += 1\n```", "```\n# if buffer has 25 entries, time for an update \nif len(episode_buffer) == 25 and d != True and episode_step_count != max_episode_steps - 1:\n    v1 = sess.run(self.local_AC.value, feed_dict={self.local_AC.inputs:[s]})[0,0]\n    value_loss, policy_loss, entropy = self.train(episode_buffer,sess,gamma,v1)\n    episode_buffer = []\n    sess.run(self.update_local_ops)\n\n# idiot check to ensure we did not miss update for some unforseen reason \nif (len(episode_buffer) > 30):\n    print(self.name, \"buffer full \", len(episode_buffer))\n    sys.exit()\n\nif d == True:\n    break\n\nprint(\"episode: \", episode_count, \"| worker: \", self.name, \"| episode reward: \", episode_reward, \"| step count: \", episode_step_count)\n```", "```\n# Update the network using the episode buffer at the end of the episode\nif len(episode_buffer) != 0:\n    value_loss, policy_loss, entropy = self.train(episode_buffer,sess,gamma,0.0)\n\nprint(\"loss: \", self.name, value_loss, policy_loss, entropy)\n\n# write to file for worker_0\nif (self.name == 'worker_0'): \n    with open(\"performance.txt\", \"a\") as myfile:\n        myfile.write(str(episode_count) + \" \" + str(episode_reward) + \" \" + str(episode_step_count) + \"\\n\")\n\n# save model params for worker_0\nif (episode_count % 25 == 0 and self.name == 'worker_0' and episode_count != 0):\n        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\nprint (\"Saved Model\")\n\nif self.name == 'worker_0':\n    sess.run(self.increment)\nepisode_count += 1\n```", "```\nimport numpy as np\nimport tensorflow as tf\nfrom random import choice\n\n# copy model params \ndef update_target_graph(from_scope,to_scope):\n    from_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n\n    copy_ops = []\n    for from_param,to_param in zip(from_params,to_params):\n        copy_ops.append(to_param.assign(from_param))\n    return copy_ops\n```", "```\n\n# Discounting function used to calculate discounted returns.\ndef discount(x, gamma):\n    dsr = np.zeros_like(x,dtype=np.float32)\n    running_sum = 0.0\n    for i in reversed(range(0, len(x))):\n       running_sum = gamma * running_sum + x[i]\n       dsr[i] = running_sum \n    return dsr\n```", "```\npython cartpole.py\n```", "```\ndef reward_shaping(r, s, s1):\n     # check if y-coord < 0; implies lander crashed\n     if (s1[1] < 0.0):\n       print('-----lander crashed!----- ')\n       d = True \n       r -= 1.0\n\n     # check if lander is stuck\n     xx = s[0] - s1[0]\n     yy = s[1] - s1[1]\n     dist = np.sqrt(xx*xx + yy*yy) \n     if (dist < 1.0e-4):\n       print('-----lander stuck!----- ')\n       d = True \n       r -= 0.5\n     return r, d\n```", "```\n# reward shaping for lunar lander\nr, d = reward_shaping(r, s, s1)\n```", "```\nmax_episode_steps = 1000\ngamma = 0.999\ns_size = 8 \na_size = 4\n```", "```\nenv = gym.make('LunarLander-v2')\n```", "```\npython lunar.py\n```"]