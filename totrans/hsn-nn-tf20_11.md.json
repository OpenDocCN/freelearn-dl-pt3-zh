["```\nimport tensorflow as tf\nimport math\n\ndef downsample(depth):\n    return tf.keras.Sequential(\n        [\n            tf.keras.layers.Conv2D(\n                depth, 3, strides=2, padding=\"same\", kernel_initializer=\"he_normal\"\n            ),\n            tf.keras.layers.LeakyReLU(),\n        ]\n    )\n\ndef upsample(depth):\n    return tf.keras.Sequential(\n        [\n            tf.keras.layers.Conv2DTranspose(\n                depth, 3, strides=2, padding=\"same\", kernel_initializer=\"he_normal\"\n            ),\n            tf.keras.layers.ReLU(),\n        ]\n    )\n```", "```\ndef get_unet(input_size=(256, 256, 3), num_classes=21):\n    # Downsample from 256x256 to 4x4, while adding depth\n    # using powers of 2, startin from 2**5\\. Cap to 512.\n    encoders = []\n    for i in range(2, int(math.log2(256))):\n        depth = 2 ** (i + 5)\n        if depth > 512:\n            depth = 512\n        encoders.append(downsample(depth=depth))\n\n    # Upsample from 4x4 to 256x256, reducing the depth\n    decoders = []\n    for i in reversed(range(2, int(math.log2(256)))):\n        depth = 2 ** (i + 5)\n        if depth < 32:\n            depth = 32\n        if depth > 512:\n            depth = 512\n        decoders.append(upsample(depth=depth))\n\n    # Build the model by invoking the encoder layers with the correct input\n    inputs = tf.keras.layers.Input(input_size)\n    concat = tf.keras.layers.Concatenate()\n\n    x = inputs\n    # Encoder: downsample loop\n    skips = []\n    for conv in encoders:\n        x = conv(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Decoder: input + skip connection\n    for deconv, skip in zip(decoders, skips):\n        x = deconv(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n\n    # Add the last layer on top and define the model\n    last = tf.keras.layers.Conv2DTranspose(\n        num_classes, 3, strides=2, padding=\"same\", kernel_initializer=\"he_normal\")\n\n    outputs = last(x)\n    return tf.keras.Model(inputs=inputs, outputs=outputs)\n```", "```\nfrom tensorflow.keras.utils import plot_model\nmodel = get_unet()\nplot_model(model, to_file=\"unet.png\")\n```", "```\nimport tensorflow as tf \nimport tensorflow_datasets as tfds \nimport os\n\nclass Voc2007Semantic(tfds.image.Voc2007): \n    \"\"\"Pasval VOC 2007 - semantic segmentation.\"\"\" \n\n    VERSION = tfds.core.Version(\"0.1.0\") \n\n    def _info(self): \n        # Specifies the tfds.core.DatasetInfo object \n        pass  # TODO \n\n    def _split_generators(self, dl_manager): \n        # Downloads the data and defines the splits \n        # dl_manager is a tfds.download.DownloadManager that can be used to \n        # download and extract URLs \n        pass  # TODO \n\n    def _generate_examples(self): \n        # Yields examples from the dataset \n        pass  # TODO\n```", "```\n    def _info(self):\n        parent_info = tfds.image.Voc2007().info\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=parent_info.description,\n            features=tfds.features.FeaturesDict(\n                {\n                    \"image\": tfds.features.Image(shape=(None, None, 3)),\n                    \"image/filename\": tfds.features.Text(),\n                    \"label\": tfds.features.Image(shape=(None, None, 1)),\n                }\n            ),\n            urls=parent_info.urls,\n            citation=parent_info.citation,\n        )\n```", "```\n  def _split_generators(self, dl_manager):\n    trainval_path = dl_manager.download_and_extract(\n        os.path.join(_VOC2007_DATA_URL, \"VOCtrainval_06-Nov-2007.tar\"))\n    test_path = dl_manager.download_and_extract(\n        os.path.join(_VOC2007_DATA_URL, \"VOCtest_06-Nov-2007.tar\"))\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs=dict(data_path=test_path, set_name=\"test\")),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=1,\n            gen_kwargs=dict(data_path=trainval_path, set_name=\"train\")),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            num_shards=1,\n            gen_kwargs=dict(data_path=trainval_path, set_name=\"val\")),\n    ]\n```", "```\nLUT = {\n    (0, 0, 0): 0, # background\n    (128, 0, 0): 1, # aeroplane\n    (0, 128, 0): 2, # bicycle\n    (128, 128, 0): 3, # bird\n    (0, 0, 128): 4, # boat\n    (128, 0, 128): 5, # bottle\n    (0, 128, 128): 6, # bus\n    (128, 128, 128): 7, # car\n    (64, 0, 0): 8, # cat\n    (192, 0, 0): 9, # chair\n    (64, 128, 0): 10, # cow\n    (192, 128, 0): 11, # diningtable\n    (64, 0, 128): 12, # dog\n    (192, 0, 128): 13, # horse\n    (64, 128, 128): 14, # motorbike\n    (192, 128, 128): 15, # person\n    (0, 64, 0): 16, # pottedplant\n    (128, 64, 0): 17, # sheep\n    (0, 192, 0): 18, # sofa\n    (128, 192, 0): 19, # train\n    (0, 64, 128): 20, # tvmonitor\n    (255, 255, 255): 21, # undefined / don't care\n}\n```", "```\n    def _generate_examples(self, data_path, set_name):\n        set_filepath = os.path.join(\n            data_path,\n            \"VOCdevkit/VOC2007/ImageSets/Segmentation/{}.txt\".format(set_name),\n        )\n        with tf.io.gfile.GFile(set_filepath, \"r\") as f:\n            for line in f:\n                image_id = line.strip()\n\n                image_filepath = os.path.join(\n                    data_path, \"VOCdevkit\", \"VOC2007\", \"JPEGImages\", f\"{image_id}.jpg\"\n                )\n                label_filepath = os.path.join(\n                    data_path,\n                    \"VOCdevkit\",\n                    \"VOC2007\",\n                    \"SegmentationClass\",\n                    f\"{image_id}.png\",\n                )\n\n                if not tf.io.gfile.exists(label_filepath):\n                    continue\n\n                label_rgb = tf.image.decode_image(\n                    tf.io.read_file(label_filepath), channels=3\n                )\n\n                label = tf.Variable(\n                    tf.expand_dims(\n                        tf.zeros(shape=tf.shape(label_rgb)[:-1], dtype=tf.uint8), -1\n                    )\n                )\n\n                for color, label_id in LUT.items():\n                    match = tf.reduce_all(tf.equal(label_rgb, color), axis=[2])\n                    labeled = tf.expand_dims(tf.cast(match, tf.uint8), axis=-1)\n                    label.assign_add(labeled * label_id)\n\n                colored = tf.not_equal(tf.reduce_sum(label), tf.constant(0, tf.uint8))\n                # Certain labels have wrong RGB values\n                if not colored.numpy():\n                    tf.print(\"error parsing: \", label_filepath)\n                    continue\n\n                yield image_id, {\n                    # Declaring in _info \"image\" as a tfds.feature.Image\n                    # we can use both an image or a string. If a string is detected\n                    # it is supposed to be the image path and tfds take care of the\n                    # reading process.\n                    \"image\": image_filepath,\n                    \"image/filename\": f\"{image_id}.jpg\",\n                    \"label\": label.numpy(),\n                }\n```", "```\ndataset, info = tfds.load(\"voc2007_semantic\", with_info=True)\n```", "```\n[...]\n    features=FeaturesDict({\n        'image': Image(shape=(None, None, 3), dtype=tf.uint8), \n        'image/filename': Text(shape=(), dtype=tf.string, encoder=None), \n        'label': Image(shape=(None, None, 1), dtype=tf.uint8) \n    }, \n    total_num_examples=625, \n    splits={ \n        'test': <tfds.core.SplitInfo num_examples=207>, \n        'train': <tfds.core.SplitInfo num_examples=207>, \n        'validation': <tfds.core.SplitInfo num_examples=211> \n    }\n\n```", "```\ndef resize_and_scale(row):\n    # Resize and convert to float, [0,1] range\n    row[\"image\"] = tf.image.convert_image_dtype(\n        tf.image.resize(\n            row[\"image\"],\n            (256,256),\n            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),\n        tf.float32)\n    # Resize, cast to int64 since it is a supported label type\n    row[\"label\"] = tf.cast(\n        tf.image.resize(\n            row[\"label\"],\n            (256,256),\n            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),\n        tf.int64)\n    return row\n\ndef to_pair(row):\n    return row[\"image\"], row[\"label\"]\n```", "```\nbatch_size= 32\n\ntrain_set = dataset[\"train\"].map(resize_and_scale).map(to_pair)\ntrain_set = train_set.batch(batch_size).prefetch(1)\n\nvalidation_set = dataset[\"validation\"].map(resize_and_scale)\nvalidation_set = validation_set.map(to_pair).batch(batch_size)\n```", "```\n# Define the model\nmodel = get_unet()\n\n# Choose the optimizer\noptimizer = tf.optimizers.Adam()\n\n# Configure and create the checkpoint callback\ncheckpoint_path = \"ckpt/pb.ckpt\"\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n# Enable TensorBoard loggging\nTensorBoard = tf.keras.callbacks.TensorBoard(write_images=True)\n\n# Cofigure the training loop and log the accuracy\nmodel.compile(optimizer=optimizer,\n              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n```", "```\nnum_epochs = 50\nmodel.fit(train_set, validation_data=validation_set, epochs=num_epochs,\n          callbacks=[cp_callback, TensorBoard])\n```", "```\nsample = tf.image.decode_jpeg(tf.io.read_file(\"author.jpg\"))\nsample = tf.expand_dims(tf.image.convert_image_dtype(sample, tf.float32), axis=[0])\nsample = tf.image.resize(sample, (512,512))\npred_image = tf.squeeze(tf.argmax(model(sample), axis=-1), axis=[0])\n```", "```\nREV_LUT = {value: key for key, value in LUT.items()}\n\ncolor_image = tf.Variable(tf.zeros((512,512,3), dtype=tf.uint8))\npixels_per_label = []\nfor label, color in REV_LUT.items():\n    match = tf.equal(pred_image, label)\n    labeled = tf.expand_dims(tf.cast(match, tf.uint8), axis=-1)\n    pixels_per_label.append((label, tf.math.count_nonzero(labeled)))\n    labeled = tf.tile(labeled, [1,1,3])\n    color_image.assign_add(labeled * color)\n\n# Save\ntf.io.write_file(\"seg.jpg\", tf.io.encode_jpeg(color_image))\n```", "```\nfor label, count in pixels_per_label:\n print(label, \": \", count.numpy())\n```", "```\n0: 218871\n1: 0\n3: 383\n[...]\n15: 42285\n[...]\n```"]