<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">TensorFlow in Action - Some Basic Examples</h1>
                </header>
            
            <article>
                
<p class="calibre2">,In this chapter, we will explain the main computational concept behind TensorFlow, which is the computational graph model, and demonstrate how to get you on track by implementing linear regression and logistic regression.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">Capacity of a single neuron and activation functions</li>
<li class="calibre8">Activation functions</li>
<li class="calibre8">Feed-forward neural network</li>
<li class="calibre8">The need for a multilayer network</li>
<li class="calibre8">TensorFlow terminologies—recap</li>
<li class="calibre8">Linear regression model<span>—b</span>uilding and training</li>
<li class="calibre8">Logistic regression model<span>—b</span>uilding and training</li>
</ul>
<p class="calibre2">We will start by explaining what a single neuron can actually do/model, and based on this, the need for a multilayer network will arise. Next up, we will do more elaboration of the main concepts and tools that are used/available within TensorFlow and how to use these tools to build up simple examples such as linear regression and logistic regression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capacity of a single neuron</h1>
                </header>
            
            <article>
                
<p class="calibre2">A <strong class="calibre13">neural network</strong>  is a computational model that is mainly inspired by the way the biological neural networks of the human brain process the incoming information. Neural networks made a huge breakthrough in machine learning research (deep learning, specifically) and industrial applications, such as breakthrough results in computer vision, speech recognition, and text processing. In this chapter, we will try to develop an understanding of a particular type of neural network called the <strong class="calibre13">multi-layer Perceptron</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Biological motivation and connections</h1>
                </header>
            
            <article>
                
<p class="calibre2">The basic computational unit of our brains is called a <strong class="calibre13">neuron</strong>, and we have approximately 86 billion neurons in our nervous system, which are connected with approximately <img class="fm-editor-equation8" src="assets/d63a8baf-bbe2-4bae-ad11-1657839aa8bf.png"/> to <img class="fm-editor-equation8" src="assets/c889ca1f-940a-4aa4-b33d-d9631a8aa4c0.png"/> synapses.</p>
<p class="calibre2"><em class="calibre19">Figure 1</em> shows a biological neuron. <em class="calibre19">Figure 2</em> shows the corresponding mathematical model. In the drawing of the biological neuron, each neuron receives incoming signals from its dendrites and then produces output signals along its axon, where the axon gets split out and connects via synapses to other neurons.</p>
<p class="calibre2">In the corresponding mathematical computational model of a neuron, the signals that travel along the axons <img class="fm-editor-equation9" src="assets/613ebbfe-8820-436c-914e-9fcf1212b25f.png"/> interact with a multiplication operation <img class="fm-editor-equation10" src="assets/12164cb0-d15a-430c-94a1-ba0adabcfa27.png"/> with the dendrites of the other neuron in the system based on the synaptic strength at that synapse, which is represented by <img class="fm-editor-equation11" src="assets/b060c83c-50ec-4aa1-9eec-425f11521da3.png"/>. The idea is that the synaptic weights/strength <img class="fm-editor-equation12" src="assets/9ed5a5b6-6d41-4348-8719-8ee0f4812468.png"/> gets learned by the network and they're the ones that control the influence of a specific neuron on another.</p>
<p class="calibre2">Also, in the basic computational model in <em class="calibre19">Figure 2</em>, the dendrites carry the signal to the main cell body where it sums them all. If the final result is above a certain threshold, the neuron can fire in the computational model.</p>
<p class="calibre2">Also, it is worth mentioning that we need to control the frequency of the output spikes along the axon, so we use something called an <strong class="calibre13">activation function</strong>. Practically, a common choice of activation function is the sigmoid function σ, since it takes a real-valued input (the signal strength after the sum) and squashes it to be between 0 and 1. We will see the details of these activation functions later in the following section:</p>
<div class="CDPAlignCenter">        <img src="assets/903ae559-7cca-47b8-a502-97797b4a994c.png" class="calibre66"/></div>
<div class="CDPAlignCenter1">Figure 1: Computational unit of the brain (http://cs231n.github.io/assets/nn1/neuron.png)</div>
<p class="calibre2">There is the corresponding basic mathematical model for the biological one:<br class="calibre20"/></p>
<div class="CDPAlignCenter"> <img src="assets/c94c95ce-1f72-4a7b-9f55-c17a2f00ad10.jpeg" class="calibre67"/></div>
<div class="CDPAlignCenter1">Figure 2: Mathematical modeling of the Brain's computational unit (http://cs231n.github.io/assets/nn1/neuron_model.jpeg)</div>
<div class="title-page-name">
<p class="calibre2">The basic unit of computation in a neural network is the neuron, often called a <strong class="calibre13">node</strong> or <strong class="calibre13">unit</strong>. It receives input from some other nodes or from an external source and computes an output. Each input has an associated <strong class="calibre13">weight</strong> (<strong class="calibre13">w</strong>), which is assigned on the basis of its importance <span class="calibre10">relative</span><span class="calibre10"> </span><span class="calibre10">to other inputs.</span> The node applies a function <em class="calibre19">f</em> (we've defined it later) to the weighted sum of its inputs.</p>
</div>
<p class="calibre2">So, the basic computational unit of neural networks in general is called <strong class="calibre13">neuron</strong>/<strong class="calibre13">node</strong>/<strong class="calibre13">unit.</strong></p>
<p class="calibre2">This neuron receives its input from previous neurons or even an external source and then it does some processing on this input to produce a so-called activation. Each input to this neuron is associated with its own weight <img class="fm-editor-equation12" src="assets/c0332e36-48b3-4716-b43a-177ce42e409a.png"/>, which represents the strength of this connection and hence the importance of this input.</p>
<p class="calibre2">So, the final output of this basic building block of the neural network is a summed version of the inputs weighted by their importance <em class="calibre19">w</em>, and then the neuron passes the summed output through an activation function.</p>
<div class="CDPAlignCenter"><img src="assets/7cb94f65-1a2f-497f-9668-800a94d2ae6a.png" class="calibre68"/></div>
<div class="packtfigref">Figure 3: A single neuron</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p class="calibre2">The output from the neuron is computed as shown in <em class="calibre19">Figure 3</em>, and passed through an activation function that introduces non-linearity to the output. This <em class="calibre19">f</em> is called an <strong class="calibre13">activation function</strong>. The main purposes of the activation functions are to:</p>
<ul class="calibre7">
<li class="calibre8">Introduce nonlinearity into the output of a neuron. This is important because most real-world data is nonlinear and we want neurons to learn these nonlinear representations.</li>
<li class="calibre8">Squash the output to be in a specific range.</li>
</ul>
<p class="calibre2">Every activation function (or nonlinearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice.</p>
<p class="calibre2">So, we are going to briefly cover the most common activation functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sigmoid</h1>
                </header>
            
            <article>
                
<p class="calibre2">Historically, the sigmoid activation function is widely used among researchers. This function accepts a real-valued input and squashes it to a range between 0 and 1, as shown in the following figure:</p>
<div class="CDPAlignCenter"><em class="calibre25">σ(x) = 1 / (1 + exp(−x))</em></div>
<div class="CDPAlignCenter"><img src="assets/cb589e92-9a77-4fa2-b417-d69f20ea718a.png" class="calibre69"/></div>
<div class="CDPAlignCenter1">Figure 4: Sigmoid activation function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tanh</h1>
                </header>
            
            <article>
                
<p class="calibre2">Tanh is another activation function that tolerates some negative values. Tanh accepts a real-valued input and squashes them to [-1, 1]:</p>
<div class="CDPAlignCenter"><em class="calibre25">tanh(x) = 2σ(2x) − 1</em></div>
<div class="CDPAlignCenter">      <img src="assets/feef9554-16ff-4f6b-9047-cdc9b679729f.png" class="calibre70"/></div>
<div class="CDPAlignCenter1">Figure 5: Tanh activation function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ReLU</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre13">Rectified linear unit</strong> (<strong class="calibre13">ReLU</strong>) does not tolerate negative values as it accepts a real-valued input and thresholds it at zero (replaces negative values with zero):</p>
<div class="CDPAlignCenter"><em class="calibre25">f(x) = max(0, x)</em></div>
<div class="CDPAlignCenter">   <img src="assets/f1c98eda-2e60-4fb1-b43d-094110178204.jpg" class="calibre71"/></div>
<div class="CDPAlignCenter1">Figure 6: Relu activation function</div>
<p class="calibre2"><strong class="calibre13">Importance of bias</strong>: The main function of bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). See this link at <a href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks" target="_blank" class="calibre11">https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks</a> to learn more about the role of bias in a neuron.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feed-forward neural network</h1>
                </header>
            
            <article>
                
<p class="calibre2">The feed-forward neural network was the first and simplest type of artificial neural network devised. It contains multiple neurons (nodes) arranged in layers. Nodes from adjacent layers have connections or edges between them. All these connections have weights associated with them.</p>
<p class="calibre2">An example of a feed-forward neural network is shown in <em class="calibre19">Figure 7</em>:</p>
<div class="CDPAlignCenter">     <img src="assets/6b47331f-bfc0-4af2-8fd1-f08ddf2ec754.png" class="calibre72"/></div>
<div class="CDPAlignCenter1">Figure 7: An example feed-forward neural network</div>
<p class="calibre2">In a feed-forward network, the information moves in only one direction—forward<span class="calibre10">—</span>from the input nodes, through the hidden nodes (if any), and to the output nodes. There are no cycles or loops in the network (this property of feed-forward networks is different from recurrent neural networks, in which the connections between nodes form a cycle).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The need for multilayer networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">A <strong class="calibre13">multi-layer perceptron</strong> (<strong class="calibre13">MLP</strong>) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can learn <span class="calibre10">only</span><span class="calibre10"> </span><span class="calibre10">linear functions, a MLP can also learn non-linear functions.</span></p>
<p class="calibre2"><em class="calibre19">Figure 7</em> shows MLP with a single hidden layer. Note that all connections have weights associated with them, but only three weights (<em class="calibre19">w0</em>, <em class="calibre19">w1</em>, and <em class="calibre19">w2</em>) are shown in the figure.</p>
<p class="calibre2"><strong class="calibre13">Input Layer</strong>: The Input layer has three nodes. The bias node has a value of 1. The other two nodes take X1 and X2 as external inputs (which are numerical values depending upon the input dataset). As discussed before, no computation, is performed in the <strong class="calibre13">Input Layer</strong>, so the outputs from nodes in the <strong class="calibre13">Input Layer</strong> are <strong class="calibre13">1</strong>, <strong class="calibre13">X1</strong>, and <strong class="calibre13">X2</strong> respectively, which are fed into the <strong class="calibre13">Hidden Layer</strong>.</p>
<p class="calibre2"><strong class="calibre13">Hidden Layer</strong>: The <strong class="calibre13">Hidden Layer</strong> also has three nodes, with the bias node having an output of 1. The output of the other two nodes in the <strong class="calibre13">Hidden Layer</strong> depends on the outputs from the <strong class="calibre13">Input Layer</strong> (<strong class="calibre13">1</strong>, <strong class="calibre13">X1</strong>, and <strong class="calibre13">X2</strong>) as well as the weights associated with the connections (edges). Remember that <em class="calibre19">f</em> refers to the activation function. These outputs are then fed to the nodes in the <strong class="calibre13">Output Layer</strong>.</p>
<div class="CDPAlignCenter">   <img src="assets/38718db0-2454-42d0-be89-545151286aba.png" class="calibre73"/></div>
<div class="CDPAlignCenter1">Figure 8: A multi-layer perceptron having one hidden layer</div>
<p class="calibre2"><strong class="calibre13">Output Layer:</strong> The <strong class="calibre13">Output Layer</strong> has two nodes; they take inputs from the <strong class="calibre13">Hidden Layer</strong> and perform similar computations as shown for the highlighted hidden node. The values calculated (<strong class="calibre13">Y1</strong> and <strong class="calibre13">Y2</strong>) as a result of these computations act as outputs of the multi-layer perceptron.</p>
<p class="calibre2">Given a set of features <em class="calibre19">X = (x1, x2, …)</em> and a target <em class="calibre19">y</em>, a multi-layer perceptron can learn the relationship between the features and the target for either classification or regression.</p>
<p class="calibre2">Let's take an example to understand multi-layer perceptrons better. Suppose we have the following student marks dataset:</p>
<p class="calibre2"><strong class="calibre13">Table 1 – Sample student marks dataset</strong></p>
<table class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38">
<div class="CDPAlignCenter"><strong class="calibre1">Hours studied</strong></div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter"><strong class="calibre1">Mid term marks</strong></div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter"><strong class="calibre1">Final term results</strong></div>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<div class="CDPAlignCenter">35</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">67</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">Pass</div>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<div class="CDPAlignCenter">12</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">75</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">Fail</div>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<div class="CDPAlignCenter">16</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">89</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">Pass</div>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<div class="CDPAlignCenter">45</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">56</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">Pass</div>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<div class="CDPAlignCenter">10</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">90</div>
</td>
<td class="calibre38">
<div class="CDPAlignCenter">Fail</div>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">The two input columns show the number of hours the student has studied and the mid term marks obtained by the student. The <strong class="calibre13">Final Result</strong> column can have two values, <strong class="calibre13">1</strong> or <strong class="calibre13">0</strong>, indicating whether the student passed in the final term or not. For example, we can see that if the student studied 35 hours and had obtained 67 marks in the mid term, he/she ended up passing the final term.</p>
<p class="calibre2">Now, suppose we want to predict whether a student studying 25 hours and having 70 marks in the mid term will pass the final term:</p>
<div class="title-page-name">
<p class="calibre2"><strong class="calibre13">Table 2 – Sample student with unknown final term result</strong></p>
</div>
<table class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38"><strong class="calibre1">Hours studied</strong></td>
<td class="calibre38"><strong class="calibre1">Mid term marks</strong></td>
<td class="calibre38"><strong class="calibre1">Final term result</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre38">26</td>
<td class="calibre38">70</td>
<td class="calibre38">?</td>
</tr>
</tbody>
</table>
<p class="calibre2">This is a binary classification problem, where a MLP can learn from the given examples (training data) and make an informed prediction given a new data point. We will soon see how a MLP learns such relationships.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training our MLP – the backpropagation algorithm</h1>
                </header>
            
            <article>
                
<p class="calibre2">The process by which a multi-layer perceptron learns is called the <strong class="calibre13">backpropagation</strong> algorithm. I would recommend reading this Quora answer by Hemanth Kumar, <a href="https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri" target="_blank" class="calibre11">https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri</a> (quoted later), which explains backpropagation clearly.</p>
<div class="packtquote">"<strong class="calibre1">Backward Propagation of Errors</strong>, often abbreviated as BackProp is one of the several ways in which an artificial neural network (ANN) can be trained. It is a supervised training scheme, which means, it learns from labeled training data (there is a supervisor, to guide its learning).<br class="title-page-name"/>
To put in simple terms, BackProp is like "<strong class="calibre1">learning from mistakes"</strong>. The supervisor corrects the ANN whenever it makes mistakes.<br class="title-page-name"/>
An ANN consists of nodes in different layers; input layer, intermediate hidden layer(s) and the output layer. The connections between nodes of adjacent layers have "weights" associated with them. The goal of learning is to assign correct weights for these edges. Given an input vector, these weights determine what the output vector is.<br class="title-page-name"/>
In supervised learning, the training set is labeled. This means, for some given inputs, we know the desired/expected output (label).<br class="title-page-name"/>
BackProp Algorithm:<br class="title-page-name"/>
Initially all the edge weights are randomly assigned. For every input in the training dataset, the ANN is activated and its output is observed. This output is compared with the desired output that we already know, and the error is "propagated" back to the previous layer. This error is noted and the weights are "adjusted" accordingly. This process is repeated until the output error is below a predetermined threshold.<br class="title-page-name"/>
Once the above algorithm terminates, we have a "learned" ANN which, we consider is ready to work with "new" inputs. This ANN is said to have learned from several examples (labeled data) and from its mistakes (error propagation)."<br class="title-page-name"/>
—Hemanth Kumar.</div>
<p class="calibre2">Now that we have an idea of how backpropagation works, let's go back to our student marks dataset.</p>
<p class="calibre2">The MLP shown in <em class="calibre19">Figure 8</em> has two nodes in the input layer, which take the inputs hours studied and mid term marks. It also has a hidden layer with two nodes. The output layer has two nodes as well; the upper node outputs the probability of <em class="calibre19">pass</em> while the lower node outputs the probability of <em class="calibre19">fail</em>.</p>
<p class="calibre2">In classification applications, we widely use a softmax function (<a href="http://cs231n.github.io/linear-classify/#softmax" target="_blank" class="calibre11">http://cs231n.github.io/linear-classify/#softmax</a>) as the activation function in the output layer of the MLP to ensure that the outputs are probabilities and they add up to 1. The softmax function takes a vector of arbitrary real-valued scores and squashes it to a vector of values between 0 and 1 that sum up to 1. So, in this case:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation13" src="assets/e46f34df-4448-4998-aba9-9ce537e465ee.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – forward propagation</h1>
                </header>
            
            <article>
                
<p class="calibre2">All weights in the network are randomly initialized. Let's consider a specific hidden layer node and call it <em class="calibre19">V</em>. Assume that the weights of the connections from the inputs to that node are <strong class="calibre13">w1</strong>, <strong class="calibre13">w2</strong>, and <strong class="calibre13">w3</strong> (as shown).</p>
<p class="calibre2">The network then takes the first training samples as input (we know that for inputs 35 and 67, the probability of passing is 1):</p>
<ul class="calibre7">
<li class="calibre8">Input to the network = [35, 67]</li>
<li class="calibre8">Desired output from the network (target) = [1, 0]</li>
</ul>
<p class="calibre2">Then, output <em class="calibre19">V</em> from the node in consideration, which can be calculated as follows (f is an activation function such as sigmoid):</p>
<p class="CDPAlignCenter3"><em class="calibre19">V = f (1*w1 + 35*w2 + 67*w3)</em></p>
<div class="title-page-name">
<p class="calibre2">Similarly, outputs from the other node in the hidden layer are also calculated. The outputs of the two nodes in the hidden layer act as inputs to the two nodes in the output layer. This enables us to calculate output probabilities from the two nodes in the output layer.</p>
<p class="calibre2">Suppose the output probabilities from the two nodes in the output layer are 0.4 and 0.6, respectively (since the weights are randomly assigned, outputs will also be random). We can see that the calculated probabilities (0.4 and 0.6) are very far from the desired probabilities (1 and 0 respectively); hence the network is said to have an <em class="calibre19">incorrect output</em>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – backpropagation and weight updation</h1>
                </header>
            
            <article>
                
<p class="calibre2">We calculate the total error at the output nodes and propagate these errors back through the network using backpropagation to calculate the gradients. Then, we use an optimization method such as gradient descent to adjust all weights in the network with an aim of reducing the error at the output layer.</p>
<p class="calibre2">Suppose that the new weights associated with the node in consideration are <em class="calibre19">w4</em>, <em class="calibre19">w5</em>, and <em class="calibre19">w6</em> (after backpropagation and adjusting weights).</p>
<p class="calibre2">If we now feed the same sample as an input to the network, the network should perform better than the initial run since the weights have now been optimized to minimize the error in prediction. The errors at the output nodes now reduce to [0.2, -0.2] as compared to [0.6, -0.4] earlier. This means that our network has learned to correctly classify our first training sample.</p>
<p class="calibre2">We repeat this process with all other training samples in our dataset. Then, our network is said to have learned those examples.</p>
<p class="calibre2">If we now want to predict whether a student studying 25 hours and having 70 marks in the mid term will pass the final term, we go through the forward propagation step and find the output probabilities for pass and fail.</p>
<p class="calibre2">I have avoided mathematical equations and explanation of concepts such as gradient descent here and have rather tried to develop an intuition for the algorithm. For a more mathematically involved discussion of the backpropagation algorithm, refer to this link: <a href="http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html" target="_blank" class="calibre11">http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow terminologies – recap</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will provide an overview of the TensorFlow library as well as the structure of a basic TensorFlow application. TensorFlow is an open source library for creating large-scale machine learning applications; it can model computations on a wide variety of hardware, ranging from android devices to heterogeneous multi-gpu systems.</p>
<p class="calibre2">TensorFlow uses a special structure in order to execute code on different devices such as CPUs and GPUs. Computations are defined as a graph and each graph is made up of operations, also known as <strong class="calibre13">ops</strong>, so whenever we work with TensorFlow, we define the series of operations in a graph.</p>
<p class="calibre2">To run these operations, we need to launch the graph into a session. The session translates the operations and passes them to a device for execution.</p>
<p class="calibre2">For example, the following image represents a graph in TensorFlow. <em class="calibre19">W</em>, <em class="calibre19">x</em>, and <em class="calibre19">b</em> are tensors over the edges of this graph. <em class="calibre19">MatMul</em> is an operation over the tensors <em class="calibre19">W</em> and <em class="calibre19">x</em>; after that, <em class="calibre19">Add</em> is called and we add the result of the previous operator with <em class="calibre19">b</em>. The resultant tensors of each operation cross the next one until the end, where it's possible to get the desired result.</p>
<div class="CDPAlignCenter">    <img src="assets/e2f35a77-296a-4058-832f-d70cfdfbea53.png" class="calibre74"/></div>
<div class="CDPAlignCenter1">Figure 9: Sample TensorFlow computational graph</div>
<p class="calibre2">In order to use TensorFlow, we need to import the library; we'll give it the name <kbd class="calibre12">tf</kbd> so that we can access a module by writing <kbd class="calibre12">tf</kbd> dot and then the module's name:</p>
<pre class="calibre21">import tensorflow as tf</pre>
<p class="calibre2">To create our first graph, we will start by using source operations, which do not require any input. These source operations or source ops will pass their information to other operations, which will actually run computations.</p>
<p class="calibre2">Let's create two source operations that will output numbers. We will define them as <kbd class="calibre12">A</kbd> and <kbd class="calibre12">B</kbd>, which you can see in the following piece of code:</p>
<pre class="calibre21">A = tf.constant([2])</pre>
<pre class="calibre21">B = tf.constant([3])</pre>
<p class="calibre2">After that, we'll define a simple computational operation <kbd class="calibre12">tf.add()</kbd>, used to sum two elements. You can also use <kbd class="calibre12">C = A + B</kbd>, as shown in this code:</p>
<pre class="calibre21">C = tf.add(A,B)</pre>
<pre class="calibre21">#C = A + B is also a way to define the sum of the terms</pre>
<p class="calibre2">Since graphs need to be executed in the context of a session, we need to create a session object:</p>
<pre class="calibre21">session = tf.Session()</pre>
<p class="calibre2">To watch the graph, let's run the session to get the result from the previously defined <kbd class="calibre12">C</kbd> operation:</p>
<pre class="calibre21">result = session.run(C)<br class="title-page-name"/>print(result)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>[5]</pre>
<p class="calibre2">You're probably thinking that it was a lot of work just to add two numbers together, but it's extremely important that you understand the basic structure of TensorFlow. Once you do so, you can define any computations that you want; again, TensorFlow's structure allows it to handle computations on different devices (CPU or GPU), and even in clusters. If you want to learn more about this, you can run the method <kbd class="calibre12">tf.device()</kbd>.</p>
<p class="calibre2">Also feel free to experiment with the structure of TensorFlow in order to get a better idea of how it works. If you want a list of all the mathematical operations that TensorFlow supports, you can check out the documentation.</p>
<p class="calibre2">By now, you should understand the structure of TensorFlow and how to create a basic applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining multidimensional arrays using TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now we will try to define such arrays using TensorFlow:</p>
<pre class="calibre21">salar_var = tf.constant([4])<br class="title-page-name"/>vector_var = tf.constant([5,4,2])<br class="title-page-name"/>matrix_var = tf.constant([[1,2,3],[2,2,4],[3,5,5]])<br class="title-page-name"/>tensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(salar_var)<br class="title-page-name"/>    print "Scalar (1 entry):\n %s \n" % result<br class="title-page-name"/>    result = session.run(vector_var)<br class="title-page-name"/>    print "Vector (3 entries) :\n %s \n" % result<br class="title-page-name"/>    result = session.run(matrix_var)<br class="title-page-name"/>    print "Matrix (3x3 entries):\n %s \n" % result<br class="title-page-name"/>    result = session.run(tensor)<br class="title-page-name"/>    print "Tensor (3x3x3 entries) :\n %s \n" % result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Scalar (1 entry):<br class="title-page-name"/> [2] <br class="title-page-name"/><br class="title-page-name"/>Vector (3 entries) :<br class="title-page-name"/> [5 6 2] <br class="title-page-name"/><br class="title-page-name"/>Matrix (3x3 entries):<br class="title-page-name"/> [[1 2 3]<br class="title-page-name"/> [2 3 4]<br class="title-page-name"/> [3 4 5]] <br class="title-page-name"/><br class="title-page-name"/>Tensor (3x3x3 entries) :<br class="title-page-name"/> [[[ 1  2  3]<br class="title-page-name"/>  [ 2  3  4]<br class="title-page-name"/>  [ 3  4  5]]<br class="title-page-name"/><br class="title-page-name"/> [[ 4  5  6]<br class="title-page-name"/>  [ 5  6  7]<br class="title-page-name"/>  [ 6  7  8]]<br class="title-page-name"/><br class="title-page-name"/> [[ 7  8  9]<br class="title-page-name"/>  [ 8  9 10]<br class="title-page-name"/>  [ 9 10 11]]]</pre>
<p class="calibre2">Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types:</p>
<pre class="calibre21">Matrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])<br class="title-page-name"/>Matrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])<br class="title-page-name"/>first_operation = tf.add(Matrix_one, Matrix_two)<br class="title-page-name"/>second_operation = Matrix_one + Matrix_two<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(first_operation)<br class="title-page-name"/>    print "Defined using tensorflow function :"<br class="title-page-name"/>    print(result)<br class="title-page-name"/>    result = session.run(second_operation)<br class="title-page-name"/>    print "Defined using normal expressions :"<br class="title-page-name"/>    print(result)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Defined using tensorflow function :<br class="title-page-name"/>[[3 4 5]<br class="title-page-name"/> [4 5 6]<br class="title-page-name"/> [5 6 7]]<br class="title-page-name"/>Defined using normal expressions :<br class="title-page-name"/>[[3 4 5]<br class="title-page-name"/> [4 5 6]<br class="title-page-name"/> [5 6 7]]</pre>
<p class="calibre2">With the regular symbol definition and also the <kbd class="calibre12">tensorflow</kbd> function, we were able to get an element-wise multiplication, also known as <strong class="calibre13">Hadamard product</strong>. But what if we want the regular matrix product? We need to use another TensorFlow function called <kbd class="calibre12">tf.matmul()</kbd>:</p>
<pre class="calibre21">Matrix_one = tf.constant([[2,3],[3,4]])<br class="title-page-name"/>Matrix_two = tf.constant([[2,3],[3,4]])<br class="title-page-name"/>first_operation = tf.matmul(Matrix_one, Matrix_two)<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(first_operation)<br class="title-page-name"/>    print "Defined using tensorflow function :"<br class="title-page-name"/>    print(result)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Defined using tensorflow function :<br class="title-page-name"/>[[13 18]<br class="title-page-name"/> [18 25]]</pre>
<p class="calibre2">We can also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why tensors?</h1>
                </header>
            
            <article>
                
<p class="calibre2">The tensor structure helps us by giving us the freedom to shape the dataset the way we want.</p>
<p class="calibre2">This is particularly helpful when dealing with images, due to the nature of how information in images are encoded.</p>
<p class="calibre2">Thinking about images, it's easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two-dimensional structure (a matrix)... until you remember that images have colors. To add information about the colors, we need another dimension, and that's when Tensors become particularly helpful.</p>
<p class="calibre2">Images are encoded into color channels; image data is represented in each color's intensity in a color channel at a given point, the most common one being RGB (which means red, blue, and green). The information contained in an image is the intensity of each channel color in the width and height of the image, just like this:</p>
<div class="CDPAlignCenter"><img src="assets/37850946-f7a2-47ba-8780-ae325f94f654.png" class="calibre75"/></div>
<div class="CDPAlignCenter1">Figure 10: Different color channels for a specific image</div>
<p class="calibre2">So, the intensity of the red channel at each point with width and height can be represented in a matrix; the same goes for the blue and green channels. So, we end up having three matrices, and when these are combined, they form a tensor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variables</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables.</p>
<p class="calibre2">To define variables, we use the command <kbd class="calibre12">tf.variable()</kbd>. To be able to use variables in a computation graph, it is necessary to initialize them before running the graph in a session. This is done by running <kbd class="calibre12">tf.global_variables_initializer()</kbd>.</p>
<p class="calibre2">To update the value of a variable, we simply run an assign operation that assigns a value to the variable:</p>
<pre class="calibre21">state = tf.Variable(0)</pre>
<p class="calibre2">Let's first create a simple counter, a variable that increases one unit at a time:</p>
<pre class="calibre21">one = tf.constant(1)<br class="title-page-name"/>new_value = tf.add(state, one)<br class="title-page-name"/>update = tf.assign(state, new_value)</pre>
<p class="calibre2">Variables must be initialized by running an initialization operation after having launched the graph. We first have to add the initialization operation to the graph:</p>
<pre class="calibre21">init_op = tf.global_variables_initializer()</pre>
<p class="calibre2">We then start a session to run the graph.</p>
<p class="calibre2">We first initialize the variables, then print the initial value of the state variable, and finally run the operation of updating the state variable and printing the result after each update:</p>
<pre class="calibre21">with tf.Session() as session:<br class="title-page-name"/> session.run(init_op)<br class="title-page-name"/> print(session.run(state))<br class="title-page-name"/> for _ in range(3):<br class="title-page-name"/>    session.run(update)<br class="title-page-name"/>    print(session.run(state))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>0<br class="title-page-name"/>1<br class="title-page-name"/>2<br class="title-page-name"/>3</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, we know how to manipulate variables inside TensorFlow, but what about feeding data outside of a TensorFlow model?</p>
<p class="calibre2">If you want to feed data to a TensorFlow model from outside a model, you will need to use placeholders.</p>
<p class="calibre2">So, what are these placeholders and what do they do? Placeholders can be seen as <em class="calibre19">holes</em> in your model, <em class="calibre19">holes</em> that you will pass the data to. You can create them using <kbd class="calibre12">tf.placeholder(datatype)</kbd>, where <kbd class="calibre12">datatype</kbd> specifies the type of data (integers, floating points, strings, and Booleans) along with its precision (8, 16, 32, and 64) bits.</p>
<p class="calibre2">The definition of each data type with the respective Python syntax is defined as:</p>
<p class="packtfigref1"><strong class="calibre1">Table 3 – Definition of different TensorFlow data types</strong></p>
<div class="title-page-name">
<table class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><strong class="calibre13">Data type</strong></p>
</td>
<td class="calibre38">
<p class="calibre2"><strong class="calibre13">Python type</strong></p>
</td>
<td class="calibre38">
<p class="calibre2"><strong class="calibre13">Description</strong></p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_FLOAT</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.float32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">32-bits floating point.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_DOUBLE</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.float64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">64-bits floating point</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">8-bits signed integer.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT16</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int16</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">16-bits signed integer.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">32-bits signed integer.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">64-bits signed integer.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_UINT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.uint8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">8-bits unsigned integer.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_STRING</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.string</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">Variable length byte arrays. Each element of a Tensor is a byte array.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_BOOL</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.bool</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">Boolean.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_COMPLEX64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.complex64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">Complex number made of two 32-bits floating points: real and imaginary parts.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_COMPLEX128</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.complex128</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">Complex number made of two 64-bits floating points: real and imaginary parts.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_QINT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.qint8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">8-bits signed integer used in quantized ops.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_QINT32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.qint32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">32-bits signed integer used in quantized ops.</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_QUINT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.quint8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">8-bits unsigned integer used in quantized ops.</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="title-page-name">
<p class="calibre2">So let's create a placeholder:</p>
<pre class="calibre21">a=tf.placeholder(tf.float32)</pre>
<p class="calibre2">And define a simple multiplication operation:</p>
<pre class="calibre21">b=a*2</pre></div>
<p class="calibre2">Now, we need to define and run the session, but since we created a <em class="calibre19">hole</em> in the model to pass the data, when we initialize the session. We are obliged to pass an argument with the data; otherwise we get an error.</p>
<p class="calibre2">To pass the data to the model, we call the session with an extra argument, <kbd class="calibre12">feed_dict</kbd>, in which we should pass a dictionary with each placeholder name followed by its respective data, just like this:</p>
<pre class="calibre21">with tf.Session() as sess:<br class="title-page-name"/>    result = sess.run(b,feed_dict={a:3.5})<br class="title-page-name"/>    print result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>7.0</pre>
<p class="calibre2">Since data in TensorFlow is passed in the form of multidimensional arrays, we can pass any kind of tensor through the placeholders to get the answer to the simple multiplication operation:</p>
<pre class="calibre21">dictionary={a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] }<br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/>    result = sess.run(b,feed_dict=dictionary)<br class="title-page-name"/>    print result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>[[[  2.   4.   6.]<br class="title-page-name"/>  [  8.  10.  12.]<br class="title-page-name"/>  [ 14.  16.  18.]<br class="title-page-name"/>  [ 20.  22.  24.]]<br class="title-page-name"/><br class="title-page-name"/> [[ 26.  28.  30.]<br class="title-page-name"/>  [ 32.  34.  36.]<br class="title-page-name"/>  [ 38.  40.  42.]<br class="title-page-name"/>  [ 44.  46.  48.]]]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operations</h1>
                </header>
            
            <article>
                
<p class="calibre2">Operations are nodes that represent mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensors, or maybe an activation function.</p>
<p class="calibre2"><kbd class="calibre12">tf.matmul</kbd>, <kbd class="calibre12">tf.add</kbd>, and  <kbd class="calibre12">tf.nn.sigmoid</kbd> are some of the operations in TensorFlow. These are like functions in Python, but operate directly over tensors and each one does a specific thing.</p>
<p class="calibre2">Other operations can be easily found at: <a href="https://www.tensorflow.org/api_guides/python/math_ops" target="_blank" class="calibre11">https://www.tensorflow.org/api_guides/python/math_ops</a>.</p>
<p class="calibre2">Let's play around with some of these operations:</p>
<pre class="calibre21">a = tf.constant([5])<br class="title-page-name"/>b = tf.constant([2])<br class="title-page-name"/>c = tf.add(a,b)<br class="title-page-name"/>d = tf.subtract(a,b)<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(c)<br class="title-page-name"/>    print 'c =: %s' % result<br class="title-page-name"/>    result = session.run(d)<br class="title-page-name"/>    print 'd =: %s' % result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>c =: [7]<br class="title-page-name"/>d =: [3]</pre>
<p class="calibre2"><kbd class="calibre12">tf.nn.sigmoid</kbd> is an activation function: it's a little more complicated, but this function helps learning models to evaluate what kind of information is good or not good.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression model – building and training</h1>
                </header>
            
            <article>
                
<p class="calibre2">According to our explanation of linear regression in the <a href="6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml" target="_blank" class="calibre11">Chapter 2</a>, <em class="calibre19">Data Modeling in Action - The Titanic Example</em> we are going to rely on this definition to build a simple linear regression model.</p>
<p class="calibre2">Let's start off by importing the necessary packages for this implementation:</p>
<pre class="calibre21">import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.patches as mpatches<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>plt.rcParams['figure.figsize'] = (10, 6)</pre>
<p class="calibre2">Let's define an independent variable:</p>
<pre class="calibre21">input_values = np.arange(0.0, 5.0, 0.1)<br class="title-page-name"/>input_values</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,<br class="title-page-name"/>        1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,<br class="title-page-name"/>        2.2,  2.3,  2.4,  2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,<br class="title-page-name"/>        3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,<br class="title-page-name"/>        4.4,  4.5,  4.6,  4.7,  4.8,  4.9])</pre>
<pre class="calibre21">##You can adjust the slope and intercept to verify the changes in the graph<br class="title-page-name"/>weight=1<br class="title-page-name"/>bias=0<br class="title-page-name"/>output = weight*input_values + bias<br class="title-page-name"/>plt.plot(input_values,output)<br class="title-page-name"/>plt.ylabel('Dependent Variable')<br class="title-page-name"/>plt.xlabel('Indepdendent Variable')<br class="title-page-name"/>plt.show()<br class="title-page-name"/>Output:</pre>
<div class="CDPAlignCenter">  <img src="assets/8e33a510-0612-4242-8cc9-7229a1ae9265.png" class="calibre76"/></div>
<div class="CDPAlignCenter1">Figure 11: Visualization of the dependent variable versus the independent one</div>
<p class="calibre2"> </p>
<p class="calibre2">Now, let's see how this gets interpreted into a TensorFlow code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression with TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">For the first part, we will generate random data points and define a linear relation; we'll use TensorFlow to adjust and get the right parameters:</p>
<pre class="calibre21">input_values = np.random.rand(100).astype(np.float32)</pre>
<p class="calibre2">The equation for the model used in this example is:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation14" src="assets/9bc580b1-9609-40b2-9c9c-f3cffa70bd0f.png"/></div>
<p class="calibre2">Nothing special about this equation, it is just a model that we use to generate our data points. In fact, you can change the parameters to whatever you want, as you will do later. We add some Gaussian noise to the points to make it a bit more interesting:</p>
<pre class="calibre21">output_values = input_values * 2 + 3<br class="title-page-name"/>output_values = np.vectorize(lambda y: y + np.random.normal(loc=0.0, scale=0.1))(output_values)</pre>
<p class="calibre2">Here is a sample of data:</p>
<pre class="calibre21">list(zip(input_values,output_values))[5:10]</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>[(0.25240293, 3.474361759429548), <br class="title-page-name"/>(0.946697, 4.980617375175061), <br class="title-page-name"/>(0.37582186, 3.650345806087635), <br class="title-page-name"/>(0.64025956, 4.271037640404975), <br class="title-page-name"/>(0.62555283, 4.37001850440196)]</pre>
<p class="calibre2">First, we initialize the variables <img class="fm-editor-equation15" src="assets/7beac794-06f0-4f0c-95d2-8f5344ad94e2.png"/> and <img class="fm-editor-equation16" src="assets/323cad53-6532-4e69-8234-50b2b8511bc7.png"/> with any random guess, and then we define the linear function:</p>
<pre class="calibre21">weight = tf.Variable(1.0)<br class="title-page-name"/>bias = tf.Variable(0.2)<br class="title-page-name"/>predicted_vals = weight * input_values + bias</pre>
<p class="calibre2">In a typical linear regression model, we minimize the squared error of the equation that we want to adjust minus the target values (the data that we have), so we define the equation to be minimized as a loss.</p>
<p class="calibre2">To find loss's value, we use <kbd class="calibre12">tf.reduce_mean()</kbd>. This function finds the mean of a multidimensional tensor, and the result can have a different dimension:</p>
<pre class="calibre21">model_loss = tf.reduce_mean(tf.square(predicted_vals - output_values))</pre>
<p class="calibre2">Then, we define the optimizer method. Here, we will use a simple gradient descent with a learning rate of 0.5.</p>
<p class="calibre2">Now, we will define the training method of our graph, but what method will <span class="calibre10">we </span>use for minimize the loss? It's <kbd class="calibre12">tf.train.GradientDescentOptimizer</kbd>.</p>
<p class="calibre2">The <kbd class="calibre12">.minimize()</kbd> function will minimize the error function of our optimizer, resulting in a better model:</p>
<pre class="calibre21">model_optimizer = tf.train.GradientDescentOptimizer(0.5)<br class="title-page-name"/>train = model_optimizer.minimize(model_loss)</pre>
<p class="calibre2">Don't forget to initialize the variables before executing a graph:</p>
<pre class="calibre21">init = tf.global_variables_initializer()<br class="title-page-name"/>sess = tf.Session()<br class="title-page-name"/>sess.run(init)</pre>
<p class="calibre2">Now, we are ready to start the optimization and run the graph:</p>
<pre class="calibre21">train_data = []<br class="title-page-name"/>for step in range(100):<br class="title-page-name"/>    evals = sess.run([train,weight,bias])[1:]<br class="title-page-name"/>    if step % 5 == 0:<br class="title-page-name"/>       print(step, evals)<br class="title-page-name"/>       train_data.append(evals)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>(0, [2.5176678, 2.9857566])<br class="title-page-name"/>(5, [2.4192538, 2.3015416])<br class="title-page-name"/>(10, [2.5731843, 2.221911])<br class="title-page-name"/>(15, [2.6890132, 2.1613526])<br class="title-page-name"/>(20, [2.7763696, 2.1156814])<br class="title-page-name"/>(25, [2.8422525, 2.0812368])<br class="title-page-name"/>(30, [2.8919399, 2.0552595])<br class="title-page-name"/>(35, [2.9294133, 2.0356679])<br class="title-page-name"/>(40, [2.957675, 2.0208921])<br class="title-page-name"/>(45, [2.9789894, 2.0097487])<br class="title-page-name"/>(50, [2.9950645, 2.0013444])<br class="title-page-name"/>(55, [3.0071881, 1.995006])<br class="title-page-name"/>(60, [3.0163314, 1.9902257])<br class="title-page-name"/>(65, [3.0232272, 1.9866205])<br class="title-page-name"/>(70, [3.0284278, 1.9839015])<br class="title-page-name"/>(75, [3.0323503, 1.9818509])<br class="title-page-name"/>(80, [3.0353084, 1.9803041])<br class="title-page-name"/>(85, [3.0375392, 1.9791379])<br class="title-page-name"/>(90, [3.039222, 1.9782581])<br class="title-page-name"/>(95, [3.0404909, 1.9775947])</pre>
<p class="calibre2">Let's visualize the training process to fit the data points:</p>
<pre class="calibre21"><span>print</span>(<span>'Plotting the data points with their corresponding fitted line...'</span>)<br class="title-page-name"/>converter = plt.colors<br class="title-page-name"/>cr, cg, cb = (<span>1.0</span>, <span>1.0</span>, <span>0.0</span>)<br class="title-page-name"/><br class="title-page-name"/><span>for </span>f <span>in </span>train_data:<br class="title-page-name"/><br class="title-page-name"/>    cb += <span>1.0 </span>/ <span>len</span>(train_data)<br class="title-page-name"/>    cg -= <span>1.0 </span>/ <span>len</span>(train_data)<br class="title-page-name"/><br class="title-page-name"/>    <span>if </span>cb &gt; <span>1.0</span>: cb = <span>1.0<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>    </span><span>if </span>cg &lt; <span>0.0</span>: cg = <span>0.0<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>    </span>[a, b] = f<br class="title-page-name"/>    f_y = np.vectorize(<span>lambda </span>x: a*x + b)(input_values)<br class="title-page-name"/>    line = plt.plot(input_values, f_y)<br class="title-page-name"/>    plt.setp(line, <span>color</span>=(cr,cg,cb))<br class="title-page-name"/><br class="title-page-name"/>plt.plot(input_values, output_values, <span>'ro'</span>)<br class="title-page-name"/>green_line = mpatches.Patch(<span>color</span>=<span>'red'</span>, <span>label</span>=<span>'Data Points'</span>)<br class="title-page-name"/>plt.legend(<span>handles</span>=[green_line])<br class="title-page-name"/>plt.show()</pre>
<pre class="calibre21">Output:</pre>
<div class="CDPAlignCenter">  <img src="assets/1840b114-b57e-4f87-b90b-6a8082c02b13.png" class="calibre77"/></div>
<div class="CDPAlignCenter1">Figure 12: Visualization of the data points fitted by a regression line</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression model – building and training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Also based on our explanation of logistic regression in <a href="6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml" target="_blank" class="calibre11">Chapter 2</a>, <em class="calibre19">Data Modeling in Action - The Titanic Example</em>, we are going to implement the logistic regression algorithm in TensorFlow. So, briefly, logistic regression passes the input through the logistic/sigmoid but then treats the result as a probability:</p>
<div class="CDPAlignCenter">  <img src="assets/06e797f4-9584-46f6-8915-d1177426c2bb.png" class="calibre78"/></div>
<div class="CDPAlignCenter1">Figure 13: Discriminating between two linearly separable classes, 0's and 1's</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Utilizing logistic regression in TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">For us to utilize logistic regression in TensorFlow, we first need to import whatever libraries we are going to use. To do so, you can run this code cell:</p>
<pre class="calibre21"><span>import </span>tensorflow <span>as </span>tf<br class="title-page-name"/><br class="title-page-name"/><span>import </span>pandas <span>as </span>pd<br class="title-page-name"/><br class="title-page-name"/><span>import </span>numpy <span>as </span>np<br class="title-page-name"/><span>import </span>time<br class="title-page-name"/><span>from </span>sklearn.datasets <span>import </span>load_iris<br class="title-page-name"/><span>from </span>sklearn.cross_validation <span>import </span>train_test_split<br class="title-page-name"/><span>import </span>matplotlib.pyplot <span>as </span>plt</pre>
<p class="calibre2">Next, we will load the dataset we are going to use. In this case, we are utilizing the iris dataset, which is inbuilt. So, there's no need to do any preprocessing and we can jump right into manipulating it. We separate the dataset into <em class="calibre19">x</em>'s and <em class="calibre19">y</em>'s, and then into training <em class="calibre19">x</em>'s and <em class="calibre19">y</em>'s and testing <em class="calibre19">x</em>'s and <em class="calibre19">y</em>'s, (pseudo) randomly:</p>
<pre class="calibre21">iris_dataset = load_iris()<br class="title-page-name"/>iris_input_values, iris_output_values = iris_dataset.data[:-<span>1</span>,:], iris_dataset.target[:-<span>1</span>]<br class="title-page-name"/>iris_output_values= pd.get_dummies(iris_output_values).values<br class="title-page-name"/>train_input_values, test_input_values, train_target_values, test_target_values = train_test_split(iris_input_values, iris_output_values, <span>test_size</span>=<span>0.33</span>, <span>random_state</span>=<span>42</span>)</pre>
<p class="calibre2">Now, we define <em class="calibre19">x</em> and <em class="calibre19">y</em>. These placeholders will hold our iris data (both the features and label matrices) and help pass them along to different parts of the algorithm. You can consider placeholders as empty shells into which we insert our data. We also need to give them shapes that correspond to the shape of our data. Later, we will insert data into these placeholders by feeding the placeholders the data via a <kbd class="calibre12">feed_dict</kbd> (feed dictionary):</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why use placeholders?</h1>
                </header>
            
            <article>
                
<p class="calibre2">This feature of TensorFlow allows us to create an algorithm that accepts data and knows something about the shape of the data without knowing the amount of data going in. When we insert <em class="calibre19">batches</em> of data in training, we can easily adjust how many examples we train on in a single step without changing the entire algorithm:</p>
<pre class="calibre21"><span># numFeatures is the number of features in our input data.<br class="title-page-name"/></span><span># In the iris dataset, this number is '4'.<br class="title-page-name"/></span>num_explanatory_features = train_input_values.shape[<span>1</span>]<br class="title-page-name"/><br class="title-page-name"/><span># numLabels is the number of classes our data points can be in.<br class="title-page-name"/></span><span># In the iris dataset, this number is '3'.<br class="title-page-name"/></span>num_target_values = train_target_values.shape[<span>1</span>]<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><span># Placeholders<br class="title-page-name"/></span><span># 'None' means TensorFlow shouldn't expect a fixed number in that dimension<br class="title-page-name"/></span>input_values = tf.placeholder(tf.float32, [<span>None</span>, num_explanatory_features]) <span># Iris has 4 features, so X is a tensor to hold our data.<br class="title-page-name"/></span>output_values = tf.placeholder(tf.float32, [<span>None</span>, num_target_values]) <span># This will be our correct answers matrix for 3 classes.<br class="title-page-name"/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Set model weights and bias</h1>
                </header>
            
            <article>
                
<p class="calibre2">Much like linear regression, we need a shared variable weight matrix for logistic regression. We initialize both <em class="calibre19">W</em> and <em class="calibre19">b</em> as tensors full of zeros. Since we are going to learn <em class="calibre19">W</em> and <em class="calibre19">b</em>, their initial value doesn't matter too much. These variables are the objects that define the structure of our regression model, and we can save them after they've been trained so that we can reuse them later.</p>
<p class="calibre2">We define two TensorFlow variables as our parameters. These variables will hold the weights and biases of our logistic regression and they will be continually updated during training.</p>
<p class="calibre2">Notice that <em class="calibre19">W</em> has a shape of [4, 3] because we want to multiply the 4-dimensional input vectors by it to produce 3-dimensional vectors of evidence for the difference classes. <em class="calibre19">b</em> has a shape of [3], so we can add it to the output. Moreover, unlike our placeholders (which are essentially empty shells waiting to be fed data), TensorFlow variables need to be initialized with values, say, with zeros:</p>
<pre class="calibre21"><span>#Randomly sample from a normal distribution with standard deviation .01<br class="title-page-name"/></span><span><br class="title-page-name"/></span>weights = tf.Variable(tf.random_normal([num_explanatory_features,num_target_values],<br class="title-page-name"/>                                      <span>mean</span>=<span>0</span>,<br class="title-page-name"/>                                      <span>stddev</span>=<span>0.01</span>,<br class="title-page-name"/>                                      <span>name</span>=<span>"weights"</span>))<br class="title-page-name"/><br class="title-page-name"/>biases = tf.Variable(tf.random_normal([<span>1</span>,num_target_values],<br class="title-page-name"/>                                   <span>mean</span>=<span>0</span>,<br class="title-page-name"/>                                   <span>stddev</span>=<span>0.01</span>,<br class="title-page-name"/>                                   <span>name</span>=<span>"biases"</span>))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression model</h1>
                </header>
            
            <article>
                
<p class="calibre2">We now define our operations in order to properly run the logistic regression. Logistic regression is typically thought of as a single equation:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation17" src="assets/c5e9a90e-75b4-4675-878e-4be59ea25db9.png"/></div>
<p class="calibre2">However, for the sake of clarity, we can have it broken into its three main components:</p>
<ul class="calibre7">
<li class="calibre8">A weight times features matrix multiplication operation</li>
<li class="calibre8">A summation of the weighted features and a bias term</li>
<li class="calibre8">Finally, the application of a sigmoid function</li>
</ul>
<p class="calibre2">As such, you will find these components defined as three separate operations:</p>
<pre class="calibre21"><span># Three-component breakdown of the Logistic Regression equation.<br class="title-page-name"/></span><span># Note that these feed into each other.<br class="title-page-name"/></span>apply_weights = tf.matmul(input_values, weights, <span>name</span>=<span>"apply_weights"</span>)<br class="title-page-name"/>add_bias = tf.add(apply_weights, biases, <span>name</span>=<span>"add_bias"</span>)<br class="title-page-name"/>activation_output = tf.nn.sigmoid(add_bias, <span>name</span>=<span>"activation"</span>)</pre>
<p class="calibre2">As we have seen previously, the function we are going to use is the logistic function, which is fed the input data after applying weights and bias. In TensorFlow, this function is implemented as the <kbd class="calibre12">nn.sigmoid</kbd> function. Effectively, it fits the weighted input with bias into a 0-100 percent curve, which is the probability function we want.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training</h1>
                </header>
            
            <article>
                
<p class="calibre2">The learning algorithm is how we search for the best weight vector (<em class="calibre19">w</em>). This search is an optimization problem looking for the hypothesis that optimizes an error/cost measure.</p>
<p class="calibre2">So, the cost or the loss function of the model is going to tell us our model is bad, and we need to minimize this function. There are different loss or cost criteria that you can follow. In this implementation, we are going to use <strong class="calibre13">mean<span class="calibre10"> </span></strong><strong class="calibre13">squared error</strong> (<strong class="calibre13">MSE</strong>) as a loss function.</p>
<p class="calibre2">To accomplish the task of minimizing the loss function, we are going to use the gradient descent algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cost function</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before defining our cost function, we need to define how long we are going to train and how we <span class="calibre10">should</span><span class="calibre10"> </span><span class="calibre10">define the learning rate:</span></p>
<pre class="calibre21"><span>#Number of training epochs<br class="title-page-name"/></span>num_epochs = <span>700<br class="title-page-name"/></span><span># Defining our learning rate iterations (decay)<br class="title-page-name"/></span>learning_rate = tf.train.exponential_decay(<span>learning_rate</span>=<span>0.0008</span>,<br class="title-page-name"/>                                          <span>global_step</span>=<span>1</span>,<br class="title-page-name"/>                                          <span>decay_steps</span>=train_input_values.shape[<span>0</span>],<br class="title-page-name"/>                                          <span>decay_rate</span>=<span>0.95</span>,<br class="title-page-name"/>                                          <span>staircase</span>=<span>True</span>)<br class="title-page-name"/><br class="title-page-name"/><span># Defining our cost function - Squared Mean Error<br class="title-page-name"/></span>model_cost = tf.nn.l2_loss(activation_output - output_values, <span>name</span>=<span>"squared_error_cost"</span>)<br class="title-page-name"/><span># Defining our Gradient Descent<br class="title-page-name"/></span>model_train = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_cost)</pre>
<p class="calibre2">Now, it's time to execute our computational graph through the session variable.</p>
<p class="calibre2">So first off, we need to initialize our weights and biases with zeros or random values using <kbd class="calibre12">tf.initialize_all_variables()</kbd>. This initialization step will become a node in our computational graph, and when we put the graph into a session, the operation will run and create the variables:</p>
<pre class="calibre21"><span># tensorflow session<br class="title-page-name"/></span>sess = tf.Session()<br class="title-page-name"/><br class="title-page-name"/><span># Initialize our variables.<br class="title-page-name"/></span>init = tf.global_variables_initializer()<br class="title-page-name"/>sess.run(init)<br class="title-page-name"/><br class="title-page-name"/><span>#We also want some additional operations to keep track of our model's efficiency over time. We can do this like so:<br class="title-page-name"/></span><span># argmax(activation_output, 1) returns the label with the most probability<br class="title-page-name"/></span><span># argmax(output_values, 1) is the correct label<br class="title-page-name"/></span>correct_predictions = tf.equal(tf.argmax(activation_output,<span>1</span>),tf.argmax(output_values,<span>1</span>))<br class="title-page-name"/><br class="title-page-name"/><span># If every false prediction is 0 and every true prediction is 1, the average returns us the accuracy<br class="title-page-name"/></span>model_accuracy = tf.reduce_mean(tf.cast(correct_predictions, <span>"float"</span>))<br class="title-page-name"/><br class="title-page-name"/><span># Summary op for regression output<br class="title-page-name"/></span>activation_summary = tf.summary.histogram(<span>"output"</span>, activation_output)<br class="title-page-name"/><br class="title-page-name"/><span># Summary op for accuracy<br class="title-page-name"/></span>accuracy_summary = tf.summary.scalar(<span>"accuracy"</span>, model_accuracy)<br class="title-page-name"/><br class="title-page-name"/><span># Summary op for cost<br class="title-page-name"/></span>cost_summary = tf.summary.scalar(<span>"cost"</span>, model_cost)<br class="title-page-name"/><br class="title-page-name"/><span># Summary ops to check how variables weights and biases are updating after each iteration to be visualized in TensorBoard<br class="title-page-name"/></span>weight_summary = tf.summary.histogram(<span>"weights"</span>, weights.eval(<span>session</span>=sess))<br class="title-page-name"/>bias_summary = tf.summary.histogram(<span>"biases"</span>, biases.eval(<span>session</span>=sess))<br class="title-page-name"/><br class="title-page-name"/>merged = tf.summary.merge([activation_summary, accuracy_summary, cost_summary, weight_summary, bias_summary])<br class="title-page-name"/>writer = tf.summary.FileWriter(<span>"summary_logs"</span>, sess.graph)<br class="title-page-name"/><br class="title-page-name"/><span>#Now we can define and run the actual training loop, like this:<br class="title-page-name"/></span><span># Initialize reporting variables<br class="title-page-name"/></span><span><br class="title-page-name"/></span>inital_cost = <span>0<br class="title-page-name"/></span>diff = <span>1<br class="title-page-name"/></span>epoch_vals = []<br class="title-page-name"/>accuracy_vals = []<br class="title-page-name"/>costs = []<br class="title-page-name"/><br class="title-page-name"/><span># Training epochs<br class="title-page-name"/></span><span>for </span>i <span>in </span><span>range</span>(num_epochs):<br class="title-page-name"/>    <span>if </span>i &gt; <span>1 </span><span>and </span>diff &lt; <span>.0001</span>:<br class="title-page-name"/>       <span>print</span>(<span>"change in cost %g; convergence."</span>%diff)<br class="title-page-name"/>       <span>break<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>    else</span>:<br class="title-page-name"/>       <span># Run training step<br class="title-page-name"/></span><span>       </span>step = sess.run(model_train, <span>feed_dict</span>={input_values: train_input_values, output_values: train_target_values})<br class="title-page-name"/><br class="title-page-name"/>       <span># Report some stats evert 10 epochs<br class="title-page-name"/></span><span>       </span><span>if </span>i % <span>10 </span>== <span>0</span>:<br class="title-page-name"/>           <span># Add epoch to epoch_values<br class="title-page-name"/></span><span>           </span>epoch_vals.append(i)<br class="title-page-name"/><br class="title-page-name"/>           <span># Generate the accuracy stats of the model<br class="title-page-name"/></span><span>           </span>train_accuracy, new_cost = sess.run([model_accuracy, model_cost], <span>feed_dict</span>={input_values: train_input_values, output_values: train_target_values})<br class="title-page-name"/><br class="title-page-name"/>           <span># Add accuracy to live graphing variable<br class="title-page-name"/></span><span>           </span>accuracy_vals.append(train_accuracy)<br class="title-page-name"/><br class="title-page-name"/>           <span># Add cost to live graphing variable<br class="title-page-name"/></span><span>           </span>costs.append(new_cost)<br class="title-page-name"/>&gt;<br class="title-page-name"/>           <span># Re-assign values for variables<br class="title-page-name"/></span><span>           </span>diff = <span>abs</span>(new_cost - inital_cost)<br class="title-page-name"/>           cost = new_cost<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>           <span>print</span>(<span>"Training step %d, accuracy %g, cost %g, cost change %g"</span>%(i, train_accuracy, new_cost, diff))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Training step 0, accuracy 0.343434, cost 34.6022, cost change 34.6022<br class="title-page-name"/>Training step 10, accuracy 0.434343, cost 30.3272, cost change 30.3272<br class="title-page-name"/>Training step 20, accuracy 0.646465, cost 28.3478, cost change 28.3478<br class="title-page-name"/>Training step 30, accuracy 0.646465, cost 26.6752, cost change 26.6752<br class="title-page-name"/>Training step 40, accuracy 0.646465, cost 25.2844, cost change 25.2844<br class="title-page-name"/>Training step 50, accuracy 0.646465, cost 24.1349, cost change 24.1349<br class="title-page-name"/>Training step 60, accuracy 0.646465, cost 23.1835, cost change 23.1835<br class="title-page-name"/>Training step 70, accuracy 0.646465, cost 22.3911, cost change 22.3911<br class="title-page-name"/>Training step 80, accuracy 0.646465, cost 21.7254, cost change 21.7254<br class="title-page-name"/>Training step 90, accuracy 0.646465, cost 21.1607, cost change 21.1607<br class="title-page-name"/>Training step 100, accuracy 0.666667, cost 20.677, cost change 20.677<br class="title-page-name"/>Training step 110, accuracy 0.666667, cost 20.2583, cost change 20.2583<br class="title-page-name"/>Training step 120, accuracy 0.666667, cost 19.8927, cost change 19.8927<br class="title-page-name"/>Training step 130, accuracy 0.666667, cost 19.5705, cost change 19.5705<br class="title-page-name"/>Training step 140, accuracy 0.666667, cost 19.2842, cost change 19.2842<br class="title-page-name"/>Training step 150, accuracy 0.666667, cost 19.0278, cost change 19.0278<br class="title-page-name"/>Training step 160, accuracy 0.676768, cost 18.7966, cost change 18.7966<br class="title-page-name"/>Training step 170, accuracy 0.69697, cost 18.5867, cost change 18.5867<br class="title-page-name"/>Training step 180, accuracy 0.69697, cost 18.3951, cost change 18.3951<br class="title-page-name"/>Training step 190, accuracy 0.717172, cost 18.2191, cost change 18.2191<br class="title-page-name"/>Training step 200, accuracy 0.717172, cost 18.0567, cost change 18.0567<br class="title-page-name"/>Training step 210, accuracy 0.737374, cost 17.906, cost change 17.906<br class="title-page-name"/>Training step 220, accuracy 0.747475, cost 17.7657, cost change 17.7657<br class="title-page-name"/>Training step 230, accuracy 0.747475, cost 17.6345, cost change 17.6345<br class="title-page-name"/>Training step 240, accuracy 0.757576, cost 17.5113, cost change 17.5113<br class="title-page-name"/>Training step 250, accuracy 0.787879, cost 17.3954, cost change 17.3954<br class="title-page-name"/>Training step 260, accuracy 0.787879, cost 17.2858, cost change 17.2858<br class="title-page-name"/>Training step 270, accuracy 0.787879, cost 17.182, cost change 17.182<br class="title-page-name"/>Training step 280, accuracy 0.787879, cost 17.0834, cost change 17.0834<br class="title-page-name"/>Training step 290, accuracy 0.787879, cost 16.9895, cost change 16.9895<br class="title-page-name"/>Training step 300, accuracy 0.79798, cost 16.8999, cost change 16.8999<br class="title-page-name"/>Training step 310, accuracy 0.79798, cost 16.8141, cost change 16.8141<br class="title-page-name"/>Training step 320, accuracy 0.79798, cost 16.732, cost change 16.732<br class="title-page-name"/>Training step 330, accuracy 0.79798, cost 16.6531, cost change 16.6531<br class="title-page-name"/>Training step 340, accuracy 0.808081, cost 16.5772, cost change 16.5772<br class="title-page-name"/>Training step 350, accuracy 0.818182, cost 16.5041, cost change 16.5041<br class="title-page-name"/>Training step 360, accuracy 0.838384, cost 16.4336, cost change 16.4336<br class="title-page-name"/>Training step 370, accuracy 0.838384, cost 16.3655, cost change 16.3655<br class="title-page-name"/>Training step 380, accuracy 0.838384, cost 16.2997, cost change 16.2997<br class="title-page-name"/>Training step 390, accuracy 0.838384, cost 16.2359, cost change 16.2359<br class="title-page-name"/>Training step 400, accuracy 0.848485, cost 16.1741, cost change 16.1741<br class="title-page-name"/>Training step 410, accuracy 0.848485, cost 16.1141, cost change 16.1141<br class="title-page-name"/>Training step 420, accuracy 0.848485, cost 16.0558, cost change 16.0558<br class="title-page-name"/>Training step 430, accuracy 0.858586, cost 15.9991, cost change 15.9991<br class="title-page-name"/>Training step 440, accuracy 0.858586, cost 15.944, cost change 15.944<br class="title-page-name"/>Training step 450, accuracy 0.858586, cost 15.8903, cost change 15.8903<br class="title-page-name"/>Training step 460, accuracy 0.868687, cost 15.8379, cost change 15.8379<br class="title-page-name"/>Training step 470, accuracy 0.878788, cost 15.7869, cost change 15.7869<br class="title-page-name"/>Training step 480, accuracy 0.878788, cost 15.7371, cost change 15.7371<br class="title-page-name"/>Training step 490, accuracy 0.878788, cost 15.6884, cost change 15.6884<br class="title-page-name"/>Training step 500, accuracy 0.878788, cost 15.6409, cost change 15.6409<br class="title-page-name"/>Training step 510, accuracy 0.878788, cost 15.5944, cost change 15.5944<br class="title-page-name"/>Training step 520, accuracy 0.878788, cost 15.549, cost change 15.549<br class="title-page-name"/>Training step 530, accuracy 0.888889, cost 15.5045, cost change 15.5045<br class="title-page-name"/>Training step 540, accuracy 0.888889, cost 15.4609, cost change 15.4609<br class="title-page-name"/>Training step 550, accuracy 0.89899, cost 15.4182, cost change 15.4182<br class="title-page-name"/>Training step 560, accuracy 0.89899, cost 15.3764, cost change 15.3764<br class="title-page-name"/>Training step 570, accuracy 0.89899, cost 15.3354, cost change 15.3354<br class="title-page-name"/>Training step 580, accuracy 0.89899, cost 15.2952, cost change 15.2952<br class="title-page-name"/>Training step 590, accuracy 0.909091, cost 15.2558, cost change 15.2558<br class="title-page-name"/>Training step 600, accuracy 0.909091, cost 15.217, cost change 15.217<br class="title-page-name"/>Training step 610, accuracy 0.909091, cost 15.179, cost change 15.179<br class="title-page-name"/>Training step 620, accuracy 0.909091, cost 15.1417, cost change 15.1417<br class="title-page-name"/>Training step 630, accuracy 0.909091, cost 15.105, cost change 15.105<br class="title-page-name"/>Training step 640, accuracy 0.909091, cost 15.0689, cost change 15.0689<br class="title-page-name"/>Training step 650, accuracy 0.909091, cost 15.0335, cost change 15.0335<br class="title-page-name"/>Training step 660, accuracy 0.909091, cost 14.9987, cost change 14.9987<br class="title-page-name"/>Training step 670, accuracy 0.909091, cost 14.9644, cost change 14.9644<br class="title-page-name"/>Training step 680, accuracy 0.909091, cost 14.9307, cost change 14.9307<br class="title-page-name"/>Training step 690, accuracy 0.909091, cost 14.8975, cost change 14.8975<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">Now, it's time to see how our trained model performs on the <kbd class="calibre12">iris</kbd> dataset, so let's test our trained model against the test set:</p>
<pre class="calibre21"><span># test the model against the test set<br class="title-page-name"/></span><span>print</span>(<span>"final accuracy on test set: %s" </span>%<span>str</span>(sess.run(model_accuracy,<br class="title-page-name"/>                                                    <span>feed_dict</span>={input_values: test_input_values,<br class="title-page-name"/>                                                               output_values: test_target_values}))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>final accuracy on test set: 0.9</pre>
<div class="title-page-name">
<p class="calibre2">Getting 0.9 accuracy on the test set is really good and you can try to get better results by changing the number of epochs.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we went through a basic explanation of neural networks and and the need for multi-layer neural networks. We also covered the TensorFlow computational graph model with some basic examples, such as linear regression and logistic regression.</p>
<p class="calibre2">Next up, we will go through more advanced examples and demonstrate how TensorFlow can be used to build something like handwritten character recognition. We will also tackle the core idea of architecture engineering that has replaced feature engineering in traditional machine learning.</p>


            </article>

            
        </section>
    </body></html>