["```\nvgg_net = tf.keras.applications.VGG16(\n    include_top=True, weights='imagenet', input_tensor=None, \n    input_shape=None, pooling=None, classes=1000)\n```", "```\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\n\n# Sequential version:\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(5, 5), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))\n\n# Functional version:\ninputs = Input(shape=input_shape)\nconv1 = Conv2D(32, kernel_size=(5, 5))(inputs)\nmaxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\npredictions = Dense(10, activation='softmax')(Flatten()(maxpool1))\nmodel = Model(inputs=inputs, outputs=predictions)\n```", "```\nfrom keras.layers import Conv2D, MaxPooling2D, concatenate\n\ndef naive_inception_block(previous_layer, filters=[64, 128, 32]):\n    conv1x1 = Conv2D(filters[0], kernel_size=(1, 1), padding='same', \n                     activation='relu')(previous_layer)\n    conv3x3 = Conv2D(filters[1], kernel_size=(3, 3), padding='same',\n                     activation='relu')(previous_layer)\n    conv5x5 = Conv2D(filters[2], kernel_size=(5, 5), padding='same', \n                     activation='relu')(previous_layer)\n    max_pool = MaxPooling2D((3, 3), strides=(1, 1), \n                            padding='same')(previous_layer)\n    return concatenate([conv1x1, conv3x3, conv5x5, max_pool], axis=-1)\n```", "```\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nurl = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\"\nhub_feature_extractor = hub.KerasLayer( # TF-Hub model as Layer\n    url, # URL of the TF-Hub model (here, an InceptionV3 extractor)\n    trainable=False, # Flag to set the layers as trainable or not\n    input_shape=(299, 299, 3), # Expected input shape (found on tfhub.dev)\n    output_shape=(2048,), # Output shape (same, found on the model's page)\n    dtype=tf.float32) # Expected dtype\n\ninception_model = Sequential(\n    [hub_feature_extractor, Dense(num_classes, activation='softmax')], \n    name=\"inception_tf_hub\")\n```", "```\nfrom tf.keras.layers import Activation, Conv2D, BatchNormalization, add\n\ndef residual_block_basic(x, filters, kernel_size=3, strides=1):\n    # Residual Path:\n    conv_1 = Conv2D(filters=filters, kernel_size=kernel_size, \n                    padding='same', strides=strides)(x)\n    bn_1 = BatchNormalization(axis=-1)(conv_1)\n    act_1 = Activation('relu')(bn_1)\n    conv_2 = Conv2D(filters=filters, kernel_size=kernel_size, \n                    padding='same', strides=strides)(act_1)\n    residual = BatchNormalization(axis=-1)(conv_2)\n    # Shortcut Path:\n    shortcut = x if strides == 1 else Conv2D(\n        filters, kernel_size=1, padding='valid', strides=strides)(x)\n    # Merge and return :\n    return Activation('relu')(add([shortcut, residual]))\n```", "```\nfor i in range(num_layers_to_remove):\n    model.layers.pop()\n```", "```\nfor layer in model.layers:\n    if layer.name == name_of_last_layer_to_keep:\n        bottleneck_feats = layer.output\n        break\n```", "```\nbottleneck_feats = model.get_layer(last_layer_name).output\nfeature_extractor = Model(inputs=model.input, outputs=bottleneck_feats)\n```", "```\ndense1 = Dense(...)(feature_extractor.output) # ...\nnew_model = Model(model.input, dense1)\n```", "```\ndef model_function():\n    # ... define new model, reusing pretrained one as feature extractor.\n\nckpt_path = '/path/to/pretrained/estimator/model.ckpt'\nws = tf.estimator.WarmStartSettings(ckpt_path)\nestimator = tf.estimator.Estimator(model_fn, warm_start_from=ws)\n```", "```\n# Assuming the pretrained model was saved with `model.save()`:\nmodel = tf.keras.models.load_model('/path/to/pretrained/model.h5')\n# ... then pop/add layers to obtain the new model.\n```", "```\n# For instance, we want to freeze the model's layers with \"conv\" in their name:\nvars_to_train = model.trainable_variables\nvars_to_train = [v for v in vars_to_train if \"conv\" in v.name]\n\n# Applying the optimizer to the remaining model's variables:\noptimizer.apply_gradients(zip(gradient, vars_to_train))\n```", "```\nfor layer in feature_extractor_model.layers:\n    layer.trainable = False  # freezing the complete extractor\n```"]