- en: '*Chapter 9*: Video Synthesis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned about and built many models for image generation, including
    state-of-the-art **StyleGAN** and **Self-Attention GAN** (**SAGAN**) models, in
    previous chapters. You have now learned about most if not all of the important
    techniques used to generate images, and we can now move on to video generation
    (synthesis). In essence, video is simply a series of still images. Therefore,
    the most basic video generation method is to generate images individually and
    put them together in a sequence to make a video. **Video synthesis** is a complex
    and broad topic in its own right, and we won't be able to cover everything in
    a single chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will get an overview of video synthesis. We will then implement
    what is probably the most well-known video generation technique, `deepfake` online
    and you'll be impressed by how real some of them seem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Video synthesis overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing face image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deepfake model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swapping faces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving DeepFakes with GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this chapter can be accessed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook used in this chapter is this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ch9_deepfake.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video synthesis overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say your doorbell rings while you're watching a video, so you pause the
    video and go to answer the door. What would you see on your screen when you come
    back? A still picture where everything is frozen and not moving. If you press
    the play button and pause it again quickly, you will see another image that looks
    very similar to the previous one but with slight differences. Yes – when you play
    a series of images sequentially, you get a video.
  prefs: []
  type: TYPE_NORMAL
- en: We say that image data has three dimensions, or *(H, W, C)*; video data has
    four dimensions, *(T, H, W, C)*, where *T* is the temporal (time) dimension. It's
    also the case that video is just a big *batch* of images, except that we cannot
    shuffle the batch. There must be temporal consistency between the images; I'll
    explain this further.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we extract images from some video datasets and train an unconditional
    GAN to generate images from random noise input. As you can imagine, the images
    will look very different from each other. As a result, the video made from those
    images would be unwatchable. Like image generation, video generation can also
    be classified as unconditional or conditional.
  prefs: []
  type: TYPE_NORMAL
- en: In unconditional video synthesis, not only does the model need to generate good-quality
    content but it must also keep the temporal content or movement in check. As a
    result, the output video is generally quite short for some simple video content.
    Unconditional video synthesis is still not quite mature enough yet for practical
    application.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, conditional video synthesis conditions on input content and
    therefore generates better-quality results. As we learned in [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*, there is very little randomness in images generated
    by pix2pix. The lack of randomness may be a drawback in some applications, but
    the consistency in generated images is a plus in video synthesis. Thus, many video
    synthesis models are conditioned on images or videos. In particular, conditional
    face video synthesis has achieved great results and has had a real impact in commercial
    applications. We will now look at some of the most common forms of face video
    synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding face video synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common forms of face video synthesis are **face re-enactment** and
    **face swapping**. It is best to explain the difference between them by using
    pictures as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Face re-enactment and face swapping'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: Y. Nirkin et al., 2019, “FSGAN: Subject Agnostic Face Swapping and
    Reenactment,” https://arxiv.org/pdf/1908.05932)](img/B14538_09_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.1 – Face re-enactment and face swapping (Source: Y. Nirkin et al.,
    2019, “FSGAN: Subject Agnostic Face Swapping and Reenactment,” [https://arxiv.org/pdf/1908.05932](https://arxiv.org/pdf/1908.05932))'
  prefs: []
  type: TYPE_NORMAL
- en: The top row shows how face re-enactment works. In face re-enactment, we want
    to transfer the expression of the face in the target video (right) to the face
    of the source image (left) to produce the image in the middle. Digital puppetry
    is already used in computer animation and movie production, where the facial expression
    of an actor is used to control a digital avatar. Face re-enactment using AI has
    the potential to make this happen more easily. The bottom row shows face swapping.
    This time, we want to keep the facial expression of the target video but use the
    face from the source image.
  prefs: []
  type: TYPE_NORMAL
- en: Although technically different, face re-enactment and face swapping are similar.
    In terms of generated video, both could be used to create a *fake video*. As the
    name suggests, face swapping swaps just the face but not the head. Therefore,
    both the target and source faces should have a similar shape to increase the fidelity
    of the fake video. You can use this as a visual cue to differentiate between face
    swapping and face re-enactment videos. Face re-enactment is technically more challenging
    and it doesn't always require a driving video; it can use facial landmarks or
    sketches instead. We will introduce one such model in the next chapter. In the
    rest of this chapter, we will focus on implementing face swapping with the deepfake
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: DeepFake overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of you may have seen online videos where the face of an actor has been
    swapped with another celebrity's face. Quite often, that celebrity is the actor
    Nicolas Cage, and the resulting videos are quite hilarious to watch. This all
    started around the end of 2017, when an anonymous user named *deepfakes* posted
    the algorithm (which was later named after the username) on the social news website
    Reddit. This was quite unusual, considering that almost all of the breakthrough
    machine learning algorithms of the past decade had their origins in academia.
  prefs: []
  type: TYPE_NORMAL
- en: People have used the deepfake algorithm to create all sorts of videos, including
    some for TV advertisements and movies. However, as these fake videos can be very
    convincing, they have also raised some ethical issues. Researchers have demonstrated
    that it is possible to create fake videos to make the former US president Barack
    say things that he did not say. People have genuine reasons to be worried about
    deepfake and researchers have also been devising ways to detect these fake videos.
    You will want to understand how deepfake works, either to create funny videos
    or to combat fake news videos. So, let's crack on!
  prefs: []
  type: TYPE_NORMAL
- en: 'The deepfake algorithm can be roughly broken into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A deep learning model** to perform face image translation. We will first
    collect datasets for two people, say, **A** and **B**, and use an autoencoder
    to train them separately to learn their latent code, as shown in the following
    figure. There is a shared encoder, but we use separate decoders for different
    people. The top diagram in the figure shows the training architecture. The bottom
    diagram shows the face swap.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Firstly, **Face A** (the source) is encoded into a small latent face (the latent
    code). The latent code contains face representations such as the head pose (angle),
    facial expression, eyes open or shut, and more. We will then use decoder **B**
    to convert the latent code into **Face B**. The aim is to generate **Face B**
    using the pose and expression of **Face A**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2 – deepfake using autoencoders. (Top) Training with one encoder
    and two decoders. (Bottom) Reconstructing Face B from A (Redrawn from: T.T. Nguyen
    et al, 2019, “Deep Learning for deepfakes Creation and Detection: A Survey,” https://arxiv.org/abs/1909.11573)
    ](img/B14538_09_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9.2 – deepfake using autoencoders. (Top) Training with one encoder and
    two decoders. (Bottom) Reconstructing Face B from A (Redrawn from: T.T. Nguyen
    et al, 2019, “Deep Learning for deepfakes Creation and Detection: A Survey,” https://arxiv.org/abs/1909.11573)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a normal image generation setting, a model is basically what we need for
    production. All we need to do is to send an input image to the model to produce
    an output image. But the production pipeline for deepfake is more involved, as
    will be described later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We''ll need a collection of traditional computer vision techniques to perform
    pre- and post-processing, including these:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Face detection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Face landmark detection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Face alignment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Face warping
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Face mask detection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following figure shows the deepfake production pipeline:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3 – DeepFake production pipeline (Source: Y. Li, S. Lyu, 2019, “Exposing
    deepfake Videos By Detecting Face Warping Artifacts,” https://arxiv.org/abs/1811.00656)](img/B14538_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3 – DeepFake production pipeline (Source: Y. Li, S. Lyu, 2019, “Exposing
    deepfake Videos By Detecting Face Warping Artifacts,” https://arxiv.org/abs/1811.00656)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps can be grouped into three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Steps *(a)* to *(f)* are pre-processing steps to extract and align the source
    face from the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a face swap to produce target *face (g)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps *(h)* to *(j)* are post-processing steps to *paste* the target face into
    the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned about and built autoencoders in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder*, and therefore it is relatively easy for us to build
    one for deepfake. On the other hand, many of the aforementioned computer vision
    techniques have not been introduced before in this book. Therefore, in the next
    section, we will implement the face processing steps one by one. Then, we will
    implement the autoencoder and finally implement all of the techniques together
    to produce a deepfake video.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing face image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use mainly two Python libraries – `dlib` was originally a C++ toolkit
    for machine learning, it also has a Python interface, and it is the go-to machine
    learning Python library for facial landmark detection. Most of the image processing
    code used in this chapter is adapted from [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting image from video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing in the production pipeline is to extract images from video.
    A video is made up of a series of images separated by a fixed time interval. If
    you check a video file''s properties, you may find something that says `.mp4`
    video file into directory/images and name them using a number sequence – for example,
    `image_0001.png`, `image_0002.png`, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can also use OpenCV to read the video frame by frame and
    save the frames into individual image files as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will use the extracted images for all subsequent processing and not worry
    about the source video anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and localizing a face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional computer vision techniques detect faces by using the **Histogram
    of Oriented Gradients** (**HOG**). The gradient of a pixel image can be calculated
    by taking the difference of the preceding and following pixels in the horizontal
    and vertical directions. The magnitude and direction of a gradient tells us about
    the lines and corners of a face. We can then use the HOG as a feature descriptor
    to detect the shape of a face. A modern approach is, of course, to use a CNN,
    which is more accurate but slower.
  prefs: []
  type: TYPE_NORMAL
- en: '`face_recognition` is a library built on top of `dlib`. By default, it uses
    the HOG of `dlib` as a face detector, but it also has the option to use a CNN.
    Using it is simple, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return a list of coordinates for each of the faces detected in the
    image. In our code, we assume that there is only one face in the image. The coordinates
    returned are in `css` format, (top, right, bottom, left), so we''ll need an additional
    step to convert them into `dlib.rectangle` objects for the `dlib` facial landmarks
    detector, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read the bounding box coordinates from `dlib.rectangle` and crop the
    face from the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If a face is detected in the image, we can then move on to the next step to
    detect facial landmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting facial landmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`dlib` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – The 68 points the dlib facial landmarks, for the chin, eyebrows,
    nose bridge, nose tip, eyes, and lips](img/B14538_09_4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – The 68 points the dlib facial landmarks, for the chin, eyebrows,
    nose bridge, nose tip, eyes, and lips
  prefs: []
  type: TYPE_NORMAL
- en: '`dlib` makes facial landmarks detection easy. We only need to download and
    load the model into `dlib` before using it as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will also pass the face coordinates into the predictor to tell it where the
    face is. This means, we don't need to crop out the face before calling the function.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks are very useful features in machine learning problems. For
    example, if we want to know a person's facial expression, we could use the lips
    keypoints as input features to the machine learning algorithm to detect whether
    the mouth is open. This is more effective and efficient than looking at every
    single pixel in the image. We can also use facial landmarks to estimate the head
    pose.
  prefs: []
  type: TYPE_NORMAL
- en: 'In DeepFake, we use facial landmarks to perform face alignment, which I will
    explain shortly. Before that, we will need to convert the landmarks from `lib`
    format into a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now we have everything we need for face alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning a face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naturally, faces in video appear in various poses, such as looking to the left
    or open-mouthed. In order to make it easier for the autoencoder to learn, we will
    align the faces to the center of the cropped image, looking straight at the camera.
    This is known as `dlib` landmarks except for the first 18 points of the chin.
    This is because people have vastly different chin shapes, which could skew the
    alignment results, so they are not used as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: Mean face
  prefs: []
  type: TYPE_NORMAL
- en: If you still remember, we looked at mean faces in [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)*,
    Getting Started with Image Generation Using TensorFlow*. They were generated by
    directly sampling from the dataset, so not exactly the same way as used in `dlib`.
    Anyway, feel free to go to take a look if you have forgotten what mean faces look
    like.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to perform the following operations on the face to align it with
    the mean face''s position and angle:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translation (shift in location)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These operations can be represented using a 2×3 affine transformation matrix.
    The affine matrix *M* is composed of matrices *A* and *B* as shown in the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Matrix *A* contains the parameters for linear transformation (scale and rotation),
    while matrix *B* is used for translation. deepfake uses an algorithm from S. Umeyama
    to estimate the parameters. The source code of the algorithm is contained in a
    single file that I have included in our GitHub repository. We call the function
    by passing the detected facial landmarks and the mean face landmarks as shown
    in the following code. As explained earlier, we omit the chin landmarks as they
    are not included in the mean face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now pass the affine matrix into `cv2.warpAffine()` to perform affine
    transformation, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the faces before and after alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – (Left) Author’s face with facial landmarks and face detection
    bounding box. (Right) Aligned face](img/B14538_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – (Left) Author's face with facial landmarks and face detection bounding
    box. (Right) Aligned face
  prefs: []
  type: TYPE_NORMAL
- en: 'The bounding boxes in the figure show the face detection at work. The picture
    on the left is also marked with facial landmarks. On the right is the face after
    alignment. We can see that the face has now been scaled larger to fit the mean
    face. In fact, the alignment output has the face more zoomed in, covering only
    the area between the eyebrows and the chin. I added padding to zoom out a little
    to include the bounding box in the final image. We can see from the bounding box
    that the face has been rotated so that it appears vertical. Next, we will learn
    about the last image pre-processing step: face warping.'
  prefs: []
  type: TYPE_NORMAL
- en: Face warping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll need two images to train an autoencoder, the input image and the target
    image. In deepfake, the target image is the aligned face, while the input image
    is a warped version of the aligned face. A face in the image does not change its
    shape after the affine transformation that we implemented in the preceding section,
    but warping, for example, twisting one side of the face, can change the shape
    of a face. deepfake warps faces to imitate the variety of face poses in real video
    as data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In image processing, transformation is the mapping of a pixel from one location
    in a source image to a different location in a target image. For example, translation
    and rotation is a one-to-one mapping that changes location and angle but retains
    size and shape. For warping, the mapping can be irregular, and the same point
    can be mapped to multiple points, which can give the effect of twisting and bending.
    The following diagram shows an example of mapping that warps an image from dimensions
    256×256 to 64×64:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Mapping to demonstrate warping'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_09_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Mapping to demonstrate warping
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform some random warping to twist a face slightly but not so much
    as to cause major distortion. The following code shows how to perform a face warp.
    You don''t have to understand every line of the code; it is sufficient to know
    that it uses the mapping as previously described to warp a face into a smaller
    dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I guess most people think that deepfake is just a deep neural network but do
    not realize there are so many image processing steps involved. Luckily, OpenCV
    and `dlib` make things easy for us. Now, we can move on to build the whole deep
    neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a DeepFake model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep learning model used in the original deepfake is an autoencoder-based
    one. There are a total of two autoencoders, one for each face domain. They share
    the same encoder, so there is a total of one encoder and two decoders in the model.
    The autoencoders expect an image size of 64×64 for both the input and the output.
    Now, let's build the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Building the encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we learned in the previous chapter, the encoder is responsible for converting
    high-dimensional images into a low-dimensional representation. We''ll first write
    a function to encapsulate the convolutional layer; leaky ReLU activation is used
    for downsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the usual autoencoder implementation, the output of the encoder is a 1D
    vector with a size of about 100 to 200, but deepfake uses larger dimensions of
    1,024\. In addition, it reshapes the 1D latent vector and upscales it back into
    3D activation. Therefore, the output of the encoder is not a 1D vector of size
    (1,024) but a tensor of size (8, 8, 512), as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the encoder can be grouped into three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: There are convolutional layers, which downscale a `(64, 64, 3)` image all the
    way to `(4, 4, 1024)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are two dense layers. The first one produces a latent vector of size `1024`,
    then the second one projects it to a higher dimension, which gets reshaped to
    `(4, 4, 1024)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The upsampling and convolution layers bring the output to size `(8, 8, 512)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be understood better by looking at the following model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Model summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_09_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Model summary
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to construct the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Building the decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The decoder''s input comes from the encoder''s output, so it expects a tensor
    of size `(8, 8, 512)`. We use several layers of upsampling to upscale the activations
    gradually to the target image dimension of `(64, 64, 3)`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to before, we will first write a function for the upsampling block
    that contains an upsampling function, a convolutional layer, and leaky ReLU, as
    shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we stack the upsampling blocks together. The final layer is a convolutional
    layer that brings the channel number to `3` to match the RGB color channels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The decoder model summary is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Keras model summary of the decoder'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_09_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Keras model summary of the decoder
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will put the encoder and decoders together to construct the autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Training the autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the DeepFake model consists of two autoencoders that
    share the same encoder. To construct the autoencoders, the first step is to instantiate
    the encoder and decoders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we build two separate autoencoders by joining the encoder with the respective
    decoders as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to prepare the training dataset. Although the autoencoder has
    an input image size of 64 × 64, the image preprocessing pipeline expects images
    of 256 × 256\. We will need about 300 images for each face domain. There is a
    link in the GitHub repository to where you can download some prepared images.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can also create datasets yourself by cropping the faces from
    collected images or video using the image processing techniques that we learned
    earlier. The faces in the dataset do not need to be aligned as the alignment will
    be performed in the image pre-processing pipeline. The image pre-processing generator
    will return two images – an aligned face and a warped version, both at a resolution
    of 64×64\.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now pass the two generators into `train_step()` to train the autoencoder
    models as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Writing and training the autoencoder is probably the easiest part of the deepfake
    pipeline. We don't need a lot of data; probably about 300 images per face domain
    is sufficient. Of course, more data should provide better results. As both the
    dataset and model aren't big, the training can happen relatively quickly even
    without the use of a GPU. Once we have a trained model, the final step is to perform
    the face swap.
  prefs: []
  type: TYPE_NORMAL
- en: Swapping faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here comes the last step of the deepfake pipeline, but let''s first recap the
    pipeline. The deepfake production pipeline involves three main stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract a face from an image using `dlib` and OpenCV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translate the face using the trained encoder and decoders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Swap the new face back into the original image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The new face generated by the autoencoder is an aligned face of size 64×64,
    so we will need to warp it to the position, size, and angle of the face in the
    original image. We''ll use the affine matrix obtained from *step 1* in the face
    extraction stage. We''ll use `cv2.warpAffine` like before, but this time, the
    `cv2.WARP_INVERSE_MAP` flag is used to reverse the direction of image transformation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: However, directly pasting the new face onto the original image will create artifacts
    around the edges. This will be especially obvious if any part of the new face
    (which is a square 64×64) exceeds the original face boundary. To alleviate the
    artifacts, we will trim the new face with a face mask.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first mask we will create is one that contours around the facial landmarks
    in the original image. The following code will first find the contours of given
    facial landmarks and then fill inside of the contour with ones (1) and return
    it as a hull mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As the **hull mask** is bigger than the new face square, we will need to trim
    the hull mask to fit the new square. To do this, we can create a rectangle mask
    from the new face and multiply it with the hull mask. The following diagram shows
    an example of the mask for the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – (Left to right) (a) Original image (b) Rectangular mask of new
    face (c) Hull mask of original face (d) Combined mask](img/B14538_09_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – (Left to right) (a) Original image (b) Rectangular mask of new
    face (c) Hull mask of original face (d) Combined mask
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we use the mask to remove the face from the original image and fill it
    in with the new face using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The resulting face may still not look perfect. For instance, if the two faces
    have vastly different skin tone or shading, then we may need to use further and
    more sophisticated methods to iron out the artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes the face swapping. We do this for each of the images extracted
    from a video, then we convert the images back into a video sequence. One way to
    do so is to use `ffmpeg` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The deepfake model and the computer vision techniques used in this chapter are
    fairly basic, as I wanted to make it easy to understand. Therefore, this code
    may not produce a realistic fake video. If you are keen on generating good fake
    videos, I would recommend you visit the [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap)
    GitHub repository, on which a big part of this chapter's code is based. Next,
    we will quickly look at how deepfake can be improved by using GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Improving DeepFakes with GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output image of deepfake''s autoencoders can be a little blurry, so how
    can we improve that? To recap, the deepfake algorithm can be broken into two main
    techniques – face image processing and face generation. The latter can be thought
    of as an image-to-image translation problem, and we learned a lot about that in
    [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*, Image-to-Image
    Translation*. Therefore, the natural thing to do would be to use a GAN to improve
    the quality. One helpful model is **faceswap-GAN**, and we will now go over a
    high-level overview of it. The autoencoder from the original deepfake is enhanced
    with residual blocks and self-attention blocks (see [*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation*) and used as a generator in faceswap-GAN.
    The discriminator architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 - faceswap-GAN’s discriminator architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)](img/B14538_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10 - faceswap-GAN''s discriminator architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)'
  prefs: []
  type: TYPE_NORMAL
- en: We can learn a lot about the discriminator by looking at the preceding diagram
    alone. First, the input tensor has a channel dimension of `6`, which suggests
    it is a stack of two images – real and fake. Then there are two blocks of self-attention
    layers. The output has a shape of 8×8×1, so each of the output features looks
    at patches of the input image. In other words, the discriminator is PatchGAN with
    self-attention layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the encoder and decoder architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 - faceswap-GAN’s encoder and decoder architecture (Redrawn from:
    https://github.com/shaoanlu/faceswap-GAN)](img/B14538_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11 - faceswap-GAN''s encoder and decoder architecture (Redrawn from:
    https://github.com/shaoanlu/faceswap-GAN)'
  prefs: []
  type: TYPE_NORMAL
- en: There aren't a lot of changes to the encoder and decoder. Self-attention layers
    are added to both the encoder and decoder, and one residual block is added to
    the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The losses used in training are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Least-squares** (**LS**) **loss** is the adversarial loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perception loss** is the VGG features an L2 loss between the real and fake
    faces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 reconstruction loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge loss** is the L2 loss of the gradients (in the *x* and *y* directions)
    around the eyes. This helps the model to generate realistic eyes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One thing that I've been trying to achieve with this book is to instill you
    with the knowledge of most, if not all, of the fundamental building blocks of
    image generation. Once you know them, implementing a model is just like putting
    Lego bricks together. As we are already familiar with the losses (apart from edge
    loss), residual blocks, and self-attention blocks, I trust that you can now implement
    this model yourself, if you wish to. For interested readers, you can refer to
    the original implementation at [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! We have now finished all the coding in this book. We have learned
    how to use `dlib` to detect faces and facial landmarks and how to use OpenCV to
    warp and align a face. We also learned how to use warping and masking to do face
    swapping. As a matter of fact, we spent most of the chapter learning about face
    image processing and spent very little time on the deep learning side. We have
    implemented autoencoders by reusing and modifying the autoencoder code from the
    previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we went over an example of improving deepfake by using GANs. faceswap-GAN
    improves deepfake by adding a residual block, a self-attention block, and a discriminator
    for adversarial training, all of which we have already learned about in previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which is also the final chapter, we will review the techniques
    we have learned in this book and look at some of the pitfalls in training GANs
    for real-world applications. Then, we will go over a few important GAN architectures,
    looking at image inpainting and text-to-image synthesis. Finally, we will look
    at up-and-coming applications such as video retargeting and 3D-to-2D rendering.
    There won't be any coding in the next chapter, so you can sit back and relax.
    Cheers!
  prefs: []
  type: TYPE_NORMAL
