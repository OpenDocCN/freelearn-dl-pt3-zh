- en: '*Chapter 9*: Deploying Deep RL Agents on Multiple Platforms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides recipes to deploy your Deep RL agent models in applications
    targeting desktop, web, mobile, and beyond. The recipes serve as customizable
    templates that you can utilize to build and deploy your own Deep RL applications
    for your use cases. You will also learn how to export RL agent models for serving/deployment
    in various production-ready formats, such as **TensorFlow Lite**, **TensorFlow.js**,
    and **ONNX**, and learn how to leverage Nvidia **Triton** to launch production-ready
    RL-based AI services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following recipes are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Packaging Deep RL agents for mobile and IoT devices using TensorFlow Lite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying RL agents on mobile devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging Deep RL agents for the web and Node.js using TensorFlow.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a Deep RL agent as a service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging Deep RL agents for cross-platform deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04
    and should work with later versions of Ubuntu if Python 3.6+ is available. With
    Python 3.6+ installed along with the necessary Python packages, as listed before
    the start of each of the recipes, the code should run fine on Windows and Mac
    OSX too. It is advised to create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Miniconda or Anaconda installation
    for Python virtual environment management is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter will be available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Packaging Deep RL agents for mobile and IoT devices using TensorFlow Lite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will show how you can leverage the open source **TensorFlow Lite**
    (**TFLite**) framework for serving your Deep RL agents on mobile, IoT, and embedded
    devices. We will implement a complete script to build, train, and export an agent
    model that you can load into a mobile or embedded device. We will explore two
    methods to generate the TFLite model for our agent. The first method involves
    saving and exporting the agent models in TensorFlow's SavedModel file format and
    then using a command-line converter. The second method leverages the Python API
    to directly generate the TFLite models.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook''s
    code repo. If the following imports work without issues, you are ready to get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following steps, we will save space by focusing on the new and important
    pieces that are unique to this recipe. We will go through the model saving and
    export functionality and the different ways you can do that and keep the Actor,
    Critic, and Agent model definitions out of the following steps to save space.
    Please refer to the book's code repository for a complete implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it is important to set TensorFlow Keras''s backend to use `float32`
    as the default representation for float values instead of the default `float64`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s create a handler for arguments passed to the script. We will also
    define a list of options for the training environments that can be chosen from
    for the `--env` flag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will add a few other arguments to ease the training and logging configuration
    of the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also set up logging so that we can visualize the agent''s learning progress
    using TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the first export approach, we will define save methods for the `Actor`,
    `Critic`, and `Agent` classes in the following steps. We will start with the implementation
    of the `save` method in the `Actor` class to export the Actor model to TensorFlow''s
    `SavedModel` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we will implement a `save` method for the `Critic` class to export
    the Critic model to TensorFlow''s `SavedModel` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now add a `save` method for the `Agent` class that will utilize the
    `Actor` and `Critic` `save` method to save both the models needed by the Agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the `save()` method is executed, it will generate two models (one for the
    Actor and one for the Critic) and save them in the specified directory on the
    filesystem with the directory structure and files similar to the one shown in
    the following figure:![Figure 9.1 – TensorFlow SavedModel directory structure
    and file contents for the PPO RL agent ](img/B15074_09_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.1 – TensorFlow SavedModel directory structure and file contents for
    the PPO RL agent
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the `SavedModel` files are generated, we can use the `tflite_convert`
    command-line tool and specify the location of the Actor model''s save directory.
    Refer to the following command for an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we can convert the Critic model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Hooray! We now have both the Actor and Critic models in TFLite format, which
    we can ship with our mobile applications. We will look at another approach that
    doesn't need us to (manually) switch to the command line to convert the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There''s another approach to export the Agent model to the TFLite format. We
    will be implementing it in the following steps, starting with the `save_tflite`
    method for the `Actor` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we will implement the `save_tflite` method for the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Agent''s class can then call the `save_tflite` method on the Actor and
    Critic using its own `save_tflite` method, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we added the `bin` directory of the current (`tfrl-cookbook`) Python
    environment to the system's `PATH` environment variable to make sure the `toco_from_protos`
    binary is found when the TFLite converter invokes the model conversion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To sum up, we can finalize the `main` function to instantiate the agent and
    train and save the model in TFLite model file format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes our recipe. Let's recap with some important details to understand
    the recipe better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first set TensorFlow Keras's backend to use `float32` as the default representation
    for float values. This is because, otherwise, TensorFlow would use the default
    `float64` representation, which is not supported by TFLite (for performance reasons)
    as it is targeted towards running on embedded and mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we defined a list of choices for the `--env` argument. This is important
    to make sure that the environment's observation and action spaces are compatible
    with the agent's model. In this recipe, we used a PPO agent with Actor and Critic
    networks that expect image observations and produce actions in discrete space.
    You can swap the agent code with the PPO implementations from one of the earlier
    chapters that use different state/observation spaces and action spaces. You could
    also replace the agent with a different agent algorithm altogether. You will find
    a bonus recipe that exports a DDPG agent TFLite model in the book's code repository
    for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed two approaches to save and convert our Agent models to TFLite
    format. The first approach allowed us to generate a TensorFlow SavedModel file
    format first and then convert it to the TFLite model file format using the `tflite_convert`
    command-line tool. In the second approach, we used TFLite''s Python API to directly
    (in-memory) convert and save the agent''s models in TFLite (Flatbuffer) format.
    We made use of the `TFLiteConverter` module, which ships with the official TensorFlow
    2.x Python package. A summary of different ways to export the RL agent''s model
    using the API is provided in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Converting TensorFlow 2.x models to TensorFlow Lite Flatbuffer
    format ](img/B15074_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Converting TensorFlow 2.x models to TensorFlow Lite Flatbuffer
    format
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about the TFLite model format here: [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite).'
  prefs: []
  type: TYPE_NORMAL
- en: It's time to hop on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying RL agents on mobile devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mobile is the most-targeted platform due to its high customer reach compared
    to other platforms. The global mobile application market size is projected to
    reach USD 407.32 billion by 2026 according to [https://www.alliedmarketresearch.com/mobile-application-market](https://www.alliedmarketresearch.com/mobile-application-market).
    Such a huge market opens several opportunities for infusing RL-based Artificial
    Intelligence. Android and iOS are the two main OS platforms in this space. While
    IOS is a popular platform, building apps for iOS requires a Mac to develop the
    apps. We will therefore develop an Android app using the Android SDK, which is
    more widely accessible. If you are an iOS app developer, you may be able to adapt
    parts of this recipe to your app.
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe provides ways for you to deploy trained RL agent models on mobile
    and/or IoT devices using the TensorFLow Lite framework. You will also have access
    to a sample RL Table Tennis Android app that you can use as a testbed to deploy
    your RL agent or develop your own ideas and apps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – A screenshot of the RL Table Tennis app running on an Android
    device ](img/B15074_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A screenshot of the RL Table Tennis app running on an Android device
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using Android Studio to set up and develop the sample RL Android
    app. Download and install Android Studio from the official website: [https://developer.android.com/studio](https://developer.android.com/studio).
    Using the default install location is recommended. Once installed, run Android
    Studio to start the **Android Studio Setup Wizard**. Follow through the setup
    process and make sure the latest Android SDK, Android SDK command-line tools,
    and the Android SDK build tools are marked for installation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the application once complete, you have two options: 1\. Run it on your
    Android phone 2\. Run it in the Android virtual device emulator. Follow the setup
    instructions depending on your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running it on your Android phone:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a) Enable developer options and USB debugging in Android settings. Detailed
    instructions are available here: [https://developer.android.com/studio/debug/dev-options](https://developer.android.com/studio/debug/dev-options).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) If you are on Windows, install the Google USB driver: [https://developer.android.com/studio/run/win-usb](https://developer.android.com/studio/run/win-usb).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Connect your phone to your computer using a USB cable and, if prompted, allow
    your computer to access your phone.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) Run `adb devices` to make sure your phone is detected. If your phone is
    not detected, make sure the drivers are installed and ADB debugging is enabled
    on your phone. You can follow the Android official guide here for detailed instructions:
    [https://developer.android.com/studio/run/device#setting-up](https://developer.android.com/studio/run/device#setting-up).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Running it in the Android emulator:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) Launch Android Studio, click on the **AVD Manager** icon and select **Create
    Virtual Device**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Choose a device and select **Next**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Choose an x86 or x86_64 image for the Android version you want to emulate
    and complete the process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Click **Run** in the AVD Manager toolbar to launch the emulator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you have the device set up, navigate to the code directory for this recipe
    under the `src/ch9-cross-platform-deployment` directory. You will find a sample
    Android application with a directory structure and contents like the one shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Directory structure and contents of the sample Android app ](img/B15074_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Directory structure and contents of the sample Android app
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the sample code base to work with, move on to the next section
    to see how to prepare our RL agent model and build the app.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll start with the RL agent model preparation and then build a simple, two-player
    Table Tennis app where you can play against the agent. Follow the steps listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Export your RL agent's (Actor) model to TFLite format using the previous recipe
    discussed in this chapter. For example, you can run the previous recipe to train
    a PPO agent for the `Pong-v4` environment and use the generated `model.tflite`
    file in the `trained_models/actor/1/` directory. Place the model in the Android
    app's `app/src/assets/` directory as highlighted in the figure here:![Figure 9.5
    – RL agent model.tflite location in Android app src ](img/B15074_09_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.5 – RL agent model.tflite location in Android app src
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Edit the app''s `dependencies` section in the `build.gradle` file to include
    the `tensorflow-lite` dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a member method to load the `agent/model.tflite` from the `assets` folder
    and return a `MappedByteBuffer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now create a new TFLite interpreter like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The interpreter is ready. Let''s prepare the input. First, let''s define some
    constants based on what we know from our agent training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement a method to convert image data in `BitMap` format to `ByteArray`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now run the image observations from the Table Tennis game through the
    Agent model to get the action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Those are all the main ingredients for this recipe! You can run them in a loop
    to generate actions per observation/game frame or customize them however you like!
    Let's look at how to run the app on an Android device using Android Studio in
    the following steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Launch Android Studio. You will see a screen like this:![Figure 9.6 – Android
    Studio welcome screen ](img/B15074_09_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.6 – Android Studio welcome screen
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's proceed to the next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Open an Existing Project** option and you will see a popup asking
    you to choose the directory on your filesystem. Navigate to the folder where you
    have cloned the book's code repo or your fork, and browse to this recipe's folder
    under [*Chapter 9*](#_idTextAnchor244) as shown in the figure here:![Figure 9.7
    – File/project picker interface to choose the RL Android app ](img/B15074_09_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.7 – File/project picker interface to choose the RL Android app
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will notice that Android Studio has already identified our app and shows
    the directory with an Android symbol.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you click **OK**, Android Studio will open with the app's code and will
    look like the following figure:![Figure 9.8 – Android Studio with the TFRL-Cookbook's
    RL app loaded ](img/B15074_09_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.8 – Android Studio with the TFRL-Cookbook's RL app loaded
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, so good!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's build the project by clicking on the **Build** menu and choosing **Make
    Project**, as shown in the following figure (or by simply pressing *Ctrl* + *F9*):![Figure
    9.9 – Building the RL Android app using the Make Project option ](img/B15074_09_09.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.9 – Building the RL Android app using the Make Project option
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This process may take some time to complete and you may see useful status messages
    in the **Build** information tab.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the build process completes, you will see `.apk` file, which can be run
    on an Android device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's go ahead and run the app by using the **Run** menu, as shown in the following
    figure:![Figure 9.11 – The Run menu option to run the RL app in Android Studio
    ](img/B15074_09_011.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.11 – The Run menu option to run the RL app in Android Studio
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, if you have your Android device/phone connected to the machine,
    you can launch that app on your phone. Otherwise, you can use the AVD to emulate
    an Android device.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's choose an AVD device to emulate from the device menu, as shown in the
    following figure:![Figure 9.12 – Choose the AVD to emulate an Android device ](img/B15074_09_012.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.12 – Choose the AVD to emulate an Android device
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are now ready with the device to run the app.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's go ahead and launch/run the app! You can use the **Run 'app'** button
    from the **Run** menu, as shown in the following figure:![Figure 9.13 – Run 'app'
    command to launch the app ](img/B15074_09_013.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.13 – Run 'app' command to launch the app
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That should launch the app on the AVD emulator (or on your phone if you chose
    it).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The app should launch on the Android device and should look something like
    the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.14 – The TFRL-Cookbook RL app running on an Android (emulated) device
    ](img/B15074_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – The TFRL-Cookbook RL app running on an Android (emulated) device
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: That completes our recipe. Head to the next section to learn more about the
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous recipe, we saw how you can export your Deep RL agent''s model
    to the TFLite format. The previous recipe generated two `model.tflite` files:
    one for the Actor and another for the Critic.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can train any agent algorithm of your choice following the recipes previously
    discussed in this book and use the recipe titled *Packaging Deep RL agents for
    mobile and IoT devices using TensorFlow Lite* in this chapter to obtain the Actor
    `model.tflite` file used in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: As you may recall from [*Chapter 3*](B15074_03_ePub_AM.xhtml#_idTextAnchor090),
    *Implementing Advanced Deep RL Algorithms on Deep RL Agents*, the Actor component
    is responsible for generating actions according to the learned policy and the
    Critic component estimates the state or state-action value. When it comes to deploying
    RL agents, we are more interested in the action generated by the agent than the
    predicted state or state-action values. Therefore, we only used the agent's Actor
    model for deployment purposes in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We first included the TFLite dependency by updating the app's `gradle.build`
    file. We then added a method named `loadModelFile` to load the agent's model (`model.tflite`).
    This returns a `MappedByteBuffer` object, which is needed to initialize a TFLite
    interpreter instance. Once the agent's model is loaded and a TFLite interpreter
    instance is created, we can run the interpreter with valid inputs to get the agent's
    actions. In order to make sure the inputs are in a valid format, we converted
    the image data from `BitMap` format to `ByteBuffer` format. We also defined the
    image observation width, height, number of channels, and so on based on the observation
    space of the environment we used to train the RL agent.
  prefs: []
  type: TYPE_NORMAL
- en: The action returned by the agent's model in *Step 7* can be used to actuate/move,
    say, the red paddle in the Table Tennis game and repeat the preceding steps for
    each new observation in a loop to make the agent play against itself or a human!
  prefs: []
  type: TYPE_NORMAL
- en: We then saw how to launch the app using Android Studio and then concluded the
    recipe. Hope you had fun!
  prefs: []
  type: TYPE_NORMAL
- en: Let's march on to the next recipe whenever you are ready.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging Deep RL agents for the web and Node.js using TensorFlow.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JavaScript is the language of choice when it comes to developing web applications
    due to its versatility both as a frontend as well as a backend programming language
    that can be executed by a web browser or using Node.js. The ability to run out
    RL agents on the web will unlock several new pathways for deploying RL agents
    in web apps. This recipe will show how you can train and export RL agent models
    into a format that you can then use in your JavaScript applications that can be
    run directly in the browser or in a Node.js environment. The TensorFlow.js (TF.js)
    library allows us to use JavaScript to run existing models or even train/retrain
    new models. We will use the `tensorflowjs` Python module to export our agent's
    model to a supported format that can be imported into JavaScript-based web or
    desktop (Node.js/Electron) apps. We will explore two approaches to export the
    Agent model to the TF.js layers format.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook''s
    code repo. If the following imports work without issues, you are ready to get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following text, we will save space by focusing on the new and important
    pieces that are unique to this recipe. We will go through the model saving and
    export functionality and the different ways you can do that and keep the Actor,
    Critic, and Agent model definitions out of the following steps to save space.
    Please refer to the book's code repository for a complete implementation, including
    the training and logging methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first set up a command-line argument parser to allow easy customization
    of the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also set up logging so that we can visualize the agent''s learning progress
    using TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the first export approach, we will define `save_h5` methods for the `Actor`,
    `Critic`, and `Agent` classes in the following steps. We will start with the implementation
    of the `save_h5` method in the `Actor` class to export the Actor model to Keras''s
    `h5` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we will implement a `save` method for the `Critic` class to export
    the Critic model to Keras''s `h5` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now add a `save` method for the `Agent` class that will utilize the
    Actor and Critic `save` method to save both the models needed by the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the `save_h5()` method is executed, the `save` method will generate two
    models (one for the Actor and one for the Critic) and save them in the specified
    directory on the filesystem with a directory structure and files like the one
    shown in the following figure:![Figure 9.15 – Directory structure and file contents
    for the DDPG RL agent with the save_h5 model export ](img/B15074_09_015.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.15 – Directory structure and file contents for the DDPG RL agent with
    the save_h5 model export
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the `.h5` files are generated, we can use the `tensorflowjs_converter`
    command-line tool and specify the location of the Actor model''s `save` directory.
    Refer to the following command for an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we can convert the Critic model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Hooray! We now have both the Actor and Critic models in the TF.js layers format.
    We will look at another approach that doesn't need us to (manually) switch to
    the command line to convert the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There''s another approach to export the Agent model to the TF.js layers format.
    We will be implementing it in the following steps, starting with the `save_tfjs`
    method for the `Actor` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we will implement the `save_tfjs` method for the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Agent` class can then call the `save_tfjs` method on the Actor and Critic
    using its own `save_tfjs` method, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the Agent's `save_tfjs` method gets executed, the Actor and Critic models
    in the TF.js layers format will be generated and will have a directory structure
    and file contents like the one shown in the following figure:![Figure 9.16 – Directory
    structure and file contents for the DDPG RL agent with the save_tfjs model export
    ](img/B15074_09_016.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.16 – Directory structure and file contents for the DDPG RL agent with
    the save_tfjs model export
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To sum up, we can finalize the `main` function to instantiate the agent and
    train and save the model in the TF.js layers format directly using the Python
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can now take the TF.js model and deploy it in your web app, Node.js app,
    Electron app, or any other JavaScript/TypeScript-based applications. Let's recap
    some of the key items we used in this recipe in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used a DDPG agent with Actor and Critic networks that expect
    image observations and produce actions in continuous space. You can swap the agent
    code with the DDPG implementations from one of the earlier chapters that use different
    state/observation spaces and action spaces. You could also replace the agent with
    a different agent algorithm altogether. You will find a bonus recipe that exports
    a PPO agent TF.js model in the book's code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed two approaches to save and convert our agent models to TF.js format.
    The first approach allowed us to generate a Keras model in H5 format, which is
    a short form of HDF5, which is an acronym for Hierarchical Data Format version
    5 file format. We then converted it to the TF.js model using the `tensorflowjs_converter`
    command-line tool. While it is lightweight and easy to handle a single file per
    model, the Keras HDF5 model has limitations compared to the SavedModel file format.
    Specifically, the Keras HDF5 models do not contain the computation graphs of custom
    objects/layers and therefore will require the Python class/function definitions
    for these custom objects to reconstruct the model during runtime. Moreover, in
    the cases when we add loss terms and metrics outside the model class definition
    (using `model.add_loss()` or `model.add_metric()`), these are not exported in
    the HDF5 model file.
  prefs: []
  type: TYPE_NORMAL
- en: In the second approach, we used the `tensorflowjs` Python module to directly
    (in memory) convert and save the agent's models in the TF.js layers format.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about TF.js here: [https://www.tensorflow.org/js](https://www.tensorflow.org/js).'
  prefs: []
  type: TYPE_NORMAL
- en: It's time for the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Deep RL agent as a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you train your RL agent to solve a problem or business need, you will want
    to deploy it as a service – more likely than offering the trained agent model
    as a product due to several reasons, including scalability and model-staleness
    limitations. You will want to have a way to update the agent model with new versions
    and you will not want to maintain or offer support for multiple versions or older
    versions of your agent if you sell it as a product. You will need a solid and
    well-tested mechanism to offer your RL agent as an AI service that allows customizable
    runtimes (different frameworks, and CPU/GPU support), easy model upgrades, logging,
    performance monitoring, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To serve all such needs, we will be using NVIDIA's Triton server as the backend
    for serving our agent as a service. Triton serves as a unifying inference framework
    for the deployment of AI models at scale in production. It supports a wide variety
    of deep learning frameworks including TensorFlow2, PyTorch, ONNX, Caffe2, and
    others, including custom frameworks, and offers several other production-quality
    features and optimizations, such as concurrent model execution, dynamic batching,
    logging, and performance and health monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started with our recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook''s
    code repo. You will also need to make sure you have the latest NVIDIA GPU drivers
    installed on your machine that supports the GPU you have. You will also need Docker
    set up on your machine. If you haven''t installed Docker, you can follow the official
    instructions here to set up Docker for your OS: [https://docs.docker.com/get-started/](https://docs.docker.com/get-started/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following text, we will save space by focusing on the service we will
    be building. We will keep the contents of the agent training scripts out of the
    text, but you will find the scripts in the book's code repository under `ch9-cross-platform-deployment`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you will want to train, save, and export the agent that you want to
    host as a service. You can use the sample `agent_trainer_saver.py` script to train
    a PPO agent for one of the tasks in the Webgym suite of environments using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you have identified the suitable container version, say `yy.mm`, you can
    use Docker to pull the NVIDIA Triton server image using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the `yy.mm` placeholder to the version you have identified. For example,
    to pull the container version 20.09, you would run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you run the `agent_trainer_saver` script, the trained models are stored
    in the `trained_models` directory with the following directory structure and contents:![Figure
    9.17 – Directory structure and contents of the exported trained models ](img/B15074_09_017.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.17 – Directory structure and contents of the exported trained models
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `trained_models/actor` directory will be the root directory for our model
    repository store when serving with Triton.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are now ready to serve our agent''s actions as a service! To launch the
    service, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you want to serve the agent model from a machine that does not have a GPU
    (not recommended), you can simply omit the `–gpus=1` flag to instruct the Triton
    server to serve using CPUs only. The command will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you run into issues serving your agent models, check the `trained_models/actor/config.pbtxt`
    file, which describes the model configuration. While Triton can automatically
    generate the `config.pbtxt` file from TensorFlow SavedModels, it may not work
    well for all, especially a custom policy network implementation. If you are using
    the `agent_trainer_saver` script to export a trained PPO agent, you can use the
    following `config.pbtxt`. We will discuss the model config in the next few steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will continue to specify the input (state/observation) space/dimension configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will specify the output (action space):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also specify the instance group, optimization parameters, and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final set of config parameters required for the `config.pbtxt` file is
    listed here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Hooray! Our agent as a service is live. At this point, you can run the same
    commands we discussed above on a cloud/remote server/VPS if you would like to
    offer this service on the public web/a network. Let''s quickly send a query to
    the server to make sure everything went as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the agent model is being served without issues, you will see an output similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also use a full-fledged sample client app to query the agent service
    to get the prescribed action. Let''s quickly set up the tools and libraries we
    need for running a Triton client. You can use Python pip to install the dependencies,
    as shown in the following command snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optionally, to be able to run the performance analyzer (`perf_analyzer`), you
    will need to install the libb64-dev system package using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You now have all the dependencies to run the sample Triton client app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes our recipe! Let's look into some of the details of what we accomplished
    in this recipe in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our recipe had three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Train, save, and export;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first section covered the agent training, saving, and exporting routine.
    In this section, we first picked the RL environment and agent algorithm we wanted
    to train. We then utilized one of the many training strategies we discussed earlier
    in this book to train the agent model. We then used the model saving and export
    methods we discussed in the previous recipes of this chapter to export the trained
    agent model in TensorFlow's SavedModel file format. As you may recall, we followed
    a specific directory structure and file naming convention when we saved and exported
    our agent model. This convention aligns with the model repository conventions
    used by the NVIDIA Triton server and thus allows the models we export to be easily
    served with the production-ready Triton server. Moreover, the organization allows
    us to manage multiple versions of the agent model concurrently easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second section, we saw how we can deploy the exported agent model using
    NVIDIA''s Triton server. You can learn more about NVIDIA''s Triton here: [https://developer.nvidia.com/nvidia-triton-inference-server](https://developer.nvidia.com/nvidia-triton-inference-server).'
  prefs: []
  type: TYPE_NORMAL
- en: We saw how easy it is to serve our agent using a production-grade serving backend.
    We can easily run the Docker container on a remote/cloud server or VPS to deploy
    this service out on the web.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once the service was launched, we saw how a client can avail of the
    service by sending action requests with appropriate input/observation data from
    a test environment.
  prefs: []
  type: TYPE_NORMAL
- en: That's it for this recipe! Let's move on to the final recipe of this chapter
    to wrap things up.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging Deep RL agents for cross-platform deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the grandest success of Deep RL has been in the domain of game playing
    (Atari, Chess, Go, Shogi) and simulated robotics, real-world applications are
    starting to emerge where Deep RL agents show a lot of promise and value. Deploying
    Deep RL agents to a variety of physical form factors such as embedded controllers,
    computers, autonomous cars, drones, and other robots, and so on is expected soon.
    Differences in hardware processors (CPU, GPU, TPU, FPGA, ASIC), operating systems
    (Linux, Windows, OSX, Android), architectures (x86, ARM), and form factors (server,
    desktop, mobile, IoT, embedded systems, and so on) make the deployment process
    challenging. This recipe includes guidelines around how you can leverage the TensorFlow
    2.x framework's ecosystem of libraries, tools, and utilities to package Deep RL
    agent models suitable for deployments to the web, mobile, IoT, embedded systems,
    robots, and desktop platforms.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe provides a complete script to build, train, and package a Deep RL
    agent in multiple formats that can be used to deploy/serve using TensorFlow Serving,
    TensorFlow Hub, TensorFlow.js, TensorFlow Lite, NVIDIA Triton, ONNX, ONNX.js,
    Clipper, and most other serving frameworks built for deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook''s
    code repo. If the following imports work without issues, you are ready to get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following text, we will save space by focusing on the new and important
    pieces that are unique to this recipe. We will focus on the various model saving
    and export functionalities and keep the Actor, Critic, and Agent model definitions
    out of the following steps to save space. Please refer to the book's code repository
    for a complete implementation. We will start implementing the model's save/export
    methods one after the other for the Actor first and then repeat the steps for
    the Critic in the subsequent steps, and finally complete the agent implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it is important to set TensorFlow Keras''s backend to use `float32`
    as the default representation for float values instead of the default `float64`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will begin with the various save/export method implementations for the Actor
    in the following few steps. Let''s implement the `save` method to save and export
    the Actor model to TensorFlow''s `SavedModel` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add the `save_tflite` method to the `Actor` class to save and
    export the Actor model in TFLite format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s implement the `save_h5` method and add it to the `Actor` class
    to save and export the Actor model in HDF5 format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add the `save_tfjs` method to the `Actor` class to save and export
    the Actor model in TF.js format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final variant, we will add the `save_onnx` method to the `Actor` class
    to save and export the Actor model in ONNX format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes the save/export methods for the `Actor` class! In a similar
    way, let''s add the `save` methods to the `Critic` class for completeness. Starting
    with the `save` method, and then the other methods in the later steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next method in the sequence is the `save_tflite` method to save and export
    the Critic model in TFLite format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement the `save_h5` add to the `Critic` class to save and export
    the Critic model in HDF5 format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add the `save_tfjs` method to the `Critic` class to save and
    export the Critic model in TF.js format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final variant is the `save_onnx` method, which saves and exports the Critic
    model in ONNX format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes the save/export method additions to our agent''s `Critic` class.
    We now can add the corresponding `save` methods to the `Agent` class that will
    simply call the corresponding `save` methods on the Actor and Critic objects.
    Let''s complete the implementation in the following two steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The remaining methods on the `PPOAgent` class are straightforward as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our implementation for the `Agent` class! We are now ready to
    run the script to build, train, and export the Deep RL agent model! Let''s implement
    the `main` function and call all the `save` methods that we have implemented in
    the previous steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s time to execute our script! Please pull the latest copy of the recipe
    from the book''s code repository and just run it! By default, the script will
    train the agent for one episode, save the agent models, and export the model in
    various formats ready for deployment. Once the script finishes, you will see the
    exported models with the directory structure and contents similar to the one shown
    in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.18 – PPO Deep RL agent model exported to various formats for deployment
    ](img/B15074_09_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – PPO Deep RL agent model exported to various formats for deployment
  prefs: []
  type: TYPE_NORMAL
- en: That completes our final recipe for this chapter! Let's quickly revisit some
    of the key items we covered in this recipe in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first set TensorFlow Keras's backend to use `float32` as the default representation
    for float values. This is because, otherwise, TensorFlow would use the default
    `float64` representation, which is not supported by TensorFlow Lite (for performance
    reasons) as it is targeted towards running on embedded and mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used a PPO agent with Actor and Critic networks that expect
    image observations and produce actions in discrete space, designed for RL environments
    such as the procedurally generated procgen environment from OpenAI. You can swap
    the agent code with the PPO implementations from one of the earlier chapters that
    use different state/observation spaces and action spaces depending on your need/application.
    You could also replace the agent with a different agent algorithm altogether.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed several approaches to save and export your agent models, leveraging
    the whole suite of tools and libraries offered by the TensorFlow 2.x ecosystem.
    A summary of the various export options that we implemented as part of this recipe
    is provided in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Summary of various RL agent model export options discussed
    in this recipe ](img/B15074_09_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Summary of various RL agent model export options discussed in
    this recipe
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this recipe, the chapter, and – more dramatically – the book!
    We covered a lot of different topics in this cookbook to leverage the TensorFlow
    2.x framework and the ecosystem of tools and libraries built around it to build
    RL building blocks, environments, algorithms, agents, and applications. I hope
    you had an exciting journey with the book.
  prefs: []
  type: TYPE_NORMAL
- en: I can't wait to see what you build/cook with the recipes we discussed in the
    book. I will look forward to hearing about your journey with the book on the discussion
    page at [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/discussions](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/discussions).
  prefs: []
  type: TYPE_NORMAL
- en: Looking forward to getting in touch with you on the discussion/issues page.
    All the best for your future endeavors!
  prefs: []
  type: TYPE_NORMAL
