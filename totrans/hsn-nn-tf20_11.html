<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Semantic Segmentation and Custom Dataset Builder</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll analyze semantic segmentation and the challenges that come with it. Semantic segmentation is the challenging problem of classifying every single pixel of an image with the correct semantic label. The first part of this chapter presents the problem itself, why it is important, and what are the possible applications. At the end of the first part, we will discuss the well-known U-Net architecture for semantic segmentation, and we will implement it as a Keras model in pure TensorFlow 2.0 style. The model implementation is preceded by the introduction of the deconvolution operation required to implement semantic segmentation networks successfully.</p>
<p>The second part of this chapter starts with dataset creation—since, at the time of writing, there is no <kbd>tfds</kbd> builder for semantic segmentation, we take advantage of this to introduce the TensorFlow Datasets architecture and show how to implement a custom DatasetBuilder. After getting the data, we'll perform the training process of U-Net step by step, showing how straightforward it is to train this model using Keras and Keras callbacks. This chapter ends with the usual exercise section, perhaps the most crucial part of this whole chapter. The only way to understand a concept is to get your hands dirty.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Semantic segmentation</li>
<li>Create a TensorFlow DatasetBuilder</li>
<li>Model training and evaluation </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semantic segmentation</h1>
                </header>
            
            <article>
                
<p><span>Different from object detection, where the goal is to detect objects in rectangular regions, and image classification, which has the purpose of classifying the whole image with a single label, semantic segmentation is a challenging computer vision task, the goal of which is to assign the correct label to every pixel of the input image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-911 image-border" src="assets/8f1d9dcf-dd97-40ee-af55-939fb21f97ad.png" style="width:98.75em;height:30.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Examples of semantically annotated images from the CityScapes dataset. Every single pixel of the input image has a corresponding pixel-label. (Source: <a href="https://www.cityscapes-dataset.com/examples/">https://www.cityscapes-dataset.com/examples/</a>)</div>
<p><span>The applications of semantic segmentation are countless, but perhaps the most important ones are in the autonomous driving and medical imaging domains.</span></p>
<p>Automated guided vehicles and self-driving cars can take advantage of semantic segmentation results, getting a complete understanding of the whole scene captured by the cameras mounted on the vehicle. For example, having the pixel-level information of the road can help the driving software have better control of the position of the car. Localizing the road using a bounding box is far less accurate than having a pixel-level classification that localizes the road pixels independently from the perspective.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the medical imaging domain, the bounding boxes predicted by an object detector are sometimes useful and other times not. In fact, if the task is the detection of a specific type of cell, a bounding box can give the user enough information. But if, instead, the task is to localize blood vessels, then using a bounding box is not enough. As it is easy to imagine, a fine-grained classification is not an easy task, and there are several challenges to face from both the theoretical and practical points of view.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the tough challenges is to get the correct data. There are several enormous datasets of labeled images since the process of classifying an image by its main content is relatively fast. A team of professional annotators can easily label thousands of images a day since the task only consists of looking at the picture and selecting a label.</p>
<p>There are also a lot of object detection datasets, where multiple objects have been localized and classified. The process requires more annotation time with respect to the classification alone, but as it is something that does not require extreme accuracy, it is a relatively fast process.</p>
<p>The semantic segmentation dataset, instead, requires specialized software and very patient annotators that are extremely accurate in their work. In fact, the process of labeling with pixel-level accuracy is perhaps the most time-consuming process of all of the annotation types. For this reason, the number of semantic segmentation datasets is low, and their number of images is limited. As we will see in the next section, dedicated to dataset creation, PASCAL VOC 2007, which contains<span> 24,640 annotated objects for the image classification and localization task, only contains approximately 600 labeled images.</span></p>
<p>Another challenge that semantic segmentation brings is technical. Classifying every single pixel of an image requires designing convolutional architectures in a different way with respect to the ones seen so far. All of the architectures described so far followed the same structure:</p>
<ul>
<li>One input layer, which defines the input resolution expected by the network</li>
<li>The feature extractor part, which is a stack of several convolution operations with different strides or with pooling operations in between, which, layer-by-layer, reduce the spatial extent of the feature maps until it is reduced to a vector</li>
<li>The classification part which, given the feature vector produced by the feature extractor, is trained to classify this low-dimensional representation to a fixed number of classes</li>
<li>Optionally, a regression head, which uses the same features to produce a set of four coordinates</li>
</ul>
<p>The task of semantic segmentation, however, cannot follow this structure since, if the feature extractor only reduces the input resolution layer-by-layer, how can the network produce a classification for every pixel in the input image?</p>
<p>One of the proposed solutions is the deconvolution operation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deconvolution – transposed convolution</h1>
                </header>
            
            <article>
                
<p>Let's start this section by saying that the term deconvolution is misleading. In fact, in mathematics and engineering, the deconvolution operation exists but has very little in common with what deep learning practitioners intend with this term.</p>
<p>In this domain, a deconvolution operation is a transposed convolution operation, or even an image resize, followed by a standard convolution operation. Yes, two different implementations are named in the same way.</p>
<p>The deconvolution operation in deep learning just guarantees that, if a feature map is a result of a convolution between an input map and a kernel with a certain size and stride, the deconvolution operation will produce a feature map with the same spatial extent of the input, if applied with the same kernel size and stride.</p>
<p>To do that, a standard convolution is performed with a pre-processed input where a zero padding is added not only at the borders but also within the feature map cells. The following diagram should help to clarify the process:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-912 image-border" src="assets/b009c6eb-460e-4357-97de-2b631cbba889.png" style="width:41.83em;height:19.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Image and caption source: A guide to convolution arithmetic for deep learning—Vincent Dumoulin and Francesco Visin</div>
<p>TensorFlow, through the <kbd>tf.keras.layers</kbd> package, offers a ready-to-use deconvolution operation: <kbd>tf.keras.layers.Conv2DTranspose</kbd>.</p>
<p>Another possible way of performing the deconvolution is to resize the input to the desired resolution and make this operation learnable by adding a standard 2D convolution with same <span>padding </span>on top of the resized image.</p>
<p>In short, what really matters in the deep learning context is creating a learnable layer that reconstructs the original spatial resolution and performs a convolution. This is not the mathematical inverse of the convolution operation, but the practice has shown that it is enough to achieve good results.</p>
<p>One of the semantic segmentation architectures that used the deconvolution operation extensively and achieved impressive results in the task of the segmentation of medical images is the U-Net architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The U-Net architecture</h1>
                </header>
            
            <article>
                
<p>U-Net is a convolutional architecture for semantic segmentation introduced by Olaf Ronnerberg et al. in <em>Convolutional Networks for Biomedical Image Segmentation</em> with the explicit goal of segmenting biomedical images.</p>
<p>The architecture revealed itself to be general enough to be applied in every semantic segmentation task since it has been designed without any constraints about the datatypes.</p>
<p>The U-Net architecture follows the typical encoder-decoder architectural pattern with skip connections. This way of designing the architecture has proven to be very effective when the goal is to produce an output with the same spatial resolution of the input since it allows the gradients to propagate between the output and the input layer in a better way:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-913 image-border" src="assets/6befbb85-9184-4b00-9814-eece8624ae3b.png" style="width:48.33em;height:32.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><span>The U-Net architecture. The blue boxes are the feature maps produced by the blocks, denoted with their shapes. The white boxes are copied and cropped feature maps. Different arrows indicate different operations. Source: Convolutional Networks for Biomedical Image Segmentation—Olaf Ronnerberg et al.</span></span></div>
<p>The left side of the U-Net architecture is an encoder that, layer-by-layer, reduces the input size from 572 x 572 to 32 x 32 in the lowest resolution. The right side contains the encoder part of the architecture, which mixes information extracted from the encoding part to the information learned by the up-convolution (deconvolution) operations.</p>
<p>The original U-Net architecture does not produce an output with the same resolution of the input, but it has been designed to produce a slightly lower resolution output. A final 1 x 1 convolution is used as the final layer to map each feature vector (with a depth of 64) to the desired number of classes. For a complete assessment of the original architecture, carefully read the original U-Net paper <span>by Olaf Ronnerberg et al. in <em>Convolutional Networks for Biomedical Image Segmentation</em>.</span></p>
<p>Instead of implementing the original U-net architecture, we are going to show how to implement a slightly modified U-Net that produces an output with the same resolution of the input and that follows the same original block organization.</p>
<p>As can be seen from the screenshot of the architecture, there are two main blocks:</p>
<ul>
<li><strong>Encoding blocks</strong>: There are three convolutions followed by a downsampling operation.</li>
<li><strong>Decoding blocks</strong>: This is a deconvolution operation, followed by the concatenation of its output with the corresponding input feature, and two convolution operations.</li>
</ul>
<p>It is possible and really easy to use the Keras functional API to define this model and connect these logical blocks. The architecture we are going to implement differs a little from the original one, since this is a custom U-Net variation, and it shows how Keras allows the use of models as layers (or building blocks). </p>
<p>The <kbd>upsample</kbd> and <kbd>downsample</kbd> functions are implemented as a <kbd>Sequential</kbd> model, which is nothing but a convolution or deconvolution operation, with a stride of <kbd>2</kbd>, followed by an activation function:</p>
<p><kbd>(tf2)</kbd></p>
<pre>import tensorflow as tf<br/>import math<br/><br/><br/>def downsample(depth):<br/>    return tf.keras.Sequential(<br/>        [<br/>            tf.keras.layers.Conv2D(<br/>                depth, 3, strides=2, padding="same", kernel_initializer="he_normal"<br/>            ),<br/>            tf.keras.layers.LeakyReLU(),<br/>        ]<br/>    )<br/><br/><br/>def upsample(depth):<br/>    return tf.keras.Sequential(<br/>        [<br/>            tf.keras.layers.Conv2DTranspose(<br/>                depth, 3, strides=2, padding="same", kernel_initializer="he_normal"<br/>            ),<br/>            tf.keras.layers.ReLU(),<br/>        ]<br/>    )</pre>
<p>The model definition function supposes a minimum input resolution of 256 x 256, and it implements the encoding, decoding, and concatenate (skip connection) blocks of the architecture:</p>
<p><kbd>(tf2)</kbd></p>
<pre>def get_unet(input_size=(256, 256, 3), num_classes=21):<br/>    # Downsample from 256x256 to 4x4, while adding depth<br/>    # using powers of 2, startin from 2**5. Cap to 512.<br/>    encoders = []<br/>    for i in range(2, int(math.log2(256))):<br/>        depth = 2 ** (i + 5)<br/>        if depth &gt; 512:<br/>            depth = 512<br/>        encoders.append(downsample(depth=depth))<br/><br/>    # Upsample from 4x4 to 256x256, reducing the depth<br/>    decoders = []<br/>    for i in reversed(range(2, int(math.log2(256)))):<br/>        depth = 2 ** (i + 5)<br/>        if depth &lt; 32:<br/>            depth = 32<br/>        if depth &gt; 512:<br/>            depth = 512<br/>        decoders.append(upsample(depth=depth))<br/><br/>    # Build the model by invoking the encoder layers with the correct input<br/>    inputs = tf.keras.layers.Input(input_size)<br/>    concat = tf.keras.layers.Concatenate()<br/><br/>    x = inputs<br/>    # Encoder: downsample loop<br/>    skips = []<br/>    for conv in encoders:<br/>        x = conv(x)<br/>        skips.append(x)<br/><br/>    skips = reversed(skips[:-1])<br/><br/>    # Decoder: input + skip connection<br/>    for deconv, skip in zip(decoders, skips):<br/>        x = deconv(x)<br/>        x = tf.keras.layers.Concatenate()([x, skip])<br/><br/>    # Add the last layer on top and define the model<br/>    last = tf.keras.layers.Conv2DTranspose(<br/>        num_classes, 3, strides=2, padding="same", kernel_initializer="he_normal")<br/><br/>    outputs = last(x)<br/>    return tf.keras.Model(inputs=inputs, outputs=outputs)</pre>
<p>Using Keras, it is possible to visualize not only the tabular summary of the model (by using the <kbd>summary()</kbd> method of a Keras model), but also to get a graphical representation of the created model, which is often a blessing when designing complex architectures:</p>
<p><kbd>(tf2)</kbd></p>
<pre>from tensorflow.keras.utils import plot_model<br/>model = get_unet()<br/>plot_model(model, to_file="unet.png")</pre>
<p class="mce-root"/>
<p>These three lines of code, generate this great graphical representation:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-914 image-border" src="assets/0aa2463f-1c2b-4ae3-9734-7b552846b844.png" style="width:22.75em;height:51.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Graphical representation of the U-Net-like structure defined. Keras allows this kind of visualization to help the architecture design process.</div>
<p>The generated image looks like the horizontally flipped version of the U-net architecture, and this is the architecture we are going to use to tackle the semantic segmentation problem in this chapter.</p>
<p>Now that we have understood the problem and defined a deep architecture, we can move forward and gather the required data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create a TensorFlow DatasetBuilder</h1>
                </header>
            
            <article>
                
<p>In the same way as any other machine learning problem, the first step is getting the data. Since semantic segmentation is a supervised learning task, we need a classification dataset of images and corresponding labels. The peculiarity is that the label in itself is an image.</p>
<p>At the time of writing, there is no semantic dataset ready to use in TensorFlow Datasets. For this reason, we use this section not only to create <kbd>tf.data.Dataset</kbd> with the data that we need, but also to have a look at the process required to develop a <kbd>tfds</kbd> DatasetBuilder.</p>
<p>Since, in the previous section dedicated to the object detection, we used the PASCAL VOC 2007 dataset, we are going to reuse the downloaded files to create the semantic segmentation version of the PASCAL VOC 2007 dataset. The following screenshot shows how the dataset is provided. Each picture has a corresponding label, where the pixel color identifies a different class:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-915 image-border" src="assets/ee25a449-6ddc-4c91-a7b9-5461e524bcb5.png" style="width:18.42em;height:20.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">A pair (image, label) sampled from the dataset. The top image is the original image, while the bottom image contains the semantic segmentation class of the known objects. Every not know class is marked as background (color black), while the objects are delimited using the color white.</div>
<p class="mce-root">The dataset previously downloaded comes not only with the annotated bounding boxes but also with the semantic segmentation annotation for many images. TensorFlow Datasets downloaded <span>the raw data </span>in the default directory (<kbd>~/tensorflow_datasets/downloads/</kbd>) and <span>placed the extracted archive </span>in the <kbd>extracted</kbd> subfolder. We can, therefore, re-use the downloaded data to create a new dataset for semantic segmentation.</p>
<p>Before doing it, it is worth looking at the TensorFlow dataset organization to understand what we need to do to achieve our goal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hierarchical organization</h1>
                </header>
            
            <article>
                
<p>The whole TensorFlow Datasets API has been designed to be as extensible as possible. To do that, the architecture of TensorFlow Datasets is organized in several abstraction layers that transform the raw dataset data to the <kbd>tf.data.Dataset</kbd> object. The following diagram, from the TensorFlow Dataset GitHub page (<a href="https://github.com/tensorflow/datasets/">https://github.com/tensorflow/datasets/</a>), shows the logical organization of the project:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-916 image-border" src="assets/21ca6bd5-559f-4e4a-b421-e134598022f6.png" style="width:36.33em;height:25.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The logical organization of the TensorFlow Datasets project. The raw data flows from several abstraction layers that apply transformation and standardizations, in order to define the TFRecord structure and obtain a tf.data.Dataset object at the end.</div>
<p>Usually, the <kbd>FeatureConnector</kbd> and <kbd>FileFormatAdapter</kbd> classes are ready to use, while the <kbd>DatasetBuilder</kbd> class must be correctly implemented since it is the data-specific part of the pipeline.</p>
<p>Each dataset creation pipeline starts from a subclass of a <kbd>DatasetBuilder</kbd> object that must implement the following methods:</p>
<ul>
<li><kbd>_info</kbd> is used to build the <kbd>DatasetInfo</kbd> object that describes the dataset (and produces the human-readable representation that is extremely useful to have a complete understanding of the data).</li>
<li><kbd>_download_and_prepare</kbd> is used to download the data from a remote location (if any) and do some basic preprocessing (such as extracting the compressed archives). Moreover, it creates the serialized (TFRecord) representation.</li>
<li><kbd>_as_dataset</kbd>: This is the final step, to produce a <kbd>tf.data.Dataset</kbd> object from the serialized data.</li>
</ul>
<p>Subclassing directly, the <kbd>DatasetBuilder</kbd> class is often not needed since <kbd>GeneratorBasedBuilder</kbd> is a ready-to-use subclass of <kbd>DatasetBuilder</kbd> that simplifies the dataset definition. The methods to implement by subclassing it are as follows:</p>
<ul>
<li><kbd>_info</kbd> is the same method of <kbd>DatasetBuilder</kbd> (see the <kbd>_info</kbd> method description of the previous bullet list).</li>
<li><kbd>_split_generators</kbd> is used to download the raw data and do some basic preprocessing but without the need to worry about TFRecord creation.</li>
<li><kbd>_generate_examples</kbd> is used to create a Python iterator. This method yields examples in the dataset from the raw data, where every example will be automatically serialized as a row in a TFRecord.</li>
</ul>
<p>Therefore, by subclassing <kbd>GeneratorBasedBuilder</kbd>, there are only three simple methods to implement, and we can hence start implementing them.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The dataset class and DatasetInfo</h1>
                </header>
            
            <article>
                
<p>Subclassing a model and implementing the required methods is straightforward. The first step is to define the skeleton of our class and then start by implementing the methods in the order of complexity. Moreover, since our goal is to create a dataset for semantic segmentation that uses the same downloaded files of the PASCAL VOC 2007 dataset, we can override the methods of the <kbd>tfds.image.Voc2007</kbd> DatasetBuilder to reuse all of the information already present in the parent class:</p>
<p><kbd>(tf2)</kbd></p>
<pre><span>import tensorflow as tf <br/>import tensorflow_datasets as tfds <br/>import os<br/><br/>class Voc2007Semantic(tfds.image.Voc2007): <br/>    """Pasval VOC 2007 - semantic segmentation.""" <br/> <br/>    VERSION = tfds.core.Version("0.1.0") <br/> <br/>    def _info(self): <br/>        # Specifies the tfds.core.DatasetInfo object <br/>        pass  # TODO <br/> <br/>    def _split_generators(self, dl_manager): <br/>        # Downloads the data and defines the splits <br/>        # dl_manager is a tfds.download.DownloadManager that can be used to <br/>        # download and extract URLs <br/>        pass  # TODO <br/> <br/>    def _generate_examples(self): <br/>        # Yields examples from the dataset <br/>        pass  # TODO</span></pre>
<p>The most straightforward, but perhaps the most important method, to implement is <kbd>_info</kbd>, which contains all of the dataset information and the definition of the structure of a single example.</p>
<p>Since we are extending the <kbd>tfds.image.Voc2007</kbd> dataset, it is possible to reuse certain common information. The only thing to note is that the semantic segmentation requires a label, which is a single-channel image (and not a color image as we are used to seeing).</p>
<p class="mce-root"/>
<p>Implementing the <kbd>_info</kbd> method is hence straightforward:</p>
<p><kbd>(tf2)</kbd></p>
<pre>    def _info(self):<br/>        parent_info = tfds.image.Voc2007().info<br/>        return tfds.core.DatasetInfo(<br/>            builder=self,<br/>            description=parent_info.description,<br/>            features=tfds.features.FeaturesDict(<br/>                {<br/>                    "image": tfds.features.Image(shape=(None, None, 3)),<br/>                    "image/filename": tfds.features.Text(),<br/>                    "label": tfds.features.Image(shape=(None, None, 1)),<br/>                }<br/>            ),<br/>            urls=parent_info.urls,<br/>            citation=parent_info.citation,<br/>        )</pre>
<p>It is worth noting that TensorFlow Datasets already comes with a predefined set of feature connectors that have been used to define <kbd>FeatureDict</kbd>. For example, the correct way of defining an image feature with a fixed depth (4 or 1) and an unknown height and width is to use <kbd>tfds.features.Image(shape=(None, None, depth))</kbd>.</p>
<p>The <kbd>description</kbd>, <kbd>urls</kbd>, and <kbd>citation</kbd> fields have been kept from the parent, although this is not fully correct since the description and citation fields of the parent are about the object detection and classification challenges.</p>
<p>The second method to implement is <kbd>_split_generators</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the dataset splits</h1>
                </header>
            
            <article>
                
<p>The <kbd>_split_generators</kbd> <span>method </span>is used <span>to download the raw data and do some basic preprocessing </span>without<span> needing to worry about TFRecord creation.</span></p>
<p>Since we are inheriting from <kbd><span>tfds.image.Voc2007</span></kbd>, there is no need to reimplement it, but it is, instead, required to have a look at the parent source code:</p>
<p><kbd>(tf2)</kbd></p>
<pre>  def _split_generators(self, dl_manager):<br/>    trainval_path = dl_manager.download_and_extract(<br/>        os.path.join(_VOC2007_DATA_URL, "VOCtrainval_06-Nov-2007.tar"))<br/>    test_path = dl_manager.download_and_extract(<br/>        os.path.join(_VOC2007_DATA_URL, "VOCtest_06-Nov-2007.tar"))<br/>    return [<br/>        tfds.core.SplitGenerator(<br/>            name=tfds.Split.TEST,<br/>            num_shards=1,<br/>            gen_kwargs=dict(data_path=test_path, set_name="test")),<br/>        tfds.core.SplitGenerator(<br/>            name=tfds.Split.TRAIN,<br/>            num_shards=1,<br/>            gen_kwargs=dict(data_path=trainval_path, set_name="train")),<br/>        tfds.core.SplitGenerator(<br/>            name=tfds.Split.VALIDATION,<br/>            num_shards=1,<br/>            gen_kwargs=dict(data_path=trainval_path, set_name="val")),<br/>    ]</pre>
<div class="packt_figref packt_infobox">The source code is from <a href="https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/voc.py">https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/voc.py</a>, released under the Apache License, 2.0.</div>
<p>As it can be easily seen, the method uses a <kbd>dl_manager</kbd> <span>object </span>to download (and cache) and extract the archive from some remote location. The dataset split definition in <kbd>"train"</kbd>, <kbd>"test"</kbd>, and <kbd>"val"</kbd> is performed in the return line.</p>
<p>The most important part of every <kbd>tfds.core.SplitGeneratro</kbd> call is the <kbd>gen_kwargs</kbd> parameter. In fact, at this line, we are instructing how the <kbd>_generate_exaples</kbd> <span>function </span>is going to be called.</p>
<p>In short, this function creates three splits, by calling the <kbd>_generate_examples</kbd> <span>function,</span> passing the <kbd>data_path</kbd> <span>parameters </span>set to the current dataset path (<kbd>test_path</kbd> or <kbd>trainval_path</kbd>), and setting <kbd>set_name</kbd> to the correct dataset name.</p>
<p>The <kbd>set_name</kbd> parameter value comes from the PASCAL VOC 2007 directory and file organization. As we will see in the next section, where the <kbd>_generate_example</kbd> method is implemented, knowing the dataset structure and content is needed to create the splits correctly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating the example</h1>
                </header>
            
            <article>
                
<p>The <kbd>_generate_example</kbd> method can be defined with any signature. This method is called only by the <kbd>_split_generators</kbd> method, and therefore, it is up to this method to correctly invoke <kbd>_generate_example</kbd> with the correct parameters.</p>
<p class="mce-root"/>
<p>Since we haven't overwritten the parent <kbd>_split_generators</kbd> method, we have to use the same signature required by the parent. Hence, we have the <kbd>data_path</kbd> and <kbd>set_name</kbd> parameters to use, in addition to all of the other information that is available in the PASCAL VOC 2007 documentation.</p>
<p>To goal of <kbd>_generate_examples</kbd> is to yield an example every time it is invoked (behaving like a standard Python iterator).</p>
<p>From the dataset structure, we know that, inside <kbd>VOCdevkit/VOC2007/ImageSets/Segmentation/</kbd>, there are three text files—one for each split: <kbd>"train"</kbd>, <kbd>"test"</kbd>, and <kbd>"val"</kbd>. Every file contains the name of the labeled image for every split.</p>
<p>Therefore, it is straightforward to use the information contained in these files to create the three splits. We only have to open the file and read it line-by-line to know which are the images to read.</p>
<p>TensorFlow Datasets constrains us from using the Python file operations, but it explicitly requires the usage of the <kbd>tf.io.gfile</kbd> <span>package.</span> This constraint is necessary, since there are datasets that are too huge to be processed on a single machine, and <kbd>tf.io.gfile</kbd> can be easily used by TensorFlow Datasets to read and process remote and distributed datasets.</p>
<p>From the PASCAL VOC 2007 documentation, we can also extract a <strong>Look-Up Table</strong> (<strong>LUT</strong>) to create a mapping between the RGB values and the scalar labels:</p>
<p><kbd>(tf2)</kbd></p>
<pre>LUT = {<br/>    (0, 0, 0): 0, # background<br/>    (128, 0, 0): 1, # aeroplane<br/>    (0, 128, 0): 2, # bicycle<br/>    (128, 128, 0): 3, # bird<br/>    (0, 0, 128): 4, # boat<br/>    (128, 0, 128): 5, # bottle<br/>    (0, 128, 128): 6, # bus<br/>    (128, 128, 128): 7, # car<br/>    (64, 0, 0): 8, # cat<br/>    (192, 0, 0): 9, # chair<br/>    (64, 128, 0): 10, # cow<br/>    (192, 128, 0): 11, # diningtable<br/>    (64, 0, 128): 12, # dog<br/>    (192, 0, 128): 13, # horse<br/>    (64, 128, 128): 14, # motorbike<br/>    (192, 128, 128): 15, # person<br/>    (0, 64, 0): 16, # pottedplant<br/>    (128, 64, 0): 17, # sheep<br/>    (0, 192, 0): 18, # sofa<br/>    (128, 192, 0): 19, # train<br/>    (0, 64, 128): 20, # tvmonitor<br/>    (255, 255, 255): 21, # undefined / don't care<br/>}</pre>
<p>After creating this look-up table, we can use only TensorFlow operations to read the images, check for their existence (because there is no guarantee that the raw data is perfect and we have to prevent failures during the dataset creation), and create the single-channel image that contains the numerical value associated with the RGB color.</p>
<p>Read the source code carefully since it may be hard to understand the first read. In particular, the loop over the look-up table where we look for correspondences between the RGB colors and the colors available may be not easy to understand at first glance. The following code not only creates the single-channel image with the numerical values associated with the RGB colors, using <kbd>tf.Variable</kbd>, but also checks whether the RGB values are correct:</p>
<p><kbd>(tf2)</kbd></p>
<pre>    def _generate_examples(self, data_path, set_name):<br/>        set_filepath = os.path.join(<br/>            data_path,<br/>            "VOCdevkit/VOC2007/ImageSets/Segmentation/{}.txt".format(set_name),<br/>        )<br/>        with tf.io.gfile.GFile(set_filepath, "r") as f:<br/>            for line in f:<br/>                image_id = line.strip()<br/><br/>                image_filepath = os.path.join(<br/>                    data_path, "VOCdevkit", "VOC2007", "JPEGImages", f"{image_id}.jpg"<br/>                )<br/>                label_filepath = os.path.join(<br/>                    data_path,<br/>                    "VOCdevkit",<br/>                    "VOC2007",<br/>                    "SegmentationClass",<br/>                    f"{image_id}.png",<br/>                )<br/><br/>                if not tf.io.gfile.exists(label_filepath):<br/>                    continue<br/><br/>                label_rgb = tf.image.decode_image(<br/>                    tf.io.read_file(label_filepath), channels=3<br/>                )<br/><br/>                label = tf.Variable(<br/>                    tf.expand_dims(<br/>                        tf.zeros(shape=tf.shape(label_rgb)[:-1], dtype=tf.uint8), -1<br/>                    )<br/>                )<br/><br/>                for color, label_id in LUT.items():<br/>                    match = tf.reduce_all(tf.equal(label_rgb, color), axis=[2])<br/>                    labeled = tf.expand_dims(tf.cast(match, tf.uint8), axis=-1)<br/>                    label.assign_add(labeled * label_id)<br/><br/>                colored = tf.not_equal(tf.reduce_sum(label), tf.constant(0, tf.uint8))<br/>                # Certain labels have wrong RGB values<br/>                if not colored.numpy():<br/>                    tf.print("error parsing: ", label_filepath)<br/>                    continue<br/>                <br/>                yield image_id, {<br/>                    # Declaring in _info "image" as a tfds.feature.Image<br/>                    # we can use both an image or a string. If a string is detected<br/>                    # it is supposed to be the image path and tfds take care of the<br/>                    # reading process.<br/>                    "image": image_filepath,<br/>                    "image/filename": f"{image_id}.jpg",<br/>                    "label": label.numpy(),<br/>                }</pre>
<p>The <kbd>_generate_examples</kbd> method not only yields single examples, but it must yield a pair, <kbd>(id, example)</kbd>, where <kbd>id</kbd>—in this case, <kbd>image_id</kbd>—should uniquely identify the record; this field is used to shuffle the dataset globally and to avoid having repeated elements in the generated dataset.</p>
<p>Having implemented this last method, everything is correctly set up, and we can use the brand-new Voc2007Semantic loader.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use the builder</h1>
                </header>
            
            <article>
                
<p>TensorFlow Datasets can automatically detect whether a class visible in the current scope is a <kbd>DatasetBuilder</kbd> object. Therefore, having implemented the class by subclassing an existing <kbd>DatasetBuilder</kbd>, the <kbd>"voc2007_semantic"</kbd> builder is already ready to use:</p>
<pre>dataset, info = tfds.load("voc2007_semantic", with_info=True)</pre>
<p>At the first execution, the splits are created and the <kbd>_generate_examples</kbd> method is classed three times to create the TFRecord representation of the examples.</p>
<p>By inspecting the <kbd>info</kbd> variable, we can see some dataset statistics:</p>
<pre>[...]<br/>    features=FeaturesDict({<br/><span>        'image': Image(shape=(None, None, 3), dtype=tf.uint8), <br/>        'image/filename': Text(shape=(), dtype=tf.string, encoder=None), <br/>        'label': Image(shape=(None, None, 1), dtype=tf.uint8) <br/>    }, <br/>    total_num_examples=625, <br/>    splits={ <br/>        'test': &lt;tfds.core.SplitInfo num_examples=207&gt;, <br/>        'train': &lt;tfds.core.SplitInfo num_examples=207&gt;, <br/>        'validation': &lt;tfds.core.SplitInfo num_examples=211&gt; <br/>    }<br/></span></pre>
<p>The features are described by implementing the <kbd>_info</kbd> method, and the dataset size is relatively small, containing only 207 images each for train and test split and 211 for the validation split.</p>
<p>Implementing <kbd>DatasetBuilder</kbd> is a relatively straightforward operation that you are invited to do every time you start working with a new dataset—in this way, a high-efficiency pipeline can be used during the training and evaluation processes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training and evaluation</h1>
                </header>
            
            <article>
                
<p>Although the network architecture is not that of an image classifier and the labels are not scalars, semantic segmentation can be seen as a traditional classification problem and therefore the training and evaluation processes can be the same.</p>
<p>For this reason, instead of writing a custom training loop, we can use the <kbd>compile</kbd> and <kbd>fit</kbd> <span>Keras models </span>to build the training loop and execute it respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>To use the Keras <kbd>fit</kbd> model, the <kbd>tf.data.Dataset</kbd> object should generate tuples in the <kbd>(feature, label)</kbd> <span>format, </span>where <kbd>feature</kbd> is the input image and <kbd>label</kbd> is the image label.</p>
<p>Therefore, it is worth defining some functions that can be applied to the elements produced by <kbd>tf.data.Dataset</kbd>, which transforms the data from a dictionary to a tuple, and, at the same time, we can apply some useful preprocessing for the training process:</p>
<p><kbd>(tf2)</kbd></p>
<pre>def resize_and_scale(row):<br/>    # Resize and convert to float, [0,1] range<br/>    row["image"] = tf.image.convert_image_dtype(<br/>        tf.image.resize(<br/>            row["image"],<br/>            (256,256),<br/>            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),<br/>        tf.float32)<br/>    # Resize, cast to int64 since it is a supported label type<br/>    row["label"] = tf.cast(<br/>        tf.image.resize(<br/>            row["label"],<br/>            (256,256),<br/>            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),<br/>        tf.int64)<br/>    return row<br/>  <br/>def to_pair(row):<br/>    return row["image"], row["label"]</pre>
<p>It is now easy to get the validation and training sets from the <kbd>dataset</kbd> object obtained from the <kbd>tfds.load</kbd> call and apply to them the required transformations:</p>
<p><kbd>(tf2)</kbd></p>
<pre>batch_size= 32<br/><br/>train_set = dataset["train"].map(resize_and_scale).map(to_pair)<br/>train_set = train_set.batch(batch_size).prefetch(1)<br/><br/>validation_set = dataset["validation"].map(resize_and_scale)<br/>validation_set = validation_set.map(to_pair).batch(batch_size)</pre>
<p>The datasets are ready to be used in the <kbd>fit</kbd> method, and since we are developing a pure Keras solution, we can configure the hidden training loop using the Keras callbacks.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training loop and Keras callbacks</h1>
                </header>
            
            <article>
                
<p>The <kbd>compile</kbd> method is used to configure the training loop. We can specify the optimizer, the loss, the metrics to measure, and some useful callbacks.</p>
<p>Callbacks are functions that are executed at the end of every training epoch. Keras comes with a long list of predefined callbacks that are ready to use. In the next code snippet, two of the most common are going to be used, the <kbd>ModelCheckpoint</kbd> and <kbd>TensorBoard</kbd> callbacks. As it can be easily guessed, the former saves a checkpoint at the end of the epoch, while the latter logs the metrics using <kbd>tf.summary</kbd>.</p>
<p>Since semantic segmentation can be treated as a classification problem, the loss used is <kbd>SparseCategoricalCrossentropy</kbd>, configured to apply the sigmoid to the output layer of the network when computing the loss value (in the depth dimension), as stated by the <kbd>from_logits=True</kbd> parameter. This configuration is required since we haven't added an activation function to the last layer of the custom U-Net:</p>
<p><kbd>(tf2)</kbd></p>
<pre># Define the model<br/>model = get_unet()<br/><br/># Choose the optimizer<br/>optimizer = tf.optimizers.Adam()<br/><br/># Configure and create the checkpoint callback<br/>checkpoint_path = "ckpt/pb.ckpt"<br/>cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,<br/>                                                 save_weights_only=True,<br/>                                                 verbose=1)<br/># Enable TensorBoard loggging<br/>TensorBoard = tf.keras.callbacks.TensorBoard(write_images=True)<br/><br/># Cofigure the training loop and log the accuracy<br/>model.compile(optimizer=optimizer,<br/>              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),<br/>              metrics=['accuracy'])</pre>
<p>The datasets and the callbacks are passed to the <kbd>fit</kbd> method, which performs the effective training loop for the desired number of epochs:</p>
<p><kbd>(tf2)</kbd></p>
<pre>num_epochs = 50<br/>model.fit(train_set, validation_data=validation_set, epochs=num_epochs,<br/>          callbacks=[cp_callback, TensorBoard])</pre>
<p>The training loop will train the model for 50 epochs, measuring the loss and the accuracy during the training and, at the end of every epoch, the accuracy and loss value on the validation set. Moreover, having passed two callbacks, we have a checkpoint with the model parameters logged in the <kbd>ckpt</kbd> directory, and we have the logging of the metrics not only on the standard output (that is, the Keras default) but also on TensorBoard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation and inference</h1>
                </header>
            
            <article>
                
<p>During the training, we can open TensorBoard and look at the plots of the losses and metrics. At the end of the 50<sup>th</sup> epoch, we get the plots shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-917 image-border" src="assets/2b8b8a58-807d-45af-8d93-4ea5b8ee0513.png" style="width:20.17em;height:36.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The accuracy and loss values on the training set (orange) and validation set (blue). The summary usage and configuration is hidden to the user by Keras.</div>
<p>Moreover, since we have all of the parameters of the model <span>in our </span><kbd>model</kbd><span> variable</span>, we can try to feed it an image downloaded from the internet and see whether the segmentation works as expected.</p>
<p>Let's suppose that we downloaded the following image from the internet and saved it as <kbd>"author.jpg"</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-918 image-border" src="assets/9d236e4a-e3eb-4f1c-8a88-fa0c03d1e59f.png" style="width:19.25em;height:19.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Greetings!</div>
<p>We expect the model to produce a segmentation of the only known class contained in this image, that is, <kbd>"person"</kbd>, while producing the <kbd>"background"</kbd> <span>label </span>everywhere else.</p>
<p>Once we've downloaded the image, we convert it into the same format expected by the model (a float with values between <em>[0,1]</em>) and resize it to <em>512</em>. Since the model works on a batch of images, a unary dimension to the <kbd>sample</kbd> variable has to be added. Now, running the inference is as easy as <kbd>model(sample)</kbd>. After that, we use the <kbd>tf.argmax</kbd> function on the last channel to extract the predicted labels for every pixel position:</p>
<p><kbd>(tf2)</kbd></p>
<pre>sample = tf.image.decode_jpeg(tf.io.read_file("author.jpg"))<br/>sample = tf.expand_dims(tf.image.convert_image_dtype(sample, tf.float32), axis=[0])<br/>sample = tf.image.resize(sample, (512,512))<br/>pred_image = tf.squeeze(tf.argmax(model(sample), axis=-1), axis=[0])</pre>
<p class="mce-root"/>
<p>In the <kbd>pred_image</kbd> tensor, we have the dense predictions that are pretty much useless for visualization. In fact, this tensor has values in the <em>[0, 21]</em> range, and these values are indistinguishable once visualized (they all look black).</p>
<p>Hence, we can use the LUT created for the dataset to apply the inverse mapping from label to color. In the end, we can use the TensorFlow <kbd>io</kbd> package to convert the image and JPEG format and store it on the disk for easy visualization:</p>
<p><kbd>(tf2)</kbd></p>
<pre>REV_LUT = {value: key for key, value in LUT.items()}<br/><br/>color_image = tf.Variable(tf.zeros((512,512,3), dtype=tf.uint8))<br/>pixels_per_label = []<br/>for label, color in REV_LUT.items():<br/>    match = tf.equal(pred_image, label)<br/>    labeled = tf.expand_dims(tf.cast(match, tf.uint8), axis=-1)<br/>    pixels_per_label.append((label, tf.math.count_nonzero(labeled)))<br/>    labeled = tf.tile(labeled, [1,1,3])<br/>    color_image.assign_add(labeled * color)<br/><br/># Save<br/>tf.io.write_file("seg.jpg", tf.io.encode_jpeg(color_image))</pre>
<p>Here's the result of the segmentation after training a simple model for only 50 epochs on a small dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-919 image-border" src="assets/1e4d9cd8-400b-4084-a0cf-62a107ab4d83.png" style="width:21.42em;height:21.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The result of the segmentation after mapping the predicted labels to the corresponding colors.</div>
<p class="mce-root">Although coarse, because the architecture hasn't been optimized, model selection hasn't been performed, and the dataset size is small, the segmentation results already look promising!</p>
<p class="mce-root">It is possible to inspect the predicted labels by counting the number of matches per label. In the <kbd>pixels_per_label</kbd> list, we saved the pair (<kbd>label</kbd>, <kbd>match_count</kbd>) and by printing it, we can verify if the predicted class is <kbd>"person"</kbd> (id 15) as expected:</p>
<p><kbd>(tf2)</kbd></p>
<pre>for label, count in pixels_per_label:<br/> print(label, ": ", count.numpy())</pre>
<p>That produces the following:</p>
<pre><span>0: 218871<br/></span>1: 0<br/>3: 383<br/>[...]<br/>15: 42285<br/>[...]</pre>
<p>This is as expected. Of course, there is still room for improvement, and this is left to the reader as an exercise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the problem of semantic segmentation and implemented U-Net: a deep encoder-decoder architecture used to tackle this problem. A short introduction about the possible use cases and the challenges this problem poses has been presented, followed by an intuitive introduction of the deconvolution (transposed convolution) operation, used to build the decoder part of the architecture. Since, at the time of writing, there is not a dataset for semantic segmentation that's ready to use <span>in TensorFlow Datasets</span>, we took the advantage of this to show the architecture of TensorFlow Datasets and show how to implement a custom <kbd>DatasetBuilder</kbd>. Implementing it is straightforward, and it is something that's recommended to every TensorFlow user since it is a handy way of creating a high-efficiency data input pipeline (<kbd>tf.data.Dataset</kbd>). Moreover, by implementing the <kbd>_generate_examples</kbd> method, the user is forced to "have a look" at the data, and this is something that's highly recommended when doing machine learning and data science. </p>
<p>After that, we learned the implementation of the training loop for the semantic segmentation network by treating this problem as a classification problem. This chapter showed how to use the Keras <kbd>compile</kbd> and <kbd>fit</kbd> methods and presented how to customize the training loop using Keras callbacks. <span>This chapter ended with a quick example of how to use the trained model for inference and how to save the resulting image using only the TensorFlow methods. </span></p>
<p>In the next chapter, <kbd>Chapter 9</kbd>, <em>Generative Adversarial Networks</em>, an introduction to <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) and the adversarial training process is shown, and, obviously, we explain how to implement them using TensorFlow 2.0.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p><span>The following exercises are of fundamental importance and you are invited to answer to every theoretical question and solve all of the code challenges presented:</span></p>
<ol>
<li>What is the semantic segmentation?</li>
<li>Why is semantic segmentation a difficult problem?</li>
<li>What is deconvolution? Is the deconvolution operation in deep learning a real deconvolution operation?</li>
<li>It is possible to use Keras models as layers?</li>
<li>Is it possible to use a single Keras<span> </span><kbd>Sequential</kbd><span> </span>model to implement a model architecture with skip connections?</li>
<li>Describe the original U-Net architecture: what are the differences between the custom implementation presented in this chapter and the original one?</li>
<li>Implement, using Keras, the original U-Net architecture.</li>
<li>What is a DatasetBuilder?</li>
<li>Describe the hierarchical organization of TensorFlow Datasets.</li>
<li>The <kbd>_info</kbd><span> </span>method contains the description of every single example of the dataset. How is this description related to the <kbd>FeatureConnector</kbd><span> </span>object?</li>
<li>Describe the <kbd>_generate_splits</kbd><span> </span>and <kbd>_generate_examples</kbd><span> </span>methods. Explain how these methods are connected and the role of the <kbd>gen_kwargs</kbd><span> </span>parameter of <kbd>tfds.core.SplitGenerator</kbd>.</li>
<li>What is a LUT? Why is it a useful data structure when creating a dataset for semantic segmentation?</li>
<li>Why it is required to use <kbd>tf.io.gfile</kbd><span> </span>when developing a custom DatasetBuilder?</li>
</ol>
<ol start="14">
<li>(Bonus): Add a missing dataset for semantic segmentation to the TensorFlow Datasets project! Submit a<span> </span>Pull Request<span> </span>to <a href="https://github.com/tensorflow/datasets">https://github.com/tensorflow/datasets</a>, and in the message, feel free to share this exercise section and this book.</li>
<li>Train the modified U-Net architecture as shown in this chapter.</li>
<li>Change the loss function and add a term of reconstruction loss, where the goal of the minimization process is to both minimize the cross-entropy and make the predicted label similar to the ground truth label.</li>
<li>Measure the mean intersection over union using the Keras callback. The Mean IOU is already implemented in the <kbd>tf.metrics</kbd><span> </span>package.</li>
<li>Try to improve the model's performance, on the validation set, by adding dropout layers in the encoder.</li>
<li>During the training, start by dropping neurons with a probability of 0.5 and, at every epoch, increase this value of 0.1. Stop the training when the validations mean IOU stops increasing.</li>
<li>Use the trained model to run inference on a random image downloaded from the internet. Postprocess the result segmentation in order to detect a bounding box around different elements of different classes. Draw the bounding boxes, using TensorFlow, on the input image.</li>
</ol>


            </article>

            
        </section>
    </body></html>