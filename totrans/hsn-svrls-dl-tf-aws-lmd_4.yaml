- en: Working with TensorFlow on AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the architecture of deploying TensorFlow
    with AWS, and we will deploy TensorFlow on AWS Lambda using the pre-existing pack
    and the serverless framework.We will also look into the various general issues
    with deploying the various Python frameworks on AWS Lambda and then cover all
    of the solutions to the same issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of deploying TensorFlow with AWS Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General issues with deploying Python frameworks on AWS Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying TensorFlow on AWS Lambda using pre-existing pack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying TensorFlow using a serverless framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS subscription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the codes at: [https://github.com/PacktPublishing/Hands-On-Serverless-Deep-Learning-with-TensorFlow-and-AWS-Lambda](https://github.com/PacktPublishing/Hands-On-Serverless-Deep-Learning-with-TensorFlow-and-AWS-Lambda)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of the deploying TensorFlow with AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about the architecture of deploying TensorFlow
    with AWS Lambda. One of the critical questions of deployment is about where to
    keep the retrained model that will be used within AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are the following three possible options:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep the model within the deployment package alongside the code and libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the model on the S3 bucket and unload it in AWS Lambda during execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the model on the FTP or HTTP server and unload it into AWS Lambda during
    execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model within the deployment package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This option means that the model is within the deployment package. The code
    will import it from a local filesystem. This option has its own pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of the model within the deployment package are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will get a very good start speed for our deployment since there is no overhead
    on the loading model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will have a single package to start with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We won't need any outside servers or AWS services as part of our deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The disadvantages of the model within the deployment package are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There is considerable limitation on the package size and it limits the possible
    size of our model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case where you need to manage different versions of the model, it may
    be tricky to either keep them all in one package or deal with different versions
    of your package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model on the S3 bucket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This option means that we have to keep the model in the S3 bucket and unload
    it during AWS Lambda execution. This option is very limited in terms of package
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of the model on the S3 bucket are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it will be limited to only 500 MB of usage, which is the maximum
    size of TMP folder on AWS Lambda, but it is actually possible to download the
    model directly into memory by passing this limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will be a lot easier to manage multiple models as you can use AWS Lambda
    environmental variables to procure equipment links to S3 bucket for each of the
    models that you want to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The disadvantages of the model on the S3 bucket are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will get a slower start than in previous cases, since Lambda will need to
    download the model first
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be noted that, though it happens only during cold start, during warm
    start, the model will already be in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need to make the S3 bucket upload all of your models as a part of your
    deployment, and add logic for managing the different models within the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model on the HTTP/FTP server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This option is mostly useful for the case where you want to limit the use of
    AWS services, memory or integrate with services outside of AWS. The AWS Lambda
    downloads the model from HTTP or FTP server during deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of the model on the HTTP/FTP server are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use a number of publicly available services with models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't have to update your model on S3 bucket or within the package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The disadvantages of the model on the HTTP/FTP server are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It may be even slower than with the previous case, which is a downside of this
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the slower time, you will need to make sure that the server is available
    from your location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General issues with deploying Python frameworks on AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn about AWS Lambda main limit, which is also known
    as the size of the package. The current limit for the Lambda deployment package
    is 50 MB. It is supposed to include libraries and code. There are two main libraries
    that we need to fit:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These libraries are used for matrix calculations. As you may know, the libraries
    by themselves are pretty big and they just wouldn't work on AWS Lambda. As you
    have already seen in the previous section on deployment that when we deploy them
    through S3, we don't have this limitation, and we only have 250 MB limitation
    for the unzipped package. In this case to make it work, we need to reduce the
    size of the package.
  prefs: []
  type: TYPE_NORMAL
- en: Solutions for issues with deploying Python frameworks on AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of ways as to how we can reduce the package size. Here are
    the solutions for the issues in question:'
  prefs: []
  type: TYPE_NORMAL
- en: We can compress the shared libraries; this usually enables us to get the best
    reduction of size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can remove the `.pyc` files as they do not influence the library work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we can remove tests and visualization folders from the libraries as they
    are not useful in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we can remove libraries that already exist on AWS Lambda.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we can check and remove the libraries that aren't used during execution,
    for example, wheel or PIP libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, in the following code, there is the part that finds and compresses all
    shared libraries. Then, we find and delete all `.pyc` files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the commands for the preceding explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20c2e405-bef7-4cf5-a70a-bcf2add76128.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we need to delete libraries that won't be used during execution, such
    as `.pip` and `wheel`. Finally, we can also delete some folders from TensorFlow
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the different commands for the preceding explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b707219e-0514-45d0-ba06-2ae2ab8777fa.png)'
  prefs: []
  type: TYPE_IMG
- en: The whole process of preparing a package for AWS Lambda can be done through
    Docker. You don't need to use it for the project we will create, but it is good
    to keep in mind how to prepare this kind of package.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Docker, you just need to run three comments in your comment line:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to get the latest Amazon Linux image on which we will run the script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You need to start a Docker container with the managing output folder inside
    the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can run the script inside the container and it will assemble the package
    for you. The following screenshot displays all of the commands to install Docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5af485f9-ae76-479d-9a5a-3e4d5144bcf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying TensorFlow on AWS Lambda using the pre-existing pack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn to deploy TensorFlow on AWS Lambda using the
    pre-existing pack. In the project files, we have model files that are also known
    as the model itself and files that enable us to translate model response through
    labels. In `Inception` folder and Lambda package, which is also known as the code
    and libraries in `lambdapack` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the code, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll create S3 bucket where we will keep the model and upload the model itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then,we'll modify code for the specific bucket and add the created bucket name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we can package it and upload to add the AWS Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will create S3 bucket using the AWS Console and upload files there.
    We will open the code and add the bucket that we have just created. Then, let's
    package it and upload it to add AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will have to follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to go to the S3 service and click Create bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b5a2b678-1b9c-4697-adbc-1135d167ae79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can choose the bucket name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00a6f22d-931b-4cf4-b9ce-c845814d4794.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have the bucket in place, we can upload files there. You just need
    to click Upload and then choose files. So, here we just upload the package with
    libraries, and it will start the upload process, along with the package itself.
    We will need to upload model files, which are present in the `Inception` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f34cd6e4-a3e2-4a48-9398-4199e7a14df6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that now we have a package inside our S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/682c7e5f-5f9c-4f88-905f-b0ad32720507.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have to create the role for our AWS Lambda, which we can do from the
    IAM service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ae17137a-3ad4-4bef-87e2-301f05dec8dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to choose Lambda and click on Next: Permissions, which lies at the
    bottom-right of your screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/588d2e5d-3f17-47ca-9e68-57637d30be29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, it is easier to choose administrator access and click on Next:
    Tags, which lies at the bottom-right of your screen. This would allow our Lambda
    to access all services. Usually in production, the role is limited to accessing
    only specific services. We will cover this when we work with serverless frameworks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5c6a709f-8ec2-48a7-9bf9-212c95d8eb07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a role name: `lambdaAdminRole`, which will create the role in Lambda:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/193478df-a06a-41a1-93cd-c7d66ab776b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To create Lambda, navigate to Lambda function and create the function. Here,
    enter the name `testensorflolambda`, Runtime as Python 3.6\. For Roles, select
    Choose an existing role, and in Existing role, select `lambdaAdminRole`, then
    click on Create function at the bottom-right corner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/72181902-1d22-486f-9804-3251a600c875.png)'
  prefs: []
  type: TYPE_IMG
- en: '10\. After the function is created, we need to change the Handler to `index.handler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b80f11de-8c3c-43c2-ba43-afb498cc4ae6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the same screen, scroll down and, in the Basic setting tab, add enough resources
    as presented in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aac2594e-7e37-4cd4-be1b-676607b34a82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Pass the link with the URL of our package (S3 bucekt) and click on Save in
    the top right corner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ee4da3e6-39d0-4dc0-8790-13c716385262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see the function is created. To test the function, click on the Test
    on the top- right corner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/53e3a850-3fe8-4ae4-b926-85058b791050.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the function is tested, it will successfully produce the following result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9da215f9-adda-42eb-afd4-bba15ac46475.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying TensorFlow using a serverless framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will look at the project files. We have model files in the `Inception`
    folder, and Lambda code with `Serverless.yml`, the configuration file in the `Lambdapack`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of deployment will be the same as in the previous section. One of
    the main differences will be that, instead of providing an AWS Lambda admin role,
    we will provide access to bucket by serverless CML file. The only thing we need
    to add is `bucketname`, and run the properties of access as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/410584be-c45d-4925-b33b-75144ce6eb88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will need to create an S3 bucket, upload files there, and then deploy AWS
    Lambda. We will create an S3 bucket and upload files from the command line: `aws
    s3 sync.s3://<bucket>/`.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bucket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first need to create a bucket, then we will need to upload model files
    to the bucket, run serverless, and start the AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Index.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the available files. We will look at the `index.py` file as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The main difference is that we run the code inside the `handler` function and
    we need to download the model files and image file from the S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can use one of the advantages of AWS Lambda. We can save model files
    as global variables. Basically, we can define session as a global variable. With
    these, if we start Lambda right after the previous Lambda was executed, all model
    files will be in the RAM memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Serverless.yml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the `Serverless.yml` file, we need to define access to the S3 bucket as
    that''s where we will keep our model. Other than that, it will look exactly as
    the previously mentioned serverless CML files for other Lambdas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Also, the we need `inputimage.jpg` image for the inception model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the files that we need to upload to S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bfbd63b-4255-4328-bb0d-41f3e314ff40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two very convenient commands; one allows us to create a bucket, and
    another allows us to easily upload files into the bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aws s3api create-bucket --bucket serverlessdeeplearning`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws s3 sync . s3://serverlessdeeplearning/imagenet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we already have model files in this bucket, there is no need to hold it
    now, but you can use this command to upload to your bucket. Next, we can return
    back to the folder with our function and run `serverless deploy` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will invoke the function with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it successfully recognized the image. Also, if we invoke the
    function one more time after that, it will work faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10ae6ce4-b240-4ed1-b05a-50de48fc9c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the architecture of the deploying TensorFlow
    with AWS Lambda, in which we covered the possible options of deploying TensorFlow
    with AWS Lambda along with each of its pros and cons. We also discussed the general
    issues with deploying Python frameworks in AWS Lambda along with its solutions.
    Lastly, we deployed TensorFlow on AWS Lambda using the pre-existing pack and using
    a serverless framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll  create deep learning API using AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
