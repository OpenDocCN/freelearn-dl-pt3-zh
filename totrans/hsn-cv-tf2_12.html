<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Optimizing Models and Deploying on Mobile Devices</h1>
                </header>
            
            <article>
                
<p>Computer vision applications are various and multifaceted. While most of the training steps take place on a server or a computer, deep learning models are used on a variety of frontend devices, such as mobile phones, self-driving cars, and <strong>Internet-of-Things</strong> (<strong>IoT</strong>) devices. With limited computing power, performance optimization becomes paramount.</p>
<p>In this chapter, we will introduce techniques to limit your model size and improve inference speed while maintaining good prediction quality. As a practical example, we will create a simple mobile application to recognize facial expressions on iOS and Android devices, as well as in the browser.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>How to reduce model size and boost speed without impacting accuracy</li>
<li>Analyzing model computational performance in depth</li>
<li>Running models on mobile phones (iOS and Android)</li>
<li>Introducing TensorFlow.js to run models in the browser</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The code for this chapter is available from <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter09">https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter09</a>.</p>
<p>When developing applications for mobile phones, you will need knowledge of <strong>Swift</strong> (for iOS) or <strong>Java</strong> (for Android). For computer vision in the browser, you will require knowledge of <strong>JavaScript</strong>. The examples in this chapter are simple and thoroughly explained, making it easy to understand for developers who are more familiar with Python.</p>
<p>Moreover, to run the example iOS app, you will need a compatible device as well as a Mac computer with Xcode installed. To run the Android app, you will need an Android device.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing computational and disk footprints</h1>
                </header>
            
            <article>
                
<p>When using a computer vision model, some characteristics are crucial. Optimizing a model for <em>speed</em> may allow it to run in real time, opening up many new uses. Improving a model's <em>accuracy</em> by even a few percent may make the difference between a toy model and a real-life application.</p>
<p>Another important characteristic is <em>size</em>, which impacts how much storage the model will use and how long it will take to download it. For some platforms, such as mobile phones or web browsers, the size of the model matters to the end user.</p>
<p>In this section, we will describe techniques to improve the model inference speed and how to reduce its size.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring inference speed</h1>
                </header>
            
            <article>
                
<p><strong>Inference</strong> describes the process of using a deep learning model to get predictions. It is measured in images per second or seconds per image. Models must run between 5 and 30 images per second to be considered real-time processing. Before we can improve inference speed, we need to measure it properly.</p>
<p>If a model can process <em>i</em> images per second, we can always run <em>N</em> inference pipelines simultaneously to boost performance—the model will then be able to process <em>N</em> × <em>i</em> images per second. While parallelism benefits many applications, it would not work for real-time applications.</p>
<p>In a real-time context, such as with a self-driving car, no matter how many images can be processed in parallel, what matters is <strong>latency</strong>—how long it takes to compute predictions for a single image. Therefore, for real-time applications, we only measure the latency of a model—how much time it takes to <em>process a single image</em>.</p>
<p>For non-real-time applications, you can run as many inference processes in parallel as necessary. For instance, for a video, you can analyze <em>N</em> chunks of video in parallel and concatenate the predictions at the end of the process. The only impact will be in terms of financial cost, as you will need more hardware to process the frames in parallel.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring latency</h1>
                </header>
            
            <article>
                
<p>As stated, to measure how fast a model performs, we want to compute the time it takes to process a <em>single image</em>. However, to minimize measuring error, we will actually measure the processing time for several images. We will then divide the time obtained by the number of images.</p>
<p>We are not measuring the computing time over a single image for several reasons. First, we want to remove measurement error. When running the inference for the first time, the machine could be busy, the GPU might not be initialized yet, or many other technicalities could be causing the slowdown. Running several times allows us to minimize this error.</p>
<p>The second reason is TensorFlow's and CUDA's warmup. When running an operation for the first time, deep learning frameworks are usually slower—they have to initialize variables, allocate memory, move data around, and so on. Moreover, when running repetitive operations, they usually automatically optimize for it.</p>
<p class="mce-root">For all those reasons, it is recommended to measure inference time with multiple images to simulate a real environment.</p>
<div class="packt_tip">When measuring inference time, it is also very important to include data loading, data preprocessing, and post-processing times, as these can be significant.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using tracing tools to understand computational performance</h1>
                </header>
            
            <article>
                
<p>While measuring the total inference time of a model informs you of the feasibility of an application, you might sometimes need a more detailed performance report. To do so, TensorFlow offers several tools. In this section, we will discuss the <strong>trace tool</strong>, which is part of the TensorFlow summary package.</p>
<div class="packt_infobox">In <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets</em>, we described how to analyze the performance of input pipelines. Refer to this chapter to monitor preprocessing and data ingestion performance.</div>
<p>To use it, call <kbd>trace_on</kbd> and set <kbd>profiler</kbd> to <kbd>True</kbd>. You can then run TensorFlow or Keras operations and export the trace to a folder:</p>
<pre>logdir = './logs/model'<br/>writer = tf.summary.create_file_writer(logdir)<br/><br/>tf.summary.trace_on(profiler=True)<br/>model.predict(train_images)<br/>with writer.as_default():<br/>  tf.summary.trace_export('trace-model', profiler_outdir=logdir)</pre>
<div class="packt_tip">Omitting the call to <kbd>create_file_writer</kbd> and <kbd>with writer.as_default()</kbd> will still create a trace of the operations. However, the model graph representation will not be written to disk.</div>
<p>Once the model starts running with tracing enabled, we can point TensorBoard to this folder by executing the following command in the command line:</p>
<pre><strong>$ tensorboard --logdir logs</strong></pre>
<p>After opening TensorBoard in the browser and clicking on the <strong>Profile</strong> tab, we can then review the operations:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/436d7ed2-5384-4bb7-881e-7012bf36af70.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9-1: </span>Trace of the operations for a simple fully connected model over multiple batches of data</div>
<p>As seen in the preceding timeline, the model is composed of many small operations. By clicking on an operation, we can obtain its name and its duration. For instance, here are the details for a dense matrix multiplication (a fully-connected layer):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/47f42293-835f-4e3e-9640-9919d2b3ff8b.png" style="width:25.67em;height:6.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9-2: The d</span>etails of a matrix multiplication operation</div>
<div class="packt_tip">TensorFlow traces can end up taking a large amount of disk space. For this reason, we recommend running the operations you want to trace on a few batches of data only.</div>
<p>On the TPU, a dedicated <span class="packt_screen">Capture Profile</span> button is available in TensorBoard. The TPU name, IP, and the trace recording time need to be specified.</p>
<p>In practice, the tracing tool is used on much larger models to determine the following:</p>
<ul>
<li>Which layers are taking the most computing time.</li>
<li>Why a model is taking more time than usual after a modification to the architecture.</li>
<li>Whether TensorFlow is always computing numbers or is waiting on data. This can happen if preprocessing takes too long or if there is a lot of back and forth between CPUs.</li>
</ul>
<p>We encourage you to trace the models you are using to get a better understanding of the computational performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving model inference speed</h1>
                </header>
            
            <article>
                
<p>Now that we know how to properly measure a model inference speed, we can use several approaches to improve it. Some involve changing the hardware used, while others imply changing the model architecture itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing for hardware </h1>
                </header>
            
            <article>
                
<p>As we saw previously, the hardware used for inference is crucial for speed. From the slowest option to the fastest, it is recommended to use the following:</p>
<ul>
<li><strong>CPU</strong>: While slower, it is often the cheapest option.</li>
<li><strong>GPU</strong>: Faster but more expensive. Many smartphones have integrated GPUs that can be used for real-time applications.</li>
<li><strong>Specialized hardware</strong>: For instance, Google's <em>TPU</em> (for servers), Apple's <em>Neural Engine</em> (on mobile), or <em>NVIDIA Jetson</em> (for portable hardware). They are chips made specifically for running deep learning operations.</li>
</ul>
<p>If speed is crucial for your application, it is important to use the fastest hardware available and to adapt your code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing on CPUs</h1>
                </header>
            
            <article>
                
<p>Modern Intel CPUs can compute matrix operations more quickly through special instructions. This is done using the <strong>Math Kernel Library for Deep Neural Networks</strong> (<strong>MKL-DNN</strong>). Out-of-the-box TensorFlow does not exploit those instructions. Using them requires either compiling TensorFlow with the right options or installing a special build of TensorFlow called <kbd>tensorflow-mkl</kbd>.</p>
<div class="packt_infobox">Information on how to build TensorFlow with MKL-DNN is available at <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>. Note that the toolkit currently only works on Linux.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing on GPUs</h1>
                </header>
            
            <article>
                
<p>To run models on NVIDIA GPUs, two libraries are mandatory—<kbd>CUDA</kbd> and <kbd>cuDNN</kbd>. TensorFlow natively exploits the speed-up offered by those libraries.</p>
<div class="packt_tip">To properly run operations on the GPU, the <kbd>tensorflow-gpu</kbd> package must be installed. Moreover, the CUDA version of <kbd>tensorflow-gpu</kbd> must match the one installed on the computer.</div>
<p>Some modern GPUs offer <strong>Floating Point 16</strong> (<strong>FP16</strong>) instructions. The idea is to use reduced precision floats (16 bits instead of the 32 bits commonly used) in order to speed up inference while not impacting the output quality by much. Not all GPUs are compatible with FP16.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing on specialized hardware</h1>
                </header>
            
            <article>
                
<p>Since every chip is different, the techniques ensuring faster inference vary from one manufacturer to another. The steps necessary for running a model are well documented by the manufacturer.</p>
<p>A rule of thumb is to not use exotic operations. If one of the layers is running operations that include conditions or branching, it is likely that the chip will not support it. The operations will have to run on the CPU, making the whole process slower. It is therefore recommended to <em>only use standard operations</em>—convolution, pooling, and fully connected layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing input</h1>
                </header>
            
            <article>
                
<p>The inference speed of a computer vision model is directly proportional to the size of the input image. Moreover, dividing the dimensions of an image by two means four times fewer pixels for the model to process. Therefore, using <em>smaller images improves inference speed</em>.</p>
<p>When using smaller images, the model has less information and fewer details to work with. This often has an impact on the quality of the results. It is necessary to experiment with image size to find a good <em>trade-off between speed and accuracy</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing post-processing</h1>
                </header>
            
            <article>
                
<p>As we saw previously in the book, most models require post-processing operations. If implemented using the wrong tools, post-processing can take a lot of time. While most post-processing happens on the CPU, it is sometimes possible to run some operations on the GPU.</p>
<p>Using tracing tools, we can analyze the time taken by post-processing to optimize it. <strong>Non-Maximum Suppression</strong> (<strong>NMS</strong>) is an operation that can take a lot of time if not implemented correctly (refer to <a href="593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml">Chapter 5</a>, <em>Object Detection Models</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/abea7881-df99-4226-b65b-e3686c106fd0.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9-3: </span>Evolution of NMS computing time with the number of boxes</div>
<p>Notice in the preceding diagram that the slow implementation takes linear computing time, while the fast implementation is almost constant. Though four milliseconds may seem quite low, keep in mind that some models can return an even larger number of boxes, resulting in a post-processing time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When the model is still too slow</h1>
                </header>
            
            <article>
                
<p>Once the model has been optimized for speed, it can sometimes still be too slow for real-time applications. There are a few techniques to work around the slowness while maintaining a real-time feeling for the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpolating and tracking</h1>
                </header>
            
            <article>
                
<p>Object detection models are notoriously computationally-intensive. Running on every frame of a video is sometimes impractical. A common technique is to use the model every few frames only. In-between frames, linear interpolation is used to follow the tracked object.</p>
<p>While this technique does not work for real-time applications, another one that is commonly used is <strong>object tracking.</strong> Once an object is detected with a deep learning model, a more simple model is used to follow the boundaries of the object.</p>
<p>Object tracking can work on almost any kind of object as long as it is well distinguishable from its background and its shape does not change excessively. There are many object tracking algorithms (some of them are available through OpenCV's tracker module,  documented here <a href="https://docs.opencv.org/master/d9/df8/group__tracking.html">https://docs.opencv.org/master/d9/df8/group__tracking.html</a>); many of them are available for mobile applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model distillation</h1>
                </header>
            
            <article>
                
<p>When none of the other techniques work, one last option is <strong>model distillation</strong>. The general idea is to train a small model to learn the output of a bigger model. Instead of training the small model to learn the raw labels (we could use the data for this), we train it to learn the output of the bigger model.</p>
<p>Let's see an example—we trained a very large network to predict an animal's breed from a picture. The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e9ab31a5-6241-4236-9da3-948a0d32c6b6.png" style="width:33.75em;height:11.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9-4: </span>Examples of predictions made by our network</div>
<p>Because our model is too large to run on mobile, we decided to train a smaller model. Instead of training it with the labels we have, we decided to distill the knowledge of the larger network. To do so, we will use the output of the larger network as targets.</p>
<p>For the first picture, instead of training the new model with a target of <em>[1, 0, 0]</em>, we will use the output of the larger network, a target of <em>[0.9, 0.7, 0.1]</em>. This new target is called a <strong>soft target</strong>. This way, the smaller network will be taught that, while the animal in the first picture is not a husky, it does look similar to one according to more advanced models, as the picture has a score of <em>0.7</em> for the <em>husky</em> class.</p>
<p>The larger network managed to directly learn from the original labels (<em>[1, 0, 0]</em> in our example) because it has more computing and memory power. During training, it was able to deduce that breeds of dogs look like one another but belong to different classes. A smaller model would not have the capacity to learn such abstract relations in the data by itself, but it can be guided by the other network. Following the aforementioned procedure, the inferred knowledge from the first model will be passed to the new one, hence the name <strong>knowledge distillation</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing model size</h1>
                </header>
            
            <article>
                
<p>When using a deep learning model in the browser or on mobile, the model needs to be downloaded on the device. It needs to be as lightweight as possible for the following reasons:</p>
<ul>
<li>Users are often using their phone on a cellular connection that is sometimes metered.</li>
<li>The connection can also be slow.</li>
<li>Models can be frequently updated.</li>
<li>Disk space on portable devices is sometimes limited.</li>
</ul>
<p>With hundreds of millions of parameters, deep learning models are notoriously disk space-consuming. Thankfully, there are techniques to reduce their size.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quantization</h1>
                </header>
            
            <article>
                
<p>The most common technique is to reduce the precision of the parameters. Instead of storing them as 32-bit floats, we can store them as 16- or 8-bit floats. There have been experiments for using binary parameters, taking only 1 bit to store.</p>
<p><strong>Quantization</strong> is often done at the end of training, when converting the model for use on the device. This conversion impacts the accuracy of the model. Because of this, it is very important to evaluate the model after quantization.</p>
<p>Among all the compression techniques, quantization is often the one with the highest impact on size and the least impact on performance. It is also very easy to implement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Channel pruning and weight sparsification</h1>
                </header>
            
            <article>
                
<p>Other techniques exist but can be harder to implement. There is no straightforward way to apply them because they rely mostly on trial and error.</p>
<p>The first one, <strong>channel pruning</strong>, consists of removing some convolutional filters or some channels. Convolutional layers usually have between 16 and 512 different filters. At the end of the training phase, it often appears that some of them are not useful. We can remove them to avoid storing weights that will not help the model performance.</p>
<p>The second one is called <strong>weight sparsification</strong>. Instead of storing weights for the whole matrix, we can store only the ones that are deemed important or not close to zero.</p>
<p>For instance, instead of storing a weight vector such as <em>[0.1, 0.9, 0.05, 0.01, 0.7, 0.001]</em>, we could keep weights that are not close to zero. The result is a list of tuples in the form <em>(position, value)</em>. In our example, it would be <em>[(1, 0.9), (4, 0.7)]</em>. If many of the vector's values are close to zero, we could expect a large reduction in stored weights.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">On-device machine learning</h1>
                </header>
            
            <article>
                
<p>Due to their high computational requirements, deep learning algorithms are most commonly run on powerful servers. They are computers specifically designed for this task. For latency, privacy, or cost reasons, it is sometimes more interesting to run inference on customers' devices: smartphones, connected objects, cars, or microcomputers.</p>
<p>What all those devices have in common are lower computational power and low power requirements. Because they are at the end of the data life cycle, on-device machine learning is also referred to as <strong>edge computing</strong> or <strong>machine learning on the edge</strong>.</p>
<p>With regular machine learning, the computation usually happens in the data center. For instance, when you upload a photo to Facebook, a deep learning model is run in Facebook's data center to detect your friends' faces and help you tag them.</p>
<p>With on-device machine learning, the inference happens on your device. A common example is Snapchat face filters—the model that detects your face position is run directly on the device. However, model training still happens in data centers—the device uses a trained model fetched from the server:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/62deba1a-1bb9-4848-a42a-f56c56320bdc.png" style="width:34.50em;height:21.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9-5: </span>Diagram comparing on-device machine learning with conventional machine learning</div>
<div class="packt_infobox">Most on-device machine learning happens for inference. The training of models is still mostly done on dedicated servers.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considerations of on-device machine learning</h1>
                </header>
            
            <article>
                
<p>The use of <strong>on-device machine learning</strong> (<strong>on-device ML</strong>) is usually motivated by a combination of reasons, but also has its limitations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits of on-device ML</h1>
                </header>
            
            <article>
                
<div>
<p>The following paragraphs list the main benefits of running machine learning algorithms directly on users' devices.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Latency</h1>
                </header>
            
            <article>
                
<div>
<p>The most common motivation is <strong>latency</strong>. Because sending data to a server for processing takes time, real-time applications make it impossible to use conventional machine learning. The most striking illustration is self-driving cars. To react quickly to its environment, the car must have the lowest latency possible. Therefore, it is crucial to run the model in the car. Moreover, some devices are used in places where internet access is simply not available.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Privacy</h1>
                </header>
            
            <article>
                
<div>
<p>As consumers care more and more about their privacy, companies are devising techniques to run deep learning models while respecting this demand.</p>
<p>Let's use a large-scale example from Apple. When browsing through photos on an iOS device, you may notice that it is possible to search for objects or things—<kbd>cat</kbd>, <kbd>bottle</kbd>, <kbd>car</kbd> will return the corresponding images. This is the case even if the pictures are not sent to the cloud. For Apple, it was important to make that feature available while respecting the privacy of its users. Sending pictures for processing without the users' consent would have been impossible.</p>
<p>Therefore, Apple decided to use on-device ML. Every night, when the phone is charging, a computer vision model is run on the iPhone to detect objects in the image and make this feature available.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cost</h1>
                </header>
            
            <article>
                
<div>
<p>On top of respecting user privacy, this feature also reduces costs for Apple because the company does not have to pay the bill for servers to process the hundreds of millions of images that their customers produce. </p>
<p>On a much smaller scale, it is now possible to run some deep learning models in the browser. This is especially useful for demos—by running the models on the user's computer, you can avoid paying for a costly GPU-enabled server to run inference at scale. Moreover, there will not be any overloading issues because the more users that access the page, the more computing power that is available.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of on-device ML</h1>
                </header>
            
            <article>
                
<p>While it has many benefits, this concept also has a number of limitations. First of all, the limited computing power of devices means that some of the most powerful models cannot be considered.</p>
<p>Also, many on-device deep learning frameworks are not compatible with the most innovative or the most complex layers. For instance, TensorFlow Lite is not compatible with custom LSTM layers, making it hard to port advanced recurrent neural networks on mobile using this framework.</p>
<p>Finally, making models available on devices implies sharing the weights and the architecture with users. While encryption and obfuscation methods exist, it increases the risk of reverse engineering or model theft.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Practical on-device computer vision</h1>
                </header>
            
            <article>
                
<p>Before discussing the practical application of on-device computer vision, we will have a look at the general considerations for running deep learning models on mobile devices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">On-device computer vision particularities</h1>
                </header>
            
            <article>
                
<p>When running computer vision models on mobile devices, the focus switches from raw performance metrics to user experience. On mobile phones, this means minimizing battery and disk usage: we don't want to drain the phone's battery in minutes or fill up all the available space on the device. When running on mobile, it is recommended to use smaller models. As they contain fewer parameters, they use less disk space. Moreover, as they require fewer operations, this leads to reduced battery usage.</p>
<p>Another particularity of mobile phones is orientation. In training datasets, most pictures are provided with the correct orientation. While we sometimes change this orientation during data augmentation, the images are rarely upside down or completely sideways. However, there are many ways to hold a mobile phone. For this reason, we must monitor the device's orientation to make sure that we are feeding the model with images that are correctly oriented.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a SavedModel</h1>
                </header>
            
            <article>
                
<p>As we mentioned earlier, on-device machine learning is typically used for inference. Therefore, a prerequisite is to have a <em>trained model</em>. Hopefully, this book will have given you a good idea of how to implement and prepare your network. We now need to convert the model to an intermediate file format. It will then be converted by a library for mobile use.</p>
<p>In TensorFlow 2, the intermediate format of choice is <strong>SavedModel</strong>. A SavedModel contains the model architecture (the graph) and the weights.</p>
<p>Most TensorFlow objects can be exported as a SavedModel. For instance, the following code exports a trained Keras model:</p>
<pre>tf.saved_model.save(model, export_dir='./saved_model')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a frozen graph</h1>
                </header>
            
            <article>
                
<p>Before introducing the <strong>SavedModel</strong> API, TensorFlow mainly used the <strong>frozen graphs</strong> format. In practice, a SavedModel is a wrapper around a frozen graph. The former includes more metadata and can include the preprocessing function needed to serve the model. While SavedModel is gaining in popularity, some libraries still require frozen models.</p>
<p>To convert a SavedModel to a frozen graph, the following code can be used:</p>
<pre>from tensorflow.python.tools import freeze_graph<br/><br/>output_node_names = ['dense/Softmax']<br/>input_saved_model_dir = './saved_model_dir'<br/>input_binary = False<br/>input_saver_def_path = False<br/>restore_op_name = None<br/>filename_tensor_name = None<br/>clear_devices = True<br/>input_meta_graph = False<br/>checkpoint_path = None<br/>input_graph_filename = None<br/>saved_model_tags = tag_constants.SERVING<br/><br/>freeze_graph.freeze_graph(input_graph_filename, input_saver_def_path,<br/>                          input_binary, checkpoint_path, output_node_names,<br/>                          restore_op_name, filename_tensor_name,<br/>                          'frozen_model.pb', clear_devices, "", "", "",<br/>                          input_meta_graph, input_saved_model_dir,<br/>                          saved_model_tags)</pre>
<p>On top of specifying the input and output, we also need to specify <kbd>output_node_names</kbd>. Indeed, it is not always clear what the inference output of a model is. For instance, image detection models have several outputs—the box coordinates, the scores, and the classes. We need to specify which one(s) to use.</p>
<div class="packt_tip">Note that many arguments are <kbd>False</kbd> or <kbd>None</kbd> because this function can accept many different formats, and SavedModel is only one of them.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importance of preprocessing</h1>
                </header>
            
            <article>
                
<p>As explained in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>, input images have to be <strong>preprocessed</strong>. The most common preprocessing method is to divide each channel by <em>127.5</em> (<em>127.5</em> = <em>255/2</em> = middle value of an image pixel) and subtract 1. This way, we represent images with values between -1 and 1:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2d8034f3-e309-4bc6-836f-a34d00cb4057.png" style="width:34.50em;height:7.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9-6: </span>Example of preprocessing for a <em>3 x 3</em> image with a single channel</div>
<p>However, there are many ways to represent images, depending on the following:</p>
<ul>
<li>The order of the channels: RGB or BGR</li>
<li>Whether the image is between <em>0</em> and <em>1</em>, <em>-1</em> and <em>1</em>, or <em>0</em> and <em>255</em></li>
<li>The order of the dimensions: <em>[W, H, C]</em> or <em>[C, W, H]</em></li>
<li>The orientation of the image</li>
</ul>
<p>When porting a model, it is paramount to use the <em>exact same preprocessing</em> on a device as during training. Failing to do so will lead the model to infer poorly, sometimes even to fail completely, as the input data will be too different compared with the training data.</p>
<p>All mobile deep learning frameworks provide some options to specify preprocessing settings. It is up to you to set the correct parameters.</p>
<p>Now that we have obtained a <strong>SavedModel</strong> and that we know the importance of pre-processing, we are ready to use our model on different devices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example app – recognizing facial expressions</h1>
                </header>
            
            <article>
                
<p>To directly apply the notions presented in this chapter, we will develop an app making use of a lightweight computer vision model, and we will deploy it to various platforms.</p>
<p>We will build an app that classifies facial expressions. When pointed to a person's face, it will output the expression of that person—happy, sad, surprised, disgusted, angry, or neutral. We will train our model on the <strong>Facial Expression Recognition</strong> (<strong>FER</strong>) dataset available at <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge</a>, put together by Pierre-Luc Carrier and Aaron Courville. It is composed of 28,709 grayscale images of <em>48 × 48</em> in size:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c5a7d006-6b39-4efd-8d63-1c6381414dee.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 9-7: </span>Images sampled from the FER dataset</div>
<p>Inside the app, the naive approach would be to capture images with the camera and then feed them directly to our trained model. However, this would yield poor results as objects in the environment would impair the quality of the prediction. We need to crop the face of the user before feeding it to the user:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9e7666d7-4bd5-49f6-a429-f33dc2299d60.png" style="width:36.25em;height:11.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9-8: </span>Two-step flow of our facial expression classification app</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>While we could build our own model for the first step (face detection), it is much more convenient to use out-of-the-box APIs. They are available natively on iOS and through libraries on Android and in the browser. The second step, expression classification, will be performed using our custom model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing MobileNet</h1>
                </header>
            
            <article>
                
<p>The architecture we will use for classification is named <strong>MobileNet</strong>. It is a convolutional model designed to run on mobile. Introduced in 2017, in the paper <em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</em>, by Andrew G Howard et al.<em>,</em> it uses a special kind of convolution to reduce the number of parameters as well as the computations necessary to generate predictions.</p>
<p>MobileNet uses <strong>depthwise separable</strong> convolutions. In practice, this means that the architecture is composed of an alternation of two types of convolutions:</p>
<ol>
<li><strong>Pointwise</strong> <strong>convolutions</strong>: These are just like regular convolutions, but with a <em>1</em> × <em>1</em> kernel. The purpose of pointwise convolutions is to combine the different channels of the input. Applied to an RGB image, they will compute a weighted sum of all channels.</li>
<li><strong>Depthwise</strong> <strong>convolutions</strong>: These are like regular convolutions, but do not combine channels. The role of depthwise convolutions is to filter the content of the input (detect lines or patterns). Applied to an RGB image, they will compute a feature map for each channel.</li>
</ol>
<p>When combined, these two types of convolutions perform similarly to regular convolutions. However, due to the small size of their kernels, they require fewer parameters and computational power, making this architecture suited for mobile devices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying models on-device</h1>
                </header>
            
            <article>
                
<p>To illustrate on-device machine learning, we will port a model to iOS and Android devices, as well as for web browsers. We will also describe the other types of devices available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running on iOS devices using Core ML</h1>
                </header>
            
            <article>
                
<p>With the release of its latest devices, Apple is putting the emphasis on machine learning. They designed a custom chip—the <strong>neural engine</strong>. This can achieve fast deep learning operations while maintaining a low power usage. To fully benefit from this chip, developers must use a set of official APIs called <strong>Core ML</strong> (refer to the documentation at <a href="https://developer.apple.com/documentation/coreml">https://developer.apple.com/documentation/coreml</a>).</p>
<p>To use an existing model with Core ML, developers need to convert it to the <kbd>.mlmodel</kbd> format. Thankfully, Apple provides Python tools to convert from Keras or TensorFlow.</p>
<p>In addition to speed and energy efficiency, one of the strengths of Core ML is its integration with other iOS APIs. Powerful native methods exist for augmented reality, face detection, object tracking, and much more.</p>
<div class="packt_tip">While TensorFlow Lite supports iOS, as of now, we still recommend using Core ML. This allows faster inference time and broader feature compatibility.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting from TensorFlow or Keras</h1>
                </header>
            
            <article>
                
<p>To convert our model from Keras or TensorFlow, another tool is needed—<kbd>tf-coreml</kbd> (<a href="https://github.com/tf-coreml/tf-coreml">https://github.com/tf-coreml/tf-coreml</a>).</p>
<div class="packt_infobox">At the time of writing, <kbd>tf-coreml</kbd> is not compatible with TensorFlow 2. We have provided a modified version while the library's developers are updating it. Refer to the chapter's notebook for the latest installation instructions.</div>
<p>We can then convert our model to <kbd>.mlmodel</kbd>:</p>
<pre>import tfcoreml as tf_converter<br/><br/>tf_converter.convert('frozen_model.pb',<br/>                     'mobilenet.mlmodel',<br/>                     class_labels=EMOTIONS,<br/>                     image_input_names=['input_0:0'],<br/>                     output_feature_names=[output_node_name + ':0'],<br/>                     red_bias=-1,<br/>                     green_bias=-1,<br/>                     blue_bias=-1,<br/>                     image_scale=1/127.5,<br/>                     is_bgr=False)</pre>
<p>A few arguments are important:</p>
<ul>
<li><kbd>class_labels</kbd>: The list of labels. Without this, we would end up with class IDs instead of readable text.</li>
<li><kbd>input_names</kbd>: The name of the input layer.</li>
<li><kbd>image_input_names</kbd>: This is used to specify to the Core ML framework that our input is an image. This will be useful later on because the library will handle all preprocessing for us.</li>
<li><kbd>output_feature_names</kbd>: As with the frozen model conversion, we need to specify the outputs we will target in our model. In this case, they are not operations but outputs. Therefore, <kbd>:0</kbd> must be appended to the name.</li>
<li><kbd>image_scale</kbd>: The scale used for preprocessing.</li>
<li><kbd>bias</kbd>: The bias of the preprocessing for each color.</li>
<li><kbd>is_bgr</kbd>: Must be <kbd>True</kbd> if channels are in the BGR order, or <kbd>False</kbd> if RGB.</li>
</ul>
<p>As stated earlier, <kbd>scale</kbd>, <kbd>bias</kbd>, and <kbd>is_bgr</kbd> must match the ones used during training.</p>
<p>After converting the model to a <kbd>.mlmodel</kbd> file, it can be opened in Xcode:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/35e480af-9e51-41fb-8a8c-92ad059ebdaa.png" style="width:33.33em;height:22.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9-9: </span>Screenshot of Xcode showing the details of a model</div>
<p>Note that the input is recognized as an <kbd>Image</kbd> since we specified <kbd>image_input_names</kbd>. Thanks to this, Core ML will be able to handle the preprocessing of the image for us.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the model</h1>
                </header>
            
            <article>
                
<p>The full app is available in the chapter repository. A Mac computer and an iOS device are necessary to build and run it. Let's briefly detail the steps to get predictions from the model. Note that the following code is written in Swift. It has a similar syntax to Python:</p>
<pre>private lazy var model: VNCoreMLModel = try! VNCoreMLModel(for: mobilenet().model)<br/><br/>private lazy var classificationRequest: VNCoreMLRequest = {<br/>    let request = VNCoreMLRequest(model: model, completionHandler: { [weak self] request, error in<br/>        self?.processClassifications(for: request, error: error)<br/>    })<br/>    request.imageCropAndScaleOption = .centerCrop<br/>    return request<br/>}()</pre>
<p>The code consists of three main steps:</p>
<ol>
<li>Loading the model. All the information about it is available in the <kbd>.mlmodel</kbd> file.</li>
<li>Setting a custom callback. In our case, after the image is classified, we will call <kbd>processClassifications</kbd>.</li>
<li>Setting <kbd>imageCropAndScaleOption</kbd>. Our model was designed to accept square images, but the input often has a different ratio. Therefore, we configure Core ML to crop the center of the image by setting it to <kbd>centerCrop</kbd>.</li>
</ol>
<p>We also load the model used for face detection using the native <kbd>VNDetectFaceRectanglesRequest</kbd> and <kbd>VNSequenceRequestHandler</kbd> functions:</p>
<pre>private let faceDetectionRequest = VNDetectFaceRectanglesRequest()<br/>private let faceDetectionHandler = VNSequenceRequestHandler()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the model</h1>
                </header>
            
            <article>
                
<p>As an input, we access <kbd>pixelBuffer</kbd>, which contains the pixel of the video feed from the camera of the device. We run our face detection model and obtain <kbd>faceObservations</kbd>. This will contain the detection results. If the variable is empty, it means that no face was detected and we do not go further in the function:</p>
<pre>try faceDetectionHandler.perform([faceDetectionRequest], on: pixelBuffer, orientation: exifOrientation)<br/><br/>guard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation], faceObservations.isEmpty == false else {<br/>    return<br/>}</pre>
<p>Then, for each <kbd>faceObservation</kbd> in <kbd>faceObservations</kbd>, we classify the area containing the face:</p>
<pre>let classificationHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: .right, options: [:])<br/><br/>let box = faceObservation.boundingBox<br/>let region = CGRect(x: box.minY, y: 1 - box.maxX, width: box.height, height:box.width)<br/>self.classificationRequest.regionOfInterest = region<br/><br/>try classificationHandler.perform([self.classificationRequest])</pre>
<p>To do so, we specify <kbd>regionOfInterest</kbd> of the request. This notifies the Core ML framework that the input is this specific area of the image. This is very convenient as we do not have to crop and resize the image—the framework handles this for us. Finally, we call the <kbd>classificationHandler.perform</kbd> <span>native method.</span></p>
<div class="packt_infobox">Note that we had to change the coordinate system. The face coordinates are returned with an origin at the top left of the image, while <kbd>regionOfInterest</kbd> must be specified with an origin in the bottom left.</div>
<p>Once the predictions are generated, our custom callback, <kbd>processClassifications</kbd>, will be called with the results. We will then be able to display the results to the user. This part is covered in the full application available in the book's GitHub repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running on Android using TensorFlow Lite</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow Lite</strong> is a mobile framework that allows you to run TensorFlow models on mobile and embedded devices. It supports Android, iOS, and Raspberry Pi. Unlike Core ML on iOS devices, it is not a native library but an external dependency that must be added to your app.</p>
<p>While Core ML was optimized for iOS device hardware, TensorFlow Lite performance may vary from device to device. On some Android devices, it can use the GPU to improve inference speed.</p>
<p>To use TensorFlow Lite for our example application, we will first convert our model to the library's format using the TensorFlow Lite converter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting the model from TensorFlow or Keras</h1>
                </header>
            
            <article>
                
<p>TensorFlow integrates a function to transform a SavedModel model to the TF Lite format. To do so, we first create a TensorFlow Lite converter object:</p>
<pre># From a Keras model<br/>converter = tf.lite.TFLiteConverter.from_keras_model(model)<br/>## Or from a SavedModel<br/>converter = tf.lite.TFLiteConverter('./saved_model')</pre>
<p>Then, the model is saved to disk:</p>
<pre>tflite_model = converter.convert()<br/>open("result.tflite", "wb").write(tflite_model)</pre>
<div class="packt_infobox">You will notice that the TensorFlow Lite function offers fewer options than the Apple Core ML equivalent. Indeed, TensorFlow Lite does not handle preprocessing and resizing of the images automatically. This has to be handled by the developer in the Android app.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the model</h1>
                </header>
            
            <article>
                
<p>After converting the model to the <kbd>.tflite</kbd> format, we can add it to the assets folder of our Android application. We can then load the model using a helper function, <kbd>loadModelFile</kbd>:</p>
<pre><span>tfliteModel </span>= loadModelFile(activity)<span>;</span><span><br/></span></pre>
<div class="packt_infobox">Because our model is in the assets folder of our app, we need to pass the current activity. If you are not familiar with Android app development, you can think of an activity as a specific screen of an app.</div>
<p>We can then create <kbd>Interpreter</kbd>. In TensorFlow Lite, the interpreter is necessary to run a model and return predictions. In our example, we pass the default <kbd>Options</kbd> constructor. The <kbd>Options</kbd> constructor could be used to change the number of threads or the precision of the model:</p>
<pre>Interpreter.Options <span>tfliteOptions </span>= <span>new </span>Interpreter.Options()<span>;<br/></span><span>tflite </span>= <span>new </span>Interpreter(<span>tfliteModel</span><span>, </span><span>tfliteOptions</span>)<span>;<br/></span></pre>
<p>Finally, we will create <kbd>ByteBuffer</kbd>. This is a data structure that contains the input image data:</p>
<pre><span>imgData </span>=<br/>    ByteBuffer.<span>allocateDirect</span>(<br/>        <span>DIM_BATCH_SIZE<br/></span><span>            </span>* getImageSizeX()<br/>            * getImageSizeY()<br/>            * <span>DIM_PIXEL_SIZE<br/></span><span>            </span>* getNumBytesPerChannel())<span>;<br/></span></pre>
<p><kbd>ByteBuffer</kbd> is an array that will contain the pixels of the image. Its size depends on the following:</p>
<ul>
<li>The batch size—in our case, 1.</li>
<li>The dimensions of the input image.</li>
<li>The number of channels (<kbd>DIM_PIXEL_SIZE</kbd>)—3 for RGB, 1 for grayscale.</li>
<li>Finally, the number of bytes per channel. As <em>1 byte = 8 bits</em>, a 32-bits input will require 4 bytes. If using quantization, an 8-bits input will require 1 byte.</li>
</ul>
<p>To process predictions, we will later fill this <kbd>imgData</kbd> buffer and pass it to the interpreter. Our facial expression detection model is ready to be used. Before we can start using our full pipeline, we only need to instantiate the face detector:</p>
<pre><span>faceDetector </span>= <span>new </span>FaceDetector.Builder(<span>this</span>.getContext())<br/>        .setMode(FaceDetector.<span>FAST_MODE</span>)<br/>        .setTrackingEnabled(<span>false</span>)<br/>        .setLandmarkType(FaceDetector.<span>NO_LANDMARKS</span>)<br/>        .build()<span>;</span></pre>
<div class="packt_infobox">Note that this <kbd>FaceDetector</kbd> class comes from the Google Vision framework and has nothing to do with TensorFlow Lite.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the model</h1>
                </header>
            
            <article>
                
<p>For our example app, we will work with bitmap images. You can see bitmaps as a matrix of raw pixels. They are compatible with most of the image libraries on Android. We obtain this bitmap from the view that displays the video feed from the camera, called <kbd>textureView</kbd>:</p>
<pre>Bitmap bitmap = <span>textureView</span>.getBitmap(<span>previewSize</span>.getHeight() / <span>4</span><span>, </span><span>previewSize</span>.getWidth() / <span>4</span>)<span><br/></span></pre>
<div class="packt_tip">We do not capture the bitmap at full resolution. Instead, we divide its dimensions by <kbd>4</kbd> (this number was picked by trial and error). Choosing a size that's too large would result in very slow face detection, reducing the inference time of our pipeline.</div>
<p>We then proceed to create <kbd>vision.Frame</kbd> from the bitmap. This step is necessary to pass the image to <kbd>faceDetector</kbd>:</p>
<pre>Frame frame = <span>new </span>Frame.Builder().setBitmap(bitmap).build()<span>;<br/></span><span>faces </span>= <span>faceDetector</span>.detect(frame);</pre>
<p>Then, for each <kbd>face</kbd> in <kbd>faces</kbd>, we can crop the face of the user in the bitmap. Provided in the GitHub repository, the <kbd>cropFaceInBitmap</kbd> <span>helper function </span>does precisely this—it accepts the coordinates of the face and crops the corresponding area in the bitmap:</p>
<pre><span>Bitmap faceBitmap = cropFaceInBitmap(face, bitmap);<br/>Bitmap resized = Bitmap.createScaledBitmap(faceBitmap, <br/>classifier.getImageSizeX(), classifier.getImageSizeY(), true)</span></pre>
<p>After resizing the bitmap to fit the input of our model, we fill <kbd>imgData</kbd>, <kbd>ByteBuffer</kbd>, which <kbd>Interpreter</kbd> accepts:</p>
<pre><span>imgData.rewind();<br/>resized.getPixels(intValues, 0, resized.getWidth(), 0, 0, resized.getWidth(), resized.getHeight());<br/><br/>int pixel = 0;<br/>for (int i = 0; i &lt; getImageSizeX(); ++i) {<br/>  for (int j = 0; j &lt; getImageSizeY(); ++j) {<br/>    final int val = intValues[pixel++];<br/>    addPixelValue(val);<br/>  }<br/>}</span></pre>
<p>As you can see, we iterate over the bitmap's pixels to add them to <kbd>imgData</kbd>. To do so, we use <kbd>addPixelValue</kbd>. This function handles the preprocessing of each pixel. It will be different depending on the characteristics of the model. In our case, the model is using a grayscale image. We must therefore convert each pixel from color to grayscale:</p>
<pre><span>protected void </span><span>addPixelValue</span>(<span>int </span>pixelValue) {<br/>  <span>float </span>mean =  (((pixelValue &gt;&gt; <span>16</span>) &amp; <span>0xFF</span>) + ((pixelValue &gt;&gt; <span>8</span>) &amp; <span>0xFF</span>) + (pixelValue &amp; <span>0xFF</span>)) / <span>3.0f</span><span>;<br/></span><span>  </span><span>imgData</span>.putFloat(mean / 127.5f - <span>1.0f</span>)<span>;<br/></span>}</pre>
<p>In this function, we are using bit-wise operations to compute the mean of the three colors of each pixel. We then divide it by <kbd>127.5</kbd> and subtract <kbd>1</kbd> as this is the preprocessing step of our model.</p>
<p class="mce-root"/>
<p>At the end of this process, <kbd>imgData</kbd> contains the input information in the correct format. Finally, we can run the inference:</p>
<pre><span>float[][] labelProbArray = new float[1][getNumLabels()];<br/>tflite</span>.run(<span>imgData</span><span>, </span><span>labelProbArray</span>)<span>;</span></pre>
<p>The predictions will be inside <kbd>labelProbArray</kbd>. We can then process and display them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running in the browser using TensorFlow.js</h1>
                </header>
            
            <article>
                
<p>With web browsers packing more and more features every year, it was only a matter of time before they could run deep learning models. Running models in the browser has many advantages:</p>
<ul>
<li>The user does not have anything to install.</li>
<li>The computing is done on the user's machine (mobile or computer).</li>
<li>The model can sometimes make use of the device's GPU.</li>
</ul>
<p>The library to run in the browser is called TensorFlow.js (refer to the documentation at <a href="https://github.com/tensorflow/tfjs">https://github.com/tensorflow/tfjs</a>). We will implement our face expression classification application using it.</p>
<div class="packt_infobox">While TensorFlow cannot take advantage of non-NVIDIA GPUs, TensorFlow.js can use GPUs on almost any device. GPU support in the browser was first implemented to display graphical animations through WebGL (a computer graphics API for web applications, based on OpenGL). Since it involves matrix calculus, it was then repurposed to run deep learning operations.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting the model to the TensorFlow.js format</h1>
                </header>
            
            <article>
                
<p>To use TensorFlow.js, the model must first be converted to the correct format using <kbd>tfjs-converter</kbd>. It can convert Keras models, frozen models, and SavedModels. Installation instructions are provided in the GitHub repository.</p>
<p>Then, converting a model is very similar to the process done for TensorFlow Lite. Instead of being done in Python, it is done from the command line:</p>
<pre>$ tensorflowjs_converter ./saved_model --input_format=tf_saved_model my-tfjs --output_format tfjs_graph_model</pre>
<p>Similar to TensorFlow Lite, we need to specify the names of the output nodes.</p>
<p>The output is composed of multiple files:</p>
<ul>
<li><kbd>optimized_model.pb</kbd>: Contains the model graph</li>
<li><kbd>weights_manifest.json</kbd>: Contains information about the list of weights</li>
<li><kbd>group1-shard1of5</kbd>, <kbd>group1-shard2of5</kbd>, ..., <kbd>group1-shard5of5</kbd>: Contains the weights of the model split into multiple files</li>
</ul>
<p>The model is split into multiple files because parallel downloads are usually faster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the model</h1>
                </header>
            
            <article>
                
<p>In our JavaScript app, after importing TensorFlow.js, we can load our model. Note that the following code is in JavaScript. It has a similar syntax to Python:</p>
<pre><span>import </span>* <span>as </span>tf <span>from </span><span>'@tensorflow/tfjs'</span><span>;</span><br/>const model = <span>await </span>tf.<span>loadModel</span>(MOBILENET_MODEL_PATH)<span>;<br/></span></pre>
<p>We will also use a library called <kbd>face-api.js</kbd> to extract faces:</p>
<pre><span>import </span>* <span>as </span>faceapi <span>from </span><span>'face-api.js'</span><span>;<br/></span><span>await </span>faceapi.loadTinyFaceDetectorModel(DETECTION_MODEL_PATH)</pre>
<p>Once both models are loaded, we can start processing images from the user:</p>
<pre><span>const </span>video = document.<span>getElementById</span>(<span>'video'</span>)<span>;</span><span><br/>const </span>detection = <span>await </span>faceapi.<span>detectSingleFace</span>(video<span>, </span><span>new </span>faceapi.TinyFaceDetectorOptions())<span><br/><br/>if </span>(detection) {<br/> <span>const </span>faceCanvases = <span>await </span>faceapi.<span>extractFaces</span>(video<span>, </span>[detection])<br/> <span>const </span>values = <span>await </span><span>predict</span>(faceCanvases[<span>0</span>])<br/>}</pre>
<p>Here, we grab a frame from the <kbd>video</kbd> element displaying the webcam of the user. The <kbd>face-api.js</kbd> library will attempt to detect a face in this frame. If it detects a frame, the part of the image containing the frame is extracted and fed to our model.</p>
<p>The <kbd>predict</kbd> function handles the preprocessing of the image and the classification. This is what it looks like:</p>
<pre><span>async function predict(imgElement) {<br/>  let img = await tf.browser.fromPixels(imgElement, 3).toFloat();<br/><br/>  const logits = tf.tidy(() =&gt; {<br/>    // tf.fromPixels() returns a Tensor from an image element.<br/>    img = tf.image.resizeBilinear(img, [IMAGE_SIZE, IMAGE_SIZE]);<br/>    img = img.mean(2);<br/>    const offset = tf.scalar(127.5);<br/>    // Normalize the image from [0, 255] to [-1, 1].<br/>    const normalized = img.sub(offset).div(offset);<br/>    const batched = normalized.reshape([1, IMAGE_SIZE, IMAGE_SIZE, 1]);<br/><br/>    return mobilenet.predict(batched);<br/>  });<br/><br/>  return logits<br/>}</span></pre>
<p>We first resize the image using <kbd>resizeBilinear</kbd> and convert it from color to grayscale using <kbd>mean</kbd>. We then preprocess the pixels, normalizing them between <kbd>-1</kbd> and <kbd>1</kbd>. Finally, we run the data through <kbd>model.predict</kbd> to get predictions. At the end of this pipeline, we end up with predictions that we can display to the user.</p>
<div class="packt_tip">Note the use of <kbd>tf.tidy</kbd>. This is very important because TensorFlow.js creates intermediate tensors that might never be removed from memory. Wrapping our operations inside <kbd>tf.tidy</kbd> automatically purges intermediate elements from memory.</div>
<p>Over the last few years, technology improvements have made new applications in the browser possible—image classification, text generation, style transfer, and pose estimation are now available to anyone without needing to install anything.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running on other devices</h1>
                </header>
            
            <article>
                
<p>We have covered the conversion of models to run in the browser, and on iOS and Android devices. TensorFlow Lite can also run on the Raspberry Pi, a pocket-sized computer running Linux.</p>
<p>Moreover, devices designed specifically to run deep learning models started to emerge over the years. Here are a few examples:</p>
<ul>
<li><strong>NVIDIA Jetson TX2</strong>: The size of a palm; it is often used for robotics applications.</li>
<li><strong>Google Edge TPU</strong>: A chip designed by Google for IoT applications. It is the size of a nail, and is available with a developer kit.</li>
<li><strong>Intel Neural Compute Stick</strong>: The size of a USB flash drive; it can be connected to any computer (including the Raspberry Pi) to improve its machine learning capabilities.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign">These devices all focus on maximizing computing power while minimizing power consumption. With each generation getting more powerful, the on-device ML field is moving extremely quickly, opening new applications every year.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered several topics on performance. First, we learned how to properly measure the inference speed of a model, and then we went through techniques to reduce inference time: choosing the right hardware and the right libraries, optimizing input size, and optimizing post-processing. We covered techniques to make a slower model appear, to the user, as if it were processing in real time, and to reduce the model size.</p>
<p>Then, we introduced on-device ML, along with its benefits and limitations. We learned how to convert TensorFlow and Keras models to a format that's compatible with on-device deep learning frameworks. With examples on iOS and Android, and in the browser, we covered a wide range of devices. We also introduced some existing embedded devices.</p>
<p>Throughout this book, we have presented TensorFlow 2 in detail, applying it to multiple computer vision tasks. We have covered a variety of state-of-the-art solutions, providing both a theoretical background and some practical implementations. With this last chapter tackling their deployment, it is now up to you to harness the power of TensorFlow 2 and to develop computer vision applications for the use cases of your choice!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>When measuring a model's inference speed, should you measure with a single image or multiple ones?</li>
<li>Is a model with <em>float32</em> weights smaller or larger than one with <em>float16</em> weights?</li>
<li>On iOS devices, should Core ML or TensorFlow Lite be favored? What about Android devices?</li>
<li>What are the benefits and limitations of running a model in the browser?</li>
<li>What is the most important requirement for embedded devices running deep learning algorithms?</li>
</ol>


            </article>

            
        </section>
    </body></html>