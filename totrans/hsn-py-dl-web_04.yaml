- en: Getting Started with Deep Learning Using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, we had a very close look at deep learning and how it is
    related to machine learning and artificial intelligence. In this chapter, we are
    going to delve deeper into this topic. We will start off by learning about what
    sits at the heart of deep learning—namely, neural networks and their fundamental
    components, such as neurons, activation units, backpropagation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this chapter is not going to be too math heavy, but at the same time,
    we are not going to cut short the most important formulas that are fundamental
    to the world of neural networks. For a more math-heavy study of the subject, readers
    are encouraged to read the book *Deep Learning* ([deeplearningbook.org](http://deeplearningbook.org))
    by Goodfellow et al.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an overview of what we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A whirlwind tour of neural networks and their related concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning versus shallow learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a deep-learning-based cloud environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Jupyter Notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start this section by finding the answers to the question, “Why are neural
    networks called 'neural'?”. What is the significance behind this term?
  prefs: []
  type: TYPE_NORMAL
- en: Our intuition says that it has something to do with our brains, which is correct,
    but only partially. Before we get to the reason why it is only partially correct,
    we need to have some familiarity with the structure of a brain. For this purpose,
    let's look at the anatomy of our own brains.
  prefs: []
  type: TYPE_NORMAL
- en: A human brain is composed of approximately 10 billion *neurons*, each connected
    to about 10,000 other neurons, which gives it a network-like structure. The inputs
    to the neurons are called *dendrites* and the outputs are called *axons*. The
    body of a neuron is called a *soma*. So, on a high level, a particular soma is
    connected to another soma. The word "neural" comes from the word "neuron," and
    in fact, neural is the adjective form of the word "neuron." In our brains, neurons
    are the most granular units that form this dense network we just discussed. We
    are slowly understanding the resemblance of an artificial neural network to the
    brain, and in order to continue our understanding of this similarity, we will
    briefly learn about the functionalities of a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: A network is nothing but a graph-like structure that contains a set of nodes
    and edges that are connected to each other. In the case of our brains, or any
    brain in general, neurons are referred to as nodes and the dendrites are referred
    to as the vertices.
  prefs: []
  type: TYPE_NORMAL
- en: A neuron receives inputs from other neurons via their dendrites. These inputs
    are electrochemical in nature. Not all the inputs are equally powerful. If the
    inputs are powerful enough, then the connected neurons are activated and continue
    the process of passing the input to the other neurons. Their power is determined
    by a predefined threshold that allows the activation process to be selective so
    that it does not activate all the neurons that are present in the network at the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, neurons receive a collective sum of inputs from other neurons,
    this sum is compared to a threshold, and the neurons are activated accordingly.
    An **artificial neural network** (**ANN**), or simply a **neural network** (**NN**),
    is based on this important fact, hence the resemblance.
  prefs: []
  type: TYPE_NORMAL
- en: So, what makes a network a *neural* one? What does it take to form an NN?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following quote from the book *Deep Learning For Computer Vision With Python*
    by Adrian Rosebrock answers this question in a very commendable way:'
  prefs: []
  type: TYPE_NORMAL
- en: Each node performs a simple computation. Each connection then carries a signal
    (i.e., the output of the computation) from one node to another, labeled by a weight
    indicating the extent to which the signal is amplified or diminished. Some connections
    have large, positive weights that amplify the signal, indicating that the signal
    is very important when making a classification. Others have negative weights,
    diminishing the strength of the signal, thus specifying that the output of the
    node is less important in the final classification. We call such a system an Artificial
    Neural Network if it consists of a graph structure with connection weights that
    are modifiable using a learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned about the resemblance of neural networks to brains. We will
    now take this information and learn more about the granular units of ANNs. Let's
    start by learning what a simple neuron has to do in an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s call the neurons that are used in ANNs artificial neurons. Broadly speaking,
    artificial neurons can be of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonlinear neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anatomy of a linear neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A neuron is the most granular unit in a neural network. Let''s look at the
    second word of "neural network." A network is nothing but a set of vertices (also
    called nodes) whose edges are connected to each other. In the case of a neural
    network, neurons serve as the nodes. Let''s consider the following neural network
    architecture and try to dissect it piece by piece:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5502868-fd41-47b9-859a-f36bcfd8020e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What we can see in the preceding diagram is a neural network with two hidden
    layers (in a neural network, a layer is a set of neurons) with a single output.
    In fact, this is called a two-layer neural network. The neural network consists
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One single input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two hidden layers, where the first hidden layer has three neurons and the second
    hidden layer contains two neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One single output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no deeper psychological significance in calling the layers hidden they
    are called hidden simply because the neurons involved in these layers are neither
    parts of the input nor output. One thing that is very evident here is that there
    is a layer before the first hidden layer. Why are we not counting that layer?
    In the world of neural networks, that initial layer and output are not counted
    in the stack of layers. In simple words, if there are *n* hidden layers, it is
    an *n*-layer neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The initial layer (also called an input layer) is used for receiving primary
    input to the neural network. After receiving the primary input, the neurons present
    in the input layer pass them to the next set of neurons that are present in the
    subsequent hidden layers. Before this propagation happens, the neurons add weights
    to the inputs and a bias term to the inputs. These inputs can be from various
    domains—for example, the inputs can be the raw pixels of an image, the frequencies
    of an audio signal, a collection of words, and so on. Generally, these inputs
    are given as feature vectors to the neural network. In this case, the input data
    has only one feature.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what are the neurons from the next two layers doing here? This is an important
    question. We can consider the addition of weights and biases to the inputs as
    the first level/layer of learning (also called the decision making layer). The
    neurons in the initial hidden layer repeat this process, but before sending the
    calculated output to the neurons that are present in the next hidden layer, they
    compare this value to a threshold. If the threshold criteria are satisfied, then
    only the outputs are propagated to the next level. This part of the whole neural
    network learning process bears a solid resemblance to the biological process that
    we discussed earlier. This also supports the philosophy of learning complex things
    in a layered fashion.
  prefs: []
  type: TYPE_NORMAL
- en: A question that is raised here is, "What happens if no hidden layers are used?".
    It turns out that adding more levels of complexity (by adding more layers) in
    a neural network allows it to learn the underlying representations of the input
    data in a more concise manner than a network with just the input layer and the
    output. But how many layers would we need? We will get to that later.
  prefs: []
  type: TYPE_NORMAL
- en: Let's introduce some mathematical formulas here to formalize what we just studied.
  prefs: []
  type: TYPE_NORMAL
- en: 'We express the input features as *x*, the weights as *w*, and the bias term
    as *b*. The neural network model that we are currently trying to dissect builds
    upon the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73d8417a-6a5d-48b3-9d3c-2df95ad05b55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The rule says that after calculating the sum of weighted input and the bias,
    if the result is greater than 0, then the neuron is going to yield 1, and if the
    result is less than or equal to 0, then the neuron is simply going to produce
    0 in other words, the neuron is not going to fire. In the case of multiple input
    features, the rule remains exactly the same and the multivariate version of the
    rule looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd99fc73-d225-45bc-9dcb-8ae25e9d5647.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *i* means that we have a total of *i* input features. The preceding rule
    can be broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We take the features individually, and then we multiply them by the weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After finishing this process for all the individual input features, we take
    all of the weighted inputs and sum them and finally add the bias term.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding process is continued for the number of layers we have in our network.
    In this case, we have two hidden layers, so the output of one layer would be fed
    to the next.
  prefs: []
  type: TYPE_NORMAL
- en: The elements we just studied were proposed by Frank Rosenblatt in the 1960s.
    The idea of assigning 0 or 1 to the weighted sum of the inputs based on a certain
    threshold is also known as the **step-function**. There are many rules like this
    in the literature, these are called update rules.
  prefs: []
  type: TYPE_NORMAL
- en: The neurons we studied are **linear neurons** that are capable of learning linear
    functions. They are not suited for learning representations that are nonlinear
    in nature. Practically, almost all the inputs that neural networks are fed with
    are nonlinear in nature. In the next section, we are going to introduce another
    type of neuron that is capable of capturing the nonlinearities that may be present
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Some of you might be wondering whether this NN model is called an **MLP** (**multilayer**
    **perceptron**). Well, it is. In fact, Rosenblatt proposed this way back in the
    1960s. Then what are neural networks? We are going to learn the answer to this
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of a nonlinear neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A nonlinear neuron means that it is capable of responding to the nonlinearities
    that may be present in the data. Nonlinearity in this context essentially means
    that for a given input, the output does not change in a linear way. Look at the
    following diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43498898-28e2-44bf-a26c-a071493c6de0.png)'
  prefs: []
  type: TYPE_IMG
- en: Both of the preceding figures depict the relationship between the inputs that
    are given to a neural network and the outputs that the network produces. From
    the first figure, it is clear that the input data is linearly separable, whereas
    the second figure tells us that the inputs cannot be linearly separated. In cases
    like this, a linear neuron will miserably fail, hence the need for nonlinear neurons.
  prefs: []
  type: TYPE_NORMAL
- en: In the training process of a neural network, conditions can arise where a small
    change in the bias and weight values may affect the output of the neural network
    in a drastic way. Ideally, this should not happen. A small change to either the
    bias or weight values should cause only a small change in the output. When a step
    function is used, the changes in weight and bias terms can affect the output to
    a great extent, hence the need for something other than a step function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Behind the operation of a neuron sits a function. In the case of the linear
    neuron, we saw that its operations were based on a step function. We have a bunch
    of functions that are capable of capturing the nonlinearities. The sigmoid function
    is such a function, and the neurons that use this function are often called sigmoid
    neurons. Unlike the step function, the output in the case of a sigmoid neuron
    is produced using the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c07baaa1-6d33-4e1b-b759-e81faa535390.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, our final, updated rule becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f641c5d4-03a7-4cf6-8f46-eacadf6206f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But why is the sigmoid function better than a step function in terms of capturing
    nonlinearities? Let''s compare their performance in graphical to understand this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c67899b6-21c7-48ed-adf3-4fd1efd01d29.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding two figures give us a clear picture about the two functions regarding
    their intrinsic nature. It is absolutely clear that the sigmoid function is more
    sensitive to the nonlinearities than the step function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the sigmoid function, the following are some widely known and used
    functions that are used to give a neuron a nonlinear character:'
  prefs: []
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the literature, these functions, along with the two that we have just studied,
    are called activation functions. Currently, ReLU and its variants are by far the
    most successful activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are still left with a few other basic things related to artificial neural
    networks. Let''s summarize what we have learned so far:'
  prefs: []
  type: TYPE_NORMAL
- en: Neurons and their two main types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are now in a position to draw a line between MLPs and neural networks. Michael
    Nielson in his online book *Neural Networks and Deep Learning* describes this
    quite well:'
  prefs: []
  type: TYPE_NORMAL
- en: Somewhat confusingly, and for historical reasons, such multiple layer networks
    are sometimes called *multilayer perceptrons* or *MLPs*, despite being made up
    of sigmoid neurons, not perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the neural network and deep neural network terminologies
    throughout this book. We will now move forward and learn more about the input
    and output layers of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: A note on the input and output layers of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to understand what can be given as inputs to a neural network.
    Do we feed raw images or raw text data to a neural network? Or are there other
    ways to provide input to a neural network? In this section, we will learn how
    a computer really interprets an image to show what exactly can be given as input
    to a neural network when it is dealing with images (yes, neural networks are pretty
    great at image processing). We will also learn the ways to show what it takes
    to feed a neural network with raw text data. But before that, we need to have
    a clear understanding of how a regular tabular dataset is given as an input to
    a neural network. Because tabular datasets are everywhere, in the form of SQL
    tables, server logs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take the following toy dataset for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ffcb3f-b5eb-4b0c-8ec6-a364fa7a3775.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take note of the following points regarding this toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: It has two predictor variables, *x1* and *x2*, and these predictors are generally
    called input feature vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is common to assign *x1* and *x2* to a vector, *X* (more on this later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response variable is *y*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have 10 instances (containing *x1*, *x2*, and *y* attributes) that are categorized
    into two classes, 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given *x1* and *x2*, our (neural network's) task is to predict *y*, which essentially
    makes this a classification task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we say that the neural network predicts something, we mean that it is supposed
    to learn the underlying representations of the input data that best approximate
    a certain function (we saw what function plotting look like a while ago).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see how this data is given as inputs to a neural network. As our
    data has two predictor variables (or two input vectors), the input layer of the
    neural network has to contain two neurons. We will use the following neural network
    architecture for this classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdab7ceb-ab2d-45c2-8c77-c896a0bfb2c5.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture is quite identical to the one that we saw a while ago, but
    in this case, we have an added input feature vector. The rest is exactly the same.
  prefs: []
  type: TYPE_NORMAL
- en: To keep it simple, we are not considering the data preprocessing that might
    be needed before we feed the data to the network. Now, let's see how the data
    is combined with the weights and the bias term, and how the activation function
    is applied to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the feature vectors and the response variable (which is *y*)
    are interpreted separately by the neural network the response variable is used
    in the later stage in the network''s training process. Most importantly, it is
    used for evaluating how the neural network is performing. The input data is organized
    as a matrix form, like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75262c2f-fdbc-41c3-b461-9a04b2d24de7.png)'
  prefs: []
  type: TYPE_IMG
- en: The kind of NN architecture that we are using now is a fully connected architecture,
    which means that all of the neurons in a particular layer are connected to all
    the other neurons in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight matrix is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c7cc6d2-b697-492b-b277-1a3217a04fe2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For now, let''s not bother about the weight values. The dimensions of the weight
    matrix is interpreted as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of rows equals the number of feature vectors (*x1* and *x2*, in our
    case).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of columns equals the number of neurons in the first hidden layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some suffixes and superscripts associated with each of the weight
    values in the matrix. If we take the general form of the weight as ![](img/c16cd84f-3e3a-42e5-b619-a628b3160c0c.png),
    then it should be interpreted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*l* denotes the layer from which the weight is coming. In this case, the weight
    matrix that we just saw is going to be associated with the input layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*j* denotes the position of the neuron in ![](img/ef96922d-fbab-4bf0-bac4-07dc23ece429.png),
    whereas *k* denotes the position of the neuron in the next layer that the value
    is propagated to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The weights are generally randomly initialized, which adds a *stochastic* character
    to the neural network. Let''s randomly initialize a weight matrix for the input
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28fe1ff8-96fa-4c85-ac98-fa1c4d0ec5db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we calculate the values that are to be given to the first hidden layer
    of the NN. This is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4716dd77-2dfc-490d-b796-aa1215cab251.png)'
  prefs: []
  type: TYPE_IMG
- en: The first matrix contains all the instances from the training set (without the
    response variable *y*) and the second matrix is the weight matrix that we just
    defined. The result of this multiplication is stored in a variable, ![](img/a85b6a85-7af3-46bc-aa93-70b13630a83d.png)
    (this variable can be named anything, and the superscript denotes that it is related
    to the first hidden layer of the network).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are still left with one more step before we send these results to the neurons
    in the next layer, where the activation functions will be applied. The sigmoid
    activation function and the final output from the input layer would look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20a23d3e-6c8c-44ac-874b-3e088653fec9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *a^((1))* is our final output for the next layer of neurons. Note that
    the sigmoid function is applied to each and every element of the ![](img/ad7faaaf-0c2c-4576-92b4-6b1cf41f2489.png)
    matrix. The final matrix will have a dimension of 10 X 3, where each row is for
    each instance from the training set and each column is for each neuron of the
    first hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole calculation that we saw is without the bias term, *b*, that we initially
    talked about. Well, that is just a matter the of addition of another dimension
    to the picture. In that case, before we apply the sigmoid function to each of
    the elements of the ![](img/2f62dd1e-ca80-4973-a7da-b0b7622410bc.png) matrix,
    the matrix itself would be changed to something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/541cba3d-b30e-416a-8eac-8a9b0ec6f6f1.png)'
  prefs: []
  type: TYPE_IMG
- en: After this matrix multiplication process, the sigmoid function is applied and
    the output is sent to the neurons in the next layers, and this whole process repeats
    for each hidden layer and output layer that we have in the NN. As we proceed,
    we are supposed to get ![](img/7ca8c778-786e-4432-b25f-0409a6d2ef8c.png) from
    the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid activation function outputs values ranging from 0–1, but we are
    dealing with a binary classification problem, and we only want 0 or 1 as the final
    output from the NN. We can do this with a little tweak. We can define a threshold
    at the output layer of the NN—for the values that are less than 0.5 they should
    be identified as class 0 and the values that are greater than or equal to 0.5
    should be identified as class 1\. Note that this is called forward pass or forward
    propagation.
  prefs: []
  type: TYPE_NORMAL
- en: The NN we just saw is referred to as a feed-forward network with no further
    optimization in its learning process. But wait! What does the network even learn?
    Well, an NN typically learns the weights and bias terms so that the final output
    is as accurate as possible. And this happens with gradient descent and backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent and backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start learning about what gradient descent and backpropagation have
    to do in the context of neural networks, let's learn what is meant by an optimization
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'An optimization problem, briefly, corresponds to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing a certain cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximizing a certain profit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now try to map this to a neural network. What happens if, after getting
    the output from a feed-forward neural network, we find that its performance is
    not up to the mark (which is the case almost all the time)? How are we going to
    enhance the performance of the NN? The answer is gradient descent and backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to optimize the learning process of the neural network with these
    two techniques. But what are we going to optimize? What are we going to minimize
    or maximize? We require a specific type of cost that we will attempt to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: We will define the cost in terms of a function. Before we define a cost function
    for the NN model, we will have to decide the parameters of the cost function.
    In our case, the weights and the biases are the parameters of the function that
    the NN is trying to learn to give us accurate results (see the information box
    just before this section). In addition, we will have to calculate the amount of
    loss that the network is inculcating at each step of its training process.
  prefs: []
  type: TYPE_NORMAL
- en: For a binary classification problem, a loss function called a **cross-entropy**
    loss function (for a binary classification problem it is called a binary cross
    cross-entropy loss function) is widely used, and we are going to use it. So, what
    does this function look like?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d3375fb-d8cf-49aa-ba7e-26efe4d6b342.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* denotes the ground truth or true label (remember the response variable,
    *y*, in the training set) of a given instance and ![](img/e50fa991-8408-4440-88e2-cd37d88e29a1.png) denotes
    the output as yielded by the NN model. This function is convex in nature, which
    is just perfect for convex optimizers such as gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the reasons that we didn't pick up a simpler and nonconvex loss
    function. (Don't worry if you are not familiar with terms like convex and nonconvex.)
  prefs: []
  type: TYPE_NORMAL
- en: We have our loss function now. Keep in mind that this is just for one instance
    of the entire set of data this is not the function on which we are going to apply
    gradient descent. The preceding function is going to help us define the cost function
    that we will eventually optimize using gradient descent. Let's see what that cost
    function looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fe2f681-2cd6-4422-b942-fba491e140e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w* and *b* are the weights and biases that the network is trying to
    learn. The letter *m* represents the number of training instances, which is 10
    in this case. The rest seems familiar. Let''s put the original form of the function,
    *L()*, and see what *J()* looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc0b7b74-497f-4bae-8f14-a5661a0c48af.png)'
  prefs: []
  type: TYPE_IMG
- en: The function may look a bit confusing, so just slow it down and make sure you
    understand it well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can finally move toward the optimization process. Broadly, gradient descent
    is trying to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Give us a point where the cost function is as minimal as possible (this point
    is called the minima).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give us the right values of the weights and biases so that the cost function
    reaches that point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To visualize this, let''s take a simple convex function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b648cee7-9c85-4912-a310-764c72904530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, say we start the journey at a random point, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7d2ca94-6643-41d4-ac7d-aa8bdb593cff.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the point at the top right corner is the point at which we started. And
    the point (directed by the dotted arrow) is the point we wish to arrive at. So,
    how do we do this in terms of simple computations?
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to arrive at this point the following update rule is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/932dc7a9-29ef-438c-ae22-d0397b9fab67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we are taking the partial derivative of *J(w,b)* with respect to the
    weights. We are taking a partial derivative because *J(w,b)* contains *b* as one
    of the parameters. 𝝰 is the learning rate that speeds up this process. This update
    rule is applied multiple times to find the right values of the weights. But what
    about the bias values? The rule remains exactly the same only the equation is
    changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0505bb51-e6be-440d-86e5-3774b8532cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: These new assignments of weights and biases are essentially referred to as *backpropagation*,
    and it is done in conjunction with *gradient descent*. After computing the new
    values of the weights and the biases, the whole forward propagation process is
    repeated until the NN model generalizes well. Note that these rules are just for
    one single instance, provided that the instance has only one feature. Doing this
    for several instances that contain several features can be difficult, so we are
    going to skip that part however, those who are interested in seeing the fully
    fledged version of this may refer to a lecture online by Andrew Ng.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have covered the necessary fundamental units of a standard neural network,
    and it was not easy at all. We started by defining neurons and we ended with backprop
    (the nerdy term of backpropagation). We have already laid the foundations of a
    deep neural network. Readers might be wondering whether that was a deep neural
    net that we just studied. As **Andriy Burkov** says (from his book titled *The
    Hundred Page Machine-Learning Book*):'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning refers to training neural networks with more than two non-output
    layers. … the term “deep learning” refers to training neural networks using the
    modern algorithmic and mathematical toolkit independently of how deep the neural
    network is. In practice, many business problems can be solved with neural networks
    having 2-3 layers between the input and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will learn about the difference between deep learning
    and shallow learning. We will also take a look at two different types of neural
    networks—namely, convolutional neural networks and recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned what feed-forward neural networks look like and how
    techniques such as backpropagation and gradient descent are applied to it in order
    to optimize their training process. The binary classification problem we studied
    earlier appears to be too naive and too impractical, doesn't it?
  prefs: []
  type: TYPE_NORMAL
- en: Well, there are many problems that a simple NN model can solve well. But as
    the complexity of the problem increases, improvements to the basic NN model become
    necessary. These complex problems include object detection, object classification,
    image-caption generation, sentiment analysis, fake-news classification, sequence
    generation, speech translation, and so on. For problems like these, a basic NN
    model is not sufficient. It needs some architectural improvements so that it can
    solve these problems. In this section, we are going to study two of the most powerful
    and widely used NN models—convolutional neural networks and recurrent neural networks.
    At the heart of the stunning applications of deep learning that we see nowadays
    sit these NN models.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever uploaded a photo of your friends' group to Facebook? If yes, have
    you ever wondered how Facebook detects all the faces in the photo automatically
    just after the upload finishes? In short, the answer is **convolutional neural
    networks** (**CNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: A feed-forward network generally consists of several fully connected layers,
    whereas a CNN consists of several layers of convolution, along with other types
    of sophisticated layers, including fully-connected layers. These fully-connected
    layers are generally placed towards the very end and are typically used for making
    predictions. But what kinds of predictions? In an image-processing and computer-vision
    context, a prediction task can encompass many use cases, such as identifying the
    type of object present in the image that is given to the network. But are CNNs
    only good for image-related tasks? CNNs were designed and proposed for image-processing
    tasks (such as object detection, object classification, and so on) but it has
    found its use in many text-processing tasks as well. We are going to learn about
    CNNs in an image-processing context because CNNs are most popular for the wonders
    they can work in the domains of image processing and computer vision. But before
    we move on to this topic, it would be useful to understand how an image can be
    represented in terms of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'An image consists of numerous pixels and dimensions—height x width x depth.
    For a color image, the depth dimension is generally 3, and for a grayscale image,
    the dimension is 1\. Let''s dig a bit deeper into this. Consider the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79d278c8-9d3b-409e-ae79-de957bb6251c.png)'
  prefs: []
  type: TYPE_IMG
- en: The dimension of the preceding image is 626 x 675 x 3, and numerically, it is
    nothing but a matrix. Each pixel represents a particular intensity of red, green,
    and blue (according to the RGB color system). The image contains a total of 422,550
    pixels (675 x 626).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pixels are denoted by a list of three values of red, green, and blue colors.
    Let''s now see what a pixel (corresponding to the twentieth row and the hundredth
    column in the matrix of 422,550 pixels) looks like in coding terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each value corresponds to a particular intensity of the colors red, green, and
    blue. For the purpose of understanding CNNs, we will look at a much smaller dimensional
    image in grayscale. Keep in mind that each pixel in a grayscale image is between
    0 and 255, where 0 corresponds to black and 255 corresponds to white.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a dummy matrix of pixels representing a grayscale image (we
    will refer to this as an image matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a24eda9c-4190-41e1-bc67-d4d3a775f1d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we proceed, let''s think intuitively about how can we train a CNN to
    learn the underlying representations of an image and make it perform some tasks.
    Images have a special property inherent to them: the pixels in an image that contain
    a similar type of information generally remain close to each other. Consider the
    image of a standard human face: the pixels denoting the hair are darker and are
    closely located on the image, whereas the pixels denoting the other parts of the
    face are generally lighter and also stay very close to each other. The intensities
    may vary from face to face, but you get the idea. We can use this spatial relationship
    of the pixels in an image and train a CNN to detect the similar pixels and the
    edges that they create in between them to distinguish between the several regions
    present in an image (in an image of a face, there are arbitrary edges in between
    the hair, eyebrows, and so on). Let''s see how this can be done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN typically has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the heart of a CNN sits an operation called convolution (which is also known
    as cross relation in the literature of computer vision and image processing).
    Adrian Rosebrock of PyImageSearch describes the operation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of deep learning, an (image) convolution is an element-wise multiplication
    of two matrices followed by a sum.
  prefs: []
  type: TYPE_NORMAL
- en: 'This quote tells us how an (image) convolution operator works. The matrices
    mentioned in the quote are the image matrix itself and another matrix known as
    the kernel. The original image matrix can be higher than the kernel matrix and
    the convolution operation is performed on the image matrix in a left–right top–bottom
    direction. Here is an example of a convolution operation involving the preceding
    dummy matrix and a kernel of size 2 x 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81b66b3b-aaa5-45a5-82d4-e14bfc38ecc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The kernel matrix actually serves as the weight matrix for the network, and
    to keep it simple, we ignore the bias term for now. It is also worth noting that
    our favorite image filters (sharpening, blurring, and so on) are nothing but outputs
    of certain kinds of convolution applied to the original images. A CNN actually
    learns these filter (kernel) values so that it can best capture the spatial representation
    of an image. These values can be further optimized using gradient descent and
    backpropagation. The following figure depicts four convolution operations applied
    to the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f1deaa5-9ebe-413d-8869-8a3b87ddc294.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note how the kernel is sliding and how the convoluted pixels are being calculated.
    But if we proceed like this, then the original dimensionality of the image gets
    lost. This can cause information loss. To prevent this, we apply a technique called
    padding and retain the dimensionality of the original image. There are many padding
    techniques, such as replicate padding, zero padding, wrap around, and so on. Zero
    padding is very popular in deep learning. We will now see how zero padding can
    be applied to the original image matrix so that the original dimensionality of
    the image is retained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c102f556-b78a-45cb-b402-302b91561d97.png)'
  prefs: []
  type: TYPE_IMG
- en: Zero padding means that the pixel value matrix will be padded by zero on all
    sides, as shown in the preceding image.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to instruct the network how it should slide the image matrix.
    This is controlled using a parameter called stride. The choice of stride depends
    on the dataset and the correct use of stride 2 is standard practice in deep learning.
    Let''s see how stride 1 differs from stride 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee0eddc-8450-4997-ab80-615b4b4155c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A convoluted image typically looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e10b613-f9a8-49eb-b1d6-d6e2072fcff4.png)'
  prefs: []
  type: TYPE_IMG
- en: The convoluted image largely depends on the kernel that is being used. The final
    output matrix is passed to an activation function and the function is applied
    to the matrix's elements. Another important operation in a CNN is pooling, but
    we will skip this for now. By now, you should have a good understanding of how
    a CNN works on a high level, which is sufficient for continuing to follow the
    book. If you want to have a deeper understanding of how a CNN works, then refer
    to the blog post at [https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/](https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent neural networks (RNNs)** are another type of neural network, and
    are tremendously good at NLP tasks—for example, sentiment analysis, sequence prediction,
    speech-to-text translation, language-to-language translation, and so on. Consider
    an example: you open up Google and you start searching for recurrent neural networks.
    The moment you start typing a word, Google starts giving you a list of suggestions
    which is most likely to be topped by the complete word, or the most commonly searched
    phrase that begins with the letters you have typed by then. This is an example
    of sequence prediction where the task is to predict the next sequence of the given
    phrase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take another example: you are given a bunch of English sentences containing
    one blank per sentence. Your task is to appropriately fill the gaps with the correct
    words. Now, in order to do this, you will need to use your previous knowledge
    of the English language in general and make use of the context as much as possible.
    To use previously encountered information like this, you use your memory. But
    what about neural networks? Traditional neural networks cannot do this because
    they do not have any memory. This is exactly where RNNs come into the picture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The question that we need to answer is how can we empower neural networks with
    memory? An absolutely naive idea would be to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Feed a certain sequence into a neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the output of the neuron and feed it to the neuron again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It turns out that this idea is not that naive, and in fact constitutes the
    foundation of the RNN. A single layer of an RNN actually looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/598b05d6-11ee-4c08-9c52-9bebfd97f229.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loop seems to be a bit mysterious. You might already be thinking about
    what happens in each iteration of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4865606d-533c-4d58-a488-c425003e96e4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, an RNN (the figure on the left) is unrolled to show
    three simple feedforward networks. But what do these unrolled networks do? Let's
    find this out now.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the task of sequence prediction. To keep it simple, we will look
    at how an RNN can learn to predict the next letter to complete a word. For example,
    if we train the network with a set of letters, *{w, h, a, t}*, and after giving
    the letters *w,h*, and *a* sequentially, the network should be able to predict
    that the letter should be *t* so that the meaningful word "what" is produced.
    Just like the feed-forward networks we saw earlier, *X* serves as the input vector
    to the network in RNN terminology, this vector is also referred to as the vocabulary
    of the network. The vocabulary of the network is, in this case, *{w, h, a, t}*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network is fed with the letters *w,h*, and *a* sequentially. Let''s try
    to give indices to the letters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/454ba3b9-649e-4cae-877b-6b2cde5bf155.png)→ ![](img/f653ecc8-9aae-468b-983e-256285270df6.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/173cfead-90f6-4776-83e6-208f4919ba20.png)→ ![](img/a2112f9b-9250-42c1-85c0-9be6fab58c48.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/3e03023b-b603-4d60-bb97-cb51c112e8c1.png)→ ![](img/2815c5c2-68c2-44ae-a545-8a9227b5685e.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: These indices are known as time-steps (the superscripts in the figure presenting
    the unrolling of an RNN). A recurrent layer makes use of the input that is given
    at previous time-steps, along with a function when operating on the current time-step.
    Let's see how the output is produced by this recurrent layer step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding the letters to the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we see how a recurrent layer produces the output, it is important to
    learn how we can feed the set of letters to the networks. One-hot encoding lets
    us do this in a very efficient way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82595ac0-af4d-4777-8394-1939beb4c4b7.png)'
  prefs: []
  type: TYPE_IMG
- en: So, in one-hot encoding, our input vectors/vocabulary of letters are nothing
    but four 4 x 1 matrices, each denoting a particular letter. One-hot encoding is
    standard practice for these tasks. This step is actually a data-preprocessing
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the weight matrix and more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When there are neural networks, there are weights. This is true, right? But
    before we start to deal with the weights for our RNN, let's see exactly where
    they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different weight matrices in the case of an RNN—one for the input
    neuron (remember that we feed feature vectors only through neurons) and one for
    the recurrent neuron. A particular state in an RNN is produced using the following
    two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a36036b-7a4b-4307-b60b-5031283c9060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand what each term means in the first equation, refer to the following
    image (don''t worry, we will get to the second equation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bbaa47e-7702-433d-aac9-870000b96aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first pass of the RNN ![](img/4b9aaa2a-87d5-487b-9d8c-6a54cb705d09.png)is
    the letter *w*. We will randomly initialize the two weight matrices as present
    in the equation (1). Assume that the matrix ![](img/06871357-5a95-4e0b-aee0-90bfb11dcc7c.png)
    after getting initialized looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94ec44a4-d673-4c34-854b-e1d35c371270.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ![](img/6ccce969-b83d-44cd-87d9-647322b6f44a.png) matrix is 3 x 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* = 3, as we have three recurrent neurons in the recurrent layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h* = 4, as our vocabulary is 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix ![](img/a1666498-816b-4b2b-87a6-4c777b0e2976.png) is a 1 x 1 matrix.
    Let's take its value as 0.35028053\. Let's also introduce the bias term *b* here,
    which is also a 1 x 1 matrix, 0.6161462\. In the next step, we will put these
    values together and determine the value of ![](img/8d4bf29d-dca0-4114-9f7b-79469774b4a9.png).
    (We will deal with the second equation later.)
  prefs: []
  type: TYPE_NORMAL
- en: Putting the weight matrices together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s determine ![](img/c1ccae7d-7e2d-4c5c-9d8f-8f3d7d29a2bf.png) first. ![](img/d9784c61-5546-4ef2-8485-d134d1b8c132.png)is
    a 4 x 1 matrix representing the letter *w*, which we defined earlier. The standard
    rules of matrix multiplication apply here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/234a5eeb-d4fd-4fb8-ad16-468a49bb1010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will calculate the term ![](img/a1b2ed80-6b11-4cdd-b69f-4dd54bf630b9.png).
    We will shortly see the significance of the bias term. Since *w* is the first
    letter that we are feeding to the network, it does not have any previous state
    therefore, we will take ![](img/47774b6e-33a9-495f-9c90-7a98bc56ad42.png) as a
    matrix of 3 x 1 consisting of zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dcad348-20fc-4827-96c5-f4e62ffd10bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that if we didn''t take the bias term, we would have got a matrix consisting
    of only zeros. We will now add these two matrices as per the equation (1). The
    result of this addition is a 3 x 1 matrix and is stored in ![](img/fe80e464-21d6-468f-bbc8-1a23d3a7a9a5.png)
    (which in this case is ![](img/def168f0-bef1-4905-8c0d-1ced2411df48.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d9339a0-4050-4c6f-8756-c05e881ccb3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Following the equation (1), all we need to do is apply the activation function
    to this matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Applying activation functions and the final output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to RNNs, ![](img/ff3c61ce-bd96-4088-8fbe-4cf175154e52.png) is
    a good choice as an activation function. So, after applying ![](img/00494d15-6793-4ec3-9fb8-3c3f40d6c9d9.png),
    the matrix looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fefb7cb2-a21e-4ac2-8aaa-84401ab2bf71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have got the result of ![](img/5a3592f6-3ebd-4459-aa50-dfaa77b7eedd.png).
    *ht* acts as ![](img/1f0a601e-c001-4c1a-86b6-84ffa79cc16e.png) for the next time-step.
    We will now calculate the value of ![](img/62216a8f-4753-45e0-853e-a73540ac5f36.png)
    using equation (2). We will require another weight matrix ![](img/7e9e4820-3c32-47c8-9529-a914a16cdd59.png)
    (of shape 4 x 3) that is randomly initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/281bbcd0-d1ba-46a1-9998-7609b6319c5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After applying the second equation, the value of ![](img/d7a94c7b-b682-4671-99b8-63180437e306.png)
    becomes a 4 x 1 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a969531-c833-44fd-8c6f-f034c65b267d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, in order to predict what might be the next letter that comes after *w*
    (remember, we started all our calculations with the letter *w* and we still left
    with the first pass of the RNN) to make a suitable word from the given vocabulary,
    we will apply the softmax function to ![](img/220321b9-76f4-476f-9fac-b77514106737.png).
    This will output a set of probabilities for each of the letters from the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20663bb4-96fe-4e90-bf5e-5ced2e6e9e10.png)'
  prefs: []
  type: TYPE_IMG
- en: If anyone is curious about learning what a softmax function looks like, there
    is an extremely helpful article at [http://bit.ly/softmaxfunc](http://bit.ly/softmaxfunc).
  prefs: []
  type: TYPE_NORMAL
- en: So, the RNN tells us that the next letter after *w* is more likely to be an
    ![](img/fdcd5b37-b1e7-4248-8ec0-1569c96a25a8.png). With this, we finish the initial
    pass of the RNN. As an exercise, you can play around with the *ht* value we got
    from this pass and apply it (along with the next letter *h*) to the next pass
    of the RNN to see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get to the most important question—what is the network learning?
    Again, weights and biases! You might have guessed the next sentence already. These
    weights are further optimized using backpropagation. Now, this backpropagation
    is a little bit different from what we have seen earlier. This version of backpropagation
    is referred to as **backpropagation through time**. We won''t be learning about
    this. Before finishing off this section, let''s summarize the steps (after one-hot
    encoding of the vocabulary) that were performed during the forward pass of the
    RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weight matrices randomly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate ![](img/06efe53c-1ce5-45a5-9526-995ad648f78c.png) using equation (1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate ![](img/71c99373-2b99-42eb-9199-be1382be273b.png) using equation (2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the softmax function to ![](img/fbe962da-d664-4cf8-9c26-31ddc850c175.png)to
    get the probabilities of each of the letters in the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is good to know that apart from CNNs and RNNs, there are other types of neural
    networks, such as auto-encoders, generative adversarial networks, capsule networks,
    and so on. In the previous two sections, we learned about two of the most powerful
    types of neural network in detail. But when we talk about cutting-edge deep-learning
    applications, are these networks good enough to be used? Or do we need more enhancements
    on top of these? It turns out that although these architectures perform well,
    they fail to scale, hence the need for more sophisticated architectures. We will
    get to some of these specialized architectures in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a good amount of theory since [Chapter 1](f97d928f-3614-4d12-ad37-d5736008f542.xhtml),
    *Demystifying Artificial Intelligence and Fundamentals of Machine Learning*. In
    the next few sections, we will be diving into some hands-on examples.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Jupyter Notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While working on a project relating to deep learning, you must deal with a huge
    amount of variables of various types and arrays of various dimensions. Also, since
    the data contained in them is massive and keeps changing after nearly every step,
    we need a tool that helps us to observe the output produced by each step so that
    we can proceed accordingly. A Jupyter Notebook is one such tool. Jupyter Notebooks
    are known for their simplicity, and their wide support of features and platforms
    are currently the standard tool for developing deep-learning solutions. The reasons
    for their popularity can be understood by considering the fact that several of
    the top tech giants offer their own version of the tool, such as Google Colaboratory
    and Microsoft Azure Notebooks. Moreover, the popular code-hosting website GitHub
    has been providing a native rendering of Jupyter Notebook since 2016.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's begin with the installation of Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Installation using pip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you already have Python installed on your system, you can install the Jupyter
    package from the `pip` repository to start using Jupyter Notebooks quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Python 3, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For Python 2, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For Mac users, if the `pip` installation is not found, you can download the
    latest Python version, which carries `pip` bundled with it.
  prefs: []
  type: TYPE_NORMAL
- en: Installation using Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it is possible to install Jupyter as a single package from `pip`, it is
    strongly recommended that you install the Anaconda distribution for Python, which
    automatically installs Python, Jupyter, and several other packages required for
    machine learning and data science. Anaconda makes it very easy to deal with the
    various package versions and update dependency packages or dependent packages.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, download the correct Anaconda distribution for your system and requirements
    from [https://www.anaconda.com/downloads](https://www.anaconda.com/downloads)
    and then follow the corresponding installation steps given on the website.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To check whether Jupyter has correctly installed, run the following command
    in Command Prompt (Windows) or Terminal (Linux/Mac):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will be able to see some logging output at the terminal (henceforth, the
    default term for Command Prompt on Windows and Terminal on Linux or Mac). After
    that, your default browser, will open up and you will be taken to a link on the
    browser which will resemble the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/580396c4-cea4-4e26-9fa3-b206c340510a.png)'
  prefs: []
  type: TYPE_IMG
- en: Under the Files tab, a basic file manager is provided that the user can use
    to create, upload, rename, delete, and move files.
  prefs: []
  type: TYPE_NORMAL
- en: The Running tab lists all the currently running Jupyter Notebooks, which can
    be shut down from the listing displayed.
  prefs: []
  type: TYPE_NORMAL
- en: The Clusters tab provides an overview of all the available IPython clusters.
    In order to use this feature, you are required to install the IPython Parallel
    extension for your Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Jupyter Notebook by default is identified by the `.ipynb` extension. Upon
    clicking on the name of once such notebook in the file manager provided by Jupyter,
    you''ll be presented with a screen resembling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9585b379-18ef-43e1-9a0a-60b00b5e6efb.png)'
  prefs: []
  type: TYPE_IMG
- en: The topmost section, where you can see a menu bar, a toolbar, and the title
    of the notebook, is called the **header**. On the right side of the header you
    can see the environment in which the notebook is executing, and when any task
    is running, the white circle beside the environment language's name turns gray.
  prefs: []
  type: TYPE_NORMAL
- en: Below the header is the body of the notebook, which is composed of cells stacked
    vertically. Each cell in the body of the notebook is either a block of code, a
    markdown cell, or a raw cell. A code cell can have an output cell attached below
    it, which the user cannot edit manually. This holds the output produced by the
    code cell associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter Notebook, the keyboard behaves differently for different **modes**
    of a cell For this reason, these notebooks are called **modal**. There are two
    modes in which a notebook cell can operate: the **command** mode and the **editx**
    mode.'
  prefs: []
  type: TYPE_NORMAL
- en: While a cell is in command mode, it has a gray border. In this mode, the cell
    contents cannot be changed. The keys of the keyboard in this mode are mapped to
    several shortcuts that can be used to modify the cell or the notebook as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: While in command mode, if you press the *Enter* key on the keyboard, the cell
    mode changes to the edit mode. While in this mode, the contents of the cell can
    be changed and the basic keyboard shortcuts that are available in the usual textboxes
    in the browser can be invoked.
  prefs: []
  type: TYPE_NORMAL
- en: 'To exit the edit mode, the user can use the *Esc* key. To run the particular
    cell, the user has to input *Shift* + *Return*, which will do one of the following
    in each case:'
  prefs: []
  type: TYPE_NORMAL
- en: For a markdown cell, the rendered markdown shall be displayed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a raw cell, the raw text as entered shall be visible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a code cell, the code will be executed and if it produces some output, an
    output cell attached to the code cell will be created and the output will get
    displayed there. If the code in the cell asks for an input, an input field will
    appear and the cell's code execution stalls until the input is provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter also allows the manipulation of text files and Python script files using
    its in-built text editor. It is also possible to invoke the system terminal from
    within the Jupyter environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a deep-learning-based cloud environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin setting up a cloud-based deep learning environment, we might
    wonder why would we need it or how a cloud-based deep learning environment would
    benefit us. Deep learning requires a massive amount of mathematical calculation.
    At every layer of the neural network, there is a mathematical matrix undergoing
    multiplication with another or several other such matrices. Furthermore, every
    data point itself can be a vector instead of a singular entity. Now, to train
    over several repetitions, such a deep learning model would require a lot of time
    just because of the number of mathematical operations involved.
  prefs: []
  type: TYPE_NORMAL
- en: A GPU-enabled machine would be much more efficient at executing these operations
    because a GPU is made specifically for high-speed mathematical calculations however,
    GPU-enabled machines are costly and may not be affordable to everyone. Furthermore,
    considering that multiple developers work on the same software in a work environment,
    it might be a very costly option to buy GPU-enabled machines for all the developers
    on the team. For these reasons, the idea of a GPU-enabled cloud computing environment
    has a strong appeal.
  prefs: []
  type: TYPE_NORMAL
- en: Companies nowadays are increasingly leaning towards the usage of GPU-enabled
    cloud environments for their development teams, which can lead to the creation
    of a common environment for all of the developers as well as the facilitation
    of high-speed computation.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an AWS EC2 GPU deep learning environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to set up a deep learning specific instance
    on AWS. Before you can begin working with AWS, you will need to create an account
    on the AWS console. To do so, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://console.aws.amazon.com](https://console.aws.amazon.com) and you'll
    be presented with a login/sign up screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you do not already have an AWS account, click on Create a new AWS account
    and follow the steps to create one, which might require you to enter your debit/credit
    card details to enable billing for your account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upon logging into your account, on the dashboard, click on EC2 in the All services
    section, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a1e52502-90c0-477b-82fa-70b9ee8e3450.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you have reached the EC2 management page within the AWS console, you'll
    need to go through the steps in the following sections to create an instance for
    your deep learning needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Creating an EC2 GPU-enabled instance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, select the Ubuntu 16.04 or 18.04 LTS AMI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41e6aa63-bb63-4a50-bbc7-3ae7249e74f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, choose a GPU-enabled instance configuration. The `g2.2xlarge` is a good
    choice for a starter deep learning environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b74f7f3-e953-46a0-a975-3d9b78497042.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, configure the required instance settings or leave them as their default
    and proceed to the storage step. Here, a recommended size of the volume is 30
    GB. You can then proceed to launch the instance with the default options.
  prefs: []
  type: TYPE_NORMAL
- en: Assign an EC2 key pair to your instance so that you can access the instance's
    terminal over SSH from your system. If you name the key pair `abc`, then a file
    named `abc.pem` would download automatically to your browser's default download
    location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: SSHing into your EC2 instance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open up a terminal on your system and using `cd`, navigate to the directory
    that your `abc.pem` file is stored in.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re unfamiliar with the `cd` command, consider a scenario in which you
    are inside a folder named `Folder1`, which has the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To access any files inside the folder named `Folder2`, you''ll have to change
    your working directory to that folder. To do so, you can use the following example
    of the `cd` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that this command only works when you're already inside `Folder1`, which
    can be reached with a similar usage of the `cd` command from anywhere on the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about the usage of any command on a Linux system by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, set the permissions required for SSH using the key file by entering the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to SSH into your instance, you will need its public IP or instance public
    DNS. For example, if the public IP is `1.2.3.4`, then use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The public IP of the AWS instance can be found on the details panel below the
    list of running instances on the AWS console in the EC2 management page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Installing CUDA drivers on the GPU instance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, update/install the NVIDIA graphics drivers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, `xxx` can be replaced with the graphics hardware version installed on
    your instance, which can be found in the instance details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, download the CUDA deb file (this code is for the latest version at the
    time of writing, from Jan, 2019):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, proceed with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify whether everything was installed successfully, run the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If both the commands produce output without any warnings or errors, then the
    installation is successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Installing the Anaconda distribution of Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, download the Anaconda installer script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, set the script to executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the installation script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The installer will ask for several options. To verify successful installation,
    use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The Python3 REPL loads into the terminal with a banner reflecting the Anaconda
    distribution version installed on your instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Run Jupyter'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use the following command to get the Jupyter Notebook server started on the
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The output on the terminal will contain a URL on opening, with which you will
    be able to access the Jupyter Notebook running on your EC2 GPU instance.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning on Crestle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While a customized deep learning environment can be of use when you need greater
    control over the system—such as when you want to have third-party applications
    working along with your deep learning model—at other times, you may not have such
    needs, and you'll only be interested in performing deep learning on the cloud,
    quickly and in a collaborative manner. In such circumstances, paying the cost
    of an AWS `g2.2xlarge` instance would be much higher than that of paying only
    for computing time or GPU time used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Crestle is a service that provides GPU-enabled Jupyter Notebooks online at
    very affordable pricing. To begin using Crestle, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log on to [www.crestle.com](http://www.crestle.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Sign Up and fill up the sign-up form that appears.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check your email for an account confirmation link. Activate your account and
    sign in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You'll be taken to the dashboard where you'll find a button reading Start Jupyter.
    You will have the option of using the GPU or keeping it disabled. Click on the
    Start Jupyter button with the GPU option enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be presented with a Jupyter environment running on the cloud with GPU
    support. While the pricing is subject to change with the passage of time, it is
    one of the most affordable solutions available on the internet as of January 2020.
  prefs: []
  type: TYPE_NORMAL
- en: Other deep learning environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As well as the aforementioned ways of performing GPU-enabled deep learning on
    the cloud, you can also, in certain circumstances, choose to use other platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colaboratory is a freely available Jupyter Notebook service that is accessible
    at [https://colab.research.google.com](https://colab.research.google.com). Colaboratory
    notebooks are stored on the user's Google Drive and so have a storage limit of
    15 GB. It is possible to store large datasets on Google Drive and include them
    in the project with the help of the Google Drive Python API. By default, the GPU
    is disabled on Colaboratory and has to be manually turned on.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle is yet another platform that was specifically built to carry out contests
    on data science. It provides a Jupyter-Notebooks-like environment called a **kernel**.
    Each kernel is provided with a large amount of RAM and free GPU power however,
    there are more strict storage limits on Kaggle than on Google Colaboratory, and
    so it is an effective option when the computation is intensive but the data that
    is to be used and the output is not very large.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring NumPy and pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NumPy and pandas are the backbone of nearly every data-science-related library
    available in the Python language. While pandas is built on top of NumPy, NumPy
    itself is a wrapping of Python around high-performance C code to facilitate superior
    mathematical computing in Python than Python itself in its pure form can provide.
  prefs: []
  type: TYPE_NORMAL
- en: Almost all deep learning software developed in Python in one way or another
    depends upon NumPy and pandas. It is therefore important to have a good understanding
    of both libraries and the features that they can provide.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NumPy is an acronym for **Numerical Python**. Vanilla Python lacks the implementation
    of arrays, which are close analogs of the mathematical matrices used to develop
    machine learning models. NumPy brings to Python support for multidimensional arrays
    and high-performance computing features. It can be included into any Python code
    by using the following import statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`np` is a commonly used convention for importing NumPy.'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several methods to create arrays in NumPy. The following are some
    notable ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`np.array`: To convert Python lists to NumPy arrays:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/bea02ad3-cb06-4762-8ddd-1ecd8acd8a7a.png)'
  prefs: []
  type: TYPE_IMG
- en: '`np.ones` or `np.zeros`: To create a NumPy array of all 1s or all 0s:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/be91b2e2-dce0-40eb-ae79-746dd9ddb97c.png)'
  prefs: []
  type: TYPE_IMG
- en: '`np.random.rand`: To generate an array of random numbers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e5956406-688a-4c1b-986c-d73d3ef30bff.png)'
  prefs: []
  type: TYPE_IMG
- en: '`np.eye`: To generate an identity matrix of given square matrix dimensions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3468a284-89f9-41d5-ba9e-8cadbfd6ca35.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's now look at basic NumPy array operations.
  prefs: []
  type: TYPE_NORMAL
- en: Basic NumPy array operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NumPy arrays are Python analogues of mathematical matrices, and so they support
    the arithmetic manipulation of all basic types, such as addition, subtraction,
    division, and multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s declare two NumPy arrays and store them as `array1` and `array2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s look at some examples of each arithmetic operation on these arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Addition**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/170428ce-55de-4a81-bf59-4ea0b5e9af4a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Subtraction**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ad215dbc-939b-4eff-a0be-e217c9cc2bf5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Multiplication**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/780c5000-1e48-466d-8e60-de7561b8d815.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Division**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d1097939-d2ab-4455-a459-056d54e7e468.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's now compare NumPy arrays with Python lists.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy arrays versus Python lists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now see how NumPy arrays offer advantages over Python lists.
  prefs: []
  type: TYPE_NORMAL
- en: Array slicing over multiple rows and columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While it is not possible to slice lists of lists in Python in such a way as
    to select a specific number of rows and columns in the list of lists, NumPy array
    slicing works according to the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Array [ rowStartIndex : rowEndIndex, columnStartIndex : columnEndIndex ]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f6ed47b-cf3a-42f9-9413-9e6a146e5577.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we are able to select two rows and all elements of
    those rows in NumPy array `a`.
  prefs: []
  type: TYPE_NORMAL
- en: Assignment over slicing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While it is not possible to assign values to slices of Python lists, NumPy
    allows the assignment of values to NumPy arrays. For example, to assign 4 to the
    third to the fifth element of a NumPy one-dimensional array, we can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will be looking at pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Built on top of NumPy, pandas is one of the most widely used libraries for data
    science using Python. It facilitates high-performance data structures and data-analysis
    methods. Pandas provides an in-memory two-dimensional table object called a DataFrame,
    which in turn is made of a one-dimensional, array-like structure called a series.
  prefs: []
  type: TYPE_NORMAL
- en: Each DataFrame in pandas is in the form of a spreadsheet-like table with row
    labels and column headers. It is possible to carry out row-based or column-based
    operations, or both together. Pandas strongly integrates with matplotlib to provide
    several intuitive visualizations of data that are often very useful when making
    presentations or during exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import pandas into a Python project, use the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, `pd` is a common name for importing pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas provides the following data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Series**: One-dimensional array or vector, similar to a column in a table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFrames**: Two-dimensional table, with table headers and labels for the
    rows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Panels**: A dictionary of DataFrames, much like a MySQL database that contains
    several tables inside'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A pandas series can be created using the `pd.Series( )` method, while a DataFrame
    can be created using the `pd.DataFrame( )` method—for example, in the following
    code, we create a pandas DataFrame object using multiple series objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0eac47bd-7093-4c7e-a082-545d3e511900.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some of the most important methods available for a pandas DataFrame are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`head(n)` or `tail(n)`: To display the top or bottom *n* rows of the DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info( )`: To display information on all the columns, dimensions, and types
    of data in the columns of the DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe( )`: To display handy aggregate and statistical information about
    each of the columns in the DataFrame. Columns that are not numeric are omitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered a lot of different things in this chapter. We started by learning
    the basics of a neural network and then we gradually proceeded. We learned the
    two most powerful types of neural networks used today—CNNs and RNNs—and we also
    learned about them on a high level, but without skipping their foundational units.
    We learned that as the complexity in a neural network increases, it requires a
    lot of computational power, which standard computers may fail to cater for we
    saw how this problem can be overcome by configuring a deep learning development
    environment using two different providers—AWS and Crestle. We explored Jupyter
    Notebooks, a powerful tool for performing deep learning tasks. We learned about
    the usage of two very popular Python libraries—NumPy and pandas. Both of these
    libraries are extensively used when performing deep learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building applications and integrating deep learning
    to make them perform intelligently. But before we did this, it was important for
    us to know the basics that were covered in this chapter. We are now in a good
    position to move on to the next chapter.
  prefs: []
  type: TYPE_NORMAL
