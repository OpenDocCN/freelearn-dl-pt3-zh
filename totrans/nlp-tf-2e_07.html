<html><head></head><body>
  <div id="_idContainer398" class="Basic-Text-Frame">
    <h1 class="chapterNumber">7</h1>
    <h1 id="_idParaDest-170" class="chapterTitle">Understanding Long Short-Term Memory Networks</h1>
    <p class="normal">In this chapter, we will discuss the fundamentals behind a more advanced RNN variant known<a id="_idIndexMarker662"/> as <strong class="keyWord">Long Short-Term Memory Networks</strong> (<strong class="keyWord">LSTMs</strong>). Here, we will focus on understanding the theory behind LSTMs, so we can discuss their implementation in the next chapter. LSTMs are widely used in many sequential tasks (including stock market prediction, language modeling, and machine translation) and have proven to perform better than older sequential models (for example, standard RNNs), especially given the availability of large amounts of data. LSTMs are designed to avoid the problem of the vanishing gradient that we discussed in the previous chapter.</p>
    <p class="normal">The main practical limitation posed by the vanishing gradient is that it prevents the model from learning long-term dependencies. However, by avoiding the vanishing gradient problem, LSTMs have the ability to store memory for longer than ordinary RNNs (for hundreds of time steps). In contrast to RNNs, which only maintain a single hidden state, LSTMs have many more parameters as well as better control over what memory to store and what to discard at a given training step. For example, RNNs are not able to decide which memory to store and which to discard, as the hidden state is forced to be updated at every training step.</p>
    <p class="normal">Specifically, we will discuss what an LSTM is at a very high level and how the functionality of LSTMs allows them to store long-term dependencies. Then we will go into the actual underlying mathematical framework governing LSTMs and discuss an example to highlight why each computation matters. We will also compare LSTMs to vanilla RNNs and see that LSTMs have a much more sophisticated architecture that allows them to surpass vanilla RNNs in sequential tasks. </p>
    <p class="normal">Revisiting the problem of the vanishing gradient and illustrating it through an example will lead us to understand how LSTMs solve the problem.</p>
    <p class="normal">Thereafter, we will <a id="_idIndexMarker663"/>discuss several techniques that have been introduced to improve the predictions produced by a standard LSTM (for example, improving the quality/variety of generated text in a text generation task). For example, generating several predictions at once instead of predicting them one by one can help to improve the quality of generated predictions. We will also<a id="_idIndexMarker664"/> look at <strong class="keyWord">bidirectional LSTMs (BiLSTMs)</strong>, which <a id="_idIndexMarker665"/>are an extension to the standard LSTM, that have greater capabilities for capturing the patterns present in a sequence than a standard LSTM.</p>
    <p class="normal">Finally, we will discuss two recent LSTM variants. First, we will look at <strong class="keyWord">peephole connections</strong>, which<a id="_idIndexMarker666"/> introduce more parameters and information to the LSTM gates, allowing LSTMs to perform better. Next, we will discuss <strong class="keyWord">Gated Recurrent Units</strong> (<strong class="keyWord">GRUs</strong>), which<a id="_idIndexMarker667"/> are gaining increasing popularity as they have a much simpler structure compared to standard LSTMs and also do not degrade performance.</p>
    <p class="normal">Specifically, this chapter will cover the following main topics:</p>
    <ul>
      <li class="bulletList">Understanding Long Short-Term Memory Networks</li>
      <li class="bulletList">How LSTMs solve the vanishing gradient problem</li>
      <li class="bulletList">Improving LSTMs</li>
      <li class="bulletList">Other variants of LSTMs</li>
    </ul>
    <div class="note">
      <p class="normal">Transformer models have emerged as a more powerful alternative for sequence learning. Transformer models deliver better performance as these models have access to the full history of the sequence at a given step, whereas LSTM models can only see the previous output at a given step. We will discuss Transformer models in detail in <em class="chapterRef">Chapter 10</em>, <em class="italic">Transformers</em> and <em class="chapterRef">Chapter 11</em>, <em class="italic">Image Captioning with Transformers</em>. However, it’s still worth learning about LSTMs as they have laid the foundation for next-generation models like Transformers. Additionally, LSTMs are still used to some extent, especially when working on time-series problems in memory-constrained environments.</p>
    </div>
    <h1 id="_idParaDest-171" class="heading-1">Understanding Long Short-Term Memory Networks</h1>
    <p class="normal">In this section, we will first explain how an LSTM cell operates. We will see that in addition to the hidden states, a gating mechanism is in place to control information flow inside the cell. </p>
    <p class="normal">Then <a id="_idIndexMarker668"/>we will work through a detailed example and see how gates and states help at various stages of the example to achieve desired behaviors, finally leading to the desired output. Finally, we will compare an LSTM against a standard RNN to learn how an LSTM differs from a standard RNN.</p>
    <h2 id="_idParaDest-172" class="heading-2">What is an LSTM?</h2>
    <p class="normal">LSTMs can be seen as a more complex and capable family of RNNs. Though LSTMs are a complicated beast, the underlying principles of LSTMs are as same as of RNNs; they process a sequence of items by working on one input at a time in a sequential order. An LSTM is mainly composed of five different components:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Cell state</strong>: This is<a id="_idIndexMarker669"/> the internal cell state (that is, memory) of an LSTM cell</li>
      <li class="bulletList"><strong class="keyWord">Hidden state</strong>: This <a id="_idIndexMarker670"/>is the external hidden state exposed to other layers and used to calculate predictions</li>
      <li class="bulletList"><strong class="keyWord">Input gate</strong>: This<a id="_idIndexMarker671"/> determines how much of the current input is read into the cell state</li>
      <li class="bulletList"><strong class="keyWord">Forget gate</strong>: This<a id="_idIndexMarker672"/> determines how much of the previous cell state is sent into the current cell state</li>
      <li class="bulletList"><strong class="keyWord">Output gate</strong>: This <a id="_idIndexMarker673"/>determines how much of the cell state is output into the hidden state</li>
    </ul>
    <p class="normal">We can wrap the RNN to a cell architecture as follows: the cell will output some state (with a nonlinear activation function) that is dependent on the previous cell state and the current input. However, in RNNs, the cell state is continuously updated with every incoming input. This behavior is quite undesirable for storing long-term dependencies.</p>
    <p class="normal">LSTMs can decide when to add, update, or forget information stored in each neuron in the cell state. In other words, LSTMs are equipped with a mechanism to keep the cell state unchanged (if warranted for better performance), giving them the ability to store long-term dependencies.</p>
    <p class="normal">This is achieved by introducing a gating mechanism. LSTMs possess gates for each operation the cell needs to perform. The gates are continuous (often sigmoid functions) between 0 and 1, where 0 means no information flows through the gate and 1 means all the information flows through the gate. An LSTM uses one such gate for each neuron in the cell. As explained in the introduction, these gates control the following:</p>
    <ul>
      <li class="bulletList">How much of the current input is written to the cell state (input gate)</li>
      <li class="bulletList">How much information is forgotten from the previous cell state (forget gate)</li>
      <li class="bulletList">How much information is output into the final hidden state from the cell state (output gate)</li>
    </ul>
    <p class="normal"><em class="italic">Figure 7.1</em> illustrates this<a id="_idIndexMarker674"/> functionality for a hypothetical scenario. Each gate decides how much of various data (for example, the current input, the previous hidden state, or the previous cell state) flows into the states (that is, the final hidden state or the cell state). The thickness of each line represents how much information is flowing from/to that gate (in some hypothetical scenarios). For example, in this figure, you can see that the input gate is allowing more from the current input than from the previous final hidden state, where the forget gate allows more from the previous final hidden state than from the current input:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_01.png" alt="What is an LSTM?"/></figure>
    <p class="packt_figref">Figure 7.1: An abstract view of the data flow in an LSTM</p>
    <h2 id="_idParaDest-173" class="heading-2">LSTMs in more detail</h2>
    <p class="normal">Here we will <a id="_idIndexMarker675"/>walk through the actual mechanism of LSTMs. We will first briefly discuss the overall view of an LSTM cell and <a id="_idIndexMarker676"/>then start discussing each of the computations crunched within an LSTM cell, along with an example of text generation.</p>
    <p class="normal">As we discussed earlier, LSTMs have<a id="_idIndexMarker677"/> a gating mechanism composed of the following three gates:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Input gate</strong>: A gate <a id="_idIndexMarker678"/>that outputs values between 0 (the current input is not written to the cell state) and 1 (the current input is fully written to the cell state). Sigmoid activation is used to squash the output to between 0 and 1.</li>
      <li class="bulletList"><strong class="keyWord">Forget gate</strong>: A <a id="_idIndexMarker679"/>sigmoidal gate that outputs values between 0 (the previous cell state is fully forgotten for calculating the current cell state) and 1 (the previous cell state is fully read in when calculating the current cell state).</li>
      <li class="bulletList"><strong class="keyWord">Output gate</strong>: A <a id="_idIndexMarker680"/>sigmoidal gate that outputs values between 0 (the current cell state is fully discarded for calculating the final state) and 1 (the current cell state is fully used when calculating the final hidden state).</li>
    </ul>
    <p class="normal">This can be shown as in <em class="italic">Figure 7.2</em>. This is a very high-level diagram, and some details have been omitted in order to avoid clutter. We present LSTMs, both with loops and without loops, to improve understanding. The figure on the right-hand side depicts an LSTM with loops, and the one on the left-hand side shows the same LSTM with the loops unfolded so that no loops are present in the model:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_02.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.2: An LSTM with recurrent links (that is, loops) (right) and an LSTM with recurrent links unfolded (left)</p>
    <p class="normal">Now, to get a better understanding of LSTMs, let’s consider a language modeling example. We will discuss the actual update rules and equations side by side with the example to ground our understanding of LSTMs better.</p>
    <p class="normal">Let’s consider an <a id="_idIndexMarker681"/>example of generating text starting from the following sentence:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy.</em></p>
    <p class="normal">The story that we output should be about <em class="italic">John</em>, <em class="italic">Mary</em>, and the <em class="italic">puppy</em>. Let’s assume our LSTM outputs two sentences following the given sentence:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy. ____________________. _____________________.</em></p>
    <p class="normal">The following is the output given by our LSTM:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy. It barks very loudly. They named it Luna.</em></p>
    <p class="normal">We are still far from outputting realistic phrases such as these. However, LSTMs can learn relationships such as between nouns and pronouns. For example, <em class="italic">it</em> is related to the <em class="italic">puppy</em>, and <em class="italic">they</em> to <em class="italic">John</em> and <em class="italic">Mary</em>. Then, it should learn the relationship between the noun/pronoun and the verb. For example, for <em class="italic">it</em>, the verb should have an <em class="italic">s</em> at the end. We illustrate these relationships/dependencies in <em class="italic">Figure 7.3</em>. As we can see, both long-term (for example, <em class="italic">Luna --&gt; puppy</em>) and short-term (for example, <em class="italic">It --&gt;barks</em>) dependencies are present in this phrase. The solid arrows depict links between nouns and pronouns and dashed arrows show links between nouns/pronouns and verbs:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_03.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.3: Sentences given and predicted by the LSTM with various relationships between words highlighted</p>
    <p class="normal">Now let’s consider how LSTMs, using their various operations, can model such relationships and dependencies to output sensible text, given a starting sentence.</p>
    <p class="normal">The input gate (<em class="italic">i</em><sub class="subscript">t</sub>) takes the current input (<em class="italic">x</em><sub class="subscript">t</sub>) and the previous final hidden state (<em class="italic">h</em><sub class="subscript">t-1</sub>) as the input and calculates <em class="italic">i</em><sub class="subscript">t</sub>, as follows:</p>
    <p class="center"><img src="../Images/B14070_07_001.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">The input gate <em class="italic">i</em><sub class="subscript">t</sub> can be understood as the calculation performed at the hidden layer of a single-hidden-layer standard RNN with the sigmoidal activation. Remember that we calculated the hidden state of a standard RNN as follows:</p>
    <p class="center"><img src="../Images/B14070_07_002.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">Therefore, the <a id="_idIndexMarker682"/>calculation of <em class="italic">i</em><sub class="subscript">t</sub> of the LSTM looks quite analogous to the calculation of <em class="italic">h</em><sub class="subscript">t</sub> of a standard RNN, except for the change in the activation function and the addition of bias.</p>
    <p class="normal">After the calculation, a value of 0 for <em class="italic">i</em><sub class="subscript">t</sub> will mean that no information from the current input will flow to the cell state, where a value of 1 means that all the information from the current input will flow to the cell state.</p>
    <p class="normal">Next, another <a id="_idIndexMarker683"/>value (which is called <strong class="keyWord">candidate value</strong>) is calculated as follows, which is fed in to calculate the current cell state later. This value will be treated as a potential candidate for the final cell state of this time step:</p>
    <p class="center"><img src="../Images/B14070_07_003.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">We can visualize these calculations in <em class="italic">Figure 7.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_04.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.4: Calculation of i<sub class="subscript">t</sub> and <img src="../Images/B14070_07_004.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> (in bold) in the context of all the calculations (grayed out) that take place in an LSTM</p>
    <p class="normal">In our <a id="_idIndexMarker684"/>example, at the very beginning of the learning, the input gate needs to be highly activated, as the model has no prior knowledge of the task. The first word that the LSTM outputs is <em class="italic">it</em>. Also, in order to do so, the LSTM must learn that <em class="italic">puppy</em> is also referred to as <em class="italic">it</em>. Let’s assume our LSTM has five neurons to store the state. We would like the LSTM to store the information that <em class="italic">it</em> refers to <em class="italic">puppy</em>. Another piece of information we would like the LSTM to learn (in a different neuron) is that the present tense verb should have an <em class="italic">s</em> at the end of the verb when the pronoun <em class="italic">it</em> is used. </p>
    <p class="normal">One<a id="_idIndexMarker685"/> more thing the LSTM needs to know is that the <em class="italic">puppy barks loud</em>. <em class="italic">Figure 7.5</em> illustrates how this knowledge might be encoded in the cell state of the LSTM. Each circle represents a single neuron (that is, a hidden unit) of the cell state:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_05.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.5: The knowledge that should be encoded in the cell state to output the first sentence</p>
    <p class="normal">With this information, we can output the first new sentence:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy. It barks very loudly.</em></p>
    <p class="normal">Next, the forget gate is calculated as follows:</p>
    <p class="center"><img src="../Images/B14070_07_005.png" alt="" style="height: 1.45em !important; vertical-align: -0.01em !important;"/></p>
    <p class="normal">The forget gate does the following. A value of 0 for the forget gate means that no information from <em class="italic">c</em><sub class="subscript">t-1</sub> will be passed to calculate <em class="italic">c</em><sub class="subscript">t</sub>, and a value of 1 means that all the information of <em class="italic">c</em><sub class="subscript">t-1</sub> will propagate into the calculation of <em class="italic">c</em><sub class="subscript">t</sub>. It may sound counter-intuitive, as switching on the forget gate causes the model to remember from the previous step and vice versa. But to respect the original naming conventions and design, we’ll continue to use them as they are.</p>
    <p class="normal">Now we will see how the forget gate helps in predicting the next sentence:</p>
    <p class="normal"><em class="italic">They named it Luna.</em></p>
    <p class="normal">Now, as you can see, the new relationship we are looking at is between <em class="italic">John</em> and <em class="italic">Mary</em> and <em class="italic">they</em>. Therefore, we no longer need information about <em class="italic">it</em> and how the verb <em class="italic">bark</em> behaves, as the subjects are <em class="italic">John</em> and <em class="italic">Mary</em>. We can use the forget gate in combination with the current subject <em class="italic">they</em> and the corresponding verb <em class="italic">named</em> to replace the information stored in the <strong class="keyWord">Current subject</strong> and <strong class="keyWord">Verb for current subject</strong> neurons (see <em class="italic">Figure 7.6</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_06.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.6: The knowledge in the third neuron from the left (it --&gt; barks) is replaced with new information (they --&gt; named)</p>
    <p class="normal">In terms <a id="_idIndexMarker686"/>of the values of weights, we illustrate this transformation in <em class="italic">Figure 7.7</em>. We do not change the state of the neuron maintaining the <em class="italic">it --&gt; puppy</em> relationship, because <em class="italic">puppy</em> appears as an object in the last sentence. This is done by setting weights connecting <em class="italic">it --&gt; puppy</em> from <em class="italic">c</em><sub class="subscript">t-1</sub> to <em class="italic">c</em><sub class="subscript">t</sub> to 1. Then we will replace the neurons maintaining the current subject and verb information with a new subject and verb. This is achieved by setting the forget weights of <em class="italic">f</em><sub class="subscript">t</sub>, for that neuron, to 0. Then we will set the weights of <em class="italic">i</em><sub class="subscript">t</sub>, connecting the current subject and verb to the corresponding state neurons, to 1. We can think of <img src="../Images/B14070_07_004.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> (the candidate value) as a potential candidate for the cell’s memory, as it contains information from the current input <em class="italic">x</em><sub class="subscript">t</sub>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_07.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.7: How the cell state c<sub class="subscript">t</sub> is calculated with the previous state c<sub class="subscript">t-1</sub> and the candidate value <img src="../Images/B14070_07_004.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/></p>
    <p class="normal">The current cell state will be updated as follows:</p>
    <p class="center"><img src="../Images/B14070_07_008.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">In other <a id="_idIndexMarker687"/>words, the current state is a combination of the following:</p>
    <ul>
      <li class="bulletList">What information to forget/remember from the previous cell state</li>
      <li class="bulletList">What information to add/discard to the current input</li>
    </ul>
    <p class="normal">Next, in <em class="italic">Figure 7.8</em>, we highlight what we have calculated so far with respect to all the calculations that are taking place inside an LSTM:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_08.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.8: Calculations covered so far, including i<sub class="subscript">t</sub>, f<sub class="subscript">t</sub>, <img src="../Images/B14070_07_004.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, and c<sub class="subscript">t</sub></p>
    <p class="normal">After <a id="_idIndexMarker688"/>learning the full cell state, it would look like <em class="italic">Figure 7.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_09.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.9: The full cell state will look like this after outputting both the sentences</p>
    <p class="normal">Next, we will look at how the final state of the LSTM cell (<em class="italic">h</em><sub class="subscript">t</sub>) is computed:</p>
    <p class="center"><img src="../Images/B14070_07_010.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_011.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">In our example, we want to output the following sentence:</p>
    <p class="normal"><em class="italic">They named it Luna.</em></p>
    <p class="normal">For this, we <a id="_idIndexMarker689"/>do <strong class="keyWord">not</strong> need the second to last neuron to compute this sentence, as it contains information about how the puppy barks, whereas this sentence is about the name of the puppy. Therefore, we can ignore this neuron (containing the <em class="italic">bark -&gt; loud</em> relationship) during the predictions of the last sentence. This is exactly what <em class="italic">o</em><sub class="subscript">t</sub> does; it ignores the unnecessary memory and only retrieves the related memory from the cell state when calculating the final output of the LSTM cell. Also, in <em class="italic">Figure 7.10</em>, we illustrate what a full LSTM cell would look like at a glance:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_10.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.10: What the full LSTM looks like</p>
    <p class="normal">Here, we summarize<a id="_idIndexMarker690"/> all the equations<a id="_idIndexMarker691"/> relating to the operations taking place within an LSTM cell:</p>
    <p class="center"><img src="../Images/B14070_07_001.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_005.png" alt="" style="height: 1.45em !important; vertical-align: -0.01em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_003.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_008.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_010.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_011.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">Now in the<a id="_idIndexMarker692"/> bigger picture, for a sequential learning problem, we can unroll the LSTM cells over time to show how they would link together so that they receive the previous state of the cell to compute the next state, as shown in <em class="italic">Figure 7.11</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_11.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.11: How LSTMs would be linked over time</p>
    <p class="normal">However, this is not adequate to do something useful. We typically use machine learning models to solve a task formulated as a classification or regression problem. As you can see, we still don’t have an output layer to output predictions. But if we want to use what the LSTM actually learned, we need a way to extract the final output from the LSTM. Therefore, we will fit a<a id="_idIndexMarker693"/> <code class="inlineCode">softmax</code> layer (with weights <em class="italic">W</em><sub class="subscript">s</sub> and bias <em class="italic">b</em><sub class="subscript">s</sub>) on top of the LSTM. The final output is obtained using the following equation:</p>
    <p class="center"><img src="../Images/B14070_07_018.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Now the final picture of the LSTM with the softmax layer looks like <em class="italic">Figure 7.12</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_12.png" alt="LSTMs in more detail"/></figure>
    <p class="packt_figref">Figure 7.12: LSTMs with a softmax output layer linked over time</p>
    <p class="normal">With the <a id="_idIndexMarker694"/>softmax head attached to the LSTM, it can now perform a given classification task end to end. Now let’s compare and contrast LSTMs and the standard RNN model we discussed in the previous chapter.</p>
    <h2 id="_idParaDest-174" class="heading-2">How LSTMs differ from standard RNNs</h2>
    <p class="normal">Let’s now<a id="_idIndexMarker695"/> investigate how LSTMs compare to standard RNNs. An LSTM has a more intricate structure compared to a standard RNN. One of the primary differences is that an LSTM has two different states: a cell state <em class="italic">c</em><sub class="subscript">t</sub> and a final hidden state <em class="italic">h</em><sub class="subscript">t</sub>. However, an RNN only has a single hidden state <em class="italic">h</em><sub class="subscript">t</sub>. The next primary difference is that, since an LSTM has three different gates, an LSTM has much more control over how the current input and the previous cell state are handled when computing the final hidden state <em class="italic">h</em><sub class="subscript">t</sub>.</p>
    <p class="normal">Having the <a id="_idIndexMarker696"/>two different states is quite advantageous. With this mechanism, we can decouple the model’s short-term and long-term memory. In other words, even when the cell state is changing quickly, the final hidden state will still be changed more slowly. So, while the cell state is learning both short-term and long-term dependencies, the final hidden state can reflect either only the short-term dependencies, only the long-term dependencies, or both.</p>
    <p class="normal">Next, the gating mechanism is composed of three gates: the input, forget, and output gates.</p>
    <p class="normal">It is quite evident that this is a more principled approach (especially compared to the standard RNNs) that permits better control over how much the current input and the previous cell state contribute to the current cell state. Also, the output gate gives better control over how much the cell state contributes to the final hidden state. </p>
    <p class="normal">In <em class="italic">Figure 7.13</em>, we compare schematic diagrams of a standard RNN and an LSTM to emphasize the difference in terms of the functionality of the two models:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_13.png" alt="How LSTMs differ from standard RNNs"/></figure>
    <p class="packt_figref">Figure 7.13: A side-by-side comparison of a standard RNN and an LSTM cell</p>
    <p class="normal">In summary, with the design of maintaining two different states, an LSTM can learn both short-term and long-term dependencies, which helps solve the problem of the vanishing gradient, which we’ll discuss in the following section.</p>
    <h1 id="_idParaDest-175" class="heading-1">How LSTMs solve the vanishing gradient problem</h1>
    <p class="normal">As we <a id="_idIndexMarker697"/>discussed earlier, even though RNNs are theoretically sound, in practice they suffer from a serious <a id="_idIndexMarker698"/>drawback. That is, when <strong class="keyWord">Backpropagation Through Time</strong> (<strong class="keyWord">BPTT</strong>) is used, the gradient diminishes quickly, which allows us to propagate the information of only a few time steps. Consequently, we can only store the information of very few time steps, thus possessing only short-term memory. This in turn limits the usefulness of RNNs in real-world sequential tasks.</p>
    <p class="normal">Often, useful and interesting sequential tasks (such as stock market predictions or language modeling) require the ability to learn and store long-term dependencies. Think of the following example for predicting the next word:</p>
    <p class="normal"><em class="italic">John is a talented student. He is an A-grade student and plays rugby and cricket. All the other students envy ______.</em></p>
    <p class="normal">For us, this is a very easy task. The answer would be <em class="italic">John</em>. However, for an RNN, this is a difficult task. We are trying to predict an answer that lies at the very beginning of the text. Also, to solve this task, we need a way to store long-term dependencies in the state of the RNN. This is exactly the type of task LSTMs are designed to solve.</p>
    <p class="normal">In <em class="chapterRef">Chapter 6</em>, <em class="italic">Recurrent Neural Networks</em>, we discussed how a vanishing/exploding gradient can appear without any nonlinear functions present. We will now see that it could still happen even with the nonlinear term present. For this, we will derive the term <img src="../Images/B14070_07_019.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> for a standard RNN and <img src="../Images/B14070_07_020.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> for an LSTM network to understand the differences. This is the crucial term that causes the vanishing gradient, as we learned in the previous chapter.</p>
    <p class="normal">Let’s assume the hidden state is calculated as follows for a standard RNN:</p>
    <p class="center"><img src="../Images/B14070_07_021.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">To simplify the calculations, we can ignore the current input related terms and focus on the recurrent part, which will give us the following equation:</p>
    <p class="center"><img src="../Images/B14070_07_022.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">If we calculate <img src="../Images/B14070_07_019.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> for the preceding equations, we will get the following:</p>
    <p class="center"><img src="../Images/B14070_07_024.png" alt="" style="height: 3.45em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_025.png" alt="" style="height: 3.45em !important;"/></p>
    <p class="normal">Now<a id="_idIndexMarker699"/> let’s see what happens when <img src="../Images/B14070_07_026.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> or <img src="../Images/B14070_07_027.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> (which will happen as learning continues). In both cases, <img src="../Images/B14070_07_019.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> will start to approach 0, giving rise to the vanishing gradient. Even when <img src="../Images/B14070_07_029.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, where the gradient is maximum (0.25) for sigmoid activation, when multiplied for many time steps, the overall gradient becomes quite small. Moreover, the term <img src="../Images/B14070_07_030.png" alt="" style="height: 1.25em !important; vertical-align: -0.24em !important;"/> (possibly due to bad initialization) can cause exploding or vanishing of the gradients as well. However, compared to the gradient vanishing due to <img src="../Images/B14070_07_026.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> or <img src="../Images/B14070_07_027.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, the gradient vanishing/explosion caused by the term <img src="../Images/B14070_07_030.png" alt="" style="height: 1.25em !important; vertical-align: -0.24em !important;"/> is relatively easy to solve (with careful initialization of weights and gradient clipping).</p>
    <p class="normal">Now let’s look at an LSTM cell. More specifically, we’ll look at the cell state, given by the following equation:</p>
    <p class="center"><img src="../Images/B14070_07_008.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">This is the product of all the forget gate applications happening in the LSTM. However, if you calculate <img src="../Images/B14070_07_020.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>in a similar way for LSTMs (that is, ignoring the <img src="../Images/B14070_07_036.png" alt="" style="height: 1.35em !important; vertical-align: -0.39em !important;"/> terms and <em class="italic">b</em><sub class="subscript">f</sub>, as they are non-recurrent), we get the following:</p>
    <p class="center"><img src="../Images/B14070_07_037.png" alt="" style="height: 3.45em !important;"/></p>
    <p class="normal">In this case, though the gradient will vanish if <img src="../Images/B14070_07_038.png" alt="" style="height: 1.15em !important; vertical-align: -0.23em !important;"/>, on the other hand, if <img src="../Images/B14070_07_039.png" alt="" style="height: 1.15em !important; vertical-align: -0.23em !important;"/>, the derivative will decrease much slower than it would in a standard RNN. Therefore, we have one alternative, where the gradient will not vanish. Also, as the squashing function is used, the gradients will not explode due to <img src="../Images/B14070_07_020.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> being large (which is the thing likely to be the cause of a gradient explosion). In addition, when <img src="../Images/B14070_07_027.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, we get a maximum gradient close to 1, meaning that the gradients will not rapidly decrease as we saw with RNNs (when the gradient is at maximum). Finally, there is no term such as <img src="../Images/B14070_07_030.png" alt="" style="height: 1.25em !important; vertical-align: -0.24em !important;"/> in the derivation. However, derivations are trickier for <img src="../Images/B14070_07_043.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>. Let’s see if such terms are present in the derivation of <img src="../Images/B14070_07_043.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>. If you calculate the derivatives of this, you will get something of the following form:</p>
    <p class="center"><img src="../Images/B14070_07_045.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">Once you solve this, you will get something of this form: </p>
    <p class="center"><img src="../Images/B14070_07_046.png" alt="" style="height: 1.05em !important; vertical-align: 0.05em !important;"/></p>
    <p class="normal">We do <a id="_idIndexMarker700"/>not care about the content within <img src="../Images/B14070_07_047.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> or <img src="../Images/B14070_07_048.png" alt="" style="height: 1.15em !important; vertical-align: -0.23em !important;"/>, because no matter the value, it will be bounded by (0,1) or (-1,1). If we further reduce the notation by replacing the <img src="../Images/B14070_07_047.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, <img src="../Images/B14070_07_050.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, <img src="../Images/B14070_07_051.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> and <img src="../Images/B14070_07_052.png" alt="" style="height: 1.25em !important; vertical-align: -0.25em !important;"/> terms with a common notation such as <img src="../Images/B14070_07_053.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/>, we get something of this form: </p>
    <p class="center"><img src="../Images/B14070_07_054.png" alt="" style="height: 1.45em !important; vertical-align: -0.01em !important;"/></p>
    <p class="normal">Alternatively, we get the following (assuming that the outside <img src="../Images/B14070_07_055.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> gets absorbed by each <img src="../Images/B14070_07_055.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> term present within the square brackets): </p>
    <p class="center"><img src="../Images/B14070_07_057.png" alt="" style="height: 1.35em !important; vertical-align: 0.01em !important;"/></p>
    <p class="normal">This will give the following: </p>
    <p class="center"><img src="../Images/B14070_07_058.png" alt="" style="height: 3.45em !important;"/></p>
    <p class="normal">This means that though the term <img src="../Images/B14070_07_020.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> is safe from any <img src="../Images/B14070_07_030.png" alt="" style="height: 1.25em !important; vertical-align: -0.24em !important;"/> terms, <img src="../Images/B14070_07_019.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> is not. Therefore, we <a id="_idIndexMarker701"/>must be careful when initializing the weights of the LSTM and we should use gradient clipping as well.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">However, <em class="italic">h</em><sub class="subscript">t</sub> of LSTMs being unsafe from vanishing gradient is not as crucial as it is for RNNs, because <em class="italic">c</em><sub class="subscript">t</sub> still can store the long-term dependencies without being affected by vanishing gradient, and <em class="italic">h</em><sub class="subscript">t</sub> can retrieve the long-term dependencies from <em class="italic">c</em><sub class="subscript">t</sub>, if required to.</p>
    </div>
    <h1 id="_idParaDest-176" class="heading-1">Improving LSTMs</h1>
    <p class="normal">Having a model <a id="_idIndexMarker702"/>backed up by solid foundations does not always guarantee pragmatic success when used in the real world. Natural language is quite complex. Sometimes seasoned writers struggle to produce quality content. So we can’t expect LSTMs to magically output meaningful, well-written content all of a sudden. Having a sophisticated design—allowing for better modeling of long-term dependencies in the data—does help, but we need more techniques during inference to produce better text. Therefore, numerous extensions have been developed to help LSTMs perform better at the prediction stage. Here we will discuss several such improvements: greedy sampling, beam search, using word vectors instead of a one-hot-encoded representation of words, and using bidirectional LSTMs. It is important to note that these optimization techniques are not specific to LSTMs; rather, any sequential model can benefit from them.</p>
    <h2 id="_idParaDest-177" class="heading-2">Greedy sampling</h2>
    <p class="normal">If we try<a id="_idIndexMarker703"/> to always<a id="_idIndexMarker704"/> predict the word with the highest probability, the LSTM will tend to produce very monotonic results. For example, due to the frequent occurrence of stop words (e.g. <em class="italic">the</em>), it may repeat them many times before switching to another word.</p>
    <p class="normal">One way to get around this is to use <strong class="keyWord">greedy sampling</strong>, where we pick the predicted best <em class="italic">n</em> and sample from that set. This helps to break the monotonic nature of the predictions.</p>
    <p class="normal">Let’s <a id="_idIndexMarker705"/>consider the<a id="_idIndexMarker706"/> first sentence of the previous example:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy.</em></p>
    <p class="normal">Say, we start with the first word and want to predict the next four words:</p>
    <p class="normal"><em class="italic">John ____ ____ _ _____.</em></p>
    <p class="normal">If we attempt <a id="_idIndexMarker707"/>to choose samples deterministically, the LSTM might output something like the following:</p>
    <p class="normal"><em class="italic">John gave Mary gave John.</em></p>
    <p class="normal">However, by sampling the next word from a subset of words in the vocabulary (most highly probable ones), the LSTM is forced to vary the prediction and might output the following:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy.</em></p>
    <p class="normal">Alternatively, it might give the following output:</p>
    <p class="normal"><em class="italic">John gave puppy a puppy.</em></p>
    <p class="normal">However, even though greedy sampling helps to add more flavor/diversity to the generated text, this method does not guarantee that the output will always be realistic, especially when outputting longer sequences of text. Now we will see a better search technique that actually looks ahead several steps before predictions.</p>
    <h2 id="_idParaDest-178" class="heading-2">Beam search</h2>
    <p class="normal"><strong class="keyWord">Beam search</strong> is <a id="_idIndexMarker708"/>a way of helping with the quality of the<a id="_idIndexMarker709"/> predictions produced by the LSTM. In this, the predictions are found by solving a search problem. Particularly, we predict several steps ahead for multiple <a id="_idIndexMarker710"/>candidates at each step. This gives rise to a tree-like structure with candidate sequences of words (<em class="italic">Figure 7.14</em>). The crucial idea of beam search is to produce the <em class="italic">b</em> outputs (that is, <img src="../Images/B14070_07_062.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/>) at once instead of a single output <em class="italic">y</em><sub class="subscript">t</sub>. Here, <em class="italic">b</em> is known as the <strong class="keyWord">length</strong> of <a id="_idIndexMarker711"/>the beam, and the <em class="italic">b</em> outputs produced are known as the <strong class="keyWord">beam</strong>. More <a id="_idIndexMarker712"/>technically, we pick the beam that has the highest joint probability <img src="../Images/B14070_07_063.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> instead of picking the highest probable <img src="../Images/B14070_07_064.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/>. We are looking farther into the future before making a prediction, which usually leads to better results.</p>
    <p class="normal">Let’s <a id="_idIndexMarker713"/>understand beam search through the previous example:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy.</em></p>
    <p class="normal">Say, we are predicting word by word and initially we have the following:</p>
    <p class="normal"><em class="italic">John ____ ____ _ _____.</em></p>
    <p class="normal">Let’s assume <a id="_idIndexMarker714"/>hypothetically that our LSTM produces the example sentence using beam search. Then the probabilities for each word might look like what we see in <em class="italic">Figure 7.14</em>. Let’s assume beam length <em class="italic">b</em> = <em class="italic">2</em>, and we will consider the <em class="italic">n</em> = <em class="italic">3</em> best candidates at each stage of the search. </p>
    <p class="normal">The search tree would look like the following figure:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_14.png" alt="Beam search"/></figure>
    <p class="packt_figref">Figure 7.14: The search space of beam search for a b=2 and n=3</p>
    <p class="normal">We start with the word <em class="italic">John</em> and get the probabilities for all the words in the vocabulary. In our example, as <em class="italic">n</em> = <em class="italic">3</em>, we pick the best three candidates for the next level of the tree: <em class="italic">gave</em>, <em class="italic">Mary</em>, and <em class="italic">puppy</em>. (Note that these might not be the candidates found by an actual LSTM and are only used as an example.) Then from these selected candidates, the next level of the tree is grown. And from that, we will pick the best three candidates, and the search will repeat until we reach a depth of <em class="italic">b</em> in the tree.</p>
    <p class="normal">The path that gives the highest joint probability (that is, <img src="../Images/B14070_07_065.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/>) is highlighted with heavier arrows. Also, this is a better prediction mechanism, as it would return a higher probability, or a reward, for a phrase such as <em class="italic">John gave Mary</em> than <em class="italic">John Mary John</em> or <em class="italic">John John gave</em>.</p>
    <p class="normal">Note that the <a id="_idIndexMarker715"/>outputs produced by both greedy sampling and beam search are identical in our example, which is a simple sentence containing five words. However, this is not the case when<a id="_idIndexMarker716"/> we scale this to output a small paragraph. Then the results produced by beam search will be much more realistic and meaningful than the ones produced by greedy sampling.</p>
    <h2 id="_idParaDest-179" class="heading-2">Using word vectors</h2>
    <p class="normal">Another<a id="_idIndexMarker717"/> popular way of improving the performance <a id="_idIndexMarker718"/>of LSTMs is to use word vectors instead of using one-hot-encoded vectors as the input to the LSTM. Let’s <a id="_idIndexMarker719"/>understand the value of this method through an example. Let’s assume that we want to generate text starting from some random word. In our case, it would be the following:</p>
    <p class="normal"><em class="italic">John ____ ____ _ _____.</em></p>
    <p class="normal">We have already trained our LSTM on the following sentences:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy. Mary has sent Bob a kitten.</em></p>
    <p class="normal">Let’s also assume that we have the word vectors positioned as shown in <em class="italic">Figure 7.15</em>. Remember that semantically similar words will have vectors placed close to each other: </p>
    <figure class="mediaobject"><img src="../Images/B14070_07_15.png" alt="Using word vectors"/></figure>
    <p class="packt_figref">Figure 7.15: Assumed word vectors’ topology in two-dimensional space</p>
    <p class="normal">The word<a id="_idIndexMarker720"/> embeddings of these words, in their numerical form, might look like the following:</p>
    <p class="normal"><em class="italic">kitten:</em> [0.5, 0.3, 0.2]</p>
    <p class="normal"><em class="italic">puppy:</em> [0.49, 0.31, 0.25]</p>
    <p class="normal"><em class="italic">gave:</em> [0.1, 0.8, 0.9]</p>
    <p class="normal">It can be seen that <em class="italic">distance(kitten, puppy) &lt; distance(kitten, gave)</em>. However, if we use one-hot encoding, they would be as follows:</p>
    <p class="normal"><em class="italic">kitten:</em> [ 1, 0, 0, …]</p>
    <p class="normal"><em class="italic">puppy:</em> [0, 1, 0, …]</p>
    <p class="normal"><em class="italic">gave:</em> [0, 0, 1, …]</p>
    <p class="normal">Then, <em class="italic">distance(kitten, puppy) = distance(kitten, gave)</em>. As we can already see, one-hot-encoded vectors <a id="_idIndexMarker721"/>do not capture the proper relationship between words and see all the words are equally distanced from each other. However, word vectors are capable of capturing such relationships and are more suitable to represent text for machine learning models.</p>
    <p class="normal">Using word vectors, the LSTM will learn to exploit relationships between words better. For example, with <a id="_idIndexMarker722"/>word vectors, LSTM will learn <a id="_idIndexMarker723"/>the following:</p>
    <p class="normal"><em class="italic">John gave Mary a kitten.</em></p>
    <p class="normal">This is quite close to the following:</p>
    <p class="normal"><em class="italic">John gave Mary a puppy.</em></p>
    <p class="normal">Also, it is quite different from the following:</p>
    <p class="normal"><em class="italic">John gave Mary a gave.</em></p>
    <p class="normal">However, this would not be the case if one-hot-encoded vectors are used.</p>
    <h2 id="_idParaDest-180" class="heading-2">Bidirectional LSTMs (BiLSTMs)</h2>
    <p class="normal">Making <a id="_idIndexMarker724"/>LSTMs bidirectional is<a id="_idIndexMarker725"/> another way of improving the quality of the predictions of an LSTM. By this we mean training the LSTM with text read in both directions: from the beginning to the end and the end to the beginning. So far during the training of the LSTM, we would create a dataset as follows.</p>
    <p class="normal">Consider the following two sentences:</p>
    <p class="normal"><em class="italic">John gave Mary a _____. It barks very loudly.</em></p>
    <p class="normal">At this stage, there is data missing in one of the sentences that we would want our LSTM to fill sensibly.</p>
    <p class="normal">If we read from the beginning up to the missing word, it would be as follows:</p>
    <p class="normal"><em class="italic">John gave Mary a _____.</em></p>
    <p class="normal">This does not provide enough information about the context of the missing word to fill the word properly. However, if we read in both directions, it would be the following:</p>
    <p class="normal"><em class="italic">John gave Mary a _____.</em></p>
    <p class="normal"><em class="italic">_____. It barks very loudly.</em></p>
    <p class="normal">If we created data with both these pieces, it is adequate to predict that the missing word should be something like <em class="italic">dog</em> or <em class="italic">puppy</em>. Therefore, certain problems can benefit significantly from reading data from both sides. BiLSTMs also help in multilingual problems as different languages can have very different sentence structures.</p>
    <div class="note">
      <p class="normal">Another application of BiLSTMs is neural machine translation, where we translate a sentence of a source language to a target language. As there is no specific alignment between the translation of one language to another, having access to both sides of a given token in the source language can greatly help to understand the context better, thus producing better translations. As an example, consider a translation task of translating Filipino to English. In Filipino, sentences are usually written having <em class="italic">verb-object-subject</em> in that order, whereas in English, it is <em class="italic">subject-verb-object</em>. In this translation task, it will be extremely helpful to read sentences both forward and backward to make a good translation.</p>
    </div>
    <p class="normal">A BiLSTM is <a id="_idIndexMarker726"/>essentially two separate<a id="_idIndexMarker727"/> LSTM networks. One network learns data from the beginning to the end, and the other network learns data from the end to the beginning. In <em class="italic">Figure 7.16</em>, we illustrate the architecture of a BiLSTM network.</p>
    <p class="normal">Training occurs<a id="_idIndexMarker728"/> in two phases. First, the solid-colored network is trained with data created by reading the text from the beginning to the end. This network represents the normal training procedure used for standard LSTMs. Secondly, the dashed network is trained with data generated by reading the text in the reversed direction. Then, at the inference phase, we use both the solid and dashed states’ information (by concatenating both states and creating a vector) to predict the missing word:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_16.png" alt="Bidirectional LSTMs (BiLSTM)"/></figure>
    <p class="packt_figref">Figure 7.16: A schematic diagram of a BiLSTM</p>
    <p class="normal">In this section, <a id="_idIndexMarker729"/>we discussed several <a id="_idIndexMarker730"/>different ways to improve the performance of LSTM models. This involved employing better prediction strategies to introduce structural changes such as word vectors and BiLSTMs.</p>
    <h1 id="_idParaDest-181" class="heading-1">Other variants of LSTMs</h1>
    <p class="normal">Though we <a id="_idIndexMarker731"/>will mainly focus on the standard LSTM architecture, many variants have emerged that either simplify the complex architecture found in standard LSTMs, produce better performance, or both. We will look at two variants that introduce structural modifications to the cell architecture of LSTMs: peephole connections and GRUs.</p>
    <h2 id="_idParaDest-182" class="heading-2">Peephole connections</h2>
    <p class="normal"><strong class="keyWord">Peephole connections</strong> allow gates to see not only the current input and the previous final hidden state, but also <a id="_idIndexMarker732"/>the previous cell state. This<a id="_idIndexMarker733"/> increases the number of weights in the LSTM cell. Having such connections has been shown to produce better results. The equations would look like these: </p>
    <p class="center"><img src="../Images/B14070_07_066.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_003.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_068.png" alt="" style="height: 1.45em !important; vertical-align: -0.01em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_008.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_070.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_011.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="normal">Let’s briefly look at how this helps the LSTM perform better. So far, the gates see the current input and final hidden state but not the cell state. However, in this configuration, if the output gate is close to zero, even when the cell state contains information crucial to better performance, the final hidden state will be close to zero. Thus, the gates will not take the hidden state into consideration during calculation. Including the cell state directly in the gate calculation equation allows more control over the cell state, and it can perform well even in situations where the output gate is close to zero.</p>
    <p class="normal">We illustrate the architecture of the LSTM with peephole connections in <em class="italic">Figure 7.17</em>. We have grayed all the existing connections in a standard LSTM and the newly added connections are shown in black:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_17.png" alt="Peephole connections"/></figure>
    <p class="packt_figref">Figure 7.17: An LSTM with peephole connections (the peephole connections are shown in black while the other connections are grayed out)</p>
    <h2 id="_idParaDest-183" class="heading-2">Gated Recurrent Units</h2>
    <p class="normal"><strong class="keyWord">GRUs</strong> can be <a id="_idIndexMarker734"/>seen as a simplification <a id="_idIndexMarker735"/>of the standard LSTM architecture. As we have seen already, an LSTM has three different gates and two different states. This alone requires a large number of parameters even for a small state size. Therefore, scientists have investigated ways to reduce the number of parameters. GRUs are a result of one such endeavor.</p>
    <p class="normal">There are several main differences in GRUs compared to LSTMs.</p>
    <p class="normal">First, GRUs combine two states, the cell state and the final hidden state, into a single hidden state <em class="italic">h</em><sub class="subscript">t</sub>. Now, as a side effect of this simple modification of not having two different states, we can get rid of the output gate. Remember, the output gate was merely deciding how much of the cell state is read into the final hidden state. This operation greatly reduces the number of parameters in the cell.</p>
    <p class="normal">Next, GRUs <a id="_idIndexMarker736"/>introduce a reset gate that, when<a id="_idIndexMarker737"/> it’s close to 1, takes the full previous state information in when computing the current state. Also, when the reset gate is close to 0, it ignores the previous state when computing the current state: </p>
    <p class="center"><img src="../Images/B14070_07_072.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_073.png" alt="" style="height: 1.25em !important; vertical-align: 0.06em !important;"/></p>
    <p class="normal">Then, GRUs combine the input and forget gates into one <em class="italic">update gate</em>. The standard LSTM has two gates known as the input and forget gates. The input gate decides how much of the current input is read into the cell state, and the forget gate determines how much of the previous cell state is read into the current cell state. Mathematically, this can be shown as follows: </p>
    <p class="center"><img src="../Images/B14070_07_001.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_005.png" alt="" style="height: 1.45em !important; vertical-align: -0.01em !important;"/></p>
    <p class="normal">GRUs combine these two operations into a single gate known as the update gate. If the update gate is 0, then the full state information of the previous cell state is pushed into the current cell state, where none of the current input is read into the state. If the update gate is 1, then all of the current input is read into the current cell state and none of the previous cell state is propagated into the current cell state. In other words, the input gate <em class="italic">i</em><sub class="subscript">t</sub> becomes inverse of the forget gate, that is, <img src="../Images/B14070_07_076.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/>: </p>
    <p class="center"><img src="../Images/B14070_07_077.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_078.png" alt="" style="height: 1.25em !important; vertical-align: 0.06em !important;"/></p>
    <p class="normal">Now let’s bring all the equations into one place. The GRU computations would look like this: </p>
    <p class="center"><img src="../Images/B14070_07_072.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_073.png" alt="" style="height: 1.25em !important; vertical-align: 0.06em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_077.png" alt="" style="height: 1.15em !important; vertical-align: 0.10em !important;"/></p>
    <p class="center"><img src="../Images/B14070_07_078.png" alt="" style="height: 1.25em !important; vertical-align: 0.06em !important;"/></p>
    <p class="normal">This is <a id="_idIndexMarker738"/>much more compact than LSTMs. In <em class="italic">Figure 7.18</em>, we <a id="_idIndexMarker739"/>can visualize a GRU cell (left) and an LSTM cell (right) side by side:</p>
    <figure class="mediaobject"><img src="../Images/B14070_07_18.png" alt="Gated Recurrent Units"/></figure>
    <p class="packt_figref">Figure 7.18: A side-by-side comparison of a GRU (left) and the standard LSTM (right)</p>
    <p class="normal">In this section, we learned two variants of the LSTM: LSTMs with peepholes and GRUs. GRUs have become a popular choice over LSTMs, due to their simplicity and on-par performance with more complex LSTMs.</p>
    <h1 id="_idParaDest-184" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, you learned about LSTM networks. First, we discussed what an LSTM is and its high-level architecture. We also delved into the detailed computations that take place in an LSTM and discussed the computations through an example.</p>
    <p class="normal">We saw that an LSTM is composed mainly of five different things:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Cell state</strong>: The internal cell state of an LSTM cell</li>
      <li class="bulletList"><strong class="keyWord">Hidden state</strong>: The external hidden state used to calculate predictions</li>
      <li class="bulletList"><strong class="keyWord">Input gate</strong>: This determines how much of the current input is read into the cell state</li>
      <li class="bulletList"><strong class="keyWord">Forget gate</strong>: This determines how much of the previous cell state is sent into the current cell state</li>
      <li class="bulletList"><strong class="keyWord">Output gate</strong>: This determines how much of the cell state is output into the hidden state</li>
    </ul>
    <p class="normal">Having such a complex structure allows LSTMs to capture both short-term and long-term dependencies quite well.</p>
    <p class="normal">We compared LSTMs to vanilla RNNs and saw that LSTMs are actually capable of learning long-term dependencies as an inherent part of their structure, whereas RNNs can fail to learn long-term dependencies. Afterward, we discussed how LSTMs solve the vanishing gradient with its complex structure.</p>
    <p class="normal">Then we discussed several extensions that improve the performance of LSTMs. First, a very simple technique we called greedy sampling, in which, instead of always outputting the best candidate, we randomly sample a prediction from a set of best candidates. We saw that this improves the diversity of the generated text. After that, we looked at a more complex search technique called beam search. With this, instead of making a prediction for a single time step into the future, we predict several time steps into the future and pick the candidates that produce the best joint probability. Another improvement involved seeing how word vectors can help improve the quality of the predictions of an LSTM. Using word vectors, LSTMs can learn more effectively to replace semantically similar words during prediction (for example, instead of outputting <em class="italic">dog</em>, LSTM might output <em class="italic">cat</em>), leading to more realism and correctness of the generated text. The final extension we considered was BiLSTMs or bidirectional LSTMs. A popular application of BiLSTMs is filling missing words in a phrase. BiLSTMs read the text in both directions, from the beginning to the end and the end to the beginning. This gives more context as we are looking at both the past and future before predicting.</p>
    <p class="normal">Finally, we discussed two variants of vanilla LSTMs: peephole connections and GRUs. Vanilla LSTMs, when calculating the gates, only look at the current input and the hidden state. With peephole connections, we make the gate computations dependent on all: the current input, and the hidden and cell states.</p>
    <p class="normal">GRUs are a much more elegant variant of vanilla LSTMs that simplify LSTMs without compromising on performance. GRUs have only two gates and a single state, whereas vanilla LSTMs have three gates and two states.</p>
    <p class="normal">In the next chapter, we will see all these different architectures in action with implementations of each of them and see how well they perform in text generation tasks.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"><img src="../Images/QR_Code5143653472357468031.png" alt=""/> </figure>
  </div>
</body></html>