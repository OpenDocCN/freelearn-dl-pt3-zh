- en: Introduction to Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, a **convolutional neural network** (**CNN**) is specific kind
    of deep learning architecture that uses the convolution operation to extract relevant
    explanatory features for the input image. CNN layers are connected as a feed-forward
    neural network while using this convolution operation to mimic how the human brain
    functions while trying to recognize objects. Individual cortical neurons respond
    to stimuli in a restricted region of space known as the receptive field. In particular,
    biomedical imaging problems could be challenging sometimes, but in this chapter,
    we'll see how to use CNN in order to discover patterns in this image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different layers of CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNN basic example: MNIST digit classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolution operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are widely used in the area of computer vision and they outperform most
    of the traditional computer vision techniques that we have been using. CNNs combine
    the famous convolution operation and neural networks, hence the name convolutional
    neural network. So, before diving into the neural network aspect of CNNs, we are
    going to introduce the convolution operation and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main purpose of the convolution operation is to extract information or
    features from an image. Any image could be considered as a matrix of values and
    a specific group of values in this matrix will form a feature. The purpose of
    the convolution operation is to scan this matrix and try to extract relevant or
    explanatory features for that image. For example, consider a 5 by 5 image whose
    corresponding intensity or pixel values are shown as zeros and ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c7834a6-2ae2-474e-ad2b-6d51782c9525.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Matrix of pixel values'
  prefs: []
  type: TYPE_NORMAL
- en: 'And consider the following 3 x 3 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9763afb-ab0d-4025-b397-defa753e1707.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Matrix of pixel values'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can convolve the 5 x 5 image using a 3 x 3 one as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03dbfb76-d1b6-4116-96b5-1f20a7da595f.png)![](img/bc1d23c0-2eaa-4a9b-bed7-3760beeafefd.png)![](img/02ab2c65-cad7-4eb1-a4cb-58461dd8083c.png)![](img/6d592c61-4681-4347-9da7-ab30c8857ad7.png)![](img/27277f69-a023-44dd-bc20-9d4258e910a8.png)![](img/bfe47520-e728-47a6-8970-26a08c2ad7e2.png)![](img/2bacdd70-eb6a-4a11-b8ff-c5d1b70511d3.png)![](img/d4cb9f3d-e099-49b0-852f-1d6d8b1205c2.png)![](img/3a6b2050-5870-4bd4-9f29-ebbb1a88e937.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: The convolution operation. The output matrix is called a convolved
    feature or feature map'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure could be summarized as follows. In order to convolve the
    original 5 by 5 image using the 3 x 3 convolution kernel, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan the original green image using the orange matrix and each time move by
    only 1 pixel (stride)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For every position of the orange image, we do element-wise multiplication between
    the orange matrix and the corresponding pixel values in the green matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the results of these element-wise multiplication operations together to
    get a single integer which will form a single value in the output pink matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, the orange 3 by 3 matrix only operates
    on one part of the original green image at a time in each move (stride), or it
    only sees a part at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s put the previous explanation in the context of CNN terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: The orange 3 x 3 matrix is called a **kernel**, **feature detector**, or **filter**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output pink matrix that contain the results of the element-wise multiplications
    is called the **feature map**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the fact that we are getting the feature map based on the element-wise
    multiplication between the kernel and the corresponding pixels in the original
    input image, changing the values of the kernel or the filter will give different
    feature maps each time.
  prefs: []
  type: TYPE_NORMAL
- en: So, we might think that we need to figure out the values of the feature detectors
    ourselves during the training of the convolution neural networks, but this is
    not the case here. CNNs figure out these numbers during the learning process.
    So, if we have more filters, it means that we can extract more features from the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before jumping to the next section, let''s introduce some terminology that
    is usually used in the context of CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stride**: We mentioned this term briefly earlier. In general, stride is the
    number of pixels by which we move our feature detector or filter over the pixels
    of the input matrix. For example, stride 1 means moving the filter one pixel at
    a time while convolving the input image and stride 2 means moving the filter two
    pixels at a time while convolving the input image. The more stride we have, the
    smaller the generated feature maps are.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-padding**: If we wanted to include the border pixels of the input image,
    then part of our filter will be outside the input image. Zero-padding solves this
    problem by padding the input matrix with zeros around the borders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Traditional computer vision techniques were used to perform most computer vision
    tasks, such as object detection and segmentation. The performance of these traditional
    computer vision techniques was good but it was never close to being usable in
    real time, for example by autonomous cars. In 2012, Alex Krizhevsky introduced
    CNNs, which made a breakthrough on the ImageNet competition by enhancing the object
    classification error from 26% to 15%. CNNs have been widely used since then and
    different variations have been discovered. It has even outperformed the human
    classification error over the ImageNet competition, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20ae5e5f-425e-44c5-9001-b1b25ca200d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Classification error over time with human level error marked in
    red'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the breakthrough the CNNs achieved in different domains of computer vision
    and even natural language processing, most companies have integrated this deep
    learning solution into their computer vision echo system. For example, Google
    uses this architecture for its image search engine, and Facebook uses it for doing
    automatic tagging and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d938629-dc53-44fa-b7fe-7185e92f773d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: A typical CNN general architecture for object recognition'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs achieved this breakthrough because of their architecture, which intuitively
    uses the convolution operation to extract features from the images. Later on,
    you will see that it's very similar to the way the human brain works.
  prefs: []
  type: TYPE_NORMAL
- en: Different layers of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical CNN architecture consists of multiple layers that do different tasks,
    as shown in the preceding diagram. In this section, we are going to go through
    them in detail and will see the benefits of having all of them connected in a
    special way to make such a breakthrough in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the first layer in any CNN architecture. All the subsequent convolution
    and pooling layers expect the input to be in a specific format. The input variables
    will tensors, that has the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` is a random sample from the original training set that''s used
    during applying stochastic gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_width` is the width of the input images to the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_height` is the height of the input images to the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channels` are the number of color channels of the input images. This number
    could be 3 for RGB images or 1 for binary images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, consider our famous MNIST dataset. Let's say we are going to perform
    digit classification using CNNs using this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the dataset is composed of monochrome 28 x 28 pixel images like the MNIST
    dataset, then the desired shape for our input layer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To change the shape of our input features, we can do the following reshaping
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have specified the batch size to be -1, which means that
    this number should be determined dynamically based on the input values in the
    features. By doing this, we will be able to fine-tune our CNN model by controlling
    the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example for the reshape operation, suppose that we divided our input
    samples into batches of five and our feature `["x"]` array will hold 3,920 `values()`
    of the input images, where each value of this array corresponds to a pixel in
    an image. For this case, the input layer will have the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Convolution step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, the convolution step got its name from the convolution
    operation. The main purpose of having these convolution steps is to extract features
    from the input images and then feed them to a linear classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In natural images, features could be anywhere in the image. For example, edges
    could be in the middle or at the corner of the images, so the whole idea of stacking
    a bunch of convolution steps is to be able to detect these features anywhere in
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s very easy to define a convolution step in TensorFlow. For example, if
    we wanted to apply 20 filters each of size 5 by 5 to the input layer with a ReLU
    activation function, then we can use the following line of code to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter for this `conv2d` function is the input layer that we have
    defined in the preceding code, which has the appropriate shape, and the second
    argument is the filters argument which specifies the number of filters to be applied
    to the image where the higher the number of filters, the more features are extracted
    from the input image. The third parameter is the `kernel_size`, which represents
    the size of the filter or the feature detector. The padding parameters specifies
    where we use `"same"` here to introduce zero-padding to the corner pixels of the
    input image. The last argument specifies the activation function that should be
    used for the output of the convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in our MNIST example, the input tensor will have the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output tensor for this convolution step will have the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output tensor has the same dimensions as the input images, but now we have
    20 channels that represent applying the 20 filters to the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing non-linearity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the convolution step, we talked about feeding the output of the convolution
    step to a ReLU activation function to introduce non-linearity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/655f87c8-366b-4717-a6bc-8b55c6f1c4c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReLU activation function replaces all the negative pixel values with zeros
    and the whole purpose of feeding the output of the convolution step to this activation
    function is to introduce non-linearity in the output image because this will be
    useful for the training process as the data that we are using is usually non-linear.
    To clearly understand the benefit of ReLU activation function, have a look at
    the following figure, which shows the row output of the convolution step and the
    rectified version of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdcf199b-f53c-4b9f-ac13-201474a7f2f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: The result of applying ReLU to the input feature map'
  prefs: []
  type: TYPE_NORMAL
- en: The pooling step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the important steps for our learning process is the pooling step, which
    is sometimes called the subsampling or downsampling step. This step is mainly
    for reducing the dimensionality of the output of the convolution step (feature
    map). The advantage of this pooling step is reducing the size of the feature map
    while keeping the important information in the newly reduced version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows this step by scanning the image with a 2 by 2 filter
    and stride 2 while applying the max operation. This kind of pooling operation
    is called **max pool**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1278534-f054-4594-b1f5-9e534c5d0c2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: An example of a max pooling operation on a rectified feature map
    (obtained after convolution and ReLU operation) by using a 2 x 2 window (source:
    http://textminingonline.com/wp-content/uploads/2016/10/max_polling-300x256.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can connect the output of the convolution step to the pooling layer by using
    the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The pooling layer receives the input from the convolution step with the following
    shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, in our digit classification task, the input to the pooling layer
    will have the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the pooling operation will have the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we have reduced the size of the output of the convolution step
    by 50%. This step is very useful because it keeps only the important information
    and it also reduces the model's complexity and hence avoids overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After stacking up a bunch of convolution and pooling steps, we follow them
    with a fully connected layer where we feed the extracted high-level features that
    we got from the input image to this fully connected layer to use them and do the
    actual classification based on these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49cba1b5-f082-4e6a-87b4-a24e8cbcad72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Fully connected layer -each node is connected to every other node
    in the adjacent layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the case of the digit classification task, we can follow the
    convolution and pooling step with a fully connected layer that has 1,024 neurons
    and ReLU activation to perform the actual classification. This fully connected
    layer accepts the input in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we need to reshape or flatten our input feature map from `pool_layer2`
    to match this format. We can use the following line of code to reshape the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this reshape function, we have used `-1` to indicate that the batch size
    will be dynamically determined and each example from the `pool_layer1` output
    will have a width of `14` and a height of `14` with `20` channels each.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the final output of this reshape operation will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the `dense()` function of TensorFlow to define our fully
    connected layer with the required number of neurons (units) and the final activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Logits layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we need the logits layer, which will take the output of the fully
    connected layer and then produce the raw prediction values. For example, in the
    case of the digit classification, the output will be a tensor of 10 values, where
    each value represents the score of one class from 0-9\. So, let''s define this
    logit layer for the digit classification example, where we need 10 outputs only,
    and with linear activation, which is the default for the `dense()` function of
    TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/983d2783-0a86-4586-acac-3d52882d8cd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Training the ConvNet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output of this logits layer will be a tensor of the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned previously, the logits layer of the model will return the raw
    predictions our our batch. But we need to convert these values to interpretable
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: The predicted class for the input sample 0-9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scores or probabilities for each possible class. For example, the probability
    that the sample is 0, is 1, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d26f9bb1-357c-445c-932f-b8af5ec6d556.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: A visualization of the different layers of a CNN (source: http://cs231n.github.io/assets/cnn/convnet.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our predicted class will be the one that has the highest value in the 10
    probabilities. We can get this value by using the `argmax` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that the `logits_layer` has the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: So, we need to find the max values along our predictions, which is the dimension
    that has an index of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can get our next value, which represents the probabilities of each
    target class, by applying `softmax` activation to the output of the `logits_layer`,
    which will squash each value to be between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: CNN basic example – MNIST digit classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will do a complete example of implementing a CNN for digit
    classification using the MNIST dataset. We will build a simple model of two convolution
    layers and fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off by importing the libraries that will be needed for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use TensorFlow helper functions to download and preprocess the
    MNIST dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is split into three disjoint sets: training, validation, and testing.
    So, let''s print the number of images in each set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual labels of the images are stored in a one-hot encoding format, so
    we have an array of 10 values of zeros except for the index of the class that
    this image represents. For later use, we need to get the class numbers of the
    dataset as integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define some known variables to be used later in our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define a helper function to plot some images from the dataset.
    This helper function will plot the images in a grid of nine subplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot some images from the test set and see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f03662a-f7fb-4e0d-a0cb-d2f1da0cbfc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: A visualization of some examples from the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it''s time to build the core of the model. The computational graph includes
    all the layers we mentioned earlier in this chapter. We''ll start by defining
    some functions that will be used to define variables of a specific shape and randomly
    initialize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the function that will be responsible for creating a new
    convolution layer based on some input layer, input channels, filter size, number
    of filters, and whether to use pooling parameters or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned previously, the pooling layer produces a 4D tensor. We need
    to flatten this 4D tensor to a 2D one to be fed to the fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This function creates a fully connected layer which assumes that the input
    is a 2D tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Before building the network, let''s define a placeholder for the input images
    where the first dimension is `None` to represent an arbitrary number of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned previously, the convolution step expects the input images to
    be in the shape of a 4D tensor. So, we need to reshape the input images to be
    in the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s reshape the input values to match this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define another placeholder for the actual class values, which
    will in one-hot encoding format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we need to define a placeholder to hold the integer values of the actual
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s start off by building the first CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the shape of the output tensor that will be produced by the first
    convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create the second convolution network and feed the output of
    the first one to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Also, we need to double-check the shape of the output tensor of the second convolution
    layer. The shape should be `(?, 7, 7, 36)`, where the `?` mark means an arbitrary
    number of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to flatten the 4D tensor to match the expected format for the
    fully connected layer, which is a 2D tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to double-check the shape of the output tensor of the flattened layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a fully connected layer and feed the output of the flattened
    layer to it. We will also feed the output of the fully connected layer to a ReLU
    activation function before feeding it to the second fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s double-check the shape of the output tensor of the first fully connected
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to add another fully connected layer, which will take the output
    of the first fully connected layer and produce an array of size 10 for each image
    that represents the scores for each target class being the correct one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll normalize these scores from the second fully connected layer and
    feed it to a `softmax` activation function, which will squash the values to be
    between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to choose the target class that has the highest probability by
    using the `argmax` function of TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Cost function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we need to define our performance measure, which is the cross-entropy.
    The value of the cross-entropy will be 0 if the predicted class is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to average all the cross-entropy values that we got from the
    previous step to be able to get a single performance measure over the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a cost function that needs to be optimized/minimized, so we will
    be using `AdamOptimizer`, which is an optimization method like gradient descent
    but a bit more advanced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Performance measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For showing the output, let''s define a variable to check whether the predicted
    class is equal to the true one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the model accuracy by casting the boolean values then averaging them
    to sum the correctly classified ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s kick off the training process by creating a session variable that will
    be responsible for executing the computational graph that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we need to initialize the variables that we have defined so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to feed the images in batches to avoid an out-of-memory error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Before kicking the training process, we are going to define a helper function
    that will perform the optimization process by iterating through the training batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'And we''ll define some helper functions to help us visualize the results of
    the model and to see which images are misclassified by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot the confusion matrix of the predicted results compared to
    the actual true classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are going to define a helper function to help us measure the accuracy
    of the trained model over the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the accuracy of the created model over the test set without doing
    any optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get a sense of the optimization process actually enhancing the model
    capability to classify images to their correct class by running the optimization
    process for one iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s get down to business and kick off a long optimization process of
    10,000 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the output, you should be getting something very close to the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s check how the model will generalize over the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cea9722b-34a1-4a22-a216-9083a559579a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: Accuracy over the test'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71ec6e79-46a4-4511-b063-c5985c75d61d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Confusion matrix of the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: It was interesting that we actually got almost 93% accuracy over the test while
    using a basic convolution network. This implementation and the results show you
    what a simple convolution network can do.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the intuition and the technical details of
    how CNNs work. we also had a look at how to implement a basic architecture of
    a CNN in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we'll demonstrate more advanced architectures that could
    be used for detecting objects in one of the image datasets widely used by data
    scientists. We'll also see the beauty of CNNs and how they come to mimic human
    understanding of objects by first realizing the basic features of objects and
    then building up more advanced semantic features on them to come up with a classification
    for them. Although this process happens very quickly in our minds, it is what
    actually happens when we recognize objects.
  prefs: []
  type: TYPE_NORMAL
