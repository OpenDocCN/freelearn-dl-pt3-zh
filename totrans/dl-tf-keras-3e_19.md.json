["```\npip install tensorflow_hub \n```", "```\nimport tensorflow_hub as hub \n```", "```\nmodel = hub.load(handle) \n```", "```\nhub.KerasLayer(\n    handle,\n    trainable=False,\n    arguments=None,\n    _sentinel=None,\n    tags=None,\n    signature=None,\n    signature_outputs_as_dict=None,\n    output_key=None,\n    output_shape=None,\n    load_options=None,\n    **kwargs\n) \n```", "```\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import requests\n    from PIL import Image\n    from io import BytesIO\n    import matplotlib.pyplot as plt\n    import numpy as np \n    ```", "```\n    def load_image_from_url(img_url, image_size):\n      \"\"\"Get the image from url. The image return has shape [1, height, width, num_channels].\"\"\"\n      response = requests.get(img_url, headers={'User-agent': 'Colab Sample (https://tensorflow.org)'})\n      image = Image.open(BytesIO(response.content))\n      image = np.array(image)\n      # reshape image\n      img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]]) \n      # Normalize by convert to float between [0,1]\n      image = tf.image.convert_image_dtype(img_reshaped, tf.float32) \n      image_padded = tf.image.resize_with_pad(image, image_size, image_size)\n      return image_padded, image \n    ```", "```\n    def show_image(image, title=''):\n      image_size = image.shape[1]\n      w = (image_size * 6) // 320\n      plt.figure(figsize=(w, w))\n      plt.imshow(image[0], aspect='equal')\n      plt.axis('off')\n      plt.title(title)\n      plt.show() \n    ```", "```\n    image_size = 330\n    print(f\"Images will be converted to {image_size}x{image_size}\")\n    img_url =  \"https://upload.wikimedia.org/wikipedia/commons/c/c6/Okonjima_Lioness.jpg\"\n    image, original_image = load_image_from_url(img_url, image_size) \n    show_image(image, 'Scaled image') \n    ```", "```\n    labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n    #download labels and creates a maps\n    downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n    classes = []\n    with open(downloaded_file) as f:\n      labels = f.readlines()\n      classes = [l.strip() for l in labels] \n    ```", "```\n    classifier = hub.load(\"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\") \n    ```", "```\n    probabilities = tf.nn.softmax(classifier(image)).numpy() \n    ```", "```\n    top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n    show_image(image, f'{classes[top_5[0]+1]}: {probabilities[0][top_5][0]:.4f}') \n    ```", "```\npip install tensorflow-datasets \n```", "```\nimport tensorflow_datasets as tfds \n```", "```\ndatasets = tfds.list_builders()\nprint(f\"TFDS contains {len(datasets)} datasets\") \n```", "```\n### Output\nTFDS contains 224 datasets \n```", "```\ntfds.load(\n    name: str,\n    *,\n    split: Optional[Tree[splits_lib.SplitArg]] = None,\n    data_dir: Optional[str] = None,\n    batch_size: tfds.typing.Dim = None,\n    shuffle_files: bool = False,\n    download: bool = True,\n    as_supervised: bool = False,\n    decoders: Optional[TreeDict[decode.partial_decode.DecoderArg]] =\nNone,\n    read_config: Optional[tfds.ReadConfig] = None,\n    with_info: bool = False,\n    builder_kwargs: Optional[Dict[str, Any]] = None,\n    download_and_prepare_kwargs: Optional[Dict[str, Any]] = None,\n    as_dataset_kwargs: Optional[Dict[str, Any]] = None,\n    try_gcs: bool = False\n) \n```", "```\ndata, info = tfds.load(name=\"mnist\", as_supervised=True, split=['train', 'test'], with_info=True) \n```", "```\nprint(info) \n```", "```\n### output\ntfds.core.DatasetInfo(\n    name='mnist',\n    version=3.0.1,\n    description='The MNIST database of handwritten digits.',\n    homepage='http://yann.lecun.com/exdb/mnist/',\n    features=FeaturesDict({\n        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n    }),\n    total_num_examples=70000,\n    splits={\n        'test': 10000,\n        'train': 60000,\n    },\n    supervised_keys=('image', 'label'),\n    citation=\"\"\"@article{lecun2010mnist,\n      title={MNIST handwritten digit database},\n      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n      volume={2},\n      year={2010}\n    }\"\"\",\n    redistribution_info=,\n) \n```", "```\ndata_train = data[1].take(1)\nfor sample, label in data_train:\n  print(sample.shape)\n  print(label) \n```", "```\n### output\n(28, 28, 1)\ntf.Tensor(2, shape=(), dtype=int64) \n```", "```\nfig = tfds.show_examples(data[0], info) \n```", "```\n    import tensorflow as tf\n    import tensorflow_datasets as tfds \n    ```", "```\n    model = tf.keras.models.Sequential([ \n      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)), \n      tf.keras.layers.MaxPooling2D(2, 2),\n      tf.keras.layers.Conv2D(32, (3,3), activation='relu'), \n      tf.keras.layers.MaxPooling2D(2,2), \n      tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n      tf.keras.layers.MaxPooling2D(2,2), \n      tf.keras.layers.Flatten(), \n      tf.keras.layers.Dense(256, activation='relu'), \n      tf.keras.layers.Dense(1, activation='sigmoid')\n    ]) \n    ```", "```\n    model.compile(optimizer='Adam', loss='binary_crossentropy',metrics=['accuracy']) \n    ```", "```\n    data = tfds.load('horses_or_humans', split='train', as_supervised=True) \n    val_data = tfds.load('horses_or_humans', split='test', as_supervised=True) \n    ```", "```\n    def normalize_img(image, label):\n      \"\"\"Normalizes images: 'uint8' -> 'float32'.\"\"\"\n      return tf.cast(image, tf.float32) / 255., label\n    def augment_img(image, label):\n      image, label = normalize_img(image, label)\n      image = tf.image.random_flip_left_right(image)\n      return image, label \n    ```", "```\n    data = data.cache()\n    data = data.map(augment_img, num_parallel_calls=tf.data.AUTOTUNE)\n    train_data = data.shuffle(1024).batch(32)\n    train_data = train_data.prefetch(tf.data.AUTOTUNE)\n    val_data = val_data.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n    val_data = val_data.batch(32)\n    val_data = val_data.cache()\n    val_data = val_data.prefetch(tf.data.AUTOTUNE) \n    ```", "```\n    %time history = model.fit(train_data, epochs=10, validation_data=val_data, validation_steps=1) \n    ```", "```\nimport tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\ntflite_model = converter.convert()\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model) \n```", "```\nimport tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_quant_model = converter.convert()\nopen(\"converted_model.tflite\", \"wb\").write(tflite_quant_model) \n```", "```\ntry (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) {\n  interpreter.run(input, output);\n} \n```", "```\nGpuDelegate delegate = new GpuDelegate();\nInterpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);\nInterpreter interpreter = new Interpreter(tensorflow_lite_model_file, options);\ntry {\n  interpreter.run(input, output);\n} \n```", "```\ngit clone https://github.com/tensorflow/examples \n```", "```\nbrew tap adoptopenjdk/openjdk\nbrew cask install  homebrew/cask-versions/adoptopenjdk8 \n```", "```\n    keras_model = â€¦\n    keras_model.compile(...)\n    keras_federated_model = tff.learning.from_compiled_keras_model(keras_model, ..) \n    ```", "```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <!-- Import TensorFlow.js -->\n  <script src=\"img/tf.min.js\"></script>\n  <!-- Import tfjs-vis -->\n  <script src=\"img/tfjs-vis.umd.min.js\"></script>\n  <!-- Import the data file -->\n  <script src=\"img/data.js\" type=\"module\"></script>\n  <!-- Import the main script file -->\n  <script src=\"img/script.js\" type=\"module\"></script>\n</head>\n<body>\n</body>\n</html> \n```", "```\npython -m http.server \n```", "```\nwget -cO - https://storage.googleapis.com/tfjs-tutorials/mnist_data.js > data.js \n```", "```\nfunction getModel() {\n  const IMAGE_WIDTH = 28;\n  const IMAGE_HEIGHT = 28;\n  const IMAGE_CHANNELS = 1;  \n  const NUM_OUTPUT_CLASSES = 10;\n\n  const model = tf.sequential();\n  model.add(tf.layers.conv2d({\n    inputShape: [IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS],\n    kernelSize: 5,\n    filters: 8,\n    strides: 1,\n    activation: 'relu',\n    kernelInitializer: 'varianceScaling'\n  }));\n  model.add(tf.layers.maxPooling2d({\n    poolSize: [2, 2], strides: [2, 2]\n  }));\n  model.add(tf.layers.conv2d({\n    kernelSize: 5,\n    filters: 16,\n    strides: 1,\n    activation: 'relu',\n    kernelInitializer: 'varianceScaling'\n  }));\n  model.add(tf.layers.maxPooling2d({\n    poolSize: [2, 2], strides: [2, 2]\n  }));\n  model.add(tf.layers.flatten());\n  model.add(tf.layers.dense({\n    units: NUM_OUTPUT_CLASSES,\n    kernelInitializer: 'varianceScaling',\n    activation: 'softmax'\n  }));\n  const optimizer = tf.train.adam();\n  model.compile({\n    optimizer: optimizer,\n    loss: 'categoricalCrossentropy',\n    metrics: ['accuracy'],\n  });\n  return model;\n} \n```", "```\nasync function train(model, data) {\n  const metrics = ['loss', 'val_loss', 'acc', 'val_acc'];\n  const container = {\n    name: 'Model Training', tab: 'Model', styles: { height: '1000px' }\n  };\n  const fitCallbacks = tfvis.show.fitCallbacks(container, metrics);\n\n  const BATCH_SIZE = 512;\n  const TRAIN_DATA_SIZE = 5500;\n  const TEST_DATA_SIZE = 1000;\n  const [trainXs, trainYs] = tf.tidy(() => {\n    const d = data.nextTrainBatch(TRAIN_DATA_SIZE);\n    return [\n      d.xs.reshape([TRAIN_DATA_SIZE, 28, 28, 1]),\n      d.labels\n    ];\n  });\n  const [testXs, testYs] = tf.tidy(() => {\n    const d = data.nextTestBatch(TEST_DATA_SIZE);\n    return [\n      d.xs.reshape([TEST_DATA_SIZE, 28, 28, 1]),\n      d.labels\n    ];\n  });\n  return model.fit(trainXs, trainYs, {\n    batchSize: BATCH_SIZE,\n    validationData: [testXs, testYs],\n    epochs: 10,\n    shuffle: true,\n    callbacks: fitCallbacks\n  });\n} \n```", "```\nconst classNames = [\n  'Zero', 'One', 'Two', 'Three', 'Four', \n  'Five', 'Six', 'Seven', 'Eight', 'Nine'];\nfunction doPrediction(model, data, testDataSize = 500) {\n  const IMAGE_WIDTH = 28;\n  const IMAGE_HEIGHT = 28;\n  const testData = data.nextTestBatch(testDataSize);\n  const testxs = testData.xs.reshape(\n    [testDataSize, IMAGE_WIDTH, IMAGE_HEIGHT, 1]);\n  const labels = testData.labels.argMax([-1]);\n  const preds = model.predict(testxs).argMax([-1]);\n  testxs.dispose();\n  return [preds, labels];\n}\nasync function showAccuracy(model, data) {\n  const [preds, labels] = doPrediction(model, data);\n  const classAccuracy = await tfvis.metrics.perClassAccuracy(\n    labels, preds);\n  const container = {name: 'Accuracy', tab: 'Evaluation'};\n  tfvis.show.perClassAccuracy(container, classAccuracy, classNames);\n  labels.dispose();\n}\nasync function showConfusion(model, data) {\n  const [preds, labels] = doPrediction(model, data);\n  const confusionMatrix = await tfvis.metrics.confusionMatrix(\n    labels, preds);\n  const container = {name: 'Confusion Matrix', tab: 'Evaluation'};\n  tfvis.render.confusionMatrix(\n      container, {values: confusionMatrix}, classNames);\n  labels.dispose();\n} \n```", "```\nimport {MnistData} from './data.js';\nasync function run() { \n  const data = new MnistData();\n  await data.load();\n  await showExamples(data);\n  const model = getModel();\n  tfvis.show.modelSummary({name: 'Model Architecture', tab: 'Model'}, model);\n  await train(model, data);\n  await showAccuracy(model, data);\n  await showConfusion(model, data);\n}\ndocument.addEventListener('DOMContentLoaded', run); \n```", "```\ntensorflowjs_converter --input_format=keras /tmp/model.h5 /tmp/tfjs_model \n```", "```\npip install tensorflowjs \n```", "```\n<html>\n  <head>\n    <!-- Load TensorFlow.js -->\n    <script src=\"img/tfjs\"></script>\n    <!-- Load MobileNet -->\n    <script src=\"img/mobilenet\"></script>\n    <!-- Load KNN Classifier -->\n    <script src=\"img/knn-classifier\"></script>\n  </head> \n```", "```\nimport * as tf from '@tensorflow/tfjs-node' \n```", "```\nimport * as tf from '@tensorflow/tfjs-node-gpu' \n```", "```\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({ units: 1, inputShape: [400] }));\nmodel.compile({\n  loss: 'meanSquaredError',\n  optimizer: 'sgd',\n  metrics: ['MAE']\n}); \n```", "```\nconst xs = tf.randomUniform([10000, 400]);\nconst ys = tf.randomUniform([10000, 1]);\nconst valXs = tf.randomUniform([1000, 400]);\nconst valYs = tf.randomUniform([1000, 1]);\nasync function train() {\n  await model.fit(xs, ys, {\n    epochs: 100,\n    validationData: [valXs, valYs],\n  });\n}\ntrain(); \n```"]