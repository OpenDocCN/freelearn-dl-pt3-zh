<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predictive Maintenance for IoT</h1>
                </header>
            
            <article>
                
<p>In <strong>Internet of Things</strong> (<strong>IoT</strong>) devices, streaming data is generated for one event at a time. DL-based approaches can examine this data in order to diagnose the problem across the fleet in real time, and the future health of individual units can be predicted in order to enable on-demand maintenance. This strategy is known as <strong>predictive</strong> (or <strong>condition-based</strong>) <strong>maintenance</strong>. This approach is now emerging as one of the most promising and lucrative industrial applications of the IoT.</p>
<p>Considering these motivations, in this chapter, we will look at how to develop a DL solution for predictive maintenance for IoT using the <strong>Turbofan Engine Degradation Simulation</strong> dataset. The idea behind predictive maintenance is to determine whether the failure patterns of various types can be predictable. Furthermore, we will discuss how to collect data from IoT-enabled devices for the predictive maintenance. In a nutshell, the following topics will be covered in this chapter:</p>
<ul>
<li>Predictive maintenance for IoT</li>
<li>Developing a predictive maintenance application</li>
<li>Preparing the data</li>
<li>Training ML baselines</li>
<li>Training the LSTM model</li>
<li>Evaluating the model</li>
<li>FAQs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predictive maintenance for IoT</h1>
                </header>
            
            <article>
                
<p class="mce-root">With advances in real-time data capture and streaming architecture, it is now possible to have real-time data monitoring, where an organization can gain real-time insight into individual components and all processes. Monitoring still requires active involvement and quick responses—for example, an oil well sensor that is indicating increased temperature or volume or a network traffic for bot-net activity or insider threats.</p>
<p class="mce-root"/>
<p>Let's consider a real-world example called <strong>equipment failures in industrial engineering</strong>, which is always considered a costly issue. Conducting preventative maintenance at regular intervals has always been the conventional strategy. Consequently, the schedules tend to be very unadventurous, which is often based on operator experience. This manual intervention has several downsides. Firstly, it tends to increase maintenance costs. Secondly, it's impossible to adapt such a setting to a highly complex or changing industrial scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting IoT data in an industrial setting</h1>
                </header>
            
            <article>
                
<p>According to <strong>RT Insights</strong>, a single jet engine could cost $16 million, and on a transatlantic flight it can consume 36,000 gallons of fuel. Today's airline fuel prices come to around $54,000 per trip, or more than $5,000 an hour. <span class="ILfuVd">The majority of jet engines are gas turbine engines</span> in which the thermal energy is converted into kinetic energy by expanding through nozzles, then into rotational mechanical energy in a spinning rotor. Such engines produce huge amounts of IoT data. Let's try to perceive how predictive maintenance with ML could help us to reduce the maintenance costs.</p>
<p>The first step is to collect the sensor data representing healthy and faulty operations under different operating conditions, for example, temperature, flow, and pressure. In a real-life scenario, those might be deployed in different environments and locations (suppose you are in Siberia at an operating temperature of -20 degree Celsius with high fluid viscosity, and another one in a Middle Eastern country with a temperature of 45 degree Celsius with high fluid viscosity).</p>
<p>Even though both of them are supposed to work normally, one of the engines might fail sooner because of different operating conditions. Unfortunately, without having enough data, there's no further way to investigate the root cause of the failure. Once such a jet turbine engine is deployed, sensor data can be collected using streaming technologies in the following settings:</p>
<ul>
<li>Real sensor data from normal system operations</li>
<li>Real sensor data from a system operating in a faulty condition</li>
<li>Real sensor data from system failures (<em>run-to-failure</em> data)</li>
</ul>
<p class="mce-root"/>
<p>However, if we don't have many such engines deployed, we won't have much data, which would represent both healthy and faulty conditions and operations. There are two workarounds to overcome this data scarcity:</p>
<ul>
<li>Using historical data from similar/ the same engine, which might resemble the currently-deployed engines.</li>
<li>Secondly, we can build a mathematical model of the engines and estimate their parameters from the available sensor data. Based on the statistical distribution and operating conditions, we can then generate failure data.</li>
</ul>
<p>If we go with the second option, after generating the sensor data, we can combine them with the real sensor data to generate large-scale sensor data for the developing predictive maintenance model, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4910870-43c7-4ad2-86a0-fc496bcb2aa0.png" style="width:50.83em;height:34.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML techniques for predictive maintenance</h1>
                </header>
            
            <article>
                
<p><strong>Deep learning</strong> (<strong>DL</strong>) techniques can be applied to process the massive amount of IoT data and can be an appealing emerging alternative to classical machine learning algorithms. The idea is that when equipment is given with sensors and networked, a huge amount of sensor data is produced. In a more complex industrial setting, data from the sensor channels is quite noisy and fluctuates over time, but some of the data does not seem to change at all. This is more-or-less true for every industrial setting because the data produced in an IoT setting is a multivariate series of sensor measurements each with its own amount of noise containing many missing or uninformative values.</p>
<p>A key step in predictive maintenance application development is identifying the <strong>Condition Indicators</strong> (<strong>CIs</strong>) and features from the collected sensor data, inspecting the behavior changes of CIs in a predictable way as the system degrades. Usually, CIs contain features that help distinguish normal and faulty operations and predict <strong>Remaining Useful Life</strong> (<strong>RUL</strong>).</p>
<p>The RUL of an engine or machine is the expected life or usage time remaining before the engine requires repair or replacement. Consequently, predicting RUL from sensor data is key in many predictive-maintenance applications. In the following diagram, we can see that the peaks in the frequency data shifts to the left as the turbine engine degrades. Therefore, the peak frequency can serve as condition indicators:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/39ae6a54-b44e-4921-92c5-96cb0a42c3f0.png" style="width:63.42em;height:15.50em;"/></p>
<p>CIs can help us understand healthy and faulty operation in the turbine engine. However, they don't tell us what parts need to be repaired or how much time remains until the failure occurs. We either identify the fault types before fixing or predict the RUL before the scheduled maintenance. For the former option, use the extracted CIs features to train an ML or DL model and identify the fault types, such as seal leakage, blocked inlet, or worn bearing, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cd8200d6-66a0-4393-b77d-bac7fb44f88b.png" style="width:73.25em;height:14.08em;"/></p>
<p>For the latter strategy, we can also train the ML/DL model to predict the trend that the pumps will continue to transition between these two states (current condition and the failure). A DL model can capture the relationships between CI features, and the degradation path of the turbine engine will help us to predict how much time we have until the next failure or when we should schedule maintenance, as depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e543f232-57e9-49d5-9d28-cbc13f1dfcd0.png" style="width:74.58em;height:15.25em;"/></p>
<p>Finally, a stable model can be deployed in an industrial setting. The preceding steps can be summarized in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-982 image-border" src="assets/f413846e-0190-45cc-b5e9-346f65906fa6.png" style="width:44.17em;height:21.75em;"/></p>
<p>Unfortunately, due to the lack of sensor data for predicting fault types, in the next section, we will see a hands-on example on predicting RUL using both ML and DL techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – PM for an aircraft gas turbine engine</h1>
                </header>
            
            <article>
                
<p>To give a real-life glimpse into predictive maintenance, we will use the open source <strong>Turbofan Engine Degradation Simulation</strong> dataset, which was released in 2008 by the <strong>Prognostics Center of Excellence</strong> at NASA's Ames research centre. The dataset can be downloaded from <a href="https://ti.arc.nasa.gov/c/6/">https://ti.arc.nasa.gov/c/6/</a>. We're thankful to the authors of the following research for providing this dataset:</p>
<div class="packt_quote packt_infobox"><span>Turbofan Engine Degradation Simulation Data Set, A Saxena and K Goebel (2008), NASA Ames Prognostics Data Repository (<a href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/">https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/</a>), NASA Ames Research Center, Moffett Field, CA.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Describing the dataset</h1>
                </header>
            
            <article>
                
<p>The dataset consists of sensor readings from a fleet of simulated aircraft gas turbine engines operating conditions as a multiple multivariate time series. The dataset consists of separate training and test sets. The testset is similar to the training set, except that each engine's measurements are truncated some (unknown) amount of time before it fails. The data is provided as a ZIP-compressed text file with 26 columns of numbers. Each row represents a snapshot of data taken during a single operational cycle and each column represents a different variable. The columns correspond to the following attributes:</p>
<ul>
<li class="mce-root">Unit number</li>
<li class="mce-root">Time, in cycles</li>
<li class="mce-root">Operational setting 1</li>
<li class="mce-root">Operational setting 2</li>
<li class="mce-root">Operational setting 3</li>
<li class="mce-root">Sensor measurement 1</li>
<li class="mce-root">Sensor measurement 2</li>
<li class="mce-root">Sensor measurement 26</li>
</ul>
<p class="mce-root"/>
<p>In addition, the dataset has a vector of true RUL values for the data, which will be used as the ground truths for training the models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory analysis</h1>
                </header>
            
            <article>
                
<p>To give an idea of the sensor readings in areas such as the physical state of the engine (for example, with regard to the temperature of a component, the fan speed of the turbine, and so on) we decided to extract the first unit from the first dataset for all the sensors on a single engine. For this, we have written a script (see <kbd>make_dataset.py</kbd>) that gets all of the data files from the input directory. Then it parses a set of raw data files into a single DataFrame object and returns an aggregated representation of all files with the appropriate column names:</p>
<pre class="mce-root">data_sets = []<br/>    <strong>for</strong> data_file in glob(file_pattern):<br/>        <strong>if</strong> label_data:<br/>            # read in contents as a DataFrame<br/>            subset_df = pd.read_csv(data_file, header=None)<br/>            # need to create a unit_id column explicitly<br/>            unit_id = <strong>range</strong>(1, subset_df.shape[0] + 1)<br/>            subset_df.insert(0, 'unit_id', unit_id)<br/>        <strong>else</strong>:<br/>            # read in contents as a DataFrame<br/>            subset_df = pd.read_csv(data_file, sep=' ', header=None, usecols=range(26))<br/>        # extract the id of the dataset from the name and add as a column<br/>        dataset_id = basename(data_file).split("_")[1][:5]<br/>        subset_df.insert(0, 'dataset_id', dataset_id)<br/>        # add to list<br/>        data_sets.append(subset_df)<br/>    # combine dataframes<br/>    df = pd.concat(data_sets)<br/>    df.columns = columns<br/>    # return the result<br/><br/>    <strong>return</strong> df</pre>
<p>To use this script, first copy all the files in the <kbd>data/raw/</kbd> directory, and then execute the following command:</p>
<pre><strong>$python3 make_dataset.py data/raw/ /data/processed/</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This command will generate three files—<kbd>train.csv</kbd>, <kbd>test.csv</kbd>, and <kbd>RUL.csv</kbd>—for the training set, testset, and labels, respectively. Now that our dataset is ready for exploratory analysis, we can now read each CSV file as a pandas DataFrame:</p>
<pre class="mce-root"># load the processed data in CSV format<br/>train_df = pd.read_csv('train.csv')<br/>test_df = pd.read_csv('test.csv')<br/>rul_df = pd.read_csv('RUL.csv')<br/><br/># for convenience, identify the sensor and operational setting columns<br/>sensor_columns = [col for col in train_df.columns if col.startswith("sensor")]<br/>setting_columns = [col for col in train_df.columns if col.startswith("setting")]</pre>
<p>Then, extract the first unit from the first dataset:</p>
<pre>slice = train_df[(train_df.dataset_id == 'FD001') &amp; (train_df.unit_id == 1)]</pre>
<p>Then, we plot its sensor traces over time on a 7 * 3 = 21 plots grid to see all sensor channels. We have to plot the channel corresponding to this position:</p>
<pre>fig, axes = plt.subplots(7, 3, figsize=(15, 10), sharex=True)<br/><br/><strong>for</strong> index, ax <strong>in</strong> enumerate(axes.ravel()):<br/>    sensor_col = sensor_columns[index]<br/>    slice.plot(x='cycle', y=sensor_col, ax=ax, color='blue');<br/>    # label formatting<br/>    if index % 3 == 0:<br/>        ax.set_ylabel("Sensor reading", size=10);<br/>    else:<br/>        ax.set_ylabel("");<br/>    ax.set_xlabel("Time (cycle)");<br/>    ax.set_title(sensor_col.title(), size=14);<br/>    ax.legend_.remove();<br/><br/># plot formatting<br/>fig.suptitle("Sensor reading : unit 1, dataset 1", size=20, y=1.025)<br/>fig.tight_layout();</pre>
<p class="mce-root"/>
<p class="mce-root">As seen in the following diagram, data from the sensor channels is quite noisy and fluctuates over time, while other data does not seem to change at all. Each sensor's life cycle is different in terms of the starting and ending value on the <em>x</em> axis:<q><br/></q></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bb6ee94c-00be-4c6d-8df2-d6ca8e001d0b.png" style="width:54.83em;height:37.58em;"/></p>
<p class="CDPAlignLeft CDPAlign">We can see that each engine has a slightly different lifetime and failure pattern. Next, we can visualize the data from all the sensor channels against time for a random sample of 10 engines from the training set:</p>
<pre class="mce-root"># randomly select 10 units from dataset 1 to plot<br/>all_units = train_df[train_df['dataset_id'] == 'FD001']['unit_id'].unique()<br/>units_to_plot = np.random.choice(all_units, size=10, replace=False)<br/><br/># get the data for these units<br/>plot_data = train_df[(train_df['dataset_id'] == 'FD001') &amp;<br/>                     (train_df['unit_id'].isin(units_to_plot))].copy()<br/><br/># plot their sensor traces (overlaid)<br/>fig, axes = plt.subplots(7, 3, figsize=(15, 10), sharex=True)<br/><br/>for index, ax in enumerate(axes.ravel()):<br/>    sensor_col = sensor_columns[index]<br/><br/>    for unit_id, group in plot_data.groupby('unit_id'):<br/>        # plot the raw sensor trace<br/>        (group.plot(x='cycle', y=sensor_col, alpha=0.45, ax=ax, color='gray', legend=False));<br/>        # overlay the 10-cycle rolling mean sensor trace for visual clarity<br/>        (group.rolling(window=10, on='cycle')<br/>             .mean()<br/>             .plot(x='cycle', y=sensor_col, alpha=.75, ax=ax, color='black', legend=False));<br/>    <br/>    # label formatting<br/>    if index % 3 == 0:<br/>        ax.set_ylabel("Sensor Value", size=10);<br/>    else:<br/>        ax.set_ylabel("");<br/>    ax.set_title(sensor_col.title());<br/>    ax.set_xlabel("Time (Cycles)");<br/><br/># plot formatting<br/>fig.suptitle("All Sensor Traces: Dataset 1 (Random Sample of 10 Units)", size=20, y=1.025);<br/>fig.tight_layout();</pre>
<p>The preceding code segment shows the following graph of random samples of 10 units from the sensor reading from dataset 1:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aadcd010-d6a2-44dd-9850-d26e05de78e2.png" style="width:61.08em;height:42.25em;"/></p>
<p>From the preceding graph, we can inspect that an engine's progress with respect to time is not quite aligned with the others. This is an impedance that does not allow us to compare the fifth cycle of one engine to the fifth cycle of another, for example. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inspecting failure modes</h1>
                </header>
            
            <article>
                
<p>Since it is already known when each engine in the training set will fail, we can compute a <strong>time before failure</strong> value at each time step, which can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Time before failure (TBF) = engine elapsed life at failure time (EEL) - total operating lifetime (TOL)</em></p>
<p>This number can be considered as the countdown to failure for each engine, which allows us to align different engines' data to a common end:</p>
<pre class="mce-root"># generate the lifetimes series<br/>lifetimes = train_df.groupby(['dataset_id', 'unit_id'])['cycle'].max()<br/><br/># apply the above function to the data we're plotting<br/>plot_data['ctf'] = plot_data.apply(lambda r: cycles_until_failure(r, lifetimes), axis=1)<br/><br/># plot the sensor traces (overlaid)<br/>fig, axes = plt.subplots(7, 3, figsize=(15, 10), sharex=True)<br/>for index, ax in enumerate(axes.ravel()):<br/>    sensor_col = sensor_columns[index]<br/>    # use the same subset of data as above<br/>    for unit_id, group in plot_data.groupby('unit_id'):<br/>        # plot the raw sensor trace, using ctf on the time axis<br/>        (group.plot(x='ctf', y=sensor_col, alpha=0.45, ax=ax, color='gray', legend=False));<br/><br/>        # overlay the 10-cycle rolling mean sensor trace for visual clarity<br/>        (group.rolling(window=10, on='ctf')<br/>             .mean()<br/>             .plot(x='ctf', y=sensor_col, alpha=.75, ax=ax, color='black', legend=False));<br/><br/>    # label formatting<br/>    if index % 3 == 0:<br/>        ax.set_ylabel("Sensor Value", size=10);<br/>    else:<br/>        ax.set_ylabel("");<br/>    ax.set_title(sensor_col.title());<br/>    ax.set_xlabel("Time Before Failure (Cycles)");<br/><br/>    # add a vertical red line to signal common time of failure<br/>    ax.axvline(x=0, color='r', linewidth=3);<br/><br/>    # extend the x-axis to compensate<br/>    ax.set_xlim([None, 10]);<br/>fig.suptitle("All Sensor Traces: Dataset 1 (Random Sample of 10 Units)", size=20, y=1.025);<br/>fig.tight_layout();</pre>
<p class="mce-root"/>
<p>The following shows the sensor channels in the same engines. The only difference is that the previous graph is plotted against the time before failure, where each engine ends at the same instant (<em>t=0</em>). It also gives us a common pattern across different engines, which shows that some sensor readings consistently rise or fall right before a failure, while others—for example, sensor 14—exhibit different failure behaviors<q>:</q></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f855574e-b052-403b-9850-f700f005902c.png"/></p>
<p>This pattern is very common in many predictive maintenance problems: failure is often a confluence of different processes, and as a result, things in the real world are likely to exhibit multiple failure modes. Due to this unpredictable pattern of data, predicting the RUL is very challenging.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction challenges</h1>
                </header>
            
            <article>
                
<p>As shown in the following diagram, after observing the engine's sensor measurements and operating conditions for a certain amount of time (133 cycles in the diagram), the challenge is to predict the amount of time (in other words, the RUL) that the engine will continue to function before it fails:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a8291ed7-5473-4b7f-bd0a-29eb8e31a583.png" style="width:64.67em;height:9.75em;"/></p>
<p>However, making an incorrect prediction for an ML/DL model is basically underestimating the true RUL of a particular engine. This can bring the turbine engine to maintenance too early, when it could have operated for a bit longer without any issues arising. So, what would happen if our model were to overestimate the true RUL instead? In that case, we might allow a degrading aircraft to keep flying, and risk catastrophic engine failure. Clearly, the costs of these two outcomes would not be the same. Considering these challenges, in the next section, we will focus on using DL-based techniques for predicting RUL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL for predicting RLU</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>As we have discussed, we are trying to calculate the amount of time before</span> an engine <span>needs maintenance.</span> What makes this dataset special is that the engines run all the way until failure, giving us the precise RLU information for every engine at every point in time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating cut-off times</h1>
                </header>
            
            <article>
                
<p>Let's consider the <kbd>FD004</kbd> dataset, that contains as much as 249 engines (<kbd>engine_no</kbd>) monitored over time (<kbd>time_in_cycles</kbd>). Each engine has <kbd>operational_settings</kbd> and <kbd>sensor_measurements</kbd> recorded for each cycle:</p>
<pre>data_path <strong>=</strong> 'train_FD004.txt'<br/>data <strong>=</strong> utils.load_data(data_path)</pre>
<p>To train a model that will predict RUL, we can simulate real predictions by choosing a random point in the life of the engine and only using the data from before that point. We can create features with that restriction easily by using cut-off times:</p>
<pre class="mce-root">def make_cutoff_times(data):<br/>    gb = data.groupby(['unit_id'])<br/>    labels = []<br/>    for engine_no_df in gb:<br/>        instances = engine_no_df[1].shape[0]<br/>        label = [instances - i - 1 for i in range(instances)]<br/>        labels += label<br/>    return new_labels(data, labels)</pre>
<p>The preceding function generates the cut-off times by sampling for both <kbd>cutoff_time</kbd> and <kbd>label</kbd>, which can be called as follows:</p>
<pre>cutoff_times = utils.make_cutoff_times(data)<br/>cutoff_times.head()</pre>
<p>The preceding lines of code show the following RUL and cut-off time for five engines only:<q><br/></q></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2e644b28-8122-42c1-be67-27fa8e49a973.png" style="width:19.75em;height:12.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep feature synthesis</h1>
                </header>
            
            <article>
                
<p>Then, we generate the features using <strong>Deep Feature Synthesis</strong> (<strong>DFS</strong>). For this, we need to establish an entity set structure for our data. We can create an engines entity by normalizing the <kbd>engine_no</kbd> column in the raw data<span>:</span></p>
<pre>def make_entityset(data):<br/>    es = ft.EntitySet('Dataset')<br/>    es.entity_from_dataframe(dataframe=data,<br/>                             entity_id='recordings',<br/>                             index='index',<br/>                             time_index='time')<br/>    es.normalize_entity(base_entity_id='recordings',<br/>                        new_entity_id='engines',<br/>                        index='engine_no')<br/>    es.normalize_entity(base_entity_id='recordings',<br/>                        new_entity_id='cycles',<br/>                        index='time_in_cycles')<br/>    return es<br/>es = make_entityset(data)</pre>
<p>The preceding code block will generate the following statistics of the entity set:<br/></p>
<pre class="mce-root"><strong>Entityset: Dataset</strong><br/><strong>   Entities:</strong><br/><strong>     recordings [Rows: 20631, Columns: 28]</strong><br/><strong>     engines [Rows: 100, Columns: 2]</strong><br/><strong>     cycles [Rows: 362, Columns: 2]</strong><br/><strong>   Relationships:</strong><br/><strong>     recordings.engine_no -&gt; engines.engine_no</strong><br/><strong>     recordings.time_in_cycles -&gt; cycles.time_in_cycles</strong></pre>
<p>The <kbd>ft.dfs</kbd> <span>function</span><span> </span><span>takes an entity set and stacks primitives such as <kbd>max</kbd>, <kbd>min</kbd>, and <kbd>last</kbd> exhaustively across entities:</span></p>
<pre class="mce-root">fm, features = ft.dfs(entityset=es,<br/>                      target_entity='engines',<br/>                      agg_primitives=['last', 'max', 'min'],<br/>                      trans_primitives=[],<br/>                      cutoff_time=cutoff_times,<br/>                      max_depth=3,<br/>                      verbose=True)<br/>fm.to_csv('FM.csv')</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML baselines</h1>
                </header>
            
            <article>
                
<p>Now that we have generated the features, we can start training a first ML model called <kbd>RandomForestRegressor</kbd>. Then, we will gradually move to using DL using <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) network. <strong>Random forest</strong> (<strong>RF</strong>) is an ensemble technique that builds several decision trees and integrates them together to get a more accurate and stable prediction. In general, a deeper tree signifies more complex decision rules and a better-fitted model for example the following image shows <span>Decision tree for university admission data</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4dfcb6f0-e778-4951-b1a7-df7b0f07c401.png" style="width:53.58em;height:52.17em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Consequently, the deeper the tree, the more complex the decision rules and the better fitted the model is. This is a direct consequence of Random Forest. In other words, the final prediction based on the majority vote from a panel of independent juries is always better and more reliable than the best jury. The following diagram shows r<span>andom forest and its assembling technique</span>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/18820971-61e8-437d-bef1-7aab87e7e702.png" style="width:69.58em;height:31.92em;"/></div>
<p>So, let's get started by preparing the separate training set and test set:</p>
<pre class="mce-root">fm = pd.read_csv('FM.csv', index_col='engine_no')<br/>X = fm.copy().fillna(0)<br/>y = X.pop('RUL')<br/>X_train, X_test, y_train, y_test = train_test_split(X, y)</pre>
<p class="mce-root">Then, using the training set, we will check the following baselines:</p>
<ul>
<li>Always predict the median value of <kbd>y_train</kbd>.</li>
<li>Always predict the RUL as if every engine has the median lifespan in <kbd>X_train</kbd>.</li>
</ul>
<p>We will check those predictions by finding the mean of the absolute value of the errors called the <strong>Mean Absolute Error</strong> (<strong>MAE</strong>) using <kbd>RandomForestRegressor</kbd> from scikit-learn:</p>
<pre>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_absolute_error<br/><br/>yhat_median_predict = [np.median(y_train) for _ in y_test]<br/>print('Baseline by median label: MAE = {:.2f}'.format(<br/>    mean_absolute_error(yhat_median_predict, y_test)))<br/><br/># Collect sensor readings from the sensor in training set<br/>recordings_from_train = es['recordings'].df[es['recordings'].df['engine_no'].isin(y_train.index)]<br/>median_life = np.median(recordings_from_train.groupby(['engine_no']).apply(lambda df: df.shape[0]))<br/><br/># Collect sensor readings from the sensor in training set<br/>recordings_from_test = es['recordings'].df[es['recordings'].df['engine_no'].isin(y_test.index)]<br/>life_in_test = recordings_from_test.groupby(['engine_no']).apply(lambda df: df.shape[0])-y_test<br/><br/># Compute mean absolute error as the baseline by meadian of the RUL<br/>yhat_median_predict2 = (median_life - life_in_test).apply(lambda row: max(row, 0))<br/>print('Baseline by median life: MAE = {:.2f}'.format(<br/>    mean_absolute_error(yhat_median_predict2, y_test)))</pre>
<p class="mce-root">The preceding code block should produce the following output showing the baseline <kbd>MAE</kbd> values:</p>
<pre class="mce-root"><strong>Baseline by median label: MAE = 66.72</strong><br/><strong>Baseline by median life: MAE = 59.96</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making predictions</h1>
                </header>
            
            <article>
                
<p>Now we can use our created features to fit <kbd>RandomForestRegressor</kbd> to our data and see whether we can improve on the previous scores:</p>
<pre>rf = RandomForestRegressor() # first we instantiate RandomForestRegressor from scikit-learn<br/>rf.fit(X_train, y_train) # train the regressor model with traing set<br/>   <br/>preds = rf.predict(X_test) # making predictin on unseen observation <br/>scores = mean_absolute_error(preds, y_test) # Computing MAE<br/><br/>print('Mean Abs Error: {:.2f}'.format(scores))<br/>high_imp_feats = utils.feature_importances(X, reg, feats=10) # Printing feature importance</pre>
<p class="mce-root">The preceding code block should produce the following output showing the baseline MAE values and statistics about the engine recording cycles:</p>
<pre class="mce-root"><strong>Mean Abs Error: 31.04</strong><br/><strong> 1: LAST(recordings.cycles.LAST(recordings.sensor_measurement_4)) [0.395]</strong><br/><strong> 2: LAST(recordings.sensor_measurement_4) [0.192]</strong><br/><strong> 3: MAX(recordings.sensor_measurement_4) [0.064]</strong><br/><strong> 4: LAST(recordings.cycles.MIN(recordings.sensor_measurement_11)) [0.037]</strong><br/><strong> 5: LAST(recordings.cycles.MAX(recordings.sensor_measurement_12)) [0.029]</strong><br/><strong> 6: LAST(recordings.sensor_measurement_15) [0.020]</strong><br/><strong> 7: LAST(recordings.cycles.MAX(recordings.sensor_measurement_11)) [0.020]</strong><br/><strong> 8: LAST(recordings.cycles.LAST(recordings.sensor_measurement_15)) [0.018]</strong><br/><strong> 9: MAX(recordings.cycles.MAX(recordings.sensor_measurement_20)) [0.016]</strong><br/><strong> 10: LAST(recordings.time_in_cycles) [0.014]</strong></pre>
<p class="mce-root">Then, we have to prepare both the features and label, which we can do using the following code:</p>
<pre>data2 = utils.load_data('test_FD001.txt')<br/>es2 = make_entityset(data2)<br/>fm2 = ft.calculate_feature_matrix(entityset=es2, features=features, verbose=True)<br/>fm2.head()</pre>
<p class="mce-root">The loaded data should have 41,214 recordings from 249 engines in which 21 sensor measurements are used under three operational settings. Then, we have to prepare both the features and labels using the loaded data, which we can do using the following code:</p>
<pre>X = fm2.copy().fillna(0)<br/>y = pd.read_csv('RUL_FD004.txt', sep=' ', header=-1, names=['RUL'], index_col=False)<br/><br/>preds2 = rf.predict(X)<br/>print('Mean Abs Error: {:.2f}'.format(mean_absolute_error(preds2, y)))<br/><br/>yhat_median_predict = [np.median(y_train) for _ in preds2]<br/>print('Baseline by median label: MAE = {:.2f}'.format(<br/>    mean_absolute_error(yhat_median_predict, y)))<br/><br/>yhat_median_predict2 = (median_life - es2['recordings'].df.groupby(['engine_no']).apply(lambda df: df.shape[0])).apply(lambda row: max(row, 0))<br/><br/>print('Baseline by median life: MAE = {:.2f}'.format(<br/>    mean_absolute_error(yhat_median_predict2 y)))</pre>
<p class="mce-root">The preceding code block should produce the following output showing the prediced MAE and baseline MEA values:</p>
<pre class="mce-root"><strong>Mean Abs Error: 40.33</strong><br/><strong>Baseline by median label: Mean Abs Error = 52.08</strong><br/><strong>Baseline by median life: Mean Abs Error = 49.55</strong></pre>
<p>As seen, the predicted MAE value is lower than both baseline MAE values. Next, we try to improve the MAE even more using the LSTM network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving MAE with LSTM</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will use the Keras-based LSTM network to predict RUL. However, for this, we <span>first </span><span>need to convert the data so that the LSTM model, which expects data in three-dimensional format, can consume it:</span></p>
<pre class="mce-root">#Prepare data for Keras based LSTM model<br/><strong>def</strong> prepareData(X, y):<br/>    X_train, X_test, y_train, y_test = train_test_split(X, y)<br/>    X_train = X_train.as_matrix(columns=None)<br/>    X_test = X_test.as_matrix(columns=None)<br/>    y_train = y_train.as_matrix(columns=None)<br/>    y_test = y_test.as_matrix(columns=None)<br/>    y_train = y_train.reshape((y_train.shape[0], 1))<br/>    y_test = y_test.reshape((y_test.shape[0], 1))<br/>    X_train = np.reshape(X_train,(X_train.shape[0], 1, X_train.shape[1]))<br/>    X_test = np.reshape(X_test,(X_test.shape[0], 1, X_test.shape[1]))    <br/>    return X_train, X_test, y_train, y_test</pre>
<p class="mce-root">Now that we have the data appropriate for the LSTM model, we can construct the LSTM network. For this, we have a fancy LSTM network that has only an LSTM layer followed by a dense layer, before we apply a dropout layer for better regularization. Then, we have another dense layer, before we project the output from this dense layer through to the activation layer using the linear activation function so that it outputs real-value outputs. We then use the SGD version, called <kbd>RMSProp</kbd>, which tries to optimize the <strong>Mean Square Error</strong> (<strong>MSE</strong>):</p>
<pre>#Create LSTM model<br/>from keras.models import Sequential<br/>from keras.layers.core import Dense, Activation<br/>from keras.layers.recurrent import LSTM<br/>from keras.layers import Dropout<br/>from keras.layers import GaussianNoise<br/><br/><strong>def</strong> createLSTMModel(X_train, hidden_neurons):<br/>    model = Sequential()<br/>    model.add(LSTM(hidden_neurons, input_shape=(X_train.shape[1], X_train.shape[2])))<br/>    model.add(Dense(hidden_neurons))<br/>    model.add(Dropout(0.7))<br/>    model.add(Dense(1))<br/>    model.add(Activation("linear"))<br/>    model.compile(loss="mean_squared_error", optimizer="rmsprop")<br/>    return model</pre>
<p class="mce-root">Then, we train the LSTM model with the training set:</p>
<pre>X_train, X_test, y_train, y_test = prepareData(X, y)<br/>hidden_neurons = 128<br/>model = createLSTMModel(X_train, hidden_neurons)<br/>history = model.fit(X_train, y_train, batch_size=32, nb_epoch=5000, validation_split=0.20)</pre>
<p>The preceding lines of code should produce some logs, which give us an idea of whether the training and the validation losses are getting reduced across iterations:</p>
<pre><strong>Train on 60 samples, validate on 15 samples</strong><br/><strong> Epoch 1/5000</strong><br/><strong> 60/60 [==============================] - ETA: 0s - loss: 7996.37 - 1s 11ms/step - loss: 7795.0232 - val_loss: 8052.6118</strong><br/><strong> Epoch 2/5000</strong><br/><strong> 60/60 [==============================] - ETA: 0s - loss: 6937.66 - 0s 301us/step - loss: 7466.3648 - val_loss: 7833.4321</strong><br/><strong> …</strong><br/><strong> 60/60 [==============================] - ETA: 0s - loss: 1754.92 - 0s 259us/step - loss: 1822.5668 - val_loss: 1420.7977</strong><br/><strong> Epoch 4976/5000</strong><br/><strong> 60/60 [==============================] - ETA: 0s - loss: 1862.04</strong></pre>
<p class="mce-root">Now that the training has been finished, we can plot the training and validation loss:</p>
<pre># plot history<br/>plt.plot(history.history['loss'], label='Training')<br/>plt.plot(history.history['val_loss'], label='Validation')<br/>plt.legend()<br/>plt.show()</pre>
<p class="mce-root">The preceding code block should produce the following graph, in which we can see that the validation loss drops below the training loss:<q><br/></q></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ea2b8246-0002-4817-9bda-7e6507f9b7a0.png" style="width:28.58em;height:18.75em;"/></p>
<p class="mce-root">The model may be overfitting the training data. Measuring and plotting MAE during training may shed more light on this. Let's take a look at the MAE on the testset:</p>
<pre>predicted = model.predict(X_test)<br/>rmse = np.sqrt(((predicted - y_test) ** 2).mean(axis=0))<br/>print('Mean Abs Error: {:.2f}'.format(mean_absolute_error(predicted, y_test)))</pre>
<p class="mce-root">We should get an MAE of 38.32, which means the MAE error has been reduced a bit (whereas RF gave an MAE of 40.33), which is, however, still not convincing. There could be several reasons behind such a high MAE. For example, we do not have sufficient training data. Secondly, we used an inefficient method for generating the entity set. For the first problem, we can use all the dataset to train the model. However, we can also use other regularization techniques, such as a Gaussian Noise layer, by specifying the noise threshold:</p>
<pre><strong>def</strong> createLSTMModel(X_train, hidden_neurons):<br/>    model = Sequential()<br/>    model.add(LSTM(hidden_neurons, input_shape=(X_train.shape[1], X_train.shape[2])))<br/>    model.add(GaussianNoise(0.2))<br/>    model.add(Dense(hidden_neurons))<br/>    model.add(Dropout(0.7))<br/>    model.add(Dense(1))<br/>    model.add(GaussianNoise(0.5))<br/>    model.add(Activation("linear"))<br/>    model.compile(loss="mean_squared_error", optimizer="rmsprop")<br/>    return model    </pre>
<p class="mce-root">The Gaussian noise layer can be used as an input layer to add noise <span>directly </span><span>to input variables. This is the traditional use of noise as a regularization method in neural networks, which states that the noise can be added before or after the use of the activation function. It may make more sense to add this before activation, but, nevertheless, both options are possible. In our case, we added a Gaussian noise layer with a dropout of 0.2 after the LSTM layer and before the dense layer.</span></p>
<p class="mce-root">Then, we have another Gaussian noise layer that adds noise to the linear output of a dense layer before a rectified linear activation function. Then, training the LSTM model with the same data with noise introduced should produce a slightly lower MAE value of around 35.25. We can even inspect the plot showing the training and validation loss:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/bfbbf5d2-4100-4d57-b233-eac0fea3614e.png" style="width:29.42em;height:19.00em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">The preceding diagram shows that the training loss and test loss are more or less the same, which indicates a better regularization of the model. Hence, the model performed better on the testset as well. However, the MAE can still be reduced using better quality features, perhaps. Let's explore this with a better feature generation technique.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised deep feature synthesis</h1>
                </header>
            
            <article>
                
<p>We will see how the entity set structures can contribute to improve the predictive accuracy. We will build custom primitives using time-series functions from the <kbd>tsfresh</kbd> library. Before that, we will make cut-off times by selecting a random one from the life of each engine. We are going to make five sets of cut-off times to use for cross-validation:</p>
<pre>from tqdm import tqdm<br/>splits = 10<br/>cutoff_time_list = []<br/>for i in tqdm(range(splits)):<br/>    cutoff_time_list.append(utils.make_cutoff_times(data))<br/>cutoff_time_list[0].head()</pre>
<p>The preceding code block should show the cut-off time and the RUL values for five engines, as shown here:<q><br/></q></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4a06a9b2-7315-49ae-9a54-cc2bb226f2b7.png" style="width:22.75em;height:14.00em;"/></p>
<p>Then, we will use an unsupervised way of generating the entity set. As we can see, the values of operational settings <kbd>1</kbd>—<kbd>3</kbd> are continuous, but they create an implicit relation between different engines. Consequently, if two engines have a similar operational setting, the sensor measurements give a similar value. The idea is to apply the clustering technique through k-means to those settings. Then, we create a new entity from clusters with similar values:</p>
<pre>from sklearn.cluster import KMeans<br/>nclusters = 50<br/>def make_entityset(data, nclusters, kmeans=None):<br/>    X = data[['operational_setting_1', 'operational_setting_2', 'operational_setting_3']]<br/>    if kmeans:<br/>        kmeans=kmeans<br/>    else:<br/>        kmeans = KMeans(n_clusters=nclusters).fit(X)<br/>    data['settings_clusters'] = kmeans.predict(X)<br/>        es = ft.EntitySet('Dataset')<br/>    es.entity_from_dataframe(dataframe=data,<br/>                             entity_id='recordings',<br/>                             index='index',<br/>                             time_index='time')<br/>    es.normalize_entity(base_entity_id='recordings', <br/>                        new_entity_id='engines',<br/>                        index='engine_no')<br/>    es.normalize_entity(base_entity_id='recordings', <br/>                        new_entity_id='settings_clusters',<br/>                        index='settings_clusters')<br/>    return es, kmeans<br/>es, kmeans = make_entityset(data, nclusters)</pre>
<p>The preceding code segment generates an entity set showing the following relations:</p>
<pre><strong>Entityset: Dataset</strong><br/><strong>   Entities:</strong><br/><strong>     settings_clusters [Rows: 50, Columns: 2]</strong><br/><strong>     recordings [Rows: 61249, Columns: 29]</strong><br/><strong>     engines [Rows: 249, Columns: 2]</strong><br/><strong>   Relationships:</strong><br/><strong>     recordings.engine_no -&gt; engines.engine_no</strong><br/><strong>     recordings.settings_clusters -&gt; settings_clusters.settings_clusters</strong></pre>
<p>In addition to changing our entity set structure, we are also going to use the complexity time-series primitive from the <span><kbd>tsfresh</kbd> </span><span>package. Any function that takes in a pandas series and outputs a float can be converted into an aggregation primitive using the <kbd>make_agg_primitive</kbd> function, as shown here:</span></p>
<pre><strong>from</strong> featuretools.primitives import make_agg_primitive<br/><strong>import</strong> featuretools.variable_types as vtypes<br/><strong>from</strong> tsfresh.feature_extraction.feature_calculators import (number_peaks, mean_abs_change, <br/>                                                            cid_ce, last_location_of_maximum, length)<br/>Complexity = make_agg_primitive(lambda x: cid_ce(x, False),<br/>                              input_types=[vtypes.Numeric],<br/>                              return_type=vtypes.Numeric,<br/>                              name="complexity")<br/>fm, features = ft.dfs(entityset=es, <br/>                      target_entity='engines',<br/>                      agg_primitives=['last', 'max', Complexity],<br/>                      trans_primitives=[],<br/>                      chunk_size=.26,<br/>                      cutoff_time=cutoff_time_list[0],<br/>                      max_depth=3,<br/>                      verbose=True)<br/>fm.to_csv('Advanced_FM.csv')<br/>fm.head()</pre>
<p class="mce-root">Using this approach, we managed to generate 12 more features (previously, we had 290). Then, we built four more feature matrices with the same feature set but different cut-off times. This lets us test the pipeline multiple times before using it on test data:</p>
<pre class="mce-root">fm_list = [fm]<br/><strong>for</strong> i <strong>in</strong> tqdm(range(1, splits)):<br/>    fm = ft.calculate_feature_matrix(entityset=make_entityset(data, nclusters, kmeans=kmeans)[0], <br/>         features=features, chunk_size=.26, cutoff_time=cutoff_time_list[i])<br/>    fm_list.append(fm)</pre>
<p class="mce-root">Then, using the recursive feature elimination, we again model RF regressors so that the model picks only important features, so it makes better predictions:</p>
<pre class="mce-root"><strong>from</strong> sklearn.ensemble import RandomForestRegressor<br/><strong>from</strong> sklearn.model_selection import train_test_split<br/><strong>from</strong> sklearn.metrics import mean_absolute_error<br/><strong>from</strong> sklearn.feature_selection import RFE<br/><br/><strong>def</strong> pipeline_for_test(fm_list, hyperparams={'n_estimators':100, 'max_feats':50, 'nfeats':50}, do_selection=False):<br/>    scores = []<br/>    regs = []<br/>    selectors = []<br/>    for fm in fm_list:<br/>        X = fm.copy().fillna(0)<br/>        y = X.pop('RUL')<br/>        reg = RandomForestRegressor(n_estimators=int(hyperparams['n_estimators']), <br/>              max_features=min(int(hyperparams['max_feats']), int(hyperparams['nfeats'])))<br/>        X_train, X_test, y_train, y_test = train_test_split(X, y)<br/><br/>        if do_selection:<br/>            reg2 = RandomForestRegressor(n_jobs=3)<br/>            selector=RFE(reg2,int(hyperparams['nfeats']),step=25)<br/>            selector.fit(X_train, y_train)<br/>            X_train = selector.transform(X_train)<br/>            X_test = selector.transform(X_test)<br/>            selectors.append(selector)<br/>        reg.fit(X_train, y_train)<br/>        regs.append(reg)<br/>        preds = reg.predict(X_test)<br/>        scores.append(mean_absolute_error(preds, y_test))<br/>    return scores, regs, selectors  <br/>  <br/>scores, regs, selectors = pipeline_for_test(fm_list)<br/>print([float('{:.1f}'.format(score)) for score in scores])<br/>print('Average MAE: {:.1f}, Std: {:.2f}\n'.format(np.mean(scores), np.std(scores)))<br/>most_imp_feats = utils.feature_importances(fm_list[0], regs[0])</pre>
<p>The preceding code block should produce the following output showing predicted MAE in each iteration and their average. Additionally, it shows the baseline MAE values and statistics about the engine recording cycles:</p>
<pre class="mce-root"><strong>[33.9, 34.5, 36.0, 32.1, 36.4, 30.1, 37.2, 34.7,38.6, 34.4]</strong><br/><strong> Average MAE: 33.1, Std: 4.63</strong><br/><strong> 1: MAX(recordings.settings_clusters.LAST(recordings.sensor_measurement_13)) [0.055]</strong><br/><strong> 2: MAX(recordings.sensor_measurement_13) [0.044]</strong><br/><strong> 3: MAX(recordings.sensor_measurement_4) [0.035]</strong><br/><strong> 4: MAX(recordings.settings_clusters.LAST(recordings.sensor_measurement_4)) [0.029]</strong><br/><strong> 5: MAX(recordings.sensor_measurement_11) [0.028]</strong></pre>
<p class="mce-root">Now let's again try using LSTM to see whether we can reduce the MAE error:</p>
<pre>X = fm.copy().fillna(0)<br/>y = X.pop('RUL')<br/>X_train, X_test, y_train, y_test = prepareData(X, y)<br/><br/>hidden_neurons = 128<br/>model = createLSTMModel(X_train, hidden_neurons)<br/><br/>history = model.fit(X_train, y_train, batch_size=32, nb_epoch=5000, validation_split=0.20)<br/># plot history<br/>plt.plot(history.history['loss'], label='Training')<br/>plt.plot(history.history['val_loss'], label='Validation')<br/>plt.legend()<br/>plt.show()</pre>
<p>The preceding lines of code should produce the following diagram, in which the validation loss drops below the training loss:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/63d36ef1-c495-4102-b75f-98b71db81b04.png" style="width:29.50em;height:18.83em;"/></p>
<p>Finally, we can evaluate the model's performance based on the MAE:</p>
<pre>predicted = model.predict(X_test)<br/>print('Mean Abs Error: {:.2f}'.format(mean_absolute_error(predicted, y_test)))</pre>
<p>The preceding code block should produce an MAE of 52.40, which is lower than we experienced in the previous section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">FAQs</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will cover some <strong>frequently asked questions</strong> (<strong>FAQs</strong>), which will help you to extend this application:</p>
<ol>
<li class="mce-root"><strong>Can we use other deep architectures to make predictions in similar IoT settings?</strong></li>
</ol>
<p style="padding-left: 90px"><strong>Answer</strong><span>: Yes, using other deep architectures could be a viable option. For example, creating a convolutional-LSTM network by combining the predictive power of both CNN and LSTM layers has</span> proven <span>to be effective in many use cases, such as audio classification,</span> <strong>natural language processing</strong> <span>(</span><strong>NLP</strong><span>), and time-series forecasting.  </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li class="mce-root"><strong>Sometimes we do not have enough IoT data to train the model flexibly. How can we increase the amount of training data?</strong></li>
</ol>
<p class="mce-root" style="padding-left: 90px"><strong>Answer</strong>: There are many ways to do this. For example, we can try to generate the training set by combining all the engines data. For this, the generated CSV files for both training, testing, and RUL would be helpful. Another example might be to try to extend the dataset by adding more samples.</p>
<ol start="3">
<li class="mce-root"><strong>Can I perform anomaly detection in an industrial setting?</strong></li>
</ol>
<p class="mce-root" style="padding-left: 90px"><strong>Answer</strong>: Yes, you can. In fact, this is very common in an industrial setting such as production fault identification, real-time time-series anomaly detection, predictive monitoring, and so on.</p>
<ol start="4">
<li><strong>Where can I get data to perform other analytics in an IoT setting?</strong></li>
</ol>
<p style="padding-left: 90px"><strong>Answer</strong>: Time-series data from some nominal state to a failed state from the <strong>Prognostics Data Repository</strong> can be used for the development of prognostic algorithms. See the following link to learn more about the dataset: <a href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/">https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have looked at how to develop a DL solution for predictive maintenance using IoT and the Turbofan Engine Degradation Simulation dataset. We started by discussing the exploratory analysis of the dataset before we modeled the predictive maintenance using one of the most popular tree-based ensemble techniques called <strong>RF</strong>, which uses features from the turbine engines as it is. Then, we saw how to improve the predictive accuracy using an LSTM network. The LSTM network indeed helps to reduce network errors. Nevertheless, we saw how to add a Gaussian noise layer to achieve generalization in the LSTM network, along with dropout.</p>
<p>Understanding the potential of DL techniques in all layers of IoT (including the sensors/sensing, gateway, and cloud layer) is important. Consequently, developing scalable and efficient solutions for IoT-enabled healthcare devices is no exception. In the next chapter, we will present a use case that exploits DL for data analysis in all potential stages of its life cycle.</p>


            </article>

            
        </section>
    </body></html>