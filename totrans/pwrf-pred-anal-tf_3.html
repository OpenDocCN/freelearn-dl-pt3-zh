<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Clustering Your Data &#x2013; Unsupervised Learning for Predictive Analytics"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Clustering Your Data – Unsupervised Learning for Predictive Analytics</h1></div></div></div><p>In this lesson, we will dig deeper into predictive analytics and find out how we can take advantage of it to cluster records belonging to a certain group or class for a dataset of unsupervised observations. We will provide some practical examples of unsupervised learning; in particular, clustering techniques using TensorFlow will be discussed with some hands-on examples.</p><p>The following topics will be covered in this lesson:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Unsupervised learning and clustering</li><li class="listitem" style="list-style-type: disc">Using K-means for predicting neighborhood</li><li class="listitem" style="list-style-type: disc">Using K-means for clustering audio files</li><li class="listitem" style="list-style-type: disc">Using unsupervised <span class="strong"><strong>k-nearest neighborhood</strong></span> (<span class="strong"><strong>kNN</strong></span>) for predicting nearest neighbors</li></ul></div><div class="section" title="Unsupervised Learning and Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Unsupervised Learning and Clustering</h1></div></div></div><p>In this section, we will provide a brief introduction to the unsupervised <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) technique. Unsupervised learning is a type of ML algorithm used for grouping related data objects and finding hidden patterns by inferencing from unlabeled datasets, that is, a training set consisting of input data without labels.</p><p>Let's see a real-life example. Suppose you have a large collection of not-pirated-totally-legal MP3s in a crowded and massive folder on your hard drive. Now, what if you can build a predictive model that helps automatically group together similar songs and organize them into your favorite categories such as country, rap, and rock?</p><p>This is an act of assigning an item to a group so that an MP3 is added to the respective playlist in an unsupervised way. In <a class="link" href="ch01.html" title="Chapter 1. From Data to Decisions – Getting Started with TensorFlow">Lesson 1</a>, <span class="emphasis"><em>From Data to Decisions – Getting Started with TensorFlow, on classification</em></span>, we assumed that you're given a training dataset of correctly labeled data. Unfortunately, we don't always have that extravagance when we collect data in the real world. For example, suppose we would like to divide a huge collection of music into interesting playlists. How canwe possibly group together songs if we don't have direct access to their metadata? One possible approach is a mixture of various ML techniques, but clustering is often at the heart of the solution.</p><p>In other words, the main objective of the unsupervised learning algorithms is to explore the unknown/hidden patterns in the input data that are unlabeled. Unsupervised learning, however, also comprehends other techniques to explain the key features of the data in an exploratory way toward finding the hidden patterns. To overcome this challenge, clustering techniques are used widely to group unlabeled data points based on certain similarity measures in an unsupervised way.</p><p>In a clustering task, an algorithm groups related features into categories by analyzing similarities between input examples, where similar features are clustered and marked using circles.</p><p>Clustering uses include but are not limited to the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Anomaly detection for suspicious pattern finding in an unsupervised way</li><li class="listitem" style="list-style-type: disc">Text categorization for finding useful patterns in the tests for NLP</li><li class="listitem" style="list-style-type: disc">Social network analysis for finding coherent groups</li><li class="listitem" style="list-style-type: disc">Data center computing clusters for finding a way of putting related computers together</li><li class="listitem" style="list-style-type: disc">Real estate data analysis for identifying neighborhoods based on similar features</li></ul></div><p>Clustering analysis is about dividing data samples or data points and putting them into corresponding homogeneous classes or clusters. Thus, a trivial definition of clustering can be thought of as the process of organizing objects into groups whose members are similar in some way, as shown in figure 1:</p><div class="mediaobject"><img alt="Unsupervised Learning and Clustering" src="graphics/03_01.jpg"/><div class="caption"><p>Figure 1: A typical data pipelines for clustering raw data</p></div></div><p>A cluster is, therefore, a collection of objects that have a similarity between them and are dissimilar to the objects belonging to other clusters. If a collection of objects is provided, clustering algorithms put these objects into groups based on similarity. For example, a clustering algorithm such as K-means locates the centroid of the groups of data points.</p><p>However, to make clustering accurate and effective, the algorithm evaluates the distance between each point from the centroid of the cluster. Eventually, the goal of clustering is to determine intrinsic grouping in a set of unlabeled data. For example, the K-means algorithm tries to cluster related data points within the predefined <span class="emphasis"><em>3</em></span> (that is <span class="emphasis"><em>k = 3</em></span>) clusters, as shown in figure 2:</p><div class="mediaobject"><img alt="Unsupervised Learning and Clustering" src="graphics/03_02.jpg"/><div class="caption"><p>Figure 2: The results of a typical clustering algorithm and a representation of the cluster centers</p></div></div><p>Clustering is a process of intelligently categorizing items in your dataset. The overall idea is that two items in the same cluster are closer to each other than items that belong to separate clusters. This is a general definition, leaving the interpretation of closeness open. For example, perhaps cheetahs and leopards belong to the same cluster, whereas elephants belong to another when closeness is measured by the similarity of two species in the hierarchy of biological classification (family, genus, and species).</p></div></div>
<div class="section" title="Using K-means for Predictive Analytics"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Using K-means for Predictive Analytics</h1></div></div></div><p>K-means is a clustering algorithm that tries to cluster related data points together. However, we should know its working principle and mathematical operations.</p><div class="section" title="How K-means Works"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>How K-means Works</h2></div></div></div><p>Suppose we have <span class="emphasis"><em>n</em></span> data points, <span class="emphasis"><em>xi</em></span>, <span class="emphasis"><em>i = 1...n</em></span>, that need to be partitioned into <span class="emphasis"><em>k</em></span> clusters. Now that the target here is to assign a cluster to each data point, K-means aims to find the positions, <span class="emphasis"><em>μi</em></span>, <span class="emphasis"><em>i=1...k</em></span>, of the clusters that minimize the distance from the data points to the cluster. Mathematically, the K-means algorithm tries to achieve the goal by solving an equation that is an optimization problem:</p><div class="mediaobject"><img alt="How K-means Works" src="graphics/03_03.jpg"/></div><p>In the previous equation, <span class="strong"><strong>ci</strong></span> is a set of data points, which when assigned to cluster <span class="strong"><strong>i</strong></span> and<span class="inlinemediaobject"><img alt="How K-means Works" src="graphics/03_21.jpg"/></span>is the Euclidean distance to be calculated (we will explain why we should use this distance measurement shortly). Therefore, we can see that the overall clustering operation using K-means is not a trivial one, but a NP-hard optimization problem. This also means that the K-means algorithm not only tries to find the global minima but often gets stuck in different solutions.</p><p>Clustering using the K-means algorithm begins by initializing all the coordinates to the centroids. With every pass of the algorithm, each point is assigned to its nearest centroid based on some distance metric, usually the Euclidean distance stated earlier.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>
<span class="strong"><strong>Distance calculation</strong></span>: There are other ways to calculate the distance as well. For example, the Chebyshev distance can be used to measure the distance by considering only the most notable dimensions. The Hamming distance algorithm can identify the difference between two strings. Mahalanobis distance can be used to normalize the covariance matrix. The Manhattan distance is used to measure the distance by considering only axis-aligned directions. The Haversine distance is used to measure the great-circle distances between two points on a sphere from the location.</p></div></div><p>Considering these distance-measuring algorithms, it is clear that the Euclidean distance algorithm would be the most appropriate to solve our purpose of distance calculation in the K-means algorithm. The centroids are then updated to be the centers of all the points assigned to it in that iteration. This repeats until there is a minimal change in the centers. In short, the K-means algorithm is an iterative algorithm and works in two steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Cluster assignment step</strong></span>: K-means goes through each of the <span class="emphasis"><em>n</em></span> data points in the dataset that is assigned to a cluster closest to the k centroids, then the least distant one is picked.</li><li class="listitem"><span class="strong"><strong>Update step</strong></span>: For each cluster, a new centroid is calculated for all the data points in the cluster. The overall workflow of K-means can be explained using a flowchart, as follows:</li></ol></div><div class="mediaobject"><img alt="How K-means Works" src="graphics/03_04.jpg"/><div class="caption"><p>Figure 4: Flowchart of the K-means algorithm (Elbow method is an optional but also an advanced option)</p></div></div></div><div class="section" title="Using K-means for Predicting Neighborhoods"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Using K-means for Predicting Neighborhoods</h2></div></div></div><p>Now, to show an example of clustering using K-means, we will use the Saratoga NY Homes dataset downloaded from <a class="ulink" href="http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html">http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html</a> as an unsupervised learning technique. The dataset contains several features of the houses located in the suburb of the New York City; for example, price, lot size, waterfront, age, land value, new construct, central air, fuel type, heat type, sewer type, living area, Pct.College, bedrooms, fireplaces, bathrooms, and the number of rooms. However, only a few features have been shown in <span class="strong"><strong>Table 1</strong></span>:</p><div class="mediaobject"><img alt="Using K-means for Predicting Neighborhoods" src="graphics/03_05.jpg"/><div class="caption"><p>Table 1: A sample data from the Saratoga NY Homes dataset</p></div></div><p>The target of this clustering technique is to show an exploratory analysis based on the features of each house in the city for finding possible neighborhoods' of the house located in the same area. Before performing the feature extraction, we need to load and parse the Saratoga NY Homes dataset. However, we will look at this example with step-by-step source codes for better understanding:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Loading required libraries and packages.<p>We need some built-in Python libraries, such as os, random, NumPy, and Pandas, for data manipulation; PCA for dimensionality reduction; Matplotlib for plotting; and of course, TensorFlow:</p><div class="informalexample"><pre class="programlisting">import os
import random
from random import choice, shuffle
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d, Axes3D</pre></div></li><li class="listitem">Loading, parsing, and preparing a training set.<p>Here, the first line is used to ensure the reproducibility of the result. The second line basically reads the dataset from your location and converts it into the Pandas data frame:</p><div class="informalexample"><pre class="programlisting">random.seed(12345)
train = pd.read_csv(os.path.join('input', 'saratoga.csv'))
x_train = np.array(train.iloc[:, 1:], dtype='float32')</pre></div><p>If you now print the data frame (using print(train)), you should find the dataframe containing headers and data as shown in figure 3:</p><div class="mediaobject"><img alt="Using K-means for Predicting Neighborhoods" src="graphics/03_06.jpg"/><div class="caption"><p>Figure 5: A snapshot of the Saratoga NY Homes dataset</p></div></div><p>Well, we have managed to prepare the dataset. Now, the next task is to conceptualize our K-means and write a function/class for it.</p></li><li class="listitem">Implementing K-means.<p>The following is the source code of K-means, which is simple in a TensorFlow way:</p><div class="informalexample"><pre class="programlisting">def kmeans(x, n_features, n_clusters, n_max_steps=1000, early_stop=0.0):
    input_vec = tf.constant(x, dtype=tf.float32)
    centroids = tf.Variable(tf.slice(tf.random_shuffle(input_vec), [0, 0], [n_clusters, -1]), dtype=tf.float32)
    old_centroids = tf.Variable(tf.zeros([n_clusters, n_features]), dtype=tf.float32)
    centroid_distance = tf.Variable(tf.zeros([n_clusters, n_features]))
    expanded_vectors = tf.expand_dims(input_vec, 0)
    exanded_centroids = tf.expand_dims(centroids, 1)
    distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)
    assignments = tf.argmin(distances, 0)
    means = tf.concat([tf.reduce_mean(
        tf.gather(input_vec, tf.reshape(tf.where(tf.equal(assignments, c)), [1, -1])),
        reduction_indices=[1]) for c in range(n_clusters)], 0)
    save_old_centroids = tf.assign(old_centroids, centroids)
    update_centroids = tf.assign(centroids, means)
    init_op = tf.global_variables_initializer()
    performance = tf.assign(centroid_distance, tf.subtract(centroids, old_centroids))
    check_stop = tf.reduce_sum(tf.abs(performance))
    with tf.Session() as sess:
        sess.run(init_op)
        for step in range(n_max_steps):
            sess.run(save_old_centroids)
            _, centroid_values, assignment_values = sess.run(
                [update_centroids, centroids, assignments])            
            sess.run(check_stop)
            current_stop_coeficient = check_stop.eval()
            if current_stop_coeficient &lt;= early_stop:
                break
    return centroid_values, assignment_values</pre></div><p>The previous code contains all the steps required to develop the K-means model, including the distance-based centroid calculation, centroid update, and training parameters required.</p></li><li class="listitem">Clustering the houses.<p>Now the previous function can be invoked with real values, for example, our housing dataset. Since there are many houses with their respective features, it would be difficult to plot the clusters along with all the properties. This is the <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) that we discussed in the previous lessons:</p><div class="informalexample"><pre class="programlisting">centers, cluster_assignments = kmeans(x_train, len(x_train[0]), 10)
pca_model = PCA(n_components=3)
reduced_data = pca_model.fit_transform(x_train)
reduced_centers = pca_model.transform(centers)</pre></div><p>Well, now we are all set. It would be even better to visualize the clusters as shown in figure 6. For this, we will use mpl_toolkits.mplot3d for 3D projection, as follows:</p><div class="informalexample"><pre class="programlisting">plt.subplot(212, projection='3d')
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c=cluster_assignments)
plt.title("Clusters")
plt.show()
&gt;&gt;&gt;</pre></div><div class="mediaobject"><img alt="Using K-means for Predicting Neighborhoods" src="graphics/03_07.jpg"/><div class="caption"><p>Figure 6: Clustering the houses with similar properties, for example, price</p></div></div><p>Here, we can see that most houses fall in <span class="strong"><strong>0</strong></span> to <span class="strong"><strong>100,000</strong></span> range. The second highest houses fall in the range of <span class="strong"><strong>100000</strong></span> to <span class="strong"><strong>200000</strong></span>. However, it's really difficult to separate them. Moreover, the number of predefined clusters that we used is 10, which might not be the most optimal one. Therefore, we need to tune this parameter.</p></li><li class="listitem">Fine tuning and finding the optimal number of clusters.<p>Choosing the right number of clusters often depends on the task. For example, suppose you're planning an event for hundreds of people, both young and old. If you have a budget for only two entertainment options, then you can use the K-means clustering with <span class="strong"><strong>k = 2</strong></span> to separate the guests into two age groups. Other times, it's not as obvious what the value of <code class="literal">k</code> should be. Automatically figuring out the value of <code class="literal">k</code> is a bit more complicated.</p><p>As mentioned earlier, the K-means algorithm tries to minimize the sum of squares of the distance (that is, Euclidean distance), in terms of <span class="strong"><strong>Within-Cluster Sum of Squares</strong></span> (<span class="strong"><strong>WCSS</strong></span>).</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>However, if you want to minimize the sum of squares of the distance between the points of each set manually or automatically, you would end up with a model where each cluster is its own cluster center; in this case, this measure would be 0, but it would hardly be a generic enough model.</p></div></div><p>Therefore, once you have trained your model by specifying the parameters, you can evaluate the result using WCSS. Technically, it is same as the sum of distances of each observation in each K cluster. The beauty of clustering algorithms as a K-means algorithm is that it does the clustering on the data with an unlimited number of features. It is a great tool to use when you have raw data and would like to know the patterns in that data.</p><p>However, deciding the number of clusters prior to conducting the experiment might not be successful but sometimes may lead to an overfitting problem or an under-fitting one. Also, informally, determining the number of clusters is a separate but an optimization problem to be solved. So, based on this, we can redesign our K-means considering the WCSS value computation, as follows:</p><div class="informalexample"><pre class="programlisting">def kmeans(x, n_features, n_clusters, n_max_steps=1000, early_stop=0.0):
    input_vec = tf.constant(x, dtype=tf.float32)
    centroids = tf.Variable(tf.slice(tf.random_shuffle(input_vec), [0, 0], [n_clusters, -1]), dtype=tf.float32)
    old_centroids = tf.Variable(tf.zeros([n_clusters, n_features]), dtype=tf.float32)
    centroid_distance = tf.Variable(tf.zeros([n_clusters, n_features]))
    expanded_vectors = tf.expand_dims(input_vec, 0)
    expanded_centroids = tf.expand_dims(centroids, 1)
    distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)
    assignments = tf.argmin(distances, 0)
    means = tf.concat([tf.reduce_mean(        tf.gather(input_vec, tf.reshape(tf.where(tf.equal(assignments, c)), [1, -1])),
        reduction_indices=[1]) for c in range(n_clusters)], 0)
    save_old_centroids = tf.assign(old_centroids, centroids)
    update_centroids = tf.assign(centroids, means)
    init_op = tf.global_variables_initializer()

    performance = tf.assign(centroid_distance, tf.subtract(centroids, old_centroids))
    check_stop = tf.reduce_sum(tf.abs(performance))
    calc_wss = tf.reduce_sum(tf.reduce_min(distances, 0))
    with tf.Session() as sess:
        sess.run(init_op)
        for step in range(n_max_steps):
            sess.run(save_old_centroids)
            _, centroid_values, assignment_values = sess.run(
                [update_centroids, centroids, assignments])            
            sess.run(calc_wss)
            sess.run(check_stop)
            current_stop_coeficient = check_stop.eval()
            wss = calc_wss.eval()
            print(step, current_stop_coeficient)
            if current_stop_coeficient &lt;= early_stop:
                break
    return centroid_values, assignment_values, wss</pre></div><p>To fine tune the clustering performance, we can use a heuristic approach called Elbow method. We start from <span class="strong"><strong>K = 2</strong></span>. Then, we run the K-means algorithm by increasing <code class="literal">K</code> and observe the value of the cost function (CF) using WCSS. At some point, we should experience a big drop with respect to CF. Nevertheless, the improvement then becomes marginal with an increasing value of <code class="literal">K</code>.</p><p>In summary, we can pick the <code class="literal">K</code> after the last big drop of WCSS as the optimal one. The K-means includes various parameters such as withiness and betweenness, analyzing which you can find out the performance of K-means:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Betweenness</strong></span>: This is the between sum of squares, also called the intra-cluster similarity</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Withiness</strong></span>: This is the within sum of squares, also called the inter-cluster similarity</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Totwithiness</strong></span>: This is the sum of all the withiness of all the clusters, also called the total intra-cluster similarity</li></ul></div><p>Note that a robust and accurate clustering model will have a lower value of withiness and a higher value of betweenness. However, these values depend on the number of clusters that is <code class="literal">K</code>, which is chosen before building the model. Now, based on this, we will train the K-means model for different <code class="literal">K</code> values that are a number of predefined clusters. We will start <span class="strong"><strong>K = 2</strong></span> to <code class="literal">10</code>, as follows:</p><div class="informalexample"><pre class="programlisting">wcss_list = []
for i in range(2, 10):
    centers, cluster_assignments, wcss = kmeans(x_train, len(x_train[0]), i)
    wcss_list.append(wcss)</pre></div><p>Now, let's discuss how we can take the advantage of the Elbow method for determining the number of clusters. We calculated the cost function WCSS as a function of a number of clusters for the K-means algorithm applied to home data based on all the features, as follows:</p><div class="informalexample"><pre class="programlisting">plt.figure(figsize=(12, 24))
plt.subplot(211)
plt.plot(range(2, 10), wcss_list)
plt.xlabel('No of Clusters')
plt.ylabel('WCSS')
plt.title("WCSS vs Clusters")
&gt;&gt;&gt;</pre></div><div class="mediaobject"><img alt="Using K-means for Predicting Neighborhoods" src="graphics/03_08.jpg"/><div class="caption"><p>Figure 7: Number of clusters as a function of WCSS</p></div></div><p>We will try to reuse this lesson in upcoming examples using K-means too. Now, it can be observed that a big drop occurs when <span class="strong"><strong>k = 5</strong></span>. Therefore, we chose the number of clusters to be <span class="strong"><strong>5</strong></span> as discussed in figure 7. Basically, this is the one after the last big drop. This means that the optimal number of cluster for our dataset that we need to set before we start training the K-means model is <span class="strong"><strong>5</strong></span>.</p></li><li class="listitem">Clustering analysis.<p>From figure 8, it is clear that most houses fall in <span class="strong"><strong>cluster 3</strong></span> (<span class="strong"><strong>4655 houses</strong></span>) and then in <span class="strong"><strong>cluster 4</strong></span> (<span class="strong"><strong>3356 houses</strong></span>). The x-axis shows the price and the y-axis shows the lot size for each house. We can also observe that the <span class="strong"><strong>cluster 1</strong></span> has only a few houses and potentially in longer distances, but it is also expensive. So, it is most likely that you will not find a nearer neighborhood to interact with if you buy a house that falls in this cluster. However, if you like more human interaction and budget is a constraint, you should probably try buying a house from cluster 2, 3, 4, or 5:</p><div class="mediaobject"><img alt="Using K-means for Predicting Neighborhoods" src="graphics/03_09.jpg"/><div class="caption"><p>Figure 8: Clusters of neighborhoods, that is,. homogeneous houses fall in same clusters</p></div></div><p>To make the analysis, we dumped the output in RStudio and generated the clusters shown in figure 6. The R script can be found on my GitHub repositories at <a class="ulink" href="https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics">https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics</a>. Alternatively, you can write your own script and do the visualization accordingly.</p></li></ol></div></div></div>
<div class="section" title="Predictive Models for Clustering Audio Files"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Predictive Models for Clustering Audio Files</h1></div></div></div><p>For clustering music with audio data, the data points are the feature vectors from the audio files. If two points are close together, it means that their audio features are similar. We want to discover which audio files belong to the same neighborhood because these clusters will probably be a good way to organize your music files:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Loading audio files with TensorFlow and Python.<p>Some common input types in ML algorithms are audio and image files. This shouldn't come as a surprise because sound recordings and photographs are raw, redundant, ab nd often noisy representations of semantic concepts. ML is a tool to help handle these complications. These data files have various implementations, for example, an audio file can be an MP3 or WAV.</p><p>Reading files from a disk isn't exactly a ML-specific ability. You can use a variety of Python libraries to load files onto the memory, such as Numpy or Scipy. Some developers like to treat the data preprocessing step separately from the ML step. However, I believe that this is also a part of the whole analytics process.</p><p>Since this is a TensorFlow book, I will try to use something from the TensorFlow built-in operator to list files in a directory called <code class="literal">tf.train.match_filenames_once()</code>. We can then pass this information along to a <code class="literal">tf.train.string_input_producer()queue</code> operator. This way, we can access a filename one at a time, without loading everything at once. Here's the structure of this method:</p><div class="informalexample"><pre class="programlisting">match_filenames_once(pattern,name=None)</pre></div><p>This method takes two parameters: <code class="literal">pattern</code> and <code class="literal">name</code>. <code class="literal">pattern</code> signifies a file pattern or 1D tensor of file patterns. The <code class="literal">name</code> is used to signify the name of the operations. However, this parameter is optional. Once invoked, this method saves the list of matching patterns, so as the name implies, it is only computed once.</p><p>Finally, a variable that is initialized to the list of files matching the pattern(s) is returned by this method. Once we have finished reading the metadata and the audio files, we can decode the file to retrieve usable data from the given filename. Now, let's get started. First, we need to import necessary packages and Python modules, as follows:</p><div class="informalexample"><pre class="programlisting">import 
tensorflow as tf
import numpy as np
from bregman.suite import *
from tensorflow.python.framework import ops
import warnings
import random</pre></div><p>Now we can start reading the audio files from the directory specified. First, we need to store filenames that match a pattern containing a particular file extension, for example, <code class="literal">.mp3</code>, <code class="literal">.wav</code>, and so on. Then, we need to set up a pipeline for retrieving filenames randomly. Now, the code natively reads a file in TensorFlow. Then, we run the reader to extract the file data. Use can use following code for this task:</p><div class="informalexample"><pre class="programlisting">filenames = tf.train.match_filenames_once('./audio_dataset/*.wav')
count_num_files = tf.size(filenames)
filename_queue = tf.train.string_input_producer(filenames)
reader = tf.WholeFileReader()
filename, file_contents = reader.read(filename_queue)
chromo = tf.placeholder(tf.float32)
max_freqs = tf.argmax(chromo, 0)</pre></div><p>Well, once we have read the data and metadata about all the audio files, the next and immediate tasks are to capture the audio features that will be used by K-means for the clustering purpose.</p></li><li class="listitem">Extracting features and preparing feature vectors.<p>ML algorithms are typically designed to use feature vectors as input; however, sound files are a very different format. We need a way to extract features from sound files to create feature vectors.</p><p>It helps to understand how these files are represented. If you've ever seen a vinyl record, you've probably noticed the representation of audio as grooves indented in the disk. Our ears interpret audio from a series of vibrations through the air. By recording the vibration properties, our algorithm can store sound in a data format. The real world is continuous but computers store data in discrete values.</p><p>The sound is digitalized into a discrete representation through an <span class="strong"><strong>Analog to Digital Converter</strong></span> (<span class="strong"><strong>ADC</strong></span>). You can think about sound as a fluctuation of a wave over time. However, this data is too noisy and difficult to comprehend. An equivalent way to represent a wave is by examining the frequencies that make it up at each time interval. This perspective is called the frequency domain.</p><p>It's easy to convert between time domain and frequency domain using a mathematical operation called a discrete Fourier transform (commonly known as the Fast Fourier transform). We will use this technique to extract a feature vector out of our sound.</p><p>A sound may produce 12 kinds of pitch. In music terminology, the 12 pitches are C, C#, D, D#, E, F, F#, G, G#, A, A#, and B. Figure 9 shows how to retrieve the contribution of each pitch in a 0.1-second interval, resulting in a matrix with 12 rows. The number of columns grows as the length of the audio file increases. Specifically, there will be <span class="strong"><strong>10*t</strong></span> columns for a <span class="strong"><strong>t</strong></span> second audio.</p><p>This matrix is also called a chromogram of the audio. But first, we need to have a placeholder for TensorFlow to hold the chromogram of the audio and the maximum frequency:</p><div class="informalexample"><pre class="programlisting">chromo = tf.placeholder(tf.float32) 
max_freqs = tf.argmax(chromo, 0)</pre></div><p>The next task that we can perform is that we can write a method that can extract these chromograms for the audio files. It can look as follows:</p><div class="informalexample"><pre class="programlisting">def get_next_chromogram(sess):
    audio_file = sess.run(filename)
    F = Chromagram(audio_file, nfft=16384, wfft=8192, nhop=2205)
    return F.X, audio_file</pre></div><p>The workflow of the previous code is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First, pass in the filename and use these parameters to describe 12 pitches every 0.1 seconds.</li><li class="listitem" style="list-style-type: disc">Finally, represent the values of a 12-dimensional vector 10 times a second.</li></ul></div><p>The chromogram output that we extract using the previous method will be a matrix, as visualized in figure 10. A sound clip can be read as a chromogram, and a chromogram is a recipe for generating a sound clip. Now, we have a way to convert audio and matrices. As you have learned, most ML algorithms accept feature vectors as a valid form of data. That being said, the first ML algorithm we'll look at is K-means clustering:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_10.jpg"/><div class="caption"><p>Figure 9: The visualization of the chromogram matrix where the x-axis represents time and the y-axis represents pitch class. The green markings indicate a presence of that pitch at that time</p></div></div><p>To run the ML algorithms on our chromogram, we first need to decide how we're going to represent a feature vector. One idea is to simplify the audio by only looking at the most significant pitch class per time interval, as shown in figure 10:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_11.jpg"/><div class="caption"><p>Figure 10: The most influential pitch at every time interval is highlighted. You can think of it as the loudest pitch at each time interval</p></div></div><p>Now, we will count the number of times each pitch shows up in the audio file. Figure 11 shows this data as a histogram, forming a 12-dimensional vector. If we normalize the vector so that all the counts add up to <span class="strong"><strong>1</strong></span>, then we can easily compare audio of different lengths:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_12.jpg"/><div class="caption"><p>Figure 11: We count the frequency of the loudest pitches heard at each interval to generate this histogram, which acts as our feature vector</p></div></div><p>Now that we have the chromagram, we need to use it to extract the audio feature to construct a feature vector. You can use the following method for this:</p><div class="informalexample"><pre class="programlisting">def extract_feature_vector(sess, chromo_data):
    num_features, num_samples = np.shape(chromo_data)
    freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})
    hist, bins = np.histogram(freq_vals, bins=range(num_features + 
))
    normalized_hist = hist.astype(float) / num_samples
    return normalized_hist</pre></div><p>The workflow of the previous code is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Create an operation to identify the pitch with the biggest contribution.</li><li class="listitem" style="list-style-type: disc">Now, convert the chromogram into a feature vector.</li><li class="listitem" style="list-style-type: disc">After this, we will construct a matrix where each row is a data item.</li><li class="listitem" style="list-style-type: disc">Now, if you can hear the audio clip, you can imagine and differentiate between the different audio files. However, this is just intuition.</li></ul></div><p>Therefore, we cannot rely on this, but we should inspect them visually. So, we will invoke the previous method to extract the feature vector from each audio file and plot the feature. The whole operation should look as follows:</p><div class="informalexample"><pre class="programlisting">def get_dataset(sess):
    num_files = sess.run(count_num_files)
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    xs = list()
    names = list()
    plt.figure()
    for _ in range(num_files):
        chromo_data, filename = get_next_chromogram(sess)
        plt.subplot(1, 2, 1)
        plt.imshow(chromo_data, cmap='Greys', interpolation='nearest')
        plt.title('Visualization of Sound Spectrum')
        plt.subplot(1, 2, 2)
        freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})
        plt.hist(freq_vals)
        plt.title('Histogram of Notes')
        plt.xlabel('Musical Note')
        plt.ylabel('Count')
        plt.savefig('{}.png'.format(filename))
        plt.clf()
        plt.clf()
        names.append(filename)
        x = extract_feature_vector(sess, chromo_data)
        xs.append(x)
    xs = np.asmatrix(xs)
    return xs, names</pre></div><p>The previous code should plot the audio features of each audio file in the histogram as follows:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_13.jpg"/><div class="caption"><p>Figure 12: The ride audio files show a similar histogram</p></div></div><p>You can see some examples of audio files that we are trying to cluster based on their audio features. As you can see, the two on the right appear to have similar histograms. The two on the left also have similar sound spectrums:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_14.jpg"/><div class="caption"><p>Figure 13: The crash cymbal audio files show a similar histogram</p></div></div><p>Now, the target is to develop K-means so that it is able to group these sounds together accurately. We will look at the high-level view of the cough audio files, as shown in the following figure:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_15.jpg"/><div class="caption"><p>Figure 14: The cough audio files show a similar histogram</p></div></div><p>Finally, we have the scream audio files that have a similar histogram and audio spectrum, but of course are different compared to others:</p><div class="mediaobject"><img alt="Predictive Models for Clustering Audio Files" src="graphics/03_16.jpg"/><div class="caption"><p>Figure 15: The scream audio files show a similar histogram and audio spectrum</p></div></div><p>Now, we can imagine our problem. We have the features ready for training the K-means model. Let's start doing it.</p></li><li class="listitem">Training K-means model.<p>Now that the feature vector is ready, it's time to feed this to the K-means model for clustering the feature presented in figure 10. The idea is that the midpoint of all the points in a cluster is called a centroid.</p><p>Depending on the audio features we choose to extract, a centroid can capture concepts such as loud sound, high-pitched sound, or saxophone-like sound. Therefore, it's important to note that the K-means algorithm assigns non-descript labels, such as cluster 1, cluster 2, or cluster 3. First, we can write a method that computes the initial cluster centroids as follows:</p><div class="informalexample"><pre class="programlisting">def initial_cluster_centroids(X, k):
    return X[0:k, :]</pre></div><p>Now, the next task is to randomly assign the cluster number to each data point based on the initial cluster assignment. This time we can use another method:</p><div class="informalexample"><pre class="programlisting">def assign_cluster(X, centroids):
    expanded_vectors = tf.expand_dims(X, 0)
    expanded_centroids = tf.expand_dims(centroids, 1)
    distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)
    calc_wss = tf.reduce_sum(tf.reduce_min(distances, 0))
    mins = tf.argmin(distances, 0)
    return mins, calc_wss</pre></div><p>The previous method computes the minimum distance and WCSS for the clustering evaluation in the later steps. Then, we need to update the centroid to check and make sure if there are any changes that occur in the cluster assignment:</p><div class="informalexample"><pre class="programlisting">def recompute_centroids(X, Y):
    sums = tf.unsorted_segment_sum(X, Y, k)
    counts = tf.unsorted_segment_sum(tf.ones_like(X), Y, k)
    return sums / counts</pre></div><p>Now that we have defined many variables, it's time to initialize them using <code class="literal">local_variable_initializer()</code>, as follows:</p><div class="informalexample"><pre class="programlisting">init_op = tf.local_variables_initializer()</pre></div><p>Finally, we can perform the training. Forthis, the <code class="literal">audioClusterin()</code> method takes the number of tentative clusters k and iterates the training up to the maximum iteration, as follows:</p><div class="informalexample"><pre class="programlisting">def audioClustering(k, max_iterations ): 
    with tf.Session() as sess:
        sess.run(init_op)
        X, names = get_dataset(sess)
        centroids = initial_cluster_centroids(X, k)
        i, converged = 0, False
        while not converged and i &lt; max_iterations:
            i += 1.
            Y, wcss_updated = assign_cluster(X, centroids)        
            centroids = sess.run(recompute_centroids(X, Y))
        wcss = wcss_updated.eval()        
        print(zip(sess.run(Y)), names) 
    return wcss</pre></div><p>The previous method returns the cluster cost, WCSS, and also prints the cluster number against each audio file. So, we have been able to finish the training step. Now, the next task is to evaluate the K-means clustering quality.</p></li><li class="listitem">Evaluating the model.<p>Here, we will evaluate the clustering quality from two perspectives. First, we will observe the predicted cluster number. Secondly, we will also try to find the optimal value of <code class="literal">k</code> as a function of WCSS. So, we will iterate the training for <span class="strong"><strong>K = 2</strong></span> to say <span class="strong"><strong>10</strong></span> and observe the clustering result. However, first, let's create two empty lists to hold the values of <code class="literal">K</code> and WCSS in each step:</p><div class="informalexample"><pre class="programlisting">wcss_list = []
k_list = []</pre></div><p>Now, let's iterate the training using the <code class="literal">for</code> loop as follows:</p><div class="informalexample"><pre class="programlisting">for k in range(2, 9):
    random.seed(12345)
    wcss = audioClustering(k, 100)
    wcss_list.append(wcss)
    k_list.append(k)</pre></div><p>This prints the following output:</p><div class="informalexample"><pre class="programlisting"> ([(0,), (1,), (1,), (0,), (1,), (0,), (0,), (0,), (0,), (0,), (0,)],
['./audio_dataset/scream_1.wav', './audio_dataset/Crash-Cymbal-3.
wav', './audio_dataset/Ride_Cymbal_1.wav', './audio_dataset/Ride_
Cymbal_2.wav', './audio_dataset/Crash-Cymbal-2.wav', './audio_
dataset/Ride_Cymbal_3.wav', './audio_dataset/scream_3.wav', './
audio_dataset/scream_2.wav', './audio_dataset/cough_2.wav', './audio_
dataset/cough_1.wav', './audio_dataset/Crash-Cymbal-1.wav'])

([(0,), (1,), (2,), (2,), (2,), (2,), (2,), (1,), (2,), (2,), (2,)],
['./audio_dataset/Ride_Cymbal_2.wav', './audio_dataset/Crash-
Cymbal-3.wav', './audio_dataset/cough_1.wav', './audio_dataset/Crash-
Cymbal-2.wav', './audio_dataset/scream_2.wav', './audio_dataset/
Ride_Cymbal_3.wav', './audio_dataset/Crash-Cymbal-1.wav', './
udio_
dataset/Ride_Cymbal_1.wav', './audio_dataset/cough_2.wav', './audio_
dataset/scream_1.wav', './audio_dataset/scream_3.wav'])

([(0,), (1,), (2,), (3,), (2,), (2,), (2,), (2,), (2,), (2,), (2,)],
['./audio_dataset/Ride_Cymbal_2.wav', './audio_dataset/Ride_Cymbal_3.
wav', './audio_dataset/cough_1.wav', './audio_dataset/Crash-Cymbal-1.
wav', './audio_dataset/scream_3.wav', './audio_dataset/cough_2.wav',
'./audio_dataset/Crash-Cymbal-2.wav', './audio_dataset/Ride_Cymbal_1.
wav', './audio_dataset/Crash-Cymbal-3.wav', './audio_dataset/
scream_1.wav', './audio_dataset/scream_2.wav'])

([(0,), (1,), (2,), (3,), (4,), (0,), (0,), (4,), (0,), (0,), (0,)],
['./audio_dataset/cough_1.wav', './audio_dataset/scream_1.wav', './
audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/Ride_Cymbal_2.
wav', './audio_dataset/Crash-Cymbal-3.wav', './audio_dataset/
scream_2.wav', './audio_dataset/cough_2.wav', './audio_dataset/
Ride_Cymbal_1.wav', './audio_dataset/Crash-Cymbal-2.wav', './audio_
dataset/Ride_Cymbal_3.wav', './audio_dataset/scream_3.wav'])

([(0,), (1,), (2,), (3,), (4,), (5,), (2,), (2,), (2,), (4,), (2,)],
['./audio_dataset/scream_3.wav', './audio_dataset/Ride_Cymbal_2.wav',
'./audio_dataset/cough_1.wav', './audio_dataset/Crash-Cymbal-2.wav',
'./audio_dataset/Crash-Cymbal-3.wav', './audio_dataset/scream_2.wav',
'./audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/cough_2.wav',
'./audio_dataset/Ride_Cymbal_3.wav', './audio_dataset/Ride_Cymbal_1.
wav', './audio_dataset/scream_1.wav'])

([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (5,), (6,), (5,), (5,)],
['./audio_dataset/cough_2.wav', './audio_dataset/Ride_Cymbal_3.
av',
'./audio_dataset/scream_1.wav', './audio_dataset/Ride_Cymbal_2.wav',
'./audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/cough_1.wav',
'./audio_dataset/scream_2.wav', './audio_dataset/Crash-Cymbal-3.wav',
'./audio_dataset/scream_3.wav', './audio_dataset/Ride_Cymbal_1.wav',
'./audio_dataset/Crash-Cymbal-2.wav'])

([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (6,), (6,), (1,)],
['./audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/scream_3.
wav', './audio_dataset/Ride_Cymbal_3.wav', './audio_dataset/
Crash-Cymbal-3.wav', './audio_dataset/Crash-Cymbal-2.wav', './
audio_dataset/cough_2.wav', './audio_dataset/cough_1.wav', './audio_
dataset/Ride_Cymbal_1.wav', './audio_dataset/Ride_Cymbal_2.wav', './
audio_dataset/scream_1.wav', './audio_dataset/scream_2.wav'])

([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (1,), (7,)],
['./audio_dataset/scream_2.wav', './audio_dataset/Ride_Cymbal_1.wav',
'./audio_dataset/Crash-Cymbal-2.wav', './audio_dataset/Ride_Cymbal_3.
wav', './audio_dataset/Ride_Cymbal_2.wav', './audio_dataset/scream_3.
wav', './audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/cough_1.
wav', './audio_dataset/cough_2.wav', './audio_dataset/Crash-Cymbal-3.
wav', './audio_dataset/scream_1.wav'])</pre></div><p>These values signify that each audio file is clustered and the cluster number has been assigned (the first bracket is the cluster number, the contents in the second bracket is the filename). However, it is difficult to judge the accuracy from this output. One naïve approach would be to compare each file with figure 12 to figure 15. Alternatively, let's adopt a better approach that we used in the first example that is the elbow method. For this, I have created a dictionary using two lists that are <code class="literal">k_list</code> and <code class="literal">wcss_list</code> computed previously, as follows:</p><div class="informalexample"><pre class="programlisting">dict_list = zip(k_list, wcss_list)
my_dict = dict(dict_list)
print(my_dict)</pre></div><p>The previous code produces the following output:</p><div class="informalexample"><pre class="programlisting">{2: 2.8408628007260428, 3: 2.3755930780867365, 4: 0.9031724736903582,
5: 0.7849431270192495, 6: 0.872767581979385, 7: 0.62019339653673422,
8: 0.70075249251166494, 9: 0.86645706880532057}</pre></div><p>From the previous output, you can see a sharp drop in WCSS for <span class="strong"><strong>k = 4</strong></span>, and this is generated in the third iteration. So, based on this minimum evaluation, we can take a decision about the following clustering assignment:</p><div class="informalexample"><pre class="programlisting">Ride_Cymbal_1.wav =&gt; 2
Ride_Cymbal_2.wav =&gt; 0 
cough_1.wav =&gt; 2 
cough_2.wav =&gt;2
Crash-Cymbal-1.wav =&gt;3
Crash-Cymbal-2.wav =&gt; 2
scream_1.wav =&gt; 2 
scream_2.wav =&gt; 2  </pre></div><p>Now that we have seen two complete examples of using K-means, there is another example called kNN. This is typically a supervised ML algorithm. In the next section, we will see how we can train this algorithm in an unsupervised way for a regression task.</p></li></ol></div></div>
<div class="section" title="Using kNN for Predictive Analytics"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Using kNN for Predictive Analytics</h1></div></div></div><p>kNN is non-parametric and instance-based and is used in supervised learning. It is a robust and versatile classifier, frequently used as a benchmark for complex classifiers such as <span class="strong"><strong>Neural Networks</strong></span> (<span class="strong"><strong>NNs</strong></span>) and <span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVMs</strong></span>). kNN is commonly used in economic forecasting, data compression, and genetics based on their expression profiling.</p><div class="section" title="Working Principles of kNN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/>Working Principles of kNN</h2></div></div></div><p>The idea of kNN is that from a set of features <span class="strong"><strong>x</strong></span> we try to predict the labels <span class="strong"><strong>y</strong></span>. Thus, kNN falls in a supervised learning family of algorithms. Informally, this means that we are given a labeled dataset consisting of training observations (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>). Now, the task is to model the relationship between <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> so that the function <span class="emphasis"><em>f: X→Y</em></span> learns from the unseen observation <span class="emphasis"><em>x</em></span>. The function <span class="emphasis"><em>f(x)</em></span> can confidently predict the corresponding label y prediction on a point <span class="emphasis"><em>z</em></span> by looking at a set of nearest neighbors.</p><p>However, the actual method of prediction depends on whether or not we are doing regression (continuous) or classification (discrete). For discrete classification targets, the prediction may be given by a maximum voting scheme weighted by the distance to the prediction point:</p><div class="mediaobject"><img alt="Working Principles of kNN" src="graphics/03_17.jpg"/></div><p>Here, our prediction, <span class="emphasis"><em>f(z)</em></span>, is the maximum weighted value of all classes, <span class="emphasis"><em>j</em></span>, where the weighted distance from the prediction point to the training point, <span class="emphasis"><em>i</em></span>, is given by <span class="emphasis"><em>φ (dij)</em></span>, where <span class="emphasis"><em>d</em></span> indicates the distance between two points. On the other hand, <span class="emphasis"><em>Iij</em></span> is just an indicator function if point <span class="emphasis"><em>i</em></span> is in class <span class="emphasis"><em>j</em></span>.</p><p>For continuous regression targets, the prediction is given by a weighted average of all <span class="emphasis"><em>k</em></span> points nearest to the prediction:</p><div class="mediaobject"><img alt="Working Principles of kNN" src="graphics/03_18.jpg"/></div><p>From the previous two equations, it is clear that the prediction is heavily dependent on the choice of the distance metric, <span class="emphasis"><em>d</em></span>. There are many different specifications of distance metrics such as L1 and L2 metrics can be used for the textual distances:</p><p>A straightforward way to weigh the distances is by the distance itself. Points that are further away from our prediction should have less impact than the nearer points. The most common way to weigh is the normalized inverse of the distance. We will implement this method in the next section.</p></div><div class="section" title="Implementing a kNN-Based Predictive Model"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Implementing a kNN-Based Predictive Model</h2></div></div></div><p>To illustrate how making predictions with the nearest neighbors works in TensorFlow, we will use the 1970s Boston housing dataset, which is available through the UCI ML repository at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data</a>. The following table shows the basic description of the dataset:</p><div class="mediaobject"><img alt="Implementing a kNN-Based Predictive Model" src="graphics/03_19.jpg"/></div><p>Here, we will predict the median neighborhood housing value that is the last value named MEDV as a function of several features. Since we consider the training set the trained model, we will find kNNs to the prediction points and do a weighted average of the target value. Let's get started:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Loading required libraries and packages.<p>As an entry point, we import necessary libraries and packages that will be needed to do predictive analytics using kNN with TensorFlow:</p><div class="informalexample"><pre class="programlisting">import matplotlib.pyplot as plt
import numpy as np 
import random
import os
import tensorflow as tf
import requests
from tensorflow.python.framework import ops
import warnings</pre></div></li><li class="listitem">Resetting the default graph and disabling the TensorFlow warning.<p>We need to reset the default TensorFlow graph using the <code class="literal">reset_default_graph()</code> function from TensorFlow. You must also disable all warnings due to the absence of GPU on your device:</p><div class="informalexample"><pre class="programlisting">warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
ops.reset_default_graph()</pre></div></li><li class="listitem">Loading and preprocessing the dataset.<p>First, we will load and parse the dataset using the <code class="literal">get()</code> function from the requests package as follows:</p><div class="informalexample"><pre class="programlisting">housing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'
housing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
num_features = len(housing_header)
housing_file = requests.get(housing_url)
housing_data = [[float(x) for x in y.split(' ') if len(x)&gt;=1] for y in housing_file.text.split('\n') if len(y)&gt;=1]</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>For more information on how the previous code works, please see the documentation of requests package at <a class="ulink" href="http://docs.python-requests.org/en/master/user/quickstart/">http://docs.python-requests.org/en/master/user/quickstart/</a>.</p></div></div><p>Then, we will separate features (predictor) from the labels:</p><div class="informalexample"><pre class="programlisting">y_vals = np.transpose([np.array([y[len(housing_header)-1] for y in housing_data])])
x_vals = np.array([[x for i,x in enumerate(y) if housing_header[i] in housing_header] for y in housing_data])</pre></div><p>Now, to get some idea of the features and labels, let's print them as follows:</p><div class="informalexample"><pre class="programlisting">print(y_vals)
&gt;&gt;&gt;
[[ 24. ]
[ 21.6]
[ 34.7]
[ 33.4]
[ 36.2]
[ 28.7]
[ 22.9]
[ 27.1]
[ 16.5]
[ 18.9]
[ 15. ]
…]</pre></div><p>So, the labels are okay to work with, and these are also continuous values. Now, let's see the features:</p><div class="informalexample"><pre class="programlisting">print(x_vals)
&gt;&gt;&gt;
[[  6.32000000e-03   1.80000000e+01   2.31000000e+00 ...,   3.96900000e+02
    4.98000000e+00   2.40000000e+01]
 [  2.73100000e-02   0.00000000e+00   7.07000000e+00 ...,   3.96900000e+02
    9.14000000e+00   2.16000000e+01]
 [  2.72900000e-02   0.00000000e+00   7.07000000e+00 ...,   3.92830000e+02
    4.03000000e+00   3.47000000e+01]
 ..., 
 [  6.07600000e-02   0.00000000e+00   1.19300000e+01 ...,   3.96900000e+02
    5.64000000e+00   2.39000000e+01]
 [  1.09590000e-01   0.00000000e+00   1.19300000e+01 ...,   3.93450000e+02</pre></div><div class="informalexample"><pre class="programlisting">    6.48000000e+00   2.20000000e+01]
 [  4.74100000e-02   0.00000000e+00   1.19300000e+01 ...,   3.96900000e+02
    7.88000000e+00   1.19000000e+01]]</pre></div><p>Well, if you see these values, they are pretty unscaled to be fed to a predictive model. Thus, we need to apply the min-max scaling to get a better structure of the features so that an estimator scales and translates each feature individually, and it ensures that it is in the given range on the training set, that is, between zero and one. Since features are most important in predictive analytics, we should take special care of them. The following line of code does the min-max scaling:</p><div class="informalexample"><pre class="programlisting">x_vals = (x_vals - x_vals.min(0)) / x_vals.ptp(0)</pre></div><p>Now let's print them again to check to make sure what's changed:</p><div class="informalexample"><pre class="programlisting">print(x_vals)
&gt;&gt;&gt;
[[  0.00000000e+00   1.80000000e-01   6.78152493e-02 ...,   1.00000000e+008.96799117e-02   4.22222222e-01]
 [  2.35922539e-04   0.00000000e+00   2.42302053e-01 ...,   1.00000000e+002.04470199e-01   3.68888889e-01]
 [  2.35697744e-04   0.00000000e+00   2.42302053e-01 ...,   9.89737254e-016.34657837e-02   6.60000000e-01] ..., 
 [  6.11892474e-04   0.00000000e+00   4.20454545e-01 ...,   1.00000000e+001.07891832e-01   4.20000000e-01]
 [  1.16072990e-03   0.00000000e+00   4.20454545e-01 ...,   9.91300620e-01
    1.31070640e-01   3.77777778e-01]
 [  4.61841693e-04   0.00000000e+00   4.20454545e-01 ...,   1.00000000e+00
    1.69701987e-01   1.53333333e-01]]</pre></div></li><li class="listitem">Preparing the training and test set.<p>Since our features are already scaled, now it's time to split the data into train and test sets. Now, we split the x and y values into the train and test sets. We will create the training set by selecting about 75% of the rows at random and leave the remaining 25% for the test set:</p><div class="informalexample"><pre class="programlisting">train_indices = np.random.choice(len(x_vals), int(len(x_vals)*0.75), replace=False)
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))
x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]</pre></div></li><li class="listitem">Preparing the placeholders for the tensors.<p>First, we will declare the batch size. Ideally, the batch size should be equal to the size of features in the test set:</p><div class="informalexample"><pre class="programlisting">batch_size=len(x_vals_test)</pre></div><p>Then, we need to declare the placeholders for the TensorFlow tensors, as follows:</p><div class="informalexample"><pre class="programlisting">x_data_train = tf.placeholder(shape=[None, num_features], dtype=tf.float32)
x_data_test = tf.placeholder(shape=[None, num_features], dtype=tf.float32)
y_target_train = tf.placeholder(shape=[None, 1], dtype=tf.float32)
y_target_test = tf.placeholder(shape=[None, 1], dtype=tf.float32)</pre></div></li><li class="listitem">Defining the distance metrics.<p>For this example, we are going to use the L1 distance. The reason is that using L2 did not give a better result in my case:</p><div class="informalexample"><pre class="programlisting">distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2)</pre></div></li><li class="listitem">Implementing kNN.<p>Now, it's time to implement kNN. This will predict the nearest neighbors by getting the minimum distance index. The <code class="literal">kNN()</code> method does the trick. There are several steps for doing this, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get the minimum distance index.</li><li class="listitem">Compute the prediction function. To do this, we will use the <code class="literal">top_k()</code>, function, which returns the values and indices of the largest values in a tensor. Since we want the indices of the smallest distances, we will instead find the k-biggest negative distances. Since we are predicting continuous values that is regression task, we also declare the predictions and <span class="strong"><strong>Mean Squared Error</strong></span> (<span class="strong"><strong>MSE</strong></span>) of the target values.</li><li class="listitem">Calculate the number of loops over training data.</li><li class="listitem">Initialize the global variables.</li><li class="listitem">Iterate the training over the number of loops calculated in step 3.</li></ol></div><p>Now, here's the function of kNN. It takes the number of initial neighbors and starts the computation. Note that although it is a widely used convention, here I will make it a variable to do some tuning, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">def kNN(k): 
    topK_X, topK_indices = tf.nn.top_k(tf.negative(distance), k=k)
    x_sums = tf.expand_dims(tf.reduce_sum(topK_X, 1), 1)
    x_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))
    x_val_weights = tf.expand_dims(tf.div(topK_X, x_sums_repeated), 1)
    topK_Y = tf.gather(y_target_train, topK_indices)
    prediction = tf.squeeze(tf.matmul(x_val_weights,topK_Y), axis=[1])
   mse = tf.div(tf.reduce_sum(tf.square(tf.subtract(prediction, y_target_test))), batch_size)
    num_loops = int(np.ceil(len(x_vals_test)/batch_size))
    init_op = tf.global_variables_initializer()
    with tf.Session() as sess:
            sess.run(init_op) 
            for i in range(num_loops):
                min_index = i*batch_size
                max_index = min((i+1)*batch_size,len(x_vals_train))
                x_batch = x_vals_test[min_index:max_index]
                y_batch = y_vals_test[min_index:max_index]
                predictions = sess.run(prediction, feed_dict={x_
data_train: x_vals_train, x_data_test: x_batch, y_target_train: y_vals_train, y_target_test: y_batch})
                batch_mse = sess.run(mse, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch, y_target_train: y_vals_train, y_target_test: y_batch})           
    return batch_mse</pre></div></li><li class="listitem">Evaluating the classification/regression.<p>Note that this function does not return the optimal <code class="literal">mse</code> value, that is, the lowest <code class="literal">mse</code> value, but varies over different <code class="literal">k</code> values, so this is a hyperparameter to be tuned. One potential technique would be to iterate the method for <span class="emphasis"><em>k = 2</em></span> to, say, <code class="literal">11</code> and keeping track of the optimal <code class="literal">k</code> value that forces <code class="literal">kNN()</code> to produce the lowest <code class="literal">mse</code> value. First, we define a method that iterates several times from <code class="literal">2</code> to <code class="literal">11</code> and returns two separate lists for <code class="literal">mse</code> and <code class="literal">k</code> respectively:</p><div class="informalexample"><pre class="programlisting">mse_list = []
k_list = []
def getOptimalMSE_K():
    mse = 0.0
    for k in range(2, 11):
        mse = kNN(k)  
        mse_list.append(mse)
        k_list.append(k)
    return k_list, mse_list </pre></div><p>Now, it's time to invoke the previous method and find the optimal <code class="literal">k</code> value for which the kNN produces the lowest <code class="literal">mse</code> value. Upon receiving the two lists, we create a dictionary and use the <code class="literal">min()</code>method to return the optimal <code class="literal">k</code> value, as follows:</p><div class="informalexample"><pre class="programlisting">k_list, mse_list  = getOptimalMSE_K()
dict_list = zip(k_list, mse_list)
my_dict = dict(dict_list)
print(my_dict)
optimal_k = min(my_dict, key=my_dict.get)
&gt;&gt;&gt;
{2: 7.6624126, 3: 10.184645, 4: 8.9112329, 5: 11.29573, 6: 13.341181, 7: 14.406253, 8: 13.923589, 9: 14.915736, 10: 13.920851}</pre></div><p>Now, let's print <code class="literal">the Optimal k value</code> for which we get the lowest <code class="literal">mse</code> value:</p><div class="informalexample"><pre class="programlisting">print("Optimal K value: ", optimal_k)
mse = min(mse_list)
print("Minimum mean square error: ", mse)
&gt;&gt;&gt;
Optimal K value: 2 minimum mean square error: 7.66241</pre></div></li><li class="listitem">Running the best kNN.<p>Now we have the optimal <code class="literal">k</code>, so we will entertain calculating the nearest neighbor. This time we will try to return the matrices for the predicted and actual labels:</p><div class="informalexample"><pre class="programlisting">def bestKNN(k): 
    topK_X, topK_indices = tf.nn.top_k(tf.negative(distance), k=k)
    x_sums = tf.expand_dims(tf.reduce_sum(topK_X, 1), 1)
    x_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))
    x_val_weights = tf.expand_dims(tf.div(topK_X, x_sums_repeated), 1)
    topK_Y = tf.gather(y_target_train, topK_indices)
    prediction = tf.squeeze(tf.matmul(x_val_weights,topK_Y), axis=[1])
    num_loops = int(np.ceil(len(x_vals_test)/batch_size))
    init_op = tf.global_variables_initializer()
    with tf.Session() as sess:
            sess.run(init_op) 
            for i in range(num_loops):
                min_index = i*batch_size
                max_index = min((i+1)*batch_size,len(x_vals_train))
                x_batch = x_vals_test[min_index:max_index]
                y_batch = y_vals_test[min_index:max_index]</pre></div><p>predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch, y_target_train: y_vals_train, y_target_test: y_batch})</p><div class="informalexample"><pre class="programlisting">    return predictions, y_batch</pre></div></li><li class="listitem">Evaluating the best kNN.<p>Now, we will invoke the <code class="literal">bestKNN()</code> method with the optimal value of <code class="literal">k</code> that was calculated in the previous step, as follows:</p><div class="informalexample"><pre class="programlisting">predicted_labels, actual_labels = bestKNN(optimal_k)</pre></div><p>Now, I would like to measure the prediction accuracy. Are you wondering why? I know the reason. You're right. There is no significant reason for calculating the accuracy or precision since we are predicting the continuous values that is labels. Even so, I would like to show you whether it works or not:</p><div class="informalexample"><pre class="programlisting">def getAccuracy(testSet, predictions):
 correct = 0
 for x in range(len(testSet)):
     if(np.round(testSet[x]) == np.round(predictions[x])):
                correct += 1
 return (correct/float(len(testSet))) * 100.0
accuracy = getAccuracy(actual_labels, predicted_labels)
print('Accuracy: ' + repr(accuracy) + '%')
&gt;&gt;&gt;
Accuracy: 17.322834645669293%</pre></div><p>The previous <code class="literal">getAccuracy()</code> method computes the accuracy, which is quite low. This is obvious and there is no exertion. This also implies that the previous method is pointless. However, if you are about to predict discrete values, this method will obviously help you. Try it yourself with suitable data and combinations of the previous code.</p><p>But do not to be disappointed; we have another way of looking at how our predictive model performs. We can still plot a histogram showing the predicted versus actual labels that are a prediction and actual distribution:</p><div class="informalexample"><pre class="programlisting">bins = np.linspace(5, 50, 45)
plt.hist(predicted_labels, bins, alpha=1.0, facecolor='red', label='Prediction')
plt.hist(actual_labels, bins, alpha=1.0, facecolor='green', label='Actual')
plt.title('predicted vs actual values')
plt.xlabel('Median house price in $1,000s')
plt.ylabel('count')
plt.legend(loc='upper right')
plt.show()
&gt;&gt;&gt;</pre></div><div class="mediaobject"><img alt="Implementing a kNN-Based Predictive Model" src="graphics/03_20.jpg"/><div class="caption"><p>Figure 16: Predicted versus actual median prices of the houses in $1,000s</p></div></div></li></ol></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Summary</h1></div></div></div><p>In this lesson, we have discussed unsupervised learning from a theoretical and practical perspective. We have seen how we can make use of predictive analytics and find out how we can take advantage of it to cluster records belonging to a certain group or class for a dataset of unsupervised observations. We have discussed unsupervised learning and clustering using K-means. In addition, we have seen how we can fine tune the clustering using the Elbow method for better predictive accuracy. We have also seen how to predict neighborhoods using K-means, and then, we have seen another example of clustering audio clips based on their audio features. Finally, we have seen how we can use unsupervised kNN for predicting the nearest neighbors.</p><p>In the next lesson, we will discuss the wonderful field of text analytics using TensorFlow. Text analytics is a wide area in <span class="strong"><strong>natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>), and ML is useful in many use cases, such as sentiment analysis, chatbots, email spam detection, text mining, and natural language processing. You will learn how to use TensorFlow for text analytics with a focus on use cases of text classification from the unstructured spam prediction and movie review dataset. Based on the spam filtering dataset, we will develop predictive models using a LR algorithm with TensorFlow</p></div>
<div class="section" title="Assessments"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Assessments</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">kNN falls in a ______ learning family of algorithms.</li><li class="listitem">State whether the following statement is True or False: A cluster is a collection of objects that have a similarity between them and are dissimilar to the objects belonging to other clusters. If a collection of objects is provided, clustering algorithms put these objects into groups based on similarity.</li><li class="listitem">What is the main objective of unsupervised learning?</li><li class="listitem">State whether the following statement is True or False: In a clustering task, an algorithm groups related features into categories by analyzing similarities between input examples, where similar features are clustered and marked using circles.</li><li class="listitem">Clustering analysis is about dividing data samples or data points and putting them into corresponding ______ classes or clusters.<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Heterogynous</li><li class="listitem">Linear</li><li class="listitem">Homogeneous</li><li class="listitem">Similar.</li></ol></div></li></ol></div></div></body></html>