- en: Deep Learning Architectures for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the era of the **Internet of Things** (**IoT**), an enormous amount of sensory
    data for a wide range of fields and applications is being generated and collected
    from numerous sensing devices. Applying analytics over such data streams to discover
    new information, predict future insights, and make controlled decisions, is a
    challenging task, which makes IoT a worthy paradigm for business intelligence
    and quality-of-life improving technology. However, analytics on IoT—enabled devices
    requires a platform consisting of **machine learning** (**ML**) and **deep learning**
    (**DL**) frameworks, a software stack, and hardware (for example, a **Graphical
    Processing Unit** (**GPU**) and **Tensor Processing Unit** (**TPU**)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss some basic concepts of DL architectures and
    platforms, which will be used in all subsequent chapters. We will start with a
    brief introduction to ML. Then, we will move onto DL, which is a branch of ML
    based on a set of algorithms that attempts to model high-level abstractions in
    data. We will briefly discuss some of the most well-known and widely used neural
    network architectures. Then, we will look at various features of DL frameworks
    and libraries that can be used for developing DL applications on IoT-enabled devices.
    Briefly, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: A soft introduction to ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep neural network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A soft introduction to ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML approaches are based on a set of statistical and mathematical algorithms
    carrying out tasks such as classification, regression analysis, concept learning,
    predictive modeling, clustering, and mining of useful patterns. Using ML, we aim
    to improve the whole learning process automatically so that we may not need complete
    human interactions, or so that we can at least reduce the level of such interactions
    as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Working principle of a learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tom M. Mitchell explained what learning really means from a computer science
    perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this definition, we can conclude that a computer program or machine
    can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn from data and histories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve with experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteratively enhance a model that can be used to predict outcomes of questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the preceding points are at the core of predictive analytics, almost
    every ML algorithm we use can be treated as an optimization problem. This is about
    finding parameters that minimize an objective function; for example, a weighted
    sum of two terms such as a cost function and regularization. Typically, an objective
    function has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: A regularizer that controls the complexity of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss that measures the error of the model on the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the regularization parameter defines the trade-off between
    minimizing the training error and the model's complexity in an effort to avoid
    overfitting problems. Now, if both of these components are convex, then their
    sum is also convex. So, when using an ML algorithm, the goal is to obtain the
    best hyperparameters of a function that return the minimum error when making predictions.
    Therefore, by using a convex optimization technique, we can minimize the function
    until it converges toward the minimum error.
  prefs: []
  type: TYPE_NORMAL
- en: Given that a problem is convex, it is usually easier to analyze the asymptotic
    behavior of the algorithm, which shows how fast it converges as the model observes
    more and more training data. The task of ML is to train a model so that it can
    recognize complex patterns from the given input data and can make decisions in
    an automated way. Thus, making predictions is all about testing the model against
    new (that is, unobserved) data and evaluating the performance of the model itself.
    However, in the process as a whole, and for making the predictive model a successful
    one, data acts as the first-class citizen in all ML tasks. In reality, the data
    that we feed to our ML systems must be made up of mathematical objects, such as
    vectors, so that they can consume such data.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the available data and feature types, the performance of your predictive
    model can vacillate dramatically. Therefore, selecting the right features is one
    of the most important steps before the model evaluation takes place. This is called
    **feature engineering**, where the domain knowledge pertaining to the data is
    used to create only selective or useful features that help prepare the feature
    vectors to be used so that an ML algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: For example, comparing hotels is quite difficult unless we already have a personal
    experience of staying in multiple hotels. However, with the help of an ML model,
    which is already trained with quality features out of thousands of reviews and
    features (for example, how many stars does a hotel have, the size of the room,
    the location, and room service, and so on), it is pretty feasible now. We'll see
    several examples throughout the chapters. However, before developing such an ML
    model, knowing a number of ML concepts is also important.
  prefs: []
  type: TYPE_NORMAL
- en: General ML rule of thumb
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The general ML rule of thumb is that the more data there is, the better the
    predictive model. However, having more features often creates a mess, to the extent
    that the performance degrades drastically, especially if the dataset is multidimensional.
    The entire learning process requires input datasets that can be split into three
    types (or are already provided as such):'
  prefs: []
  type: TYPE_NORMAL
- en: A **training set** is the knowledge base coming from historical or live data
    that is used to fit the parameters of the ML algorithm. During the training phase,
    the ML model utilizes the training set to find optimal weights of the network
    and reach the objective function by minimizing the training error. Here, the backpropagation
    rule, or an optimization algorithm, is used to train the model, but all the hyperparameters
    are needed to be set before the learning process starts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **validation set** is a set of examples used to tune the parameters of an
    ML model. It ensures that the model is trained well and generalizes toward avoiding
    overfitting. Some ML practitioners refer to it as a development set, or dev set
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **test set** is used for evaluating the performance of the trained model on
    unseen data. This step is also referred to as **model inferencing**. After assessing
    the final model on the test set (that is, when we're fully satisfied with the
    model's performance), we do not have to tune the model any further, but the trained
    model can be deployed in a production-ready environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common practice is splitting the input data (after the necessary preprocessing
    and feature engineering) into 60% for training, 10% for validation, and 20% for
    testing, but it really depends on use cases. Sometimes, we also need to perform
    upsampling or downsampling on the data, based on the availability and quality
    of the datasets. This rule of thumb of learning on different types of training
    sets can differ across ML tasks, as we will cover in the next section. However,
    before that, let's take a quick look at a few common phenomena in ML.
  prefs: []
  type: TYPE_NORMAL
- en: General issues in ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we use this input data for training, validation, and testing, usually,
    the learning algorithms cannot learn 100% accurately, which involves training,
    validation, and test error (or loss). There are two types of errors that you may
    encounter in an ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: Irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The irreducible error cannot be reduced even with the most robust and sophisticated
    model. However, the reducible error, which has two components, called bias and
    variance, can be reduced. Therefore, to understand the model (that is, prediction
    errors), we need to focus on bias and variance only. Bias means how far the predicted
    values are from the actual values. Usually, if the average predicted values are
    very different from the actual values (labels), then the bias is higher.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ML model will have a high bias because it can''t model the relationship
    between input and output variables (can''t capture the complexity of data well)
    and becomes very simple. Thus, an overly simple model with high variance causes
    underfitting of the data. The following diagram gives some high-level insights,
    and also shows what a just-right fit model should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e475e9e-0922-45e1-8a36-afdca4dc93cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Variance signifies the variability between the predicted values and the actual
    values (how scattered they are). If the model has a high training error as well
    as the validation error or test error being the same as the training error, the
    model has a high bias. On the other hand, if the model has a low training error
    but has a high validation or high test error, the model has a high variance. An
    ML model usually performs very well on the training set, but doesn''t work well
    on the test set (because of high error rates). Ultimately, it results in an underfit
    model. We can recap the overfitting and underfitting once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting**: If your training and validation errors are both relatively
    equal and very high, then your model is most likely underfitting your training
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: If your training error is low and your validation error is
    high, then your model is most likely overfitting your training data. The just-right
    fit model learns very well and performs better on unseen data too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bias-variance trade-off: the high bias and high variance issue is often called
    bias-variance trade-off, because a model cannot be too complex or too simple at
    the same time. Ideally, we strive for the best model that has both low bias and
    low variance.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we know the basic working principle of an ML algorithm. However, based on
    problem type and the method used to solve a problem, ML tasks can be different;
    for example, supervised learning, unsupervised learning, and reinforcement learning.
    We'll discuss these learning tasks in more detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: ML tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although every ML problem is more or less an optimization problem, the way
    in which they are solved can vary. In fact, learning tasks can be categorized
    into three types: supervised learning, unsupervised learning, and reinforcement
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised learning is the simplest and most well-known automatic learning
    task. It is based on a number of predefined examples, in which the category to
    which each of the inputs should belong is already known, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7da0267f-7bf5-4811-bf54-5a19375ab274.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a typical workflow of supervised learning. An actor
    (for example, a data scientist or data engineer) performs the **extraction**,
    **transformation**, **and load** (**ETL**) and the necessary feature engineering
    (including feature extraction, selection, and so on) to get the appropriate data
    with features and labels so that they can be fed into the model. Then, they split
    the data into training, development, and test sets. The training set is used to
    train an ML model, the validation set is used to validate the training against
    the overfitting problem and regularization, and then the actor would evaluate
    the model's performance on the test set (that is, unseen data).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the performance is not satisfactory, the actor can perform additional
    tuning to get the best model based on hyperparameter optimization. Finally, they
    will deploy the best model in a production-ready environment. In the overall life
    cycle, there might be many actors involved (for example, a data engineer, data
    scientist, or an ML engineer), performing each step independently or collaboratively.
    The supervised learning context includes classification and regression tasks;
    classification is used to predict which class a data point is a part of (discrete
    value). It is also used for predicting the label of the class attribute. The following
    diagram summarizes these steps in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ec8d6c2-8f14-485a-a124-40e74f65de64.png)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, regression is used to predict continuous values and make
    a numeric prediction of the class attribute. In the context of supervised learning,
    the learning process required for the input dataset is split randomly into three
    sets; for example, 60% for the training set, 10% for the validation set, and the
    remaining 30% for the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How would you summarize and group a dataset if the labels were not given? You''ll
    probably try to answer this question by finding the underlying structure of a
    dataset and measuring the statistical properties, such as the frequency distribution,
    mean, and standard deviation. If the question is how would you effectively represent
    data in a compressed format, you''ll probably reply saying that you''ll use some
    software for doing the compression, although you might have no idea how that software
    would do it. The following diagram shows the typical workflow of an unsupervised
    learning task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf93f6bf-f0b4-486d-aae2-4e13e70a2012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These are precisely two of the main goals of unsupervised learning, which is
    largely a data-driven process. We call this type of learning unsupervised because
    you will have to deal with unlabeled data. The following quote comes from Yann
    LeCun, director of AI research (source: *Predictive Learning*, NIPS 2016, Yann
    LeCun, Facebook Research):'
  prefs: []
  type: TYPE_NORMAL
- en: '"Most human and animal learning is unsupervised learning. If intelligence was
    a cake, unsupervised learning would be the cake, supervised learning would be
    the icing on the cake, and reinforcement learning would be the cherry on the cake.
    We know how to make the icing and the cherry, but we don''t know how to make the
    cake. We need to solve the unsupervised learning problem before we can even think
    of getting to true AI."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A few most widely used unsupervised learning tasks include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: Grouping data points based on similarity (or statistical properties),
    for example, a company such as Airbnb often groups its apartments and houses into
    neighborhoods so that customers can navigate the listed ones more easily'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: Compressing the data with the structure and statistical
    properties preserved as much as possible, for example, often, the number of dimensions
    of the dataset needs to be reduced for the modelling and visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Useful in several applications, such as identification
    of credit card fraud detection, identifying faulty pieces of hardware in an industrial
    engineering process, and identifying outliers in large-scale datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Association rule mining**: Often used in market basket analysis, for example,
    asking which items are bought together frequently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning is an artificial intelligence approach that focuses
    on the learning of the system through its interactions with the environment. In
    reinforcement learning, the system''s parameters are adapted based on the feedback
    obtained from the environment, which, in turn, provides feedback on the decisions
    made by the system. The following diagram shows a person making decisions in order
    to arrive at their destination. Let''s take an example of the route you take from
    home to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78d85aba-6e21-4d7d-9256-878b8542d641.png)'
  prefs: []
  type: TYPE_IMG
- en: We can take a look at one more example in terms of a system modeling a chess
    player. In order to improve its performance, the system utilizes the result of
    its previous moves; such a system is said to be a system learning with reinforcement.
    In this case, you take the same route to work every day. However, out of the blue
    one day, you get curious and decide to try a different route with a view to finding
    the shortest path. Similarly, based on your experience and the time taken with
    the different route, you'll decide whether you should take that specific route
    more often. We can take a look at one more example in terms of a system modeling
    a chess player.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned the basic working principles of ML and different learning
    tasks. Let's have a look at each learning task with some example use cases in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Learning types with applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen the basic working principles of ML algorithms, and we have seen
    what the basic ML tasks are, and how they formulate domain-specific problems.
    However, each of these learning tasks can be solved using different algorithms.
    The following diagram provides a glimpse into this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4352c6ad-bcbf-4f21-9866-b0a6537c871e.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the preceding diagram lists only a few use cases and applications using
    different ML tasks. In practice, ML is used in numerous use cases and applications.
    We will try to cover a few of them throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Delving into DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple ML methods that were used in the normal-size data analysis are no longer
    effective and should be replaced by more robust ML methods. Although classical
    ML techniques allow researchers to identify groups or clusters of related variables,
    the accuracy and effectiveness of these methods diminish with large and multidimensional
    data.
  prefs: []
  type: TYPE_NORMAL
- en: How did DL take ML to the next level?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Simple ML methods used in small-scale data analysis are not effective when
    dealing with large and high-dimensional datasets. However, deep learning (DL),
    which is a branch of ML based on a set of algorithms that attempt to model high-level
    abstractions in data, can handle this issue. Ian Goodfellow defined DL in his
    book "*Deep Learning*, MIT Press, 2016" as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Deep learning is a particular kind of machine learning that achieves great
    power and flexibility by learning to represent the world as a nested hierarchy
    of concepts, with each concept defined in relation to simpler concepts, and more
    abstract representations computed in terms of less abstract ones."'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the ML model, a DL model also takes in an input, `X`, and learns
    high-level abstractions or patterns from it to predict an output of `Y`. For example,
    based on the stock prices of the past week, a DL model can predict the stock price
    for the next day. When performing training on such historical stock data, a DL
    model tries to minimize the difference between the prediction and the actual values.
    This way, a DL model tries to generalize to inputs that it hasn't seen before
    and makes predictions on test data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might be wondering, if an ML model can do the same tasks, why do we
    need DL for this? Well, DL models tend to perform well with large amounts of data,
    whereas old ML models stop improving after a certain point. The core concept of
    DL, inspired by the structure and function of the brain, is called **artificial
    neural networks** (**ANNs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Being at the core of DL, ANNs help you to learn the associations between sets
    of inputs and outputs in order to make more robust and accurate predictions. However,
    DL is not only limited to ANNs; there have been many theoretical advances, software
    stacks, and hardware improvements that bring DL to the masses. Let''s look at
    an example in which we want to develop a predictive analytics model, such as an
    animal recognizer, where our system has to resolve two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: To classify whether an image represents a cat or a dog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To cluster images of dogs and cats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we solve the first problem using a typical ML method, we must define the
    facial features (ears, eyes, whiskers, and so on) and write a method to identify
    which features (typically non-linear) are more important when classifying a particular
    animal. However, at the same time, we cannot address the second problem because
    classical ML algorithms for clustering images (such as k-means) cannot handle
    nonlinear features. Take a look at the following diagram, which shows a workflow
    that we would follow to classify if the given image is of a cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6774fd8b-9b85-47e3-af7e-f9b28f882e9f.png)'
  prefs: []
  type: TYPE_IMG
- en: DL algorithms take these two problems one step further, and the most important
    features will be extracted automatically after determining which features are
    the most important for classification or clustering. In contrast, when using a
    classical ML algorithm, we would have to provide the features manually. A DL algorithm
    takes more sophisticated steps instead. For example, first, it identifies the
    edges that are the most relevant when clustering cats or dogs. It then tries to
    find various combinations of shapes and edges hierarchically, which is called
    ETL.
  prefs: []
  type: TYPE_NORMAL
- en: Then, after several iterations, it carries out the hierarchical identification
    of complex concepts and features. Following that, based on the features identified,
    the DL algorithm will decide which of the features are most significant for classifying
    the animal. This step is known as feature extraction. Finally, it takes out the
    label column and performs unsupervised training using **autoencoders** (**AEs**)
    to extract the latent features to be redistributed to k-means for clustering.
    Then, the **clustering assignment hardening loss** (**CAH loss**) and reconstruction
    loss are jointly optimized toward an optimal clustering assignment.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in practice, a DL algorithm is fed with a raw image representation,
    which doesn''t see an image as we see it because it only knows the position of
    each pixel and its color. The image is divided into various layers of analysis.
    For example, at a lower level, there is the software analysis—a grid of a few
    pixels with the task of detecting a type of color or various nuances. If it finds
    something, it informs the next level, which, at this point, checks whether or
    not that given color belongs to a larger form, such as a line. The process continues
    to the upper levels until the algorithm understands what is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6be27318-8090-42f0-8d8c-005086271f80.png)'
  prefs: []
  type: TYPE_IMG
- en: Although a dog versus a cat is an example of a very simple classifier, software
    that's capable of doing these types of things is now widespread and is found in
    systems for recognizing faces, or in those for searching an image on Google, for
    example. This kind of software is based on DL algorithms. By contrast, if we are
    using a linear ML algorithm we cannot build such applications, since these algorithms
    are incapable of handling non-linear image features.
  prefs: []
  type: TYPE_NORMAL
- en: Also, using ML approaches, we typically only handle a few hyperparameters. However,
    when neural networks are brought into the mix, things become too complex. In each
    layer, there are millions or even billions of hyperparameters to tune—so many
    that the cost function becomes non-convex. Another reason for this is that the
    activation functions that are used in hidden layers are non-linear, so the cost
    is non-convex. We will discuss this phenomenon in more detail in later chapters,
    but let's take a quick look at ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs, which are inspired by how a human brain works, form the core of DL and
    its true realization. Today's revolution around DL would not have been possible
    without ANNs. Thus, to understand DL, we need to understand how neural networks
    work.
  prefs: []
  type: TYPE_NORMAL
- en: ANN and the human brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs represent one aspect of the human nervous system, and how the nervous system
    consists of a number of neurons that communicate with each other using axons.
    The receptors receive the stimuli either internally or from the external world.
    Then, they pass this information to the biological neurons for further processing.
    There are a number of dendrites, in addition to another long extension called
    the axon. Toward the axon's extremities, there are minuscule structures called
    synaptic terminals, which are used to connect one neuron to the dendrites of other
    neurons. Biological neurons receive short electrical impulses called signals from
    other neurons, and, in response, they trigger their own signals.
  prefs: []
  type: TYPE_NORMAL
- en: We can, therefore, summarize that the neuron comprises a cell body (also known
    as the **soma**), one or more dendrites for receiving signals from other neurons,
    and an axon for carrying out the signals that are generated by the neurons. A
    neuron is in an active state when it is sending signals to other neurons. However,
    when it is receiving signals from other neurons, it is in an inactive state. In
    an idle state, a neuron accumulates all the signals that are received before reaching
    a certain activation threshold. This whole process motivated researchers to test
    out ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inspired by the working principles of biological neurons, Warren McCulloch and
    Walter Pitts proposed the first artificial neuron model, in 1943, in terms of
    a computational model of nervous activity. This simple model of a biological neuron,
    also known as an **artificial neuron** (**AN**), has one or more binary (on/off)
    inputs and one output only. An AN simply activates its output when more than a
    certain number of its inputs are active.
  prefs: []
  type: TYPE_NORMAL
- en: The example sounds too trivial, but even with such a simplified model, it is
    possible to build a network of ANs. Nevertheless, these networks can be combined
    to compute complex logical expressions too. This simplified model inspired John
    von Neumann, Marvin Minsky, Frank Rosenblatt, and many others to come up with
    another model called a **perceptron,** back in 1957\. The perceptron is one of
    the simplest ANN architectures we have seen in the last 60 years. It is based
    on a slightly different AN called a **Linear Threshold Unit** (**LTU**). The only
    difference is that the inputs and outputs are now numbers instead of binary on/off
    values. Each input connection is associated with a weight. The LTU computes a
    weighted sum of its inputs, then applies a step function (which resembles the
    action of an activation function) to that sum, and outputs the result.
  prefs: []
  type: TYPE_NORMAL
- en: One of the downsides of a perceptron is that its decision boundary is linear.
    Therefore, they are incapable of learning complex patterns. They are also incapable
    of solving some simple problems, such as **Exclusive OR** (**XOR**). However,
    later on, the limitations of perceptrons were somewhat eliminated by stacking
    multiple perceptrons, called **MLP**. So, the most significant progress in ANNs
    and DL can be described in the following timeline. We have already discussed how
    the artificial neurons and perceptrons provided the base in 1943 and 1958, respectively.
    In 1969, Marvin *Minsky* and Seymour *Papert* formulated the XOR as a linearly
    non-separable problem, and later, in 1974, Paul *Werbos* demonstrated the backpropagation
    algorithm for training the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the most significant advancement happened in 1982, when John Hopfield
    proposed the Hopfield Network. Then, one of the godfathers of the neural network
    and DL—Hinton and his team—proposed the Boltzmann Machine in 1985\. However, in
    1986 Geoffrey Hinton successfully trained the MLP and Jordan M.I. proposed RNNs.
    In the same year, Paul Smolensky also proposed the improved version of the Boltzmann
    Machine, called the **Restricted Boltzmann Machine** (**RBM**). Then, in 1990,
    Lecun et al. proposed LeNet, which is a deep neural network architecture. For
    a brief glimpse, refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb122a6b-b05d-4691-8b20-edad562ebfa2.png)'
  prefs: []
  type: TYPE_IMG
- en: The most significant year of the 90's era was 1997, when Jordan et al. proposed
    a **recurrent neural network** (**RNN**). In the same year, Schuster et al. proposed
    the improved version of **long-short term memory** (**LSTM**) and the improved
    version of the original RNN called bidirectional RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Despite significant advances in computing, from 1997 to 2005, we did not experience
    much advancement. Then, in 2006, Hinton struck again when, he and his team proposed
    a **deep belief network** (**DBN**) by stacking multiple RBMs. Then in 2012, Hinton
    invented the dropout that significantly improved the regularization and overfitting
    in the deep neural network. After that, Ian Goodfellow et al. introduced the GANs—a
    significant milestone in image recognition. In 2017, Hinton proposed CapsNet to
    overcome the limitation of regular CNNs, and this is so far one of the most remarkable
    milestones. We will discuss these architectures later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How does an ANN learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the concept of biological neurons, the term and idea of ANNs arose.
    Similar to biological neurons, the artificial neuron consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One or more incoming connections that aggregate signals from neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more output connections for carrying the signal to the other neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An activation function, which determines the numerical value of the output signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides the state of a neuron, synaptic weight is considered, which influences
    the connection within the network. Each weight has a numerical value indicated
    by *W[ij]*, which is the synaptic weight connecting neuron *i* to neuron *j*.
    Now, for each neuron *i*, an input
  prefs: []
  type: TYPE_NORMAL
- en: 'vector can be defined by *x[i] = (x[1],x[2],...x[n])*, and a weight vector
    can be defined by *w[i] = (w[i1],x[i2],...x[in])*. Now, depending on the position
    of a neuron, the weights and the output function determine the behavior of an
    individual neuron. Then, during forward propagation, each unit in the hidden layer
    gets the following signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/239e1275-0719-40b1-9689-eb3ea9bcf6a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nevertheless, among the weights, there is also a special type of weight called
    a bias unit, *b*. Technically, bias units aren''t connected to any previous layer,
    so they don''t have true activity. But still, the bias *b* value allows the neural
    network to shift the activation function to the left or right. By taking the bias
    unit into consideration, the modified network output is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc603183-bf05-4ad0-b110-7847a8084d82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation signifies that each hidden unit gets the sum of inputs,
    multiplied by the corresponding weight—this is known as the **Summing junction**.
    Then, the resultant output in the **Summing junction** is passed through the activation
    function, which squashes the output, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2401dc9f-11f6-42e3-90e0-f31f4f14bfb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A practical neural network architecture, however, is composed of input, hidden,
    and output layers that are composed of nodes that make up a network structure.
    It still follows the working principle of an artificial neuron model, as shown
    in the preceding diagram. The input layer only accepts numeric data, such as features
    in real numbers, and images with pixel values. The following diagram shows a neural
    network architecture for solving a multiclass classification (that is, 10 classes)
    problem based on a data having 784 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3fd76b9-b477-4275-8b19-8217f4a4483a.png)'
  prefs: []
  type: TYPE_IMG
- en: A neural network with one input layer, three hidden layers, and an output layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the hidden layers perform most of the computation to learn the patterns,
    and the network evaluates how accurate its prediction is compared to the actual
    output using a special mathematical function called the loss function. It could
    be a complex one or a very simple mean squared error, which can be defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bea114d3-df9b-4ac8-98b1-41acf87a2dc2.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/4058e15e-01a0-4db0-9f89-4de3196c76f3.png)
    is the prediction made by the network, while *Y* represents the actual or expected
    output. Finally, when the error is no longer being reduced, the neural network
    converges and makes a prediction through the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning process for a neural network is configured as an iterative process
    of the optimization of the weights. The weights are updated in each epoch. Once
    the training starts, the aim is to generate predictions by minimizing the loss
    function. The performance of the network is then evaluated on the test set. We
    already know about the simple concept of an artificial neuron. However, generating
    only some artificial signals is not enough to learn a complex task. As such, a
    commonly used supervised learning algorithm is the backpropagation algorithm,
    which is very often used to train a complex ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, training such a neural network is an optimization problem, too,
    in which we try to minimize the error by adjusting network weights and biases
    iteratively, by using backpropagation through **gradient descent** (**GD**). This
    approach forces the network to backtrack through all its layers to update the
    weights and biases across nodes in the opposite direction of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this process using GD does not guarantee that the global minimum is
    reached. The presence of hidden units and the non-linearity of the output function
    means that the behavior of the error is very complex and has many local minima.
    This backpropagation step is typically performed thousands or millions of times,
    using many training batches, until the model parameters converge to values that
    minimize the cost function. The training process ends when the error on the validation
    set begins to increase, because this could mark the beginning of a phase of overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/964eadcf-5c58-41d9-92a5-2122d399085d.png)'
  prefs: []
  type: TYPE_IMG
- en: Searching for the minimum for the error function E, we move in the direction
    in which the gradient G of E is minimal
  prefs: []
  type: TYPE_NORMAL
- en: The downside of using GD is that it takes too long to converge, which makes
    it impossible to meet the demand of handling large-scale training data. Therefore,
    a faster GD, called **stochastic gradient descent** (**SGD**) was proposed, which
    is also a widely used optimizer in DNN training. In SGD, we use only one training
    sample per iteration from the training set to update the network parameters, which
    is a stochastic approximation of the true cost gradient.
  prefs: []
  type: TYPE_NORMAL
- en: There are other advanced optimizers nowadays such as Adam, RMSProp, ADAGrad,
    and Momentum. Each of them is either a direct or indirect optimized version of
    SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Weight and bias initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, here''s a tricky question: how do we initialize the weights? Well, if
    we initialize all the weights to the same value (for example, 0 or 1), each hidden
    neuron will get the same signal. Let''s try to break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: If all weights are initialized to 1, then each unit gets a signal equal to the
    sum of the inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all weights are 0, which is even worse, then every neuron in a hidden layer
    will get zero signal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For network weight initialization, Xavier initialization is used widely. It
    is similar to random initialization, but often turns out to work much better,
    since it can identify the rate of initialization depending on the total number
    of input and output neurons by default. You may be wondering whether you can get
    rid of random initialization while training a regular DNN.
  prefs: []
  type: TYPE_NORMAL
- en: Well, recently, some researchers have been talking about random orthogonal matrix
    initializations that perform better than just any random initialization for training
    DNNs. When it comes to initializing the biases, we can initialize them to zero.
    But setting the biases to a small constant value, such as 0.01 for all biases,
    ensures that all **rectified linear units** (**ReLU**) can propagate a gradient.
    However, it neither performs well nor shows consistent improvement. Therefore,
    sticking with zero is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To allow a neural network to learn complex decision boundaries, we apply a
    non-linear activation function to some of its layers. Commonly used functions
    include Tanh, ReLU, softmax, and variants of these. More technically, each neuron
    receives a signal of the weighted sum of the synaptic weights and the activation
    values of the neurons that are connected as input. One of the most widely used
    functions for this purpose is the so-called sigmoid logistic function, which is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d18ee45f-3d01-4f2c-9ce3-8ffc88c9d079.png)'
  prefs: []
  type: TYPE_IMG
- en: The domain of this function includes all real numbers, and the co-domain is
    (0, 1). This means that any value obtained as an output from a neuron (as per
    the calculation of its activation state) will always be between zero and one.
    The Sigmoid function, as
  prefs: []
  type: TYPE_NORMAL
- en: 'represented in the following diagram, provides an interpretation of the saturation
    rate of a neuron, from not being active (equal to 0) to complete saturation, which
    occurs at a predetermined maximum value (equal to 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09c9bd66-be56-4e39-8322-365a9339cfcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid versus Tanh activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a hyperbolic tangent, or **Tanh**, is another form of activation
    function. **Tanh** flattens a real-valued number between **-1** and **1**. The
    preceding graph shows the difference between the **Tanh** and **Sigmoid** activation
    functions. In particular, mathematically,  speaking the *tanh* activation function
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a31a5802-f491-4761-a390-73b2cbc778c3.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, in the last level of a **feedforward neural network** (**FFNN**),
    the softmax function is applied as the decision boundary. This is a common case,
    especially when solving a classification problem. The softmax function is used
    for the probability distribution over the possible classes in a multiclass classification
    problem. To conclude, choosing proper activation functions and network weight
    initializations are two problems that make a network perform at its best and help
    to obtain good training. Now that we know the brief history of neural networks,
    let's deepdive into different architectures in the next section, which will give
    us an idea of their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to now, numerous neural network architectures have been proposed and are
    in use. However, more or less all of them are based on a few core neural network
    architectures. We can categorize DL architectures into four groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emergent architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, DNNs, CNNs, and RNNs have many improved variants. Although most of
    the variants are proposed or developed for solving domain-specific research problems,
    the basic working principles still follow the original DNN, CNN, and RNN architectures.
    The following subsections will give you a brief introduction to these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DNNs are neural networks that have a complex and deeper architecture with a
    large number of neurons in each layer, and many connections between them. Although
    DNN refers to a very deep network, for simplicity, we consider MLP, **stacked
    autoencoder **(**SAE**), and **deep belief networks** (**DBNs**) as DNN architectures.
    These architectures mostly work as an FFNN, meaning information propagates from
    input to output layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple perceptrons are stacked together as MLPs, where layers are connected
    as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since
    it has three layers: an input layer, a hidden layer, and an output layer. This
    way, the signal propagates one way, from the input layer to the hidden layers
    to the output layer, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b984dc6-a80d-4037-adc9-dc61d4be1b5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Autoencoders and RBMs are the basic building blocks for SAEs and DBNs, respectively.
    Unlike MLP, which is an FFNN that''s trained in a supervised way, both SAEs and
    DBNs are trained in two phases: unsupervised pretraining and supervised fine-tuning.
    In unsupervised pretraining, layers are stacked in order and trained in a layer-wise
    manner with used unlabeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In supervised fine-tuning, an output classifier layer is stacked and the complete
    neural network is optimized by retraining with labeled data. One problem with
    MLP is that it often overfits the data, so it doesn''t generalize well. To overcome
    this issue, DBN was proposed by Hinton et al. It uses a greedy, layer-by-layer,
    pretraining algorithm. DBNs are composed of a visible layer and multiple hidden
    unit layers. The building blocks of a DBN are RBMs, as shown in the following
    diagram, where several RBMs are stacked one after another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a7b0c86-7732-461b-93b4-3512d208d1b8.png)'
  prefs: []
  type: TYPE_IMG
- en: The top two layers have undirected, symmetric connections in-between, but the
    lower layers have directed connections from the preceding layer. Despite numerous
    successes, DBNs are now being replaced with AEs.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AEs are also special types of neural networks that learn automatically from
    the input data. AEs consist of two components: the encoder and the decoder. The
    encoder compresses the input into a latent-space representation. Then, the decoder
    part tries to reconstruct the original input data from this representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: Encodes or compresses the input into a latent-space representation
    using a function known as *h = f(x)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: Decodes or reconstructs the input from the latent space representation
    using a function known as *r = g(h)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, an AE can be described by a function of* g(f(x)) = 0*, where we want 0
    as close to the original input of *x*. The following diagram shows how an AE typically
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcbf8bbf-53d7-4d16-9fdc-0bf5612e3e25.png)'
  prefs: []
  type: TYPE_IMG
- en: AEs are very useful for data denoising and dimensionality reduction for data
    visualization because they can learn data projections called representations more
    effectively than PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs have achieved much and have been widely adopted in computer vision (for
    example, image recognition). In CNN networks, the connection schemes are significantly
    different compared to an MLP or DBN. A few of the convolutional layers are connected
    in a cascade style. Each layer is backed up by an ReLU layer, a pooling layer,
    additional convolutional layers (+ReLU), and another pooling layer, which is followed
    by a fully connected layer and a softmax layer. The following diagram is a schematic
    of the architecture of a CNN that''s used for facial recognition, which takes
    facial images as input and predicts emotions such as anger, disgust, fear, happy,
    and sad:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a4a34db-3397-44cb-8825-78eb3053eae0.png)'
  prefs: []
  type: TYPE_IMG
- en: A schematic architecture of a CNN used for facial recognition
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, DNNs have no prior knowledge of how the pixels are organized because
    they do not know that nearby pixels are close. CNNs embed this prior knowledge
    using lower layers by using feature maps in small areas of the image, while the
    higher layers combine lower-level features into larger features.
  prefs: []
  type: TYPE_NORMAL
- en: This setting works well with most of the natural images, giving CNN a decisive
    head start over DNNs. The output from each convolutional layer is a set of objects,
    called feature maps, that are generated by a single kernel filter. Then, the feature
    maps can be used to define a new input to the next layer. Each neuron in a CNN
    network produces an output, followed by an activation threshold, which is proportional
    to the input and not bound.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In RNNs, connections between units form a directed cycle. The RNN architecture
    was originally conceived by Hochreiter and Schmidhuber in 1997\. RNN architectures
    have standard MLPs, plus added loops so that they can exploit the powerful nonlinear
    mapping capabilities of the MLP. They also have some form of memory. The following
    diagram shows a very basic RNN that has an input layer, two recurrent layers,
    and an output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77b89365-780e-4461-a58b-6e3c4e492336.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this basic RNN suffers from gradient vanishing and the exploding problem,
    and cannot model long-term dependencies. These architectures include LSTM, **gated
    recurrent units** (**GRUs**), bidirectional-LSTM, and other variants. Consequently,
    LSTM and GRU can overcome the drawbacks of regular RNNs: the gradient vanishing/exploding
    problem and long-short term dependency.'
  prefs: []
  type: TYPE_NORMAL
- en: Emergent architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many other emergent DL architectures have been suggested, such as **Deep SpatioTemporal
    Neural Networks** (**DST-NNs**), **Multi-Dimensional Recurrent Neural Networks**
    (**MD-RNNs**), and **Convolutional AutoEncoders** (**CAEs**). Nevertheless, there
    are a few more emerging networks, such as **CapsNets** (which is an improved version
    of a CNN, designed to remove the drawbacks of regular CNNs), RNN for image recognition,
    and **Generative Adversarial Networks** (**GANs**) for simple image generation.
    Apart from these, factorization machines for personalization and deep reinforcement
    learning are also being used widely.
  prefs: []
  type: TYPE_NORMAL
- en: Residual neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since there are sometimes millions and millions of hyperparameters and other
    practical aspects, it's really difficult to train deeper neural networks. To overcome
    this limitation, Kaiming H. et al. ( [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1))
    proposed a residual learning framework to ease the training of networks that are
    substantially deeper than those used previously.
  prefs: []
  type: TYPE_NORMAL
- en: They also explicitly reformulated the layers as learning residual functions
    with reference to the layer inputs, instead of learning non-referenced functions.
    This way, these residual networks are easier to optimize and can gain accuracy
    from considerably increased depth. The downside is that building a network by
    simply stacking residual blocks inevitably limits the optimization ability. To
    overcome this limitation, Ke Zhang et al. also proposed using a multilevel residual
    network ([https://arxiv.org/abs/1608.02908](https://arxiv.org/abs/1608.02908)).
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GANs are deep neural net architectures that consist of two networks pitted
    against each other (hence the name *adversarial*). Ian Goodfellow et al. introduced
    GANs in a paper (see more at [https://arxiv.org/abs/1406.2661v1](https://arxiv.org/abs/1406.2661v1)).
    In GANs, the two main components are the **generator and discriminator**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4920409b-74a3-482e-a9f8-638542cdeab9.png)'
  prefs: []
  type: TYPE_IMG
- en: Working principle of generative adversarial networks
  prefs: []
  type: TYPE_NORMAL
- en: 'In a GAN architecture, a generator and a discriminator are pitted against each
    other—hence the name, adversarial:'
  prefs: []
  type: TYPE_NORMAL
- en: The generator tries to generate data samples out of a specific probability distribution
    and is very similar to the actual object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discriminator will judge whether its input is coming from the original training
    set or from the generator part.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many DL practitioners think that GANs were one of the most important advancements
    because GANs can be used to mimic any distribution of data, and, based on the
    data distribution, they can be taught to create robot artist images, super-resolution
    images, text-to-image synthesis, music, speech, and more. For example, because
    of the concept of adversarial training, Facebook's AI research director, Yann
    LeCun, suggested that GANs are the most interesting idea in the last 10 years
    of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Capsule networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In CNNs, each layer understands an image at a much more granular level through
    a slow receptive field or max pooling operations. If the images have rotation,
    tilt, or very different shapes or orientation, CNNs fail to extract such spatial
    information and show very poor performance at image processing tasks. Even the
    pooling operations in CNNs cannot be much help against such positional invariance.
    This issue in CNNs has led us to the recent advancement of CapsNet through the
    paper entitled *Dynamic Routing Between Capsules* (see more at [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829))
    by Geoffrey Hinton et al:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A capsule is a group of neurons whose activity vector represents the instantiation
    parameters of a specific type of entity, such as an object or an object part."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike a regular DNN, where we keep on adding layers, in CapsNet, the idea
    is to add more layers inside a single layer. This way, a CapsNet is a nested set
    of neural layers. In CapsNet, the vector inputs and outputs of a capsule are computed
    using the routing algorithm used in physics, which iteratively transfers information
    and processes the **self-consistent field** (**SCF**) procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14df1eb2-37d6-4291-b0c9-453194b85145.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a schematic diagram of a simple three-layer CapsNet.
    The length of the activity vector of each capsule in the DigiCaps layer indicates
    the presence of an instance of each class, which is used to calculate the loss.
    Now that we have learned about the working principles of neural networks and the
    different neural network architectures, implementing something hands-on would
    be great. However, before that, let's take a look at some popular DL libraries
    and frameworks, which come with the implementation of these network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks for clustering analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several variants of k-means have been proposed to address issues with higher-dimensional
    input spaces. However, they are fundamentally limited to linear embedding. Hence,
    we cannot model non-linear relationships. Nevertheless, fine-tuning in these approaches
    is based on only cluster assignment hardening loss (see later in this section).
    Therefore, a fine-grained clustering accuracy cannot be achieved. Since the quality
    of the clustering results is dependent on the data distribution, deep architecture
    can help the model learn mapping from the data space to a lower-dimensional feature
    space in which it iteratively optimizes a clustering objective. Several approaches
    have been proposed over the last few years, trying to use the representational
    power of deep neural networks for preprocessing clustering inputs.
  prefs: []
  type: TYPE_NORMAL
- en: A few notable approaches include deep embedded clustering, deep clustering networks,
    discriminatively boosted clustering, clustering CNNs, deep embedding networks,
    convolutional deep embedded clustering, and joint unsupervised learning of deep
    representation for images. Other approaches include DL with non-parametric clustering,
    CNN-based joint clustering and representation learning with feature drift compensation,
    learning latent representations in neural networks for clustering, clustering
    using convolutional neural networks, and deep clustering with convolutional autoencoder
    embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these approaches follow more or less the same principle: that is, representation
    learning using a deep architecture to transform the inputs into a latent representation
    and using these representations as input for a specific clustering method. Such
    deep architectures include MLP, CNN, DBN, GAN, and variational autoencoders. The
    following diagram shows an example of how to improve the clustering performance
    of a DEC network using convolutional autoencoders and optimizing both reconstruction
    and CAH losses jointly. The latent space out of the encoder layer is fed to K-means
    for soft clustering assignment. Blurred genetic variants signify the existence
    of reconstruction errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d1aa2d3-1e6d-407c-9349-98e368b48591.png)'
  prefs: []
  type: TYPE_IMG
- en: 'DL based clustering (source: Karim et al., Recurrent Deep Embedding Networks
    for Genotype Clustering and Ethnicity Prediction, arXiv:1805.12218)'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, in these approaches, there are three important steps involved—extracting
    cluster-friendly deep features using deep architectures, combining clustering
    and non-clustering losses, and, finally, network updates to optimize clustering
    and non-clustering losses jointly.
  prefs: []
  type: TYPE_NORMAL
- en: DL frameworks and cloud platforms for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several popular DL frameworks. Each of them comes with some pros and
    cons. Some of them are desktop-based, and some of them are cloud-based platforms,
    where you can deploy/run your DL applications. However, most of the libraries
    that are released under an open license help when people are using graphics processors,
    which can ultimately help in speeding up the learning process. Such frameworks
    and libraries include TensorFlow, PyTorch, Keras, Deeplearning4j, H2O, and the
    **Microsoft Cognitive Toolkit** (**CNTK**). Even a few years back, other implementations,
    including Theano, Caffee, and Neon, were used widely. However, these are now obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deeplearning4j** (**DL4J**) is one of the first commercial-grade, open source,
    distributed DL libraries that was built for Java and Scala. This also provides
    integrated support for Hadoop and Spark. DL4J is built for use in business environments
    on distributed GPUs and CPUs. DL4J aims to be cutting-edge and *Plug and Play*,
    with more convention than configuration, which allows for fast prototyping for
    non-researchers. Its numerous libraries can be integrated with DL4J and will make
    your JVM experience easier, regardless of whether you are developing your ML application
    in Java or Scala. Similar to NumPy for JVM, ND4J comes up with basic operations
    of linear algebra (matrix creation, addition, and multiplication). However, ND4S
    is a scientific computing library for'
  prefs: []
  type: TYPE_NORMAL
- en: 'linear algebra and matrix manipulation. It also provides n-dimensional arrays
    for JVM-based languages. The following diagram shows last year''s Google Trends,
    illustrating how popular TensorFlow is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be613e47-6b26-477a-a6ca-d92333a53d42.png)'
  prefs: []
  type: TYPE_IMG
- en: As well as these frameworks, Chainer is a powerful, flexible, and intuitive
    DL framework, which supports CUDA computation. It only requires a few lines of
    code to leverage a GPU. It also runs on multiple GPUs with little effort. Most
    importantly, Chainer supports various network architectures, including feed-forward
    nets, convnets, recurrent nets, and recursive nets. It also supports per-batch
    architectures. One more interesting feature in Chainer is that it supports forward
    computation, by which any control flow statements of Python can be included without
    lacking the ability of backpropagation. It makes code intuitive and easy to debug.
  prefs: []
  type: TYPE_NORMAL
- en: The DL framework power scores 2018 also shows that TensorFlow, Keras, and PyTorch
    are far ahead of other frameworks (see [https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a](https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a)).
    Scores were calculated based on usage, popularity, and interest in DL frameworks
    through the following sources. Apart from the preceding libraries, there are some
    recent initiatives for DL in the cloud. The idea is to bring DL capability to
    big data with billions of data points and high-dimensional data. For example,
    **Amazon Web Services** (**AWS**), Microsoft Azure, Google Cloud Platform, and
    **NVIDIA GPU Cloud** (**NGC**) all offer machine and DL services that are native
    to their public clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In October 2017, AWS released **Deep Learning AMIs** (**DLAMIs**) for **Amazon
    Elastic Compute Cloud** (**Amazon EC2**) P3 instances. These AMIs come preinstalled
    with DL frameworks, such as TensorFlow, Gluon, and Apache MXNet, which are optimized
    for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances. The DL service
    currently offers three types of AMIs: Conda AMI, Base AMI, and AMI with source
    code.The CNTK is Azure''s open source DL service. Similar to the AWS offering,
    it focuses on tools that can help developers build and deploy DL applications.
    Azure also provides a model gallery that includes resources, such as code samples,
    to help enterprises get started with the service.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated
    containers (see [https://www. nvidia. com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)).
    The NGC features containerized DL frameworks, such as TensorFlow, PyTorch, MXNet,
    and more that are tuned, tested, and certified by NVIDIA to run on the latest
    NVIDIA GPUs on participating cloud-service providers. Nevertheless, there are
    also third-party services available through their respective marketplaces.
  prefs: []
  type: TYPE_NORMAL
- en: '**When it comes to cloud-based IoT system-development markets, currently it
    forks into three obvious routes:** off-the-shelf platforms (for example, AWS IoT
    Core, Azure IoT Suite, and Google Cloud IoT Core), which trade off vendor lock-in
    and higher-end volume pricing against cost-effective scalability and shorter lead
    times; reasonably well-established MQTT configurations over the Linux stack (example:
    Eclipse Mosquitto); and the more exotic emerging protocols and products (for example,
    Nabto''s P2P protocol) that are developing enough uptake, interest, and community
    investment to stake a claim for strong market presence in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: As a DL framework, Chainer Neural Network is a great choice for all devices
    powered by Intel Atom, NVIDIA Jetson TX2, and Raspberry Pi. Therefore, using Chainer,
    we don't need to build and configure the ML framework for our devices from scratch.
    It provides prebuilt packages for three popular ML frameworks, including TensorFlow,
    Apache MXNet, and Chainer. Chainer works in a similar fashion, which depends on
    a library on the Greengrass and a set of model files generated using Amazon SageMaker
    and/or stored directly in an Amazon S3 bucket. From Amazon SageMaker or Amazon
    S3, the ML models can be deployed to AWS Greengrass to be used as a local resource
    for ML inference. Conceptually, AWS IoT Core functions as the managing plane for
    deploying ML inference to the edge.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a number of fundamental DL themes. We started
    our journey with a basic, but comprehensive, introduction to ML. Then, we gradually
    moved on to DL and different neural architectures. We then had a brief overview
    of the most important DL frameworks that can be utilized to develop DL-based applications
    for IoT-enabled devices.
  prefs: []
  type: TYPE_NORMAL
- en: IoT applications, such as smart home, smart city, and smart healthcare, heavily
    rely on video or image data processing for decision making. In the next chapter,
    we will cover DL-based image processing for IoT applications, including image
    recognition, classification, and object detection. Additionally, we will cover
    hands-on video data processing in IoT applications.
  prefs: []
  type: TYPE_NORMAL
