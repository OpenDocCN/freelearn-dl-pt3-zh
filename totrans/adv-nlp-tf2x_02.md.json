["```\n!pip install tensorflow_datasets\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np \n```", "```\n\", \".join(tfds.list_builders()) \n```", "```\n'abstract_reasoning, aeslc, aflw2k3d, amazon_us_reviews, arc, bair_robot_pushing_small, beans, big_patent, bigearthnet, billsum, binarized_mnist, binary_alpha_digits, c4, caltech101, caltech_birds2010, caltech_birds2011, cars196, cassava, cats_vs_dogs, celeb_a, celeb_a_hq, cfq, chexpert, cifar10, cifar100, cifar10_1, cifar10_corrupted, citrus_leaves, cityscapes, civil_comments, clevr, cmaterdb, cnn_dailymail, coco, coil100, colorectal_histology, colorectal_histology_large, cos_e, curated_breast_imaging_ddsm, cycle_gan, deep_weeds, definite_pronoun_resolution, diabetic_retinopathy_detection, div2k, dmlab, downsampled_imagenet, dsprites, dtd, duke_ultrasound, dummy_dataset_shared_generator, dummy_mnist, emnist, eraser_multi_rc, esnli, eurosat, fashion_mnist, flic, flores, food101, gap, gigaword, glue, groove, higgs, horses_or_humans, i_naturalist2017, image_label_folder, imagenet2012, imagenet2012_corrupted, imagenet_resized, imagenette, imagewang, imdb_reviews, iris, kitti, kmnist, lfw, librispeech, librispeech_lm, libritts, lm1b, lost_and_found, lsun, malaria, math_dataset, mnist, mnist_corrupted, movie_rationales, moving_mnist, multi_news, multi_nli, multi_nli_mismatch, natural_questions, newsroom, nsynth, omniglot, open_images_v4, opinosis, oxford_flowers102, oxford_iiit_pet, para_crawl, patch_camelyon, pet_finder, places365_small, plant_leaves, plant_village, plantae_k, qa4mre, quickdraw_bitmap, reddit_tifu, resisc45, rock_paper_scissors, rock_you, scan, scene_parse150, scicite, scientific_papers, shapes3d, smallnorb, snli, so2sat, speech_commands, squad, stanford_dogs, stanford_online_products, starcraft_video, sun397, super_glue, svhn_cropped, ted_hrlr_translate, ted_multi_translate, tf_flowers, the300w_lp, tiny_shakespeare, titanic, trivia_qa, uc_merced, ucf101, vgg_face2, visual_domain_decathlon, voc, wider_face, wikihow, wikipedia, wmt14_translate, wmt15_translate, wmt16_translate, wmt17_translate, wmt18_translate, wmt19_translate, wmt_t2t_translate, wmt_translate, xnli, xsum, yelp_polarity_reviews' \n```", "```\nimdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", \n                               with_info=True, as_supervised=True)\nimdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", \n                      as_supervised=True) \n```", "```\nprint(ds_info) \n```", "```\ntfds.core.DatasetInfo(\n    name='imdb_reviews',\n    version=1.0.0,\n    description='Large Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n    features=FeaturesDict({\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n        'text': Text(shape=(), dtype=tf.string),\n    }),\n    total_num_examples=100000,\n    splits={\n        'test': 25000,\n        'train': 25000,\n        'unsupervised': 50000,\n    },\n    supervised_keys=('text', 'label'),\n    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n      title     = {Learning Word Vectors for Sentiment Analysis},\n      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n      month     = {June},\n      year      = {2011},\n      address   = {Portland, Oregon, USA},\n      publisher = {Association for Computational Linguistics},\n      pages     = {142--150},\n      url       = {http://www.aclweb.org/anthology/P11-1015}\n    }\"\"\",\n    redistribution_info=,\n) \n```", "```\nfor example, label in imdb_train.take(1):\n    print(example, '\\n', label) \n```", "```\ntf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\ntf.Tensor(0, shape=(), dtype=int64) \n```", "```\ntokenizer = tfds.features.text.Tokenizer()\nvocabulary_set = set()\nMAX_TOKENS = 0\nfor example, label in imdb_train:\n  some_tokens = tokenizer.tokenize(example.numpy())\n  if MAX_TOKENS < len(some_tokens):\n        MAX_TOKENS = len(some_tokens)\n  vocabulary_set.update(some_tokens) \n```", "```\nimdb_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set, \n                                              tokenizer=tokenizer)\nvocab_size = imdb_encoder.vocab_size\nprint(vocab_size, MAX_TOKENS) \n```", "```\n93931 2525 \n```", "```\nfor example, label in imdb_train.take(1):\n    print(example)\n    encoded = imdb_encoder.encode(example.numpy())\n    print(imdb_encoder.decode(encoded)) \n```", "```\ntf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\nThis was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it \n```", "```\nimdb_encoder.save_to_file(\"reviews_vocab\") \n```", "```\nenc = tfds.features.text.TokenTextEncoder.load_from_file(\"reviews_vocab\")\nenc.decode(enc.encode(\"Good case. Excellent value.\")) \n```", "```\n'Good case Excellent value' \n```", "```\nfrom tensorflow.keras.preprocessing import sequence\ndef encode_pad_transform(sample):\n    encoded = imdb_encoder.encode(sample.numpy())\n    pad = sequence.pad_sequences([encoded], padding='post', \n                                 maxlen=150)\n    return np.array(pad[0], dtype=np.int64)  \ndef encode_tf_fn(sample, label):\n    encoded = tf.py_function(encode_pad_transform, \n                                       inp=[sample], \n                                       Tout=(tf.int64))\n    encoded.set_shape([None])\n    label.set_shape([])\n    return encoded, label \n```", "```\nsubset = imdb_train.take(10)\ntst = subset.map(encode_tf_fn)\nfor review, label in tst.take(1):\n    print(review, label)\n    print(imdb_encoder.decode(review)) \n```", "```\ntf.Tensor(\n[40205  9679 51728 91747 21013  7623  6550 40338 18966 36012 64846 80722\n 81643 29176 14002 73549 52960 40359 49248 62585 75017 67425 18181  2673\n 44509 18966 87701 56336 29928 64846 41917 49779 87701 62585 58974 82970\n  1902  2754 18181  7623  2615  7927 67321 40205  7623 43621 51728 91375\n 41135 71762 29392 58948 76770 15030 74878 86231 49390 69836 18353 84093\n 76562 47559 49390 48352 87701 62200 13462 80285 76037 75121  1766 59655\n  6569 13077 40768 86201 28257 76220 87157 29176  9679 65053 67425 93397\n 74878 67053 61304 64846 93397  7623 18560  9679 50741 44024 79648  7470\n 28203 13192 47453  6386 18560 79892 49248  7158 91321 18181 88633 13929\n  2615 91321 81643 29176  2615 65285 63778 13192 82970 28143 14618 44449\n 39028     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\nThis was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it \n```", "```\nencoded_train = imdb_train.map(encode_tf_fn)\nencoded_test = imdb_test.map(encode_tf_fn) \n```", "```\nencoded_train = imdb_train.map(encode_tf_fn,\n       num_parallel_calls=tf.data.experimental.AUTOTUNE)\nencoded_test = imdb_test.map(encode_tf_fn,\n       num_parallel_calls=tf.data.experimental.AUTOTUNE) \n```", "```\ntf.keras.layers.LSTM(rnn_units) \n```", "```\ndef build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n                              mask_zero=True,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n  ])\n  return model \n```", "```\nvocab_size = imdb_encoder.vocab_size \n# The embedding dimension\nembedding_dim = 64\n# Number of RNN units\nrnn_units = 64\n# batch size\nBATCH_SIZE=100 \n```", "```\nmodel = build_model_lstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)\nmodel.summary() \n```", "```\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (100, None, 64)           6011584   \n_________________________________________________________________\nlstm_3 (LSTM)                (100, 64)                 33024     \n_________________________________________________________________\ndense_5 (Dense)              (100, 1)                  65        \n=================================================================\nTotal params: 6,044,673\nTrainable params: 6,044,673\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nmodel.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy', 'Precision', 'Recall'])\nencoded_train_batched = encoded_train.batch(BATCH_SIZE)\nmodel.fit(encoded_train_batched, epochs=10) \n```", "```\nEpoch 1/10\n250/250 [==============================] - 23s 93ms/step - loss: 0.4311 - accuracy: 0.7920 - Precision: 0.7677 - Recall: 0.8376\nEpoch 2/10\n250/250 [==============================] - 21s 83ms/step - loss: 0.1768 - accuracy: 0.9353 - Precision: 0.9355 - Recall: 0.9351\n…\nEpoch 10/10\n250/250 [==============================] - 21s 85ms/step - loss: 0.0066 - accuracy: 0.9986 - Precision: 0.9986 - Recall: 0.9985 \n```", "```\nmodel.evaluate(encoded_test.batch(BATCH_SIZE)) \n```", "```\n 250/Unknown - 20s 80ms/step - loss: 0.8682 - accuracy: 0.8063 - Precision: 0.7488 - Recall: 0.9219 \n```", "```\ndef build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n                              mask_zero=True,\n                              batch_input_shape=[batch_size, None]),\n    **tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units)),**\n    tf.keras.layers.Dense(1, activation='sigmoid')\n  ])\n  return model \n```", "```\nbilstm = build_model_bilstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)\nbilstm.summary() \n```", "```\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (50, None, 128)           12023168  \n_________________________________________________________________\ndropout (Dropout)            (50, None, 128)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (50, None, 128)           98816     \n_________________________________________________________________\ndropout_1 (Dropout)          (50, None, 128)           0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (50, 128)                 98816     \n_________________________________________________________________\ndropout_2 (Dropout)          (50, 128)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (50, 1)                   129       \n=================================================================\nTotal params: 12,220,929\nTrainable params: 12,220,929\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nbilstm.fit(encoded_train_batched, epochs=5) \n```", "```\nEpoch 1/5\n500/500 [==============================] - 80s 160ms/step - loss: 0.3731 - accuracy: 0.8270 - Precision: 0.8186 - Recall: 0.8401\n…\nEpoch 5/5\n500/500 [==============================] - 70s 139ms/step - loss: 0.0316 - accuracy: 0.9888 - Precision: 0.9886 - Recall: 0.9889\nbilstm.evaluate(encoded_test.batch(BATCH_SIZE))\n500/Unknown - 20s 40ms/step - loss: 0.7280 - accuracy: 0.8389 - Precision: 0.8650 - Recall: 0.8032 \n```"]