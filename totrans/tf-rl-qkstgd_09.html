<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assessment</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 1</h1>
                </header>
            
            <article>
                
<ol>
<li>A replay buffer is required for off-policy RL algorithms. We sample from the replay buffer a mini-batch of experiences and use it to train the <em>Q(s,a)</em> state-value function in DQN and the actor's policy in a DDPG.</li>
<li>We discount rewards, as there is more uncertainty about the long-term performance of the agent. So, immediate rewards have a higher weight, a reward earned in the next time step has a relatively lower weight, a reward earned in the subsequent time step has an even lower weight, and so on.</li>
<li>The training of the agent will not be stable if <em>γ &gt; 1</em>. The agent will fail to learn an optimal policy.</li>
<li>A model-based RL agent has the potential to perform well, but there is no guarantee that it will perform better than a model-free RL agent, as the model of the environment we are constructing need not always be a good one. It is also very hard to build an accurate enough model of the environment.</li>
<li>In deep RL, deep neural networks are used for the <em>Q(s,a)</em> and the actor's policy (the latter is true in an Actor-Critic setting). In the traditional RL algorithms, a tabular <em>Q(s, a)</em> is used but is not possible when the number of states is very large, as is usually the case in most problems.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 3</h1>
                </header>
            
            <article>
                
<ol>
<li>A replay buffer is used in DQN in order to store past experiences, sample a mini-batch of data from it, and use it to train the agent.</li>
<li>Target networks help in the stability of the training. This is achieved by keeping an additional neural network whose weights are updated using an exponential moving average of the weights of the main neural network. Alternatively, another approach that is also widely used is to copy the weights of the main neural network to the target network once every few thousand steps or so.</li>
<li>One frame as the state will not help in the Atari Breakout problem. This is because no temporal information is deductible from one frame only. For instance, in one frame alone, the direction of motion of the ball cannot be obtained. If, however, we stack up multiple frames, the velocity and acceleration of the ball can be ascertained.</li>
<li>L2 loss is known to overfit to outliers. Hence, the Huber loss is preferred, as it combines both L2 and L1 losses. See Wikipedia: <a href="https://en.wikipedia.org/wiki/Huber_loss">https://en.wikipedia.org/wiki/Huber_loss</a>.</li>
<li>RGB images can also be used. However, we will need extra weights for the first hidden layer of the neural network, as we now have three channels in each of the four frames in the state stack. This much finer detail for the state space is not required for Atari. However, RGB images can help in other applications, for example, in autonomous driving and/or robotics.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 4</h1>
                </header>
            
            <article>
                
<ol>
<li>DQN is known to overestimate the state-action value function, <em>Q(s,a)</em>. To overcome this, DDQN was introduced. DDQN has fewer problems than DQN regarding the overestimation of <em>Q(s,a)</em>.</li>
<li>Dueling network architecture has separate streams for the advantage function and the state-value function. These are then combined to obtain <em>Q(s,a)</em>. This branching out and then combining is observed to result in a more stable training of the RL agent.</li>
</ol>
<ol start="3">
<li><strong>Prioritized Experience Replay</strong> (<strong>PER</strong>) gives more importance to experience samples where the agent performs poorly, and so these samples are sampled more frequently than other samples where the agent performed well. By frequently using samples where the agent performed poorly, the agent is able to work on its weakness more often, and so PER speeds up the training.</li>
<li>In some computer games, such as Atari Breakout, the simulator has too many frames per second. If a separate action is sampled from the policy in each of these time steps, the state of the agent may not change enough in one time step, as it is too small. Hence, sticky actions are used where the same action is repeated over a finite but fixed number of time steps, say <em>n</em>, and the total reward accrued over these n time steps is used as the reward for the action performed. In these n time steps, the state of the agent has changed sufficiently enough to be able to evaluate the efficacy of the action taken. Too small a value for n can prevent the agent from learning a good policy; likewise, too large a value can also be a problem. You must choose the right number of time steps over which the same action is taken, and this depends on the simulator used.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 5</h1>
                </header>
            
            <article>
                
<ol>
<li>DDPG is an off-policy algorithm, as it uses a replay buffer.</li>
<li>In general, the same number of hidden layers and the number of neurons per hidden layer is used for the actor and the critic, but this is not required. Note that the output layer will be different for the actor and the critic, with the actor having the number of outputs equal to the number of actions; the critic will have only one output.</li>
<li>DDPG is used for continuous control, that is, when the actions are continuous and real-valued. Atari Breakout has discrete actions, and so DDPG is not suitable for Atari Breakout.</li>
<li>We use the <kbd>relu</kbd> activation function, and so the biases are initialized to small positive values so that they fire at the beginning of the training and allow gradients to back-propagate.</li>
<li>This is an exercise. See <a href="https://gym.openai.com/envs/InvertedDoublePendulum-v2/">https://gym.openai.com/envs/InvertedDoublePendulum-v2/</a>.</li>
<li>This is also an exercise. Notice what happens to the learning when the number of neurons is decreased in the first layer sequentially. In general, information bottlenecks are observed not only in an RL setting, but for any DL problem.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 6</h1>
                </header>
            
            <article>
                
<ol>
<li><span><strong>Asynchronous Advantage Actor-Critic Agents</strong> (</span><strong>A3C</strong>) is an on-policy algorithm, as we do not use a replay buffer to sample data from. However, a temporary buffer is used to collect immediate samples, which are used to train once, after which the buffer is emptied.</li>
<li>The Shannon entropy term is used as a regularizer—the higher the entropy, the better the policy is.</li>
<li>When too many worker threads are used, the training can slow down and can crash, as memory is limited. If, however, you have access to a large cluster of processors, then using a large number of worker threads/processes helps.</li>
<li>Softmax is used in the policy network to obtain probabilities of different actions.</li>
<li>An advantage function is widely used, as it decreases the variance of the policy gradient. <em>Section 3</em> of the A3C paper (<a href="https://arxiv.org/pdf/1602.01783.pdf">https://arxiv.org/pdf/1602.01783.pdf</a>) has more regarding this.</li>
<li>This is an exercise. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 7</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>Trust Region Policy Optimization</strong> (<strong>TRPO</strong>) has an objective function and a constraint. It hence requires a second order optimization such as a conjugate gradient. SGD and Adam are not applicable in TRPO.</li>
<li>The entropy term helps in regularization. It allows the agent to explore more.</li>
<li>We clip the policy ratio to limit the amount by which one update step will change the policy. If this clipping parameter epsilon is large, the policy can change drastically in each update, which can result in a sub-optimal policy, as the agent's policy is noisier and has too many fluctuations.</li>
<li>The action is bounded between a negative and a positive value, and so the <kbd>tanh</kbd> activation function is used for <kbd>mu</kbd>. For sigma, the <kbd>softplus</kbd> is used as sigma and is always positive. The <kbd>tanh</kbd> function cannot be used for sigma, as <kbd>tanh</kbd> can result in negative values for sigma, which is meaningless!</li>
<li>Reward shaping generally helps with the training. But if it is done poorly, it will not help with the training. You must ensure that the reward shaping is done to keep the <kbd>reward</kbd> function dense as well as in appropriate ranges.</li>
<li>No, reward shaping is used only in the training.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 8</h1>
                </header>
            
            <article>
                
<ol>
<li>TORCS is a continuous control problem. DQN works only for discrete actions, and so it cannot be used in TORCS.</li>
<li>The initialization is another initialization strategy; you can also use a random uniform initialization with the <kbd>min</kbd> and <kbd>max</kbd> values of the range specified; another approach is to sample from a Gaussian with a zero mean and a specified sigma value. The interested reader must try these different initializers and compare the agent's performance.</li>
<li>The <kbd>abs()</kbd> function is used in the <kbd>reward</kbd> function, as we penalize lateral drift from the center equally on either side (left or right). The first term is the longitudinal speed, and so no <kbd>abs()</kbd> function is required.</li>
<li>The Gaussian noise added to the actions for exploration can be tapered down with episode count, and this can result in smoother driving. Surely, there are many other tricks you can do!</li>
<li>DDPG is off-policy, but <span><strong>Proximal Policy Optimization</strong> (</span><strong>PPO</strong>) is the on-policy RL algorithm. Hence, DDPG requires a replay buffer to store past-experience samples, but PPO does not require a reply buffer.</li>
</ol>


            </article>

            
        </section>
    </body></html>