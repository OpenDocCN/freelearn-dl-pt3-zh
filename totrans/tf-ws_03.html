<html><head></head><body>
		<div>
			<div id="_idContainer128" class="Content">
			</div>
		</div>
		<div id="_idContainer129" class="Content">
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>3. TensorFlow Development</h1>
		</div>
		<div id="_idContainer148" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">TensorFlow provides many resources for creating efficient workflows when developing data science and machine learning applications. In this chapter, you will learn how to use TensorBoard to visualize TensorFlow graphs and operations, TensorFlow Hub to access a community of users (a great source of pre-trained models), and Google Colab, which is a collaborative environment for developing code with others. You will use these tools to accelerate development by maximizing computational resources, transferring knowledge from pre-trained models, and visualizing all aspects of the model-building process.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Introduction</h1>
			<p>In the previous chapter, you learned how to load and process a variety of data types so that they can be used in TensorFlow modeling. This included tabular data from CSV files, image data, text data, and audio files. By the end of the chapter, you were able to process all these data types and produce numerical tensors from them that can be input for model training.</p>
			<p>In this chapter, you will learn about TensorFlow resources that will aid you in your model building and help you create performant machine learning algorithms. You will explore the practical resources that practitioners can utilize to aid their development workflow, including TensorBoard, TensorFlow Hub, and Google Colab. TensorBoard is an interactive platform that offers a visual representation of the computational graphs and data produced during the TensorFlow development process. The platform solves the problem of visualizing various data types that is common in machine learning. The visualization toolkit can plot model evaluation metrics during the model-building process, display images, play audio data, and perform many more tasks that would otherwise require writing custom functions. TensorBoard provides simple functions for writing logs, which are subsequently visualized in a browser window.</p>
			<p>TensorFlow Hub is an open source library of pre-trained machine learning models with a code base that's available for all to use and modify for their own applications. Models can be imported directly into code through dedicated libraries and can be viewed at <a href="https://tfhub.dev/">https://tfhub.dev/</a>. TensorFlow Hub allows users to use state-of-the-art models created by experts in the field and can result in massively reduced training times for models that incorporate pre-trained models as part of a user's model.</p>
			<p>For example, the platform contains the ResNet-50 model, a 50-layer <strong class="bold">Artificial Neural Network</strong> (<strong class="bold">ANN</strong>) that achieved first place on the ILSVRC 2015 classification task, a competition to classify images into 1,000 distinct classes. The network has over 23 million trainable parameters and was trained on more than 14 million images. Training this model from scratch on an off-the-shelf laptop to achieve something close to the accuracy of the pre-trained model on TensorFlow Hub would take days. It is for this reason that the ability to utilize TensorFlow Hub models can accelerate development.</p>
			<p>The final resource you will learn about in this chapter is Google Colab, which is an online development environment for executing Python code and creating machine learning algorithms on Google servers. The environment even has access to hardware that contains <strong class="bold">Graphics Processing Units</strong> (<strong class="bold">GPUs</strong>) and <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>) that can speed up model training free of charge. Google Colab is available at <a href="https://colab.research.google.com/">https://colab.research.google.com/</a>. </p>
			<p>Google Colab resolves the issue of setting up a development environment for creating machine learning models that can be shared with others. For example, multiple machine learning practitioners can develop the same model and train the model on one hardware instance, as opposed to having to run the instance with their own resources. As the name suggests, the platform fosters collaboration among machine learning practitioners.</p>
			<p>Now, let's explore TensorBoard, a resource that helps practitioners understand and debug their machine learning workflow.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>TensorBoard</h1>
			<p>TensorBoard is a visualization toolkit used to aid in machine learning experimentation. The platform has dashboard functionality for visualizing many of the common data types that a data science or machine learning practitioner may need at once, such as scalar values, image batches, and audio files. While such visualizations can be created with other plotting libraries, such as <strong class="source-inline">matplotlib</strong> or <strong class="source-inline">ggplot</strong>, TensorBoard combines many visualizations in an easy-to-use environment. Moreover, all that is required to create the visualizations is to log the trace during the building, fitting, and evaluating steps. TensorBoard helps in the following tasks:</p>
			<ul>
				<li>Visualizing the model graph to view and understand the model's architecture:<div id="_idContainer130" class="IMG---Figure"><img src="image/B16341_03_01.jpg" alt="Figure 3.1: A visual representation of model graphs and functions in TensorBoard&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.1: A visual representation of model graphs and functions in TensorBoard</p>
			<ul>
				<li>Viewing histograms and distributions of variables and tracking how they change over time.</li>
				<li>Displaying images, text, and audio data. For example, the following figure displays images from the Fashion MNIST dataset (<a href="https://www.tensorflow.org/datasets/catalog/fashion_mnist">https://www.tensorflow.org/datasets/catalog/fashion_mnist</a>):<div id="_idContainer131" class="IMG---Figure"><img src="image/B16341_03_02.jpg" alt="Figure 3.2: Viewing images in TensorBoard&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.2: Viewing images in TensorBoard</p>
			<ul>
				<li>Plotting graphs of model evaluation metrics as a function of epoch during model training:<div id="_idContainer132" class="IMG---Figure"><img src="image/B16341_03_03.jpg" alt="Figure 3.3: Plotting model evaluation metrics in TensorBoard&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.3: Plotting model evaluation metrics in TensorBoard</p>
			<ul>
				<li>Dimensionality reduction for visualizing embedding vectors:<div id="_idContainer133" class="IMG---Figure"><img src="image/B16341_03_04.jpg" alt="Figure 3.4: Visualizing embedding vectors in TensorBoard&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.4: Visualizing embedding vectors in TensorBoard</p>
			<p>TensorBoard creates visualizations from logs that are written during the development process. In order to create the logs to visualize the graph, a file writer object needs to be initialized within your development code, providing the location for the logs as an argument. The file writer is typically created at the beginning of a Jupyter notebook or equivalent development environment before any logs are written. This can be done as follows:</p>
			<p class="source-code">logdir = 'logs/'</p>
			<p class="source-code">writer = tf.summary.create_file_writer(logdir)</p>
			<p>In the preceding code, the directory for writing the logs is set, and if this directory does not already exist a new one will be created automatically in the working directory after you run the preceding code. The file writer object writes a file to the log directory when the logs are exported. To begin tracing, the following code must be executed:</p>
			<p class="source-code">tf.summary.trace_on(graph=True, profiler=True)</p>
			<p>The preceding command turns on the trace that records the computation graph that occurs from the time the command is executed. Without turning on the trace, nothing is logged, and so, nothing can be visualized in TensorBoard. Once the tracing of the computational graph is complete, the logs can be written to the log directory using the file writer object, as follows:</p>
			<p class="source-code">with writer.as_default():</p>
			<p class="source-code">    tf.summary.trace_export(name="my_func_trace",\</p>
			<p class="source-code">                            step=0, profiler_outdir=logdir)</p>
			<p>When writing the logs, you will need to employ the following parameters:</p>
			<ul>
				<li><strong class="source-inline">name</strong>: This parameter describes the name of the summary.</li>
				<li><strong class="source-inline">step</strong>: This parameter describes the monotonic step value for the summary and can be set to <strong class="source-inline">0</strong> if the object does not change over time.</li>
				<li><strong class="source-inline">profiler_outdir</strong>: This parameter describes the location to write the logs and is required if not provided when the file writer object is defined.</li>
			</ul>
			<p>After logs have been written to a directory, TensorBoard can be launched through the command line using the following command, thereby passing in the directory for the logs as the <strong class="source-inline">logdir</strong> parameter:</p>
			<p class="source-code">tensorboard --logdir=./logs</p>
			<p>Some versions of Jupyter Notebooks allow TensorBoard to be run directly within the notebook. However, library dependencies and conflicts can often prevent TensorBoard from running in notebook environments, in which case you can launch TensorBoard in a separate process from the command line. In this book, you will be using TensorFlow version 2.6 and TensorBoard version 2.1, and you will always use the command line to launch TensorBoard.</p>
			<p>In the first exercise, you will learn how to use TensorBoard to visualize a graph process. You will create a function to perform tensor multiplication and then visualize the computational graph in TensorBoard.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Exercise 3.01: Using TensorBoard to Visualize Matrix Multiplication </h2>
			<p>In this exercise, you will perform matrix multiplication of <strong class="source-inline">7x7</strong> matrices with random values and trace the computation graph and profiling information. Following that, you will view the computation graph using TensorBoard. This exercise will be performed in a Jupyter notebook. Launching TensorBoard will require running a command on the command line, as shown in the final step.</p>
			<p>Follow these steps:</p>
			<ol>
				<li>Open a new Jupyter notebook and import the TensorFlow library, and then set a seed for reproducibility. Since you are generating random values, setting a seed will ensure that the values generated are the same if the seed set is the same each time the code is run:<p class="source-code">import tensorflow as tf</p><p class="source-code">tf.random.set_seed(42)</p></li>
				<li>Create a <strong class="source-inline">file_writer</strong> object and set the directory for which the logs will be stored:<p class="source-code">logdir = 'logs/'</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Create a TensorFlow function to multiply two matrices together:<p class="source-code">@tf.function</p><p class="source-code">def my_matmult_func(x, y):</p><p class="source-code">    result = tf.matmul(x, y)</p><p class="source-code">    return result</p></li>
				<li>Create sample data in the form of two tensors with the shape <strong class="source-inline">7x7</strong> with random variables:<p class="source-code">x = tf.random.uniform((7, 7))</p><p class="source-code">y = tf.random.uniform((7, 7))</p></li>
				<li>Turn on graph tracing using TensorFlow's <strong class="source-inline">summary</strong> class:<p class="source-code">tf.summary.trace_on(graph=True, profiler=True)</p></li>
				<li>Apply the function that was created in <em class="italic">step 3</em> to the sample tensors that were created in <em class="italic">step 4</em>. Next, export the trace to the <strong class="source-inline">log</strong> directory, set the <strong class="source-inline">name</strong> argument for the graph for reference, and the <strong class="source-inline">log</strong> directory for the <strong class="source-inline">profiler_outdir</strong> argument. The <strong class="source-inline">step</strong> argument indicates the monotonic step value for the summary; the value should be nonzero if the values being traced vary, in which case they can be visualized with a step size dictated by this argument. For static objects, such as your graph trace here, it should be set to zero:<p class="source-code">z = my_matmult_func(x, y)</p><p class="source-code">with writer.as_default():</p><p class="source-code">    tf.summary.trace_export(name="my_func_trace",\</p><p class="source-code">                            step=0,\</p><p class="source-code">                            profiler_outdir=logdir)</p></li>
				<li>Finally, launch TensorBoard in the current working directory using the command line to view a visual representation of the graph. TensorBoard can be viewed in a web browser by visiting the URL that is provided after launching TensorBoard:<p class="source-code">tensorboard --logdir=./logs</p><p>For those running Windows, in the Anaconda prompt, run the following:</p><p class="source-code">tensorboard --logdir=logs</p><p>By running the preceding code, you will be able to visualize the following model graph:</p><div id="_idContainer134" class="IMG---Figure"><img src="image/B16341_03_05.jpg" alt="Figure 3.5: A visual representation of matrix multiplication in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.5: A visual representation of matrix multiplication in TensorBoard</p>
			<p>In TensorBoard, you can view the process of a tensor multiplying the two matrices to produce another matrix. By selecting the various elements, you can view information about each individual object in the computational graph, depending on the type of object. Here, you have created two tensors, named <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong>, represented by the nodes at the bottom. By selecting one of the nodes, you can view attributes about the tensor, including its data type (<strong class="source-inline">float</strong>), its user-specified name (<strong class="source-inline">x</strong> or <strong class="source-inline">y</strong>), and the name of the output node (<strong class="source-inline">MatMul</strong>). These nodes representing the input tensors are then input into another node representing the tensor multiplication process labeled <strong class="source-inline">MatMul</strong> after the TensorFlow function. Selecting this node reveals attributes of the function, including the input arguments, the input nodes (<strong class="source-inline">x</strong> and <strong class="source-inline">y</strong>), and the output node (<strong class="source-inline">Identity</strong>). The final two nodes, labeled <strong class="source-inline">Identity</strong> and <strong class="source-inline">identity_RetVal</strong>, represent the creation of the output tensor.</p>
			<p>In this exercise, you used TensorBoard to visualize a computational graph. You created a simple function to multiply two tensors together and you recorded the process by tracing the graph and logging the results. After logging the graph, you were able to visualize it by launching TensorBoard and directing the tool to the location of the logs.</p>
			<p>In the first activity, you will practice using TensorBoard to visualize a more complicated tensor transformation. In fact, any tensor process and transformation can be visualized in TensorBoard and the process demonstrated in the previous exercise is a good guide for creating and writing logs.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Activity 3.01: Using TensorBoard to Visualize Tensor Transformations</h2>
			<p>You are given two tensors of shape <strong class="source-inline">5x5x5</strong>. You are required to create TensorFlow functions to perform a tensor transformation and view a visual representation of the transformation.</p>
			<p>The steps you will take are as follows:</p>
			<ol>
				<li value="1">Import the TensorFlow library and set the seed to <strong class="source-inline">42</strong>.</li>
				<li>Set a log directory and initialize a file writer object to write the trace.</li>
				<li>Create a TensorFlow function to multiply two tensors, add a value of <strong class="source-inline">1</strong> to all elements in the resulting tensor using the <strong class="source-inline">ones_like</strong> function to create a tensor of the same shape as the result of the matrix multiplication. Then, apply a <strong class="source-inline">sigmoid</strong> function to each value of the tensor.</li>
				<li>Create two tensors with the shape <strong class="source-inline">5x5x5</strong>.</li>
				<li>Turn on graph tracing.</li>
				<li>Apply the function to the two tensors and export the trace to the log directory.</li>
				<li>Launch TensorBoard in the command line and view the graph in a web browser:<div id="_idContainer135" class="IMG---Figure"><img src="image/B16341_03_06.jpg" alt="Figure 3.6: A visual representation of tensor transformation in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.6: A visual representation of tensor transformation in TensorBoard</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor259">this link</a>.</p>
			<p>However, TensorBoard is not only for visualizing computational graphs. Images, scalar variables, histograms, and distributions can all be viewed in TensorBoard by writing them to the log directory using the appropriate TensorFlow <strong class="source-inline">summary</strong> method. For example, images can be written to the logs as follows:</p>
			<p class="source-code">with file_writer.as_default():</p>
			<p class="source-code">    tf.summary.image("Training data", training_images, step=0)</p>
			<p>The output of this will be a file added to the log directory named <strong class="source-inline">Training data</strong> that contains the images written by the file writer. Images can be viewed in TensorBoard by selecting the tab labeled <strong class="source-inline">IMAGES</strong>.</p>
			<p>In the same manner, scalar variables can be written to the logs for viewing in TensorBoard as follows:</p>
			<p class="source-code">with file_writer.as_default():</p>
			<p class="source-code">    tf.summary.scalar('scalar variable', variable, step=0)</p>
			<p>Audio files can be written to the logs for playback in TensorBoard in the following way:</p>
			<p class="source-code">with file_writer.as_default():</p>
			<p class="source-code">    tf.summary.audio('audio file', data, sample_rate=44100, \</p>
			<p class="source-code">                     step=0)</p>
			<p>A histogram can be logged by passing in data as follows:</p>
			<p class="source-code">with file_writer.as_default():</p>
			<p class="source-code">    tf.summary.histogram('histogram', data, step=0)</p>
			<p>In each of these examples of writing data to the logs, the <strong class="source-inline">step</strong> argument is set to zero since this is a required argument and must not be null. Setting the argument to zero indicates that the value is static and does not change with time. Each data type will be visible in a different tab in TensorBoard.</p>
			<p>In the next exercise, you will write images to TensorBoard so that they can be viewed directly within the platform. With TensorBoard, this becomes a facile process that otherwise would require writing custom code to view images. You may want to visualize images of batches to verify the labels, check the augmentation process, or validate the images in general.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Exercise 3.02: Using TensorBoard to Visualize Image Batches</h2>
			<p>In this exercise, you will use TensorBoard to view image batches. You will create a file writer and a data generator for the images, and then write one batch of images to the log files. Finally, you will view the images in TensorBoard.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the images in the <strong class="source-inline">image_data</strong> folder here: <a href="https://packt.link/1ue46">https://packt.link/1ue46</a>.</p>
			<p>Follow these steps:</p>
			<ol>
				<li value="1">Import the TensorFlow library and the <strong class="source-inline">ImageDataGenerator</strong> class:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.preprocessing.image import \</p><p class="source-code">    ImageDataGenerator</p></li>
				<li>Create a <strong class="source-inline">file_writer</strong> object and set the directory to which the logs will be stored:<p class="source-code">logdir = 'logs/'</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Initialize an <strong class="source-inline">ImageDataGenerator</strong> object:<p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255)</p></li>
				<li>Use the data generator's <strong class="source-inline">flow_from_directory</strong> method to create a batch image loader:<p class="source-code">batch_size = 25</p><p class="source-code">training_set = train_datagen.flow_from_directory\</p><p class="source-code">               ('image_data',\ </p><p class="source-code">                target_size = (224, 224),\ </p><p class="source-code">                batch_size = batch_size,\ </p><p class="source-code">                class_mode = 'binary') </p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the location of the directory on your system. If you're running the Jupyter notebook from the same directory where the dataset is stored, you can run the preceding code without any modification.</p></li>
				<li>Take the images from the first batch and write them to the logs using the file writer:<p class="source-code">with file_writer.as_default():</p><p class="source-code">    tf.summary.image("Training data", \</p><p class="source-code">                     next(training_set)[0], \</p><p class="source-code">                     max_outputs=batch_size, \</p><p class="source-code">                     step=0)</p></li>
				<li>Launch TensorBoard in the command line to view a visual representation of the graph. TensorBoard can be viewed in a web browser by visiting the URL that is provided after launching TensorBoard. The default URL provided is <strong class="source-inline">http://localhost:6006/</strong>:<p class="source-code">tensorboard --logdir=./logs</p><p>For those running Windows, in the Anaconda prompt, run the following:</p><p class="source-code">tensorboard --logdir=logs</p><p>Images in the directory will be displayed in TensorBoard as follows:</p><div id="_idContainer136" class="IMG---Figure"><img src="image/B16341_03_07.jpg" alt="Figure 3.7: Viewing a batch of images in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.7: Viewing a batch of images in TensorBoard</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Images on your system may vary.</p>
			<p>The result in TensorBoard is the images from the first batch. You can see that they are images of boats and planes. TensorBoard also provides you with the ability to adjust the brightness and contrast of the images; however, that affects only the images in TensorBoard and not the underlying image data.</p>
			<p>In this exercise, you viewed a batch of images from an image data generator using TensorBoard. This is an excellent way to verify the quality of your training data. It may not be necessary to verify every image for quality, but sample batches can be viewed easily using TensorBoard.</p>
			<p>This section has introduced one resource that TensorFlow offers to help data science and machine learning practitioners understand and visualize their data and algorithms: TensorBoard. You have used the resource to visualize computational graphs and image batches. In the next section, you will explore TensorFlow Hub, which is a repository for machine learning modules that can be accessed and incorporated into custom applications easily. The models are created by experts in the field, and you will learn how to access them for your own applications.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>TensorFlow Hub</h1>
			<p>TensorFlow Hub is an online repository of machine learning modules. The modules contain assets with the associated weights that are needed to use any model (for instance, for predictions or transfer learning) where the knowledge gained in training one model is used to solve a different but related problem. These modules can be used directly to create applications that they were trained for, or they can be used as a starting point to build new applications. The platform can be visited at the following URL: <a href="https://tfhub.dev/">https://tfhub.dev/</a>. When you visit the website, you will be greeted by the following page:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B16341_03_08.jpg" alt="Figure 3.8: TensorFlow Hub home page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8: TensorFlow Hub home page</p>
			<p>Once here, you can browse through models of various domains. The most popular domains include image, text, and video; many models exist for these domains:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B16341_03_09.jpg" alt="Figure 3.9: The model domains available on TensorFlow Hub&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9: The model domains available on TensorFlow Hub</p>
			<p>There are many models available on TensorFlow Hub that take images as their input data. These models are generally created for tasks including image classification, segmentation, embedding, generation, augmentation, and style transfer. Models created for text data are generally used for text embedding, and models used on video data are used for video classification. There are also audio data models for tasks including command detection and pitch extraction. TensorFlow Hub is consistently updated with new state-of-the-art models that can be used for all sorts of applications. </p>
			<p>Selecting a model will land you on the following page, which will tell you information about the model, such as the size of the model, its architecture, the dataset on which it was trained, and the URL for reference:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B16341_03_10.jpg" alt="Figure 3.10: The page of a TensorFlow Hub model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10: The page of a TensorFlow Hub model</p>
			<p>When referencing models for your own applications, you will need the URL of the model's page to load it in.</p>
			<p>Models can be accessed in notebook environments from TensorFlow Hub by utilizing the <strong class="source-inline">tensorflow_hub</strong> library. The library can be imported as follows:</p>
			<p class="source-code">import tensorflow_hub as hub</p>
			<p>Models can be loaded by utilizing the library's <strong class="source-inline">load</strong> function and passing in the reference URL of the model:</p>
			<p class="source-code">module = hub.load("https://tfhub.dev/google/imagenet"\</p>
			<p class="source-code">                  "/inception_resnet_v2/classification/4")</p>
			<p>Assets of the model's module, such as its architecture, can be viewed by accessing the <strong class="source-inline">signatures</strong> attribute. Each model may have different keys within the <strong class="source-inline">signatures</strong> attribute; however, much of the pertinent information will be contained within the <strong class="source-inline">default</strong> key:</p>
			<p class="source-code">model = module.signatures['default']</p>
			<p>The model can also be used directly in training by treating the whole model like a single Keras layer using the <strong class="source-inline">KerasLayer</strong> method:</p>
			<p class="source-code">layer = hub.KerasLayer("https://tfhub.dev/google/imagenet"\</p>
			<p class="source-code">                       "/inception_resnet_v2/classification/4")</p>
			<p>The process of using the model as layers for your own application is known as <strong class="bold">transfer learning</strong>, which will be explored in more depth in later chapters.</p>
			<p>Viewing a model in TensorFlow Hub can be done by writing the model graph to the logs using a file writer as follows:</p>
			<p class="source-code">from tensorflow.python.client import session</p>
			<p class="source-code">from tensorflow.python.summary import summary</p>
			<p class="source-code">from tensorflow.python.framework import ops</p>
			<p class="source-code">with session.Session(graph=ops.Graph()) as sess:</p>
			<p class="source-code">    file_writer = summary.FileWriter(logdir)</p>
			<p class="source-code">    file_writer.add_graph(model.graph)</p>
			<p>In the following exercise, you will download a model from TensorFlow Hub. After loading in the model, you will view the model's architecture using TensorBoard.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Exercise 3.03: Downloading a Model from TensorFlow Hub </h2>
			<p>In this exercise, you will download a model from TensorFlow Hub and then view the architecture of the model in TensorBoard. The model that will be downloaded is the <strong class="source-inline">InceptionV3</strong> model. This model was created in TensorFlow 1 and so requires some additional steps for displaying the model details as we're using TensorFlow 2. This model contains two parts: a part that includes convolutional layers to extract features from the images, and a classification part with fully connected layers. </p>
			<p>The distinct layers will be visible in TensorBoard as they have been named appropriately by the original author.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can get the <strong class="source-inline">InceptionV3</strong> model here: <a href="https://tfhub.dev/google/imagenet/inception_v3/classification/5">https://tfhub.dev/google/imagenet/inception_v3/classification/5</a>.</p>
			<p>Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the following libraries from TensorFlow:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">from tensorflow.python.client import session</p><p class="source-code">from tensorflow.python.summary import summary</p><p class="source-code">from tensorflow.python.framework import ops</p><p>The TensorFlow and TensorFlow Hub libraries are required to import and build the model, and the other classes from the TensorFlow library are required to visualize models that are created in TensorFlow 1 using TensorFlow 2, which is what you are using in this book.</p></li>
				<li>Create a variable for the logs to be stored:<p class="source-code">logdir = 'logs/'</p></li>
				<li>Load in a model module by using the <strong class="source-inline">load</strong> method from the <strong class="source-inline">tensorflow_hub</strong> library and pass in the URL for the model:<p class="source-code">module = hub.load('https://tfhub.dev/google/imagenet'\</p><p class="source-code">                  '/inception_v3/classification/5')</p></li>
				<li>Load the model from the <strong class="source-inline">signatures</strong> attribute of the module:<p class="source-code">model = module.signatures['default']</p></li>
				<li>Write the model graph to TensorBoard using a file writer:<p class="source-code">with session.Session(graph=ops.Graph()) as sess:</p><p class="source-code">    file_writer = summary.FileWriter(logdir)</p><p class="source-code">    file_writer.add_graph(model.graph)</p></li>
				<li>Launch TensorBoard in the command line to view a visual representation of the graph. TensorBoard can be viewed in a web browser by visiting the URL that is provided after launching TensorBoard:<p class="source-code">tensorboard --logdir=./logs</p><p>For those running Windows, in the Anaconda prompt, run the following:</p><p class="source-code">tensorboard --logdir=logs</p><p>You should get something like the following image:</p><div id="_idContainer140" class="IMG---Figure"><img src="image/B16341_03_11.jpg" alt="Figure 3.11: The architecture of the InceptionV3 model as viewed in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.11: The architecture of the InceptionV3 model as viewed in TensorBoard</p>
			<p>The result in TensorBoard is the architecture of the <strong class="source-inline">InceptionV3</strong> model. Here, you can view all the details about each layer of the model, including the input, output, and activation functions.</p>
			<p>In this exercise, you successfully downloaded a model into a Jupyter notebook environment using the TensorFlow Hub library. Once the model was loaded into the environment, you visualized the architecture of the model using TensorBoard. This can be a helpful way to visualize your model's architecture for debugging purposes.</p>
			<p>In this section, you have explored how to use TensorFlow Hub as a way to utilize the many brilliant models that have been created by experts in the machine learning field. As you will discover in later chapters, these models can be used to solve slightly different applications than those for which they were developed; this is known as transfer learning. In the next section, you will learn how to use Google Colab, an environment similar to Jupyter Notebooks that can be used to collaboratively develop applications in Python online, on Google servers.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/>Google Colab</h1>
			<p>Google Colab enables users to execute code on Google servers and is designed specifically for data science practitioners to develop code for machine learning in a collaborative environment. The platform is available at <a href="https://colab.research.google.com/">https://colab.research.google.com/</a> and offers an opportunity to develop in the Python programming language directly within a web browser with no code executing on your local machine. The environment comes pre-loaded with up-to-date libraries for data science and machine learning and offers a convenient alternative to setting up a development environment using Jupyter Notebooks. Moreover, the platform has a free tier that includes access to GPUs and TPUs, there is no configuration required, and sharing notebooks between collaborators is easy.</p>
			<p>Google Colab has a very similar development experience to Jupyter Notebooks, and there are some advantages and disadvantages of using Google Colab over Jupyter Notebooks.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>Advantages of Google Colab</h2>
			<p>The following are a few of the main advantages of using Google Colab:</p>
			<ul>
				<li><strong class="bold">Collaborative</strong>: Many users can access the same notebook and work collaboratively together.</li>
				<li><strong class="bold">Managed environment</strong>: Google Colab runs on Google servers, which can be helpful if local computational resources are limited. There is no need to set up a development environment since many packages come pre-installed.</li>
				<li><strong class="bold">Easy accessibility</strong>: Google Colab saves directly to Google Drive, offering seamless integration. Since the notebooks are saved in the cloud, they are available wherever Google Drive can be accessed.</li>
				<li><strong class="bold">Accelerated training times</strong>: GPU and TPU servers are available, which can offer accelerated training times for training machine learning models, especially ANNs with many hidden layers.</li>
				<li><strong class="bold">Interactive widgets</strong>: Widgets can be added to a notebook that can offer a way to easily vary input parameters and variables in an interactive manner.</li>
			</ul>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>Disadvantages of Google Colab</h2>
			<p>The following are a few of the disadvantages of using Google Colab:</p>
			<ul>
				<li><strong class="bold">Restrained runtime</strong>: Only two versions of TensorFlow are available on Google Colab, 1.X and 2.X, and they are updated, so specific functions may change over time, resulting in broken code. Additionally, the versions of TensorFlow may not interact well with other packages.</li>
				<li><strong class="bold">Internet dependence</strong>: Since the Python code is executed on Google servers, Google Colab can only be accessed with an internet connection.</li>
				<li><strong class="bold">No automatic save</strong>: Notebooks must be saved consistently, which is different from the automatic saving of Jupyter Notebooks.</li>
				<li><strong class="bold">Session timeout</strong>: Notebooks running on the virtual machines have a maximum lifetime of 12 hours and environments that are left idle for too long will be disconnected.</li>
				<li><strong class="bold">Interactive library</strong>: Libraries that contain interactive elements such as OpenCV or <strong class="source-inline">geoplotlib</strong> may not be capable of displaying interactive elements due to incompatibilities with the pre-loaded libraries.</li>
			</ul>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/>Development on Google Colab</h2>
			<p>Since Google Colab uses notebooks, the development environment is very similar to Jupyter Notebooks. In fact, IPython notebooks can be loaded into Google Colab. They can be loaded in via direct upload, Google Drive, or a GitHub repository. Alternatively, the platform provides example notebooks to get started. When you navigate to the platform, <a href="https://colab.research.google.com/">https://colab.research.google.com/</a>, you will be greeted by the following screen, which provides notebooks to open or the option to select a new notebook to begin developing:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B16341_03_12.jpg" alt="Figure 3.12: The home page of Google Colab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12: The home page of Google Colab</p>
			<p>If a new notebook is selected, you are greeted by the following screen, which may be very reminiscent of developing in Jupyter Notebooks and has many of the same features. You can create code or text snippets in the exact same way and many practitioners find the transition from Jupyter seamless:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B16341_03_13.jpg" alt="Figure 3.13: A blank notebook in Google Colab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: A blank notebook in Google Colab</p>
			<p>In the next exercise, you will use Google Colab to import and manipulate data. One of the main differences between working in Google Colab compared to Jupyter Notebooks is that by working in Google Colab, you are developing on a remote server. This means that any data for analysis or training models must either be loaded on Google Drive or available directly online. In the following exercise, you will import CSV data directly from a GitHub repository for this book.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>Exercise 3.04: Using Google Colab to Visualize Data</h2>
			<p>In this exercise, you will load a dataset from a GitHub repository that has bias correction data for next-day maximum and minimum air temperature forecasts for Seoul, South Korea.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the <strong class="source-inline">Bias_correction_ucl.csv</strong> file here: <a href="https://packt.link/8kP3j">https://packt.link/8kP3j</a>.</p>
			<p>To perform the exercise, you will have to navigate to <a href="https://colab.research.google.com/">https://colab.research.google.com/</a> and create a new notebook to work in. You will need to connect to a GPU-enabled environment to speed up TensorFlow operations such as tensor multiplication. Once the data has been loaded into the development environment, you will view the first five rows. Next, you'll drop the <strong class="source-inline">Date</strong> field since matrix multiplication requires numerical fields. Then, you will perform tensor multiplication of the dataset with a tensor or uniformly random variables.</p>
			<p>Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import TensorFlow and check the version of the library:<p class="source-code">import tensorflow as tf</p><p class="source-code">print('TF version:', tf.__version__)</p><p>You should get the version of the TensorFlow library:</p><div id="_idContainer143" class="IMG---Figure"><img src="image/B16341_03_14.jpg" alt="Figure 3.14: The output of the version of TensorFlow available in Google Colab&#13;&#10;"/></div><p class="figure-caption">Figure 3.14: The output of the version of TensorFlow available in Google Colab</p></li>
				<li>Navigate to the <strong class="source-inline">Edit</strong> tab, go to <strong class="source-inline">Notebook Settings</strong>, and then select <strong class="source-inline">GPU</strong> from the <strong class="source-inline">Hardware Acceleration</strong> dropdown. Verify that the GPU is enabled by displaying the GPU device name:<p class="source-code">tf.test.gpu_device_name()</p><p>You should get the name of the GPU device:</p><div id="_idContainer144" class="IMG---Figure"><img src="image/B16341_03_15.jpg" alt="Figure 3.15: The GPU device name&#13;&#10;"/></div><p class="figure-caption">Figure 3.15: The GPU device name</p></li>
				<li>Import the <strong class="source-inline">pandas</strong> library and load in the dataset directly from the GitHub repository:<p class="source-code">import pandas as pd</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com'\</p><p class="source-code">                 '/PacktWorkshops/The-TensorFlow-Workshop'\</p><p class="source-code">                 '/master/Chapter03/Datasets'\</p><p class="source-code">                 '/Bias_correction_ucl.csv')</p></li>
				<li>View the first five rows of the dataset using the <strong class="source-inline">head</strong> method:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer145" class="IMG---Figure"><img src="image/B16341_03_16.jpg" alt="Figure 3.16: The output of the first five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 3.16: The output of the first five rows of the DataFrame</p></li>
				<li>Drop the <strong class="source-inline">Date</strong> field since you'll be performing matrix multiplication, which requires numerical fields:<p class="source-code">df.drop('Date', axis=1, inplace=True)</p></li>
				<li>Import NumPy, convert the DataFrame to a NumPy array, and then create a TensorFlow tensor of uniform random variables. The value of the first axis of the tensor will be equal to the number of fields of the dataset, and the second axis will be equal to <strong class="source-inline">1</strong>:<p class="source-code">import numpy as np</p><p class="source-code">df = np.asarray(df).astype(np.float32)</p><p class="source-code">random_tensor = tf.random.normal((df.shape[1],1))</p></li>
				<li>Perform tensor multiplication on the dataset and the random tensor using TensorFlow's <strong class="source-inline">matmul</strong> function and print the result:<p class="source-code">tf.matmul(df, random_tensor)</p><p>You should get output like the following:</p><div id="_idContainer146" class="IMG---Figure"><img src="image/B16341_03_17.jpg" alt="Figure 3.17: The output of the tensor multiplication&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.17: The output of the tensor multiplication</p>
			<p>The result from executing the multiplication is a new tensor with the shape <strong class="source-inline">7752x1</strong>.</p>
			<p>In this exercise, you learned how to use Google Colab. You observed that Google Colab provides a convenient environment to build machine learning models and comes pre-loaded with many of the libraries that may be needed for any machine learning application. You can also see that the latest versions of the libraries are used. Unfortunately, the versions of TensorFlow cannot be modified, so using Google Colab in production environments may not be the most appropriate application. However, it is great for development environments.</p>
			<p>In the following activity, you will practice further how to use Google Colab in a development environment. You will use TensorFlow Hub in the same way that was achieved in Jupyter Notebooks. This activity will be similar to what was achieved in <em class="italic">Exercise 2.04</em>, <em class="italic">Loading Text Data for TensorFlow Models</em>, in which text data was processed by using a pre-trained word embedding model. Utilizing pre-trained models will be covered in future chapters, but this activity will show how easy it is to utilize a pre-trained model from TensorFlow Hub.</p>
			<h2 id="_idParaDest-75">Activity 3.02: Pe<a id="_idTextAnchor075"/>rforming Word Embedding from a Pre-Trained Model from TensorFlow Hub</h2>
			<p>In this activity, you will practice working in the Google Colab environment. You will download a universal sentence encoder from TensorFlow Hub from the following URL: <a href="https://tfhub.dev/google/universal-sentence-encoder/4">https://tfhub.dev/google/universal-sentence-encoder/4</a>. Once the model has been loaded into memory, you will use it to encode some sample text.</p>
			<p>Follow these steps:</p>
			<ol>
				<li value="1">Import TensorFlow and TensorFlow Hub and print the version of the library.</li>
				<li>Set the handle for the module as the URL for the universal sentence encoder. </li>
				<li>Use the TensorFlow Hub <strong class="source-inline">KerasLayer</strong> class to create a hub layer, passing in the following arguments: <strong class="source-inline">module_handle</strong>, <strong class="source-inline">input_shape</strong>, and <strong class="source-inline">dtype</strong>.</li>
				<li>Create a list containing a string, <strong class="source-inline">The TensorFlow Workshop</strong>, to encode with the encoder.</li>
				<li>Apply <strong class="source-inline">hub_layer</strong> to the text to embed the sentence as a vector.<p>Your final output should be like the following:</p><div id="_idContainer147" class="IMG---Figure"><img src="image/B16341_03_18.jpg" alt="Figure 3.18: Expected output of Activity 3.02&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.18: Expected output of Activity 3.02</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor260">this link</a>.</p>
			<p>This section introduced Google Colab, an online development environment used to run Python code on Google servers. This can allow any practitioner with an internet connection to begin building machine learning models. Moreover, you can browse the selection of pre-trained models to begin creating models for your own applications using another resource you learned about in this chapter, TensorFlow Hub. Google Colab provides practitioners with a zero-configuration, up-to-date environment, and even access to GPUs and TPUs for faster model training times.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor076"/>Summary</h1>
			<p>In this chapter, you used a variety of TensorFlow resources, including TensorBoard, TensorFlow Hub, and Google Colab. TensorBoard offers users a method to visualize computational model graphs, metrics, and any experimentation results. TensorFlow Hub allows users to accelerate their machine learning development using pre-trained models built by experts in the field. Google Colab provides a collaborative environment to develop machine learning models on Google servers. Developing performant machine learning models is an iterative process of trial and error, and the ability to visualize every step of the process can help practitioners debug and improve their models. Moreover, understanding how experts in the field have built their models and being able to utilize the pre-learned weights in the networks can drastically reduce training time. All of these resources are used to provide an environment to develop and debug machine learning algorithms in an efficient workflow.</p>
			<p>In the next chapter, you will begin creating your own machine learning models in TensorFlow, beginning with regression models. Regression models aim to predict continuous variables from input data. You will make your regression models by utilizing Keras layers, which are useful for building ANNs.</p>
		</div>
		<div>
			<div id="_idContainer149" class="Content">
			</div>
		</div>
	</body></html>