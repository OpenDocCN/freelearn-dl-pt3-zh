<html><head></head><body>
		<div id="_idContainer133">
			<h1 id="_idParaDest-98"><em class="italic"><a id="_idTextAnchor104"/>Chapter 5</em>: Style Transfer</h1>
			<p>Generative models such as VAE and GAN are great at generating realistic looking images. But we understand very little about the latent variables, let alone how to control them with regard to image generation. Researchers began to explore ways to better represent images aside from pixel distribution. It was found that an image could be disentangled into <strong class="bold">content</strong> and <strong class="bold">style</strong>. Content describes the composition in the image such as a tall building in the middle of the image. On the other hand, style refers to the fine details, such as the brick or stone textures of the wall or the color of the roof. Images showing the same building at different times of the day have different hues and brightness and can be seen as having the same content but different styles. </p>
			<p>In this chapter, we will start by implementing some seminal work in <strong class="bold">neural style transfer</strong> to transfer the artistic style of an image. We will then learn to implement <strong class="bold">feed-forward neural style transfer</strong>, which is a lot faster in terms of speed. Then we will implement <strong class="bold">adaptive instance normalization</strong> (<strong class="bold">AdaIN</strong>) to perform style transfer with arbitrary numbers of styles. AdaIN has been incorporated into some state-of-the-art GANs, which are collectively known as <strong class="bold">style-based GANs</strong>. This includes <strong class="bold">MUNIT</strong> for image translation and <strong class="bold">StyleGAN</strong>, which is famous for generating realistic looking, high-fidelity faces. We will learn about their architecture in the final section of the chapter. This wraps up the evolution of style-based generative models. </p>
			<p>By the end of this chapter, you will have learned how to perform artistic neural style transfer to convert a photo into painting. You will have a good understanding of how style is used in advanced GANs.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Neural style transfer</li>
				<li>Improving style transfer</li>
				<li>Arbitrary style transfer in real time</li>
				<li>Introduction to style-based generative models</li>
			</ul>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor105"/>Technical requirements</h1>
			<p>The Jupyter notebooks and codes can be found at the following link:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter05">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter05</a></p>
			<p>The notebooks used in the chapter are as follows:</p>
			<ul>
				<li><strong class="source-inline">ch5_neural_style_transfer.ipynb</strong></li>
				<li><strong class="source-inline">ch5_arbitrary_style_transfer.ipynb</strong></li>
			</ul>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor106"/>Neural style transfer</h1>
			<p>When <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) outperformed all other <a id="_idIndexMarker306"/>algorithms in the ImageNet image classification competition, people started to realize the potential of it and began exploring it for other computer vision tasks. In the <em class="italic">A Neural Algorithm of Artistic Style</em> paper published in 2015 by Gatys et al., they demonstrated the use of CNNs to transfer the artistic style of one image to another, as shown in the following examples:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B14538_05_01.jpg" alt="Figure 5.1 – (A) Content image. (B)-(D) Bottom image is the style image and the bigger pictures are stylized images&#13;&#10;(Source: Gatys et al., 2015, “A Neural Algorithm of Artistic Style” https://arxiv.org/abs/1508.06576)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – (A) Content image. (B)-(D) Bottom image is the style image and the bigger pictures are stylized images (Source: Gatys et al., 2015, “A Neural Algorithm of Artistic Style” https://arxiv.org/abs/1508.06576)</p>
			<p>Unlike most deep<a id="_idIndexMarker307"/> learning trainings that require tons of training data, neural style transfer requires only two images – content and style images. We can use pre-trained CNN such as VGG to transfer the style from the style image to the content image. </p>
			<p>As shown in the preceding image, (<strong class="bold">A</strong>) is the content image and (<strong class="bold">B</strong>) – (<strong class="bold">D</strong>) are the style and stylized images. The results were so impressive that they blew people's minds! Some even use the algorithm to create and sell art paintings. There are websites and apps that let people upload photos to perform style transfer without having to know the underlying theory and<a id="_idIndexMarker308"/> coding. Of course, as technical folks, we want to implement things by ourselves. </p>
			<p>We will now look into the details in terms of how to implement neural style transfer, starting with extracting image features with CNNs.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor107"/>Extracting features with VGG</h2>
			<p>Classification CNNs, like VGG, can be <a id="_idIndexMarker309"/>divided into two parts. The first part is known as <strong class="bold">feature extractor</strong> and is made up <a id="_idIndexMarker310"/>of mainly convolutional layers. The latter part consists of several dense layers that give the scores of classes. This is known as <strong class="bold">classifier head</strong>. It was found<a id="_idIndexMarker311"/> that a CNN pre-trained on ImageNet for classification tasks can be used for other tasks as well. </p>
			<p>For example, if you <a id="_idIndexMarker312"/>want to create classification CNNs for other datasets that have only 10 classes instead of ImageNet's 1,000 classes, then you could keep the feature extractor and only swap out the classifier head with a new one. This is<a id="_idIndexMarker313"/> known as <strong class="bold">transfer learning</strong>, where we could transfer or reuse some learned knowledge to new networks or applications. Many deep neural networks for computer vision tasks include a feature extractor, either reusing<a id="_idIndexMarker314"/> weights or training from scratch. This includes <strong class="bold">object detection</strong> and <strong class="bold">pose estimation</strong>.</p>
			<p>In a CNN, as we go deeper toward the <a id="_idIndexMarker315"/>output, it increasingly learns representation of the content of the image compared to its detailed pixel values. To understand this better, we will build a network to reconstruct the image that the layers see. The two steps for image reconstruction are as follows:</p>
			<ol>
				<li>Forward pass the image through a CNN to extract the features.</li>
				<li>With randomly initialized input, we <em class="italic">train the input </em>so that it recreates the features that best match the reference features from <em class="italic">step 1</em>. </li>
			</ol>
			<p>Let me elaborate on <em class="italic">step 2</em>. In normal network training, the input image is fixed and the backpropagated gradients are used to update the network weights. </p>
			<p>In neural style transfer, all network layers are frozen, and we use the gradients to change the input instead. The original paper uses VGG19 and Keras does have a pre-trained model that we could use. The feature extractor part of VGG is made up of five blocks and there is one<a id="_idIndexMarker316"/> downsampling at the end of each block. Every block has between two and four convolutional layers and the <a id="_idIndexMarker317"/>entire VGG19 has 16 convolutional layers and 3 dense layers, hence the number 19 in VGG19 stands for 19 layers with trainable weights. The following table shows different VGG configurations:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B14538_05_02.jpg" alt="Figure 5.2 – Different configurations of VGG&#13;&#10;(Source: K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks For Large-Scale Image Recognition” – https://arxiv.org/abs/1409.1556)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Different configurations of VGG (Source: K. Simonyan, A. Zisserman, “Very Deep Convolutional Networks For Large-Scale Image Recognition” – https://arxiv.org/abs/1409.1556)</p>
			<p>The Jupyter notebook <a id="_idIndexMarker318"/>for this is <strong class="source-inline">ch5_neural_style_transfer.ipynb</strong>, which is the complete neural style transfer solution. </p>
			<p>However, in the<a id="_idIndexMarker319"/> following text, I'll use a simpler code to show content reconstruction, which will be expanded to perform style transfer. The following is the code for using a pretrained VGG to extract the output layer of <strong class="source-inline">block4_conv2</strong>:</p>
			<p class="source-code">vgg = tf.keras.applications.VGG19(include_top=False, 						 weights='imagenet')</p>
			<p class="source-code">content_layers = ['block4_conv2']</p>
			<p class="source-code">content_outputs = [vgg.get_layer(x).output for x in  					content_layers]</p>
			<p class="source-code">model = Model(vgg.input, content_outputs)</p>
			<p>Pre-trained Keras CNN models are grouped into two parts. The bottom part is made up of convolutional layers, commonly known <a id="_idIndexMarker320"/>as the <strong class="bold">feature extractor</strong>, while the top part is the classifier head made up<a id="_idIndexMarker321"/> of dense layers. As we only want to extract the features and not bother about the classification, we will set <strong class="source-inline">include_top=False</strong> when instantiating the VGG model. </p>
			<p class="callout-heading">VGG pre-processing</p>
			<p class="callout">A Keras pre-trained model expects an input image to be in BGR in the range [0, 255]. Thus, the first step is to reverse the color channel to convert RGB into BGR. VGG uses different mean values for different color channels. Inside <strong class="source-inline">preprocess_input()</strong>, the pixel values are subtracted by the values of 103.939, 116.779, and 123.68 for the B, G, and R channels, respectively. </p>
			<p>The following is the forward pass code where the image is first pre-processed before feeding into the model to return the content feature. We then extract the content features and use them as our target:</p>
			<p class="source-code">def extract_features(image):</p>
			<p class="source-code">    image = tf.keras.applications.vgg19.\ 			preprocess_input(image *255.)</p>
			<p class="source-code">    content_ref = model(image)</p>
			<p class="source-code">    return content_ref</p>
			<p class="source-code">content_image = tf.reverse(content_image, axis=[-1])</p>
			<p class="source-code">content_ref = extract_features(content_image) </p>
			<p>Note that the image is<a id="_idIndexMarker322"/> normalized to [0., 1.] and so we need to restore that to [0., 255.] by multiplying it by 255. We then create a randomly initialized input that will also become the stylized image:</p>
			<p class="source-code">image = tf.Variable(tf.random.normal( 					shape=content_image.shape))</p>
			<p>Next, we will use backpropagation to reconstruct the image from the content features.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor108"/>Reconstructing content</h2>
			<p>In the training step, we<a id="_idIndexMarker323"/> feed an image to the frozen VGG to extract the content features and we use L2 loss to measure against the target content features. The following is the custom <strong class="source-inline">loss</strong> function to calculate the L2 loss of each feature layer:</p>
			<p class="source-code">def calc_loss(y_true, y_pred):</p>
			<p class="source-code">    loss = [tf.reduce_sum((x-y)**2) for x, y in  					zip(y_pred, y_true)]</p>
			<p class="source-code">    return tf.reduce_mean(loss)</p>
			<p>The following training step uses <strong class="source-inline">tf.GradientTape()</strong> to calculate the gradients. In normal neural network training, the gradients are applied to the trainable variables, that is, the weights of the neural <a id="_idIndexMarker324"/>network. However, in neural style transfer, the gradients are applied to the image. After that, we clip the image value between [0., 1.] as follows:</p>
			<p class="source-code">for i in range(1,steps+1):</p>
			<p class="source-code">    with tf.GradientTape() as tape:</p>
			<p class="source-code">        content_features = self.extract_features(image)</p>
			<p class="source-code">        loss = calc_loss(content_features, content_ref)</p>
			<p class="source-code">    grad = tape.gradient(loss, <strong class="bold">image</strong>)</p>
			<p class="source-code">    optimizer.apply_gradients([(grad, <strong class="bold">image</strong>)])</p>
			<p class="source-code">    image.assign(tf.clip_by_value(image, 0., 1.))</p>
			<p>We train it for 1,000 steps, and this is what the reconstructed content looks like:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B14538_05_03.jpg" alt="Figure 5.3 – Image reconstructed from content layers&#13;&#10;(Source: https://www.pexels.com/. (Left): Original content image, (Right): Content of ‘block1_1’)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Image reconstructed from content layers (Source: https://www.pexels.com/. (Left): Original content image, (Right): Content of 'block1_1')</p>
			<p>We could reconstruct the image almost perfectly with the first few convolutional layers similar to <em class="italic">block1_1</em>, as shown in the image above:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B14538_05_04.jpg" alt="Figure 5.4 – Image reconstructed from content layers &#13;&#10;(Left): Content of ‘block4_1’. (Right): Content of ‘block5_1’"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Image reconstructed from content layers (Left): Content of 'block4_1'. (Right): Content of 'block5_1'</p>
			<p>As we go<a id="_idIndexMarker325"/> deeper into <em class="italic">block4_1</em>, we start to lose fine details, such as the window frames and the words on the building. As we go deeper into <em class="italic">block5_1</em>,we see that all the details are gone and filled with some random noise. If we look carefully, the building structure and edges are still intact and in places where<a id="_idIndexMarker326"/> they should be. Now we have extracted just the content and omitted the style. After extracting the content features, the next step is to extract the style features. </p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor109"/>Reconstructing styles with the Gram matrix</h2>
			<p>As we have seen with the <a id="_idIndexMarker327"/>style reconstruction, the<a id="_idIndexMarker328"/> feature maps, especially the first few layers, contain both style and content. So how do we extract the style representation from the image? Gats et al. uses the <strong class="bold">Gram matrix</strong>, which computes the correlations between the different filter responses. Let's say the activation of convolutional layer <em class="italic">l</em> has a shape of (H, W, C), where <em class="italic">H</em> and <em class="italic">W</em> are the spatial dimensions and <em class="italic">C</em> is the number of channels, which equals the number of filters. Each filter detects different image features; they can be horizontal lines, diagonal lines, colors, and so on. </p>
			<p>Humans perceive things as having the same textures when they share some common features, such as a color and an edge. For instance, if we feed an image of a grass field into a convolutional layer, the filters that detect <em class="italic">vertical lines</em> and <em class="italic">green color</em> will produce bigger responses in their feature maps. Hence, we can use the correlation between feature maps to represent textures in the image. </p>
			<p>To create a Gram<a id="_idIndexMarker329"/> matrix from activations with a shape of (H, W, C), we will first reshape it into C number of vectors. Each vector is a flattened feature map with a size of H×W. We perform an inner product on these C vectors to get a symmetric C×C Gram matrix. The detailed steps for calculating a Gram matrix in TensorFlow are as follows:</p>
			<ol>
				<li value="1">Use <strong class="source-inline">tf.squeeze()</strong> to remove the batch dimension (1, H, W, C) to (H, W, C) as the batch size is always <strong class="source-inline">1</strong>.</li>
				<li>Transpose the tensor to transform the shape from (H, W, C) to (C, H, W).</li>
				<li>Flatten the final two dimensions to become (C, H×W).</li>
				<li>Perform the dot product of the features to create a Gram matrix with a shape of (C, C).</li>
				<li>Normalize by dividing the matrix by the number of points (H×W) in each flattened feature map.</li>
			</ol>
			<p>The code to calculate a<a id="_idIndexMarker330"/> Gram matrix from a single convolution layer activation is as follows:</p>
			<p class="source-code">def gram_matrix(x):</p>
			<p class="source-code">    x = tf.transpose(tf.squeeze(x), (2,0,1));</p>
			<p class="source-code">    x = tf.keras.backend.batch_flatten(x)</p>
			<p class="source-code">    num_points = x.shape[-1]</p>
			<p class="source-code">    gram = tf.linalg.matmul(x, tf.transpose(x))/num_points</p>
			<p class="source-code">    return gram</p>
			<p>We can use this function to <a id="_idIndexMarker331"/>obtain Gram matrices for each VGG layer that we designated as a style layer. We then use L2 loss on Gram matrices from the target and reference images. The loss function and the rest of the code is identical to content reconstruction. The code to create a list of the Gram matrices is as follows:</p>
			<p class="source-code">def extract_features(image):</p>
			<p class="source-code">    image = tf.keras.applications.vgg19.\ 				preprocess_input(image *255.)</p>
			<p class="source-code">    styles = self.model(image)</p>
			<p class="source-code">    styles = [self.gram_matrix(s) for s in styles]</p>
			<p class="source-code">    return styles</p>
			<p>The following images are reconstructed from style features from the different VGG layers:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B14538_05_05.jpg" alt=" Figure 5.5 – (Top) Style image: Vincent Van Goh’s Starry Night. (Bottom Left) Reconstructed style from ‘block1_1’. (Bottom Right) Reconstructed style from ‘block3_1’"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 5.5 – (Top) Style image: Vincent Van Goh's Starry Night. (Bottom Left) Reconstructed style from 'block1_1'. (Bottom Right) Reconstructed style from 'block3_1'</p>
			<p>In the style image<a id="_idIndexMarker332"/> reconstructed from <em class="italic">block1_1</em>, the content information is completely gone, showing only high spatial frequency<a id="_idIndexMarker333"/> texture details. The higher layer, <em class="italic">block3_1</em>, shows some curly shapes that seem to capture the higher hierarchy of the style in the input image. The loss function for the Gram matrix is the sum of <strong class="bold">squared error</strong> instead of <strong class="bold">mean squared error</strong>. Hence, higher hierarchy style layers have higher intrinsic weights. This allows the transfer of higher style representations, such as brush strokes. If we use mean squared error, low-level style features<a id="_idIndexMarker334"/> such as texture will be more prominent visually and may appear like high frequency noise. </p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor110"/>Performing neural style transfer</h2>
			<p>We can now merge the<a id="_idIndexMarker335"/> code from both the content and style reconstruction to perform neural style transfer. </p>
			<p>We first create a model that extracts two blocks of features, one for content and the other for style. We use only one layer of <strong class="source-inline">block5_conv1</strong> for the content, and five layers, from <strong class="source-inline">block1_conv1</strong> to <strong class="source-inline">block5_conv1</strong>, to capture styles from different hierarchies as follows:</p>
			<p class="source-code">vgg = tf.keras.applications.VGG19(include_top=False, 						  weights='imagenet')</p>
			<p class="source-code">default_content_layers = ['block5_conv1']</p>
			<p class="source-code">default_style_layers = ['block1_conv1',</p>
			<p class="source-code">                        'block2_conv1',</p>
			<p class="source-code">                        'block3_conv1', </p>
			<p class="source-code">                        'block4_conv1', </p>
			<p class="source-code">                        'block5_conv1']</p>
			<p class="source-code">content_layers = content_layers if content_layers else default_content_layers</p>
			<p class="source-code">style_layers = style_layers if style_layers else default_style_layers</p>
			<p class="source-code">self.content_outputs = [vgg.get_layer(x).output for x in content_layers]</p>
			<p class="source-code">self.style_outputs = [vgg.get_layer(x).output for x in style_layers]</p>
			<p class="source-code">self.model = Model(vgg.input, [self.content_outputs, 					    self.style_outputs])</p>
			<p>Before the start of the training loop, we extract content and style features from respective images to<a id="_idIndexMarker336"/> use as the targets. While we can use randomly initialized input for content and style reconstruction, it would be faster to train by starting from the content image as follows:</p>
			<p class="source-code">content_ref, _ = self.extract_features(content_image)</p>
			<p class="source-code">_, style_ref = self.extract_features(style_image)</p>
			<p>Then, we weigh the content and style loss and add them. The code snippet is as follows:</p>
			<p class="source-code">def train_step(self, image, content_ref, style_ref):</p>
			<p class="source-code">    with tf.GradientTape() as tape:</p>
			<p class="source-code">        content_features, style_features = \ 					self.extract_features(image)</p>
			<p class="source-code">        content_loss = self.content_weight*self.calc_loss( 					  content_ref, content_features)</p>
			<p class="source-code">        style_loss = self.style_weight*self.calc_loss( 						style_ref, style_features)</p>
			<p class="source-code">        loss = content_loss + style_loss</p>
			<p class="source-code">    grad = tape.gradient(loss, image)</p>
			<p class="source-code">    self.optimizer.apply_gradients([(grad, image)])</p>
			<p class="source-code">    image.assign(tf.clip_by_value(image, 0., 1.))</p>
			<p class="source-code">    return content_loss, style_loss  </p>
			<p>The following are the two stylized images produced using different weights and content layers: </p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B14538_05_06.jpg" alt="Figure 5.6 – Stylized images using neural style transfer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Stylized images using neural style transfer</p>
			<p>Feel free to change the <a id="_idIndexMarker337"/>weights and layers to create the styles that you want. I hope you now have a better understanding of content and style representation, which will come in handy when we explore advanced generative models. Next, we will look at ways to improve the neural style transfer.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor111"/>Improving style transfer</h1>
			<p>The research community and industry<a id="_idIndexMarker338"/> were excited about neural style transfer and wasted no time in putting it to use. Some set up websites to allow users to upload photos to perform style transfer, while some used that to create merchandise to sell. Then people realized some of the shortcomings of the original neural style transfer and worked to improve it. </p>
			<p>One of the biggest limitations is that style transfer takes all the style information, including the color and brush strokes of the entire style image, and transfers it to the whole of the content image. Using the examples that we just did in the previous section, the blueish color from the style image was transferred into both the building and background. Wouldn't it be nice if we had the choice to transfer only the brush stroke but not the color, and just to the preferred regions? </p>
			<p>The lead author of neural<a id="_idIndexMarker339"/> style transfer and his team produced a new algorithm to address these issues. The following diagram shows the control the algorithm can give and an example of the results:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B14538_05_07.jpg" alt="Figure 5.7 – Different control methods of neural style transfer. (a) Content image (b) The sky and ground are stylized using different style images (c) The color of the content image is preserved (d) The fine scale and coarse scale are stylized using different style images &#13;&#10;(Source: L. Gatys, 2017, “Controlling Perceptual Factors in Neural Style Transfer”, https://arxiv.org/abs/1611.07865)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Different control methods of neural style transfer. (a) Content image (b) The sky and ground are stylized using different style images (c) The color of the content image is preserved (d) The fine scale and coarse scale are stylized using different style images (Source: L. Gatys, 2017, “Controlling Perceptual Factors in Neural Style Transfer”, https://arxiv.org/abs/1611.07865)</p>
			<p>The controls proposed in this paper are as follows:</p>
			<ul>
				<li><strong class="bold">Spatial control</strong>: This controls the<a id="_idIndexMarker340"/> spatial location of style transfer in both the content and style images. This is done by applying a spatial mask to style features before calculating the Gram matrix.</li>
				<li><strong class="bold">Color control</strong>: This can be used to preserve the color of the content image. To do this, we will convert the RGB format into <a id="_idIndexMarker341"/>color space such that HCL separates the luminance (brightness) from other color channels. We can think of the luminance channel as a grayscale image. We then perform style transfer only in the luminance channel and then merge it with color channels from the original style image to give the final stylized image.</li>
				<li><strong class="bold">Scale control</strong>: This manages the <a id="_idIndexMarker342"/>granularity of the brush strokes. The process is more involved as it requires multiple runs of style transfers and<a id="_idIndexMarker343"/> different layers of style features to be chosen in order to compute the Gram matrix. </li>
			</ul>
			<p>These perceptual controls are useful for creating better stylized images that suit your requirements. I'll leave it as an exercise for you to implement those controls if you desire, because we have more important things to cover. </p>
			<p>The following are the two major themes associated with improving style transfer that had a big influence on the development of GANs:</p>
			<ul>
				<li>Improving speed</li>
				<li>Improving style variations</li>
			</ul>
			<p>Let's go through some of these developments to lay some foundations for the next project that we will implement – performing arbitrary style transfer in real time.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor112"/>Faster style transfer with a feed-forward network</h2>
			<p>Neural style<a id="_idIndexMarker344"/> transfer is based on optimization that is akin to neural network training. It is slow and takes several minutes to run even with the use <a id="_idIndexMarker345"/>of a GPU. This limited its potential applications on mobile devices. As a result, researchers were motivated to develop faster<a id="_idIndexMarker346"/> algorithms for style transfer and <strong class="bold">feed-forward style transfer</strong> was born. The following diagram shows one of the first networks that employed such an architecture:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B14538_05_08.jpg" alt="Figure 5.8 – Block diagram of a feed-forward convolutional neural network for style transfer. &#13;&#10;(Redrawn from: J. Johnson et al., 2016 “Perceptual Losses for Real-Time Style Transfer and Super-Resolution” – https://arxiv.org/abs/1603.08155)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Block diagram of a feed-forward convolutional neural network for style transfer. (Redrawn from: J. Johnson et al., 2016 “Perceptual Losses for Real-Time Style Transfer and Super-Resolution” – https://arxiv.org/abs/1603.08155)</p>
			<p>The architecture is simpler than the how the block diagram looks. There are two networks in this architecture:</p>
			<ul>
				<li>A <strong class="bold">trainable convolutional network</strong> (normally known as a <strong class="bold">style transfer network</strong>) to translate an input image into <a id="_idIndexMarker347"/>a stylized image. This can be implemented as an encoder-decoder-like architecture, like that of U-Net or VAE.</li>
				<li>A <strong class="bold">fixed convolutional network</strong>, usually a pretrained<a id="_idIndexMarker348"/> VGG, that measures the content and style losses. </li>
			</ul>
			<p>Similar to the original neural <a id="_idIndexMarker349"/>style transfer, we first extract the content and style targets with VGG. Instead of training the input image, we now train<a id="_idIndexMarker350"/> a convolutional network to translate a content image into a stylized image. The content and style features of the stylized image are extracted by VGG, and losses are measured and backpropagated to the trainable convolutional network. We train it like a normal feed-forward CNN. During inference, we only need to perform one forward pass to translate the input image into a stylized image, which is 1,000 times faster than before!</p>
			<p>Alright, the speed<a id="_idIndexMarker351"/> problem is now solved, but there is still a problem. Such a network could only learn one style to transfer. We'll need to train<a id="_idIndexMarker352"/> one network for each of the styles we want to perform, which is a lot less flexible than the original style transfer. Then people started working on that, and as you may have guessed, that got solved too! We'll go over that shortly.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor113"/>A different style feature</h2>
			<p>The original neural style<a id="_idIndexMarker353"/> transfer paper didn't explain why the Gram matrix is effective as a style feature. Many subsequent improvements to style transfers, such as the feed-forward style transfer, continued using the Gram matrix solely as style features. That's changed with the <em class="italic">Demystifying Neural Style Transfer</em> paper published by Y, Li et al. in 2017. It was found that the style information is intrinsically represented by the <em class="italic">distributions of activations</em> in a CNN. They have shown that matching Gram <a id="_idIndexMarker354"/>matrices of activations are equivalent to minimizing the <strong class="bold">maximum mean discrepancy</strong> (<strong class="bold">MMD</strong>) of activation distributions. Therefore, we can perform style transfer by matching the activation distribution of an image to those of the style image.</p>
			<p>Therefore, the Gram matrix is not the only way in which to implement style transfer. We could use adversarial loss, too. Let's recall that GANs such as pix2pix (<a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a>, <em class="italic">Image-to-Image Translation</em>) could perform style transfer by matching the pixel distribution of a generated image with the real (style) images. The difference is that GANs try to minimize the discrepancy in pixel distribution, while style transfer does it to the layer activation's distributions. </p>
			<p>Later, researchers found that we can use just the basic statistics of the mean and variance of the activations to represent the styles. In other words, if we feed two images that are similar in style into VGG, their layer activations will have a similar mean and variance. We can therefore train a network to perform style transfer by minimizing the difference in the mean and variance of activations between a generated image and a style image. This leads to the development of using a normalization layer to control the style.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor114"/>Controlling styles with a normalization layer</h2>
			<p>A simple but effective way of <a id="_idIndexMarker355"/>controlling the activation statistics is by changing the gamma <img src="image/Formula_05_001.png" alt=""/> and beta <em class="italic">β</em> in the normalization layer. In other words, we could change the style by using different affine transform parameters (gamma and beta). As a reminder, both batch normalization and instance normalization share the same equation, as follows: </p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/Formula_05_002.jpg" alt=""/>
				</div>
			</div>
			<p>The difference is that batch normalization (<em class="italic">BN</em>) calculates the mean <em class="italic">µ</em> and standard deviation <em class="italic">σ</em> across (N, H, W) dimensions, while instance normalization (<em class="italic">IN</em>) calculates only from (H, W). </p>
			<p>However, there is only one gamma and beta pair per normalization layer, which limits the network to learning only one style. How do we make the network learn multiple styles? Well, we could use multiple sets of gammas and betas where each set remembers one style. This is <a id="_idIndexMarker356"/>exactly what <strong class="bold">conditional instance normalization</strong> (<strong class="bold">CIN</strong>) does. </p>
			<p>It builds upon instance normalization but has multiple sets of gamma and beta pairs. Each gamma and beta set is used to train a particular style; in other words, they are conditioned on the style images. The equation of conditional instance normalization is as follows: </p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/Formula_05_003.jpg" alt=""/>
				</div>
			</div>
			<p>Say we have <em class="italic">S</em> different style images, then we have <em class="italic">S</em> gammas and <em class="italic">S</em> betas in normalization layers for each of the styles. In addition to the content image, we also feed in the one-hot encoded style label into the style transfer network. In practice, gamma and beta are<a id="_idIndexMarker357"/> implemented as matrices with a shape of (S×C). We retrieve the gamma and beta for that style by performing matrix multiplication of a one-hot encoded label (1×S) with matrices (S×C) to get <em class="italic">γ</em><span class="superscript">S</span> and <em class="italic">β</em><span class="superscript">s</span> for each (1×C) channel. It is easier to understand when we implement the code. However, we will defer implementation to <a href="B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175"><em class="italic">Chapter 9</em></a>, <em class="italic">Video Synthesis</em>, when we use it to perform class condition normalization. We are introducing CIN now to prepare ourselves for the upcoming section. </p>
			<p>Now, with style encoded into the embedding spaces of gammas and betas, we could perform style interpolation by interpolating gammas and betas as shown in the following image:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B14538_05_09.jpg" alt="Figure 5.9 – Combination of artistic styles by interpolating the gammas and betas of two different styles&#13;&#10;(Source: V. Dumoulin et al., 2017 “A Learned Representation for Artistic Style” – https://arxiv.org/abs/1610.07629) "/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Combination of artistic styles by interpolating the gammas and betas of two different styles (Source: V. Dumoulin et al., 2017 “A Learned Representation for Artistic Style” – https://arxiv.org/abs/1610.07629) </p>
			<p>This is all good, but the network is still limited to the fixed <em class="italic">N</em> styles that are used in training. Next, we will learn and implement an improvement that allows any arbitrary styles! </p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor115"/>Arbitrary style transfer in real time</h1>
			<p>In this section, we will learn how to<a id="_idIndexMarker358"/> implement a network that could perform arbitrary style transfer in real time. We have already learned how to use a feed-forward network for faster inference and that solves the real-time part. We have also learned how to use conditional instance normalization to transfer a fixed number of<a id="_idIndexMarker359"/> styles. Now, we will learn one further normalization technique that allows for any arbitrary style, and then we are good to go in terms of implementing the code.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor116"/>Implementing adaptive instance normalization</h2>
			<p>Like CIN, <strong class="bold">AdaIN</strong> is also instance normalization, meaning that the mean and <a id="_idIndexMarker360"/>standard deviation are calculated across (H, W) per image, and per channel, as opposed to batch normalization, which calculates across (N, H, W). In CIN, the gammas and betas are trainable variables, and they learn the means and variances that are needed for different styles. In AdaIN, gammas and betas are replaced by standard deviations and means of style features, as follows:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/Formula_05_004.jpg" alt=""/>
				</div>
			</div>
			<p>AdaIN can still be understood as a form of conditional instance normalization where the conditions are the style features rather than the style labels. In both training and inference time, we use VGG to extract the style layer outputs and use their statistics as the style conditions. This avoids the need to pre-define a fixed set of styles. We can now implement AdaIN in TensorFlow. The notebook for this is <strong class="source-inline">ch5_arbitrary_style_transfer.ipynb</strong>.</p>
			<p>We will use TensorFlow's subclassing to create a custom <strong class="source-inline">AdaIN</strong> layer as follows: </p>
			<p class="source-code">class AdaIN(layers.Layer):</p>
			<p class="source-code">    def __init__(self, epsilon=1e-5):</p>
			<p class="source-code">        super(AdaIN, self).__init__()</p>
			<p class="source-code">        self.epsilon = epsilon        </p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        x = inputs[0] # content</p>
			<p class="source-code">        y = inputs[1] # style</p>
			<p class="source-code">        mean_x, var_x = tf.nn.moments(x, axes=(1,2), 							keepdims=True)</p>
			<p class="source-code">        mean_y, var_y = tf.nn.moments(y, axes=(1,2), 							keepdims=True)</p>
			<p class="source-code">        std_x = tf.sqrt(var_x+self.epsilon)</p>
			<p class="source-code">        std_y = tf.sqrt(var_y+self.epsilon)</p>
			<p class="source-code">        output = std_y*(x – mean_x)/(std_x) + mean_y    </p>
			<p class="source-code">        return output</p>
			<p>This is a straightforward implementation of the equation. One thing that deserves a bit of explanation is the use of <strong class="source-inline">tf.nn.moments</strong>, which is also used in the TensorFlow batch normalization implementation. It calculates the mean and variance of the feature maps, where the axes <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong> refer to H, W of the feature maps. We also set <strong class="source-inline">keepdims=True</strong> to keep the results in four dimensions with a shape of (N, 1, 1, C) as opposed to the default (N, C). The former allows TensorFlow to perform broadcast arithmetic with the input tensor that has a shape of (N, H, W, C). Here, broadcast refers to repeating one value in bigger dimensions.</p>
			<p>To be more precise, when <a id="_idIndexMarker361"/>we subtract <em class="italic">x</em> from the calculated mean for a particular instance and channel, the single mean value will first be repeated into the shape of (H, W) before the subtraction. We will now look at how to incorporate AdaIN into style transfer.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor117"/>Style transfer network architecture</h2>
			<p>The following diagram <a id="_idIndexMarker362"/>shows the architecture of a style transfer network and the training pipeline:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B14538_05_10.jpg" alt="Figure 5.10 – Overview of style transfer with AdaIN &#13;&#10;(Redrawn from: X. Huang, S. Belongie, 2017, “Arbitrary Style Transfer in Real Time with Adaptive Instance Normalization” – https://arxiv.org/abs/1703.06868)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – Overview of style transfer with AdaIN (Redrawn from: X. Huang, S. Belongie, 2017, “Arbitrary Style Transfer in Real Time with Adaptive Instance Normalization” – https://arxiv.org/abs/1703.06868)</p>
			<p>The <strong class="bold">style transfer network</strong> (<strong class="bold">STN</strong>) is an <a id="_idIndexMarker363"/>encoder-decoder <a id="_idIndexMarker364"/>network where the encoder encodes the content and style features with fixed VGG. AdaIN then encodes the style features into the statistics of content features and the decoder takes these new features to generate the stylized image.</p>
			<h3>Building the encoder</h3>
			<p>The following is the code to <a id="_idIndexMarker365"/>build the encoder from VGG:</p>
			<p class="source-code">def build_encoder(self, name='encoder'):</p>
			<p class="source-code">    self.encoder_lay<a id="_idTextAnchor118"/>ers = ['block1_conv1',</p>
			<p class="source-code">                           'block2_conv1',</p>
			<p class="source-code">                           'block3_conv1', </p>
			<p class="source-code">                           'block4_conv1']</p>
			<p class="source-code">    vgg = tf.keras.applications.VGG19(include_top=False, 							weights='imagenet') </p>
			<p class="source-code">    layer_outputs = [vgg.get_layer(x).output for x in  					self.encoder_layers] </p>
			<p class="source-code">    return Model(vgg.input, layer_outputs, name=name)</p>
			<p>This is similar to neural style<a id="_idIndexMarker366"/> transfer, except that we use the last style layer, <strong class="source-inline">'block4_conv1'</strong>, as our content layer. Thus, we don't need to define the content layer separately. We will now make a small but important improvement to the convolutional layer to improve the appearance of generated images.</p>
			<h3>Reducing block artifacts with reflection padding</h3>
			<p>Normally, when we<a id="_idIndexMarker367"/> apply padding to an input tensor in a convolutional layer, constant zeros are padded around the tensor. However, the sudden drop in value at a border creates high frequency components and results in block artefacts in the generated image. One way to reduce these frequency components is by adding <em class="italic">total variation loss</em> as the regularizer in the network training. </p>
			<p>To do that, we first calculate the high frequency components simply by shifting the image by one pixel, and then subtract by the original image to create a matrix. Total variation loss is the L1 norm or the sum of absolute values. Therefore, the training will try to minimize this loss function so as to reduce the high-frequency component. </p>
			<p>There is another alternative, which is to replace the constant zeros in padding with reflective values. For example, if we pad an array of [10, 8, 9] with zeros, this will give [0, 10, 8, 9, 0]. We can then see a sudden change in values between 0 and its neighbors. </p>
			<p>If we use reflective padding, the padded array will be [8, 10, 8, 9, 8], which provides a smoother transition toward the border. However, Keras Conv2D doesn't support reflective padding, so we will have to create a custom Conv2D using TensorFlow subclassing. The following code snippet (the code has been curtailed for brevity; please check out GitHub for the entire code) shows how to add reflective padding to the input tensor prior to the convolution:</p>
			<p class="source-code">class Conv2D(layers.Layer):</p>
			<p class="source-code">    @tf.function</p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        padded = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], 						[0, 0]], mode='REFLECT')</p>
			<p class="source-code">        # perform conv2d using low level API</p>
			<p class="source-code">        output = tf.nn.conv2d(padded, self.w, strides=1, 					   padding=”VALID”) + self.b</p>
			<p class="source-code">        if self.use_relu:</p>
			<p class="source-code">            output = tf.nn.relu(output)</p>
			<p class="source-code">        return output</p>
			<p>The preceding code is taken from <a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Image Generation Using TensorFlow</em>, but with<a id="_idIndexMarker368"/> an added low-level <strong class="source-inline">tf.pad</strong> API to pad the input tensor.</p>
			<h3>Building the decoder</h3>
			<p>Although we use 4 VGG layers (<strong class="source-inline">block1_conv1</strong> to <strong class="source-inline">block4_conv1</strong>) in the encoder code, only the last layer, <strong class="source-inline">block4_conv1</strong>, from the encoder is used by AdaIN. Therefore, the input tensor to the<a id="_idIndexMarker369"/> decoder has the same activation as <strong class="source-inline">block4_conv1</strong>. The decoder architecture is not too dissimilar to the ones we have implemented in earlier chapters. It consists of convolutional and upsampling layers, as shown in the following code:</p>
			<p class="source-code">def build_decoder(self):</p>
			<p class="source-code">    block = tf.keras.Sequential([\</p>
			<p class="source-code">            Conv2D(512, 256, 3),</p>
			<p class="source-code">            UpSampling2D((2,2)),</p>
			<p class="source-code">            Conv2D(256, 256, 3),</p>
			<p class="source-code">            Conv2D(256, 256, 3),</p>
			<p class="source-code">            Conv2D(256, 256, 3),</p>
			<p class="source-code">            Conv2D(256, 128, 3),</p>
			<p class="source-code">            UpSampling2D((2,2)),</p>
			<p class="source-code">            Conv2D(128, 128, 3),</p>
			<p class="source-code">            Conv2D(128, 64, 3),</p>
			<p class="source-code">            UpSampling2D((2,2)),</p>
			<p class="source-code">            Conv2D(64, 64, 3),</p>
			<p class="source-code">            Conv2D(64, 3, 3, use_relu=False)],</p>
			<p class="source-code">                               name='decoder')</p>
			<p class="source-code">    return block</p>
			<p>The preceding code uses custom <strong class="source-inline">Conv2D</strong> with reflective padding. All layers use the ReLU activation function, except the<a id="_idIndexMarker370"/> output layer, which does not have any non-linearity activation function. We have now completed AdaIN, the encoder, and the decoder, and can move on to the image pre-processing pipeline. </p>
			<h3>VGG processing</h3>
			<p>Like the neural style transfer we<a id="_idIndexMarker371"/> built earlier, we will need to pre-process the image by inverting the color channel to BGR and then subtracting the color means. The code is as follows:</p>
			<p class="source-code">def preprocess(self, image):</p>
			<p class="source-code">    # rgb to bgr</p>
			<p class="source-code">    image = tf.reverse(image, axis=[-1])</p>
			<p class="source-code">    return tf.keras.applications.vgg19.preprocess_input(image)</p>
			<p>We could do the same in post-processing, that is, adding back the color means and reversing the color channel. However, this is something that could be learned by the decoder as color means is<a id="_idIndexMarker372"/> equivalent to the biases in the output layer. We will let the training do the job and all we need to do is to clip the pixels to range of [0, 255], as follows:</p>
			<p class="source-code">def postprocess(self, image):</p>
			<p class="source-code">    return tf.clip_by_value(image, 0., 255.)</p>
			<p>We now have all the building blocks ready and all that is left to do is to put them together to create the STN and training pipeline.</p>
			<h3>Building the style transfer network</h3>
			<p>Constructing the <strong class="bold">STN</strong> is straightforward <a id="_idIndexMarker373"/>and simply involves connecting the encoder, AdaIN, and decoder, as shown in the preceding architectural diagram. The STN is also the model we will use to perform inference. The code to do this is as follows:</p>
			<p class="source-code">content_image = self.preprocess(content_image_input)</p>
			<p class="source-code">style_image = self.preprocess(style_image_input) </p>
			<p class="source-code">self.content_target = self.encoder(content_image)</p>
			<p class="source-code">self.style_target = self.encoder(style_image) </p>
			<p class="source-code">adain_output = AdaIN()([self.content_target[-1], 				  self.style_target[-1]]) </p>
			<p class="source-code">self.stylized_image = self.postprocess( 					self.decoder(adain_output)) </p>
			<p class="source-code">self.stn = Model([content_image_input,  			  style_image_input],  			  self.stylized_image)</p>
			<p>The content and style images are pre-processed and fed into the encoder. The last feature layer, that is, <strong class="source-inline">block4_conv1</strong> from both<a id="_idIndexMarker374"/> images, goes to <strong class="source-inline">AdaIN()</strong>. The stylized feature then goes into the decoder to generate the stylized image in RGB.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor119"/>Arbitrary style transfer training</h2>
			<p>Like neural and feed-forward style<a id="_idIndexMarker375"/> transfer, content loss and style loss are computed from an activation extracted by the fixed VGG. The content loss is also an L2 norm, but the generated stylized image's content features are now compared against AdaIN's output rather than the features from the content image, as shown in the following code. The authors of the paper found that this makes convergence faster:</p>
			<p class="source-code">content_loss =  tf.reduce_sum((output_features[-1]-\ 					    adain_output)**2)</p>
			<p>For style loss, the commonly used Gram matrix is replaced with the L2 norm of the activation statistics of mean and variance. This produces similar results to the Gram matrix but is conceptually cleaner. The following is the style loss function equation:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/Formula_05_005.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <em class="italic">φ</em><span class="superscript">i</span> denotes a layer in VGG-19 used to compute the style loss.</p>
			<p>We use <strong class="source-inline">tf.nn.moments</strong> as in the AdaIN layer to calculate the statistics and the L2 norm between the features from stylized and style images. Each style layer carries the same weight, and hence we average the content layer losses as follows:</p>
			<p class="source-code">def calc_style_loss(self, y_true, y_pred):</p>
			<p class="source-code">    n_features = len(y_true)</p>
			<p class="source-code">    epsilon = 1e-5</p>
			<p class="source-code">    loss = [] </p>
			<p class="source-code">    for i in range(n_features):</p>
			<p class="source-code">        mean_true, var_true = tf.nn.moments(y_true[i], 						axes=(1,2), keepdims=True)</p>
			<p class="source-code">        mean_pred, var_pred = tf.nn.moments(y_pred[i], 						axes=(1,2), keepdims=True)</p>
			<p class="source-code">        std_true, std_pred = tf.sqrt(var_true+epsilon), 						tf.sqrt(var_pred+epsilon)</p>
			<p class="source-code">        mean_loss = tf.reduce_sum(tf.square( 						    mean_true-mean_pred))</p>
			<p class="source-code">        std_loss = tf.reduce_sum(tf.square( 						  std_true-std_pred))</p>
			<p class="source-code">        loss.append(mean_loss + std_loss) </p>
			<p class="source-code">    return tf.reduce_mean(loss)</p>
			<p>The final step is to write the<a id="_idIndexMarker376"/> training step, as shown here:</p>
			<p class="source-code">def train_step(self, train_data):</p>
			<p class="source-code">    with tf.GradientTape() as tape:</p>
			<p class="source-code">        adain_output, output_features, style_target = \ 					self.training_model(train_data) </p>
			<p class="source-code">        content_loss = tf.reduce_sum( 				(output_features[-1]-adain_output)\ 									**2)</p>
			<p class="source-code">        style_loss = self.style_weight * \ 				  self.calc_style_loss( 					style_target, output_features)</p>
			<p class="source-code">        loss =  content_loss + style_loss </p>
			<p class="source-code">        gradients = tape.gradient(loss, 				 self.decoder.trainable_variables) </p>
			<p class="source-code">        self.optimizer.apply_gradients(zip(gradients, 				self.decoder.trainable_variables)) </p>
			<p class="source-code">    return content_loss, style_loss</p>
			<p>Instead of tweaking weights to both the content and style, we fix the content weight to be <strong class="source-inline">1</strong> and adjust just the style weight. In this example, we set the content weight to <strong class="source-inline">1</strong> and the style weight to <strong class="source-inline">1e-4</strong>. In <em class="italic">Figure 5.10</em>, it may look like there are three networks to train but two of them <a id="_idIndexMarker377"/>are fixed VGG, so the only trainable network is the decoder. Therefore, we only track and apply gradients to the decoder. </p>
			<p class="callout-heading">Tips</p>
			<p class="callout">The preceding training step can be replaced by Keras' <strong class="source-inline">train_on_batch()</strong> function (see <a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Generative Adversarial Network</em>), which uses fewer code lines. I'll leave this to you as an additional exercise.</p>
			<p>In this example, we'll use faces as content images, and <strong class="source-inline">cyclegan/vangogh2photo</strong> for the styles. Although Van Gogh's paintings are of one artistic style, from the style transfer perspective, each style image is a unique style. The <strong class="source-inline">vangoh2photo</strong> dataset contains 400 style images, meaning we are training the network with 400 different styles! The following diagram shows examples of images produced by our network:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B14538_05_11.jpg" alt="Figure 5.11 – Arbitrary style transfer. (Left) Style image (Middle) Content image (Right) Stylized image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – Arbitrary style transfer. (Left) Style image (Middle) Content image (Right) Stylized image</p>
			<p>The images in the<a id="_idIndexMarker378"/> preceding diagram shows the style transfers in inference time using style images that were not previously seen by the network. Each style transfer happens only with a single forward pass, which is a lot faster than the iterative optimization of the original neural style transfer algorithm. Having understood various techniques to perform style transfer, we are now in a good position to learn how to design GANs in style (pun intended). </p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor120"/>Introduction to style-based GANs</h1>
			<p>The innovations in style transfer<a id="_idIndexMarker379"/> made their way into influencing the development of GANs. Although GANs at that time could generate realistic images, they were generated by using random latent variables, where we had little understanding in terms of what they represented. Even though multimodal GANs could create variations in generated images, we did not know how to control the latent variables to achieve the outcome that we wanted. </p>
			<p>In an ideal world, we would love to have some knobs to independently control the features we would like to generate, as in the face manipulation exercise in <a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Variational Autoencoder</em>. This is known as <strong class="bold">disentangled representation</strong>, which is a relatively new idea in deep learning. The<a id="_idIndexMarker380"/> idea of disentangled representation is to separate an image into independent representation. For example, a face has two eyes, a nose, and a mouth, with each of them being a representation of a face. As we have <a id="_idIndexMarker381"/>learned in style transfer, an image can be disentangled into content and style. So researchers brought that idea into GANs. </p>
			<p>In the next section, we will look at a style-based GAN known as <strong class="bold">MUNIT</strong>. As we are limited by the number of pages in the book, we won't be writing the detailed code, but will go over the overall architecture to understand how style is used in these models. </p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor121"/>Multimodal Unsupervised Image-to-Image Translation (MUNIT)</h2>
			<p>MUNIT is<a id="_idIndexMarker382"/> an image-to-image translation<a id="_idIndexMarker383"/> model similar to BicycleGAN (<a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a>, <em class="italic">Image-to-Image Translation</em>). Both can generate multimodal images with continuous distributions, but BicycleGAN needs to have paired data while MUNIT does not. BicycleGAN generates multimodal images by using two models that relate the target image to latent variables. It is not very clear how these models work, nor how to control the latent variable to<a id="_idIndexMarker384"/> change the output. MUNIT's approach is conceptually a lot different, but also a lot simpler to understand. It assumes that the source and target images share the same content <a id="_idIndexMarker385"/>space, but with different styles.</p>
			<p>The following diagram shows the principal idea behind MUNIT:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B14538_05_12.jpg" alt="Figure 5.12 – Illustration of the MUNIT method.&#13;&#10;(Redrawn from: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12 – Illustration of the MUNIT method.(Redrawn from: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)</p>
			<p>Say we have two images, <strong class="bold">X</strong><span class="subscript">1</span> and <strong class="bold">X</strong><span class="subscript">2</span>. Each of them can be represented as a content code and style code pair (<strong class="bold">C</strong><span class="subscript">1</span>, <strong class="bold">S</strong><span class="subscript">1</span>) and (<strong class="bold">C</strong><span class="subscript">2</span>, <strong class="bold">S</strong><span class="subscript">2</span>), respectively. It is assumed that both <strong class="bold">C</strong><span class="subscript">1</span> and <strong class="bold">C</strong><span class="subscript">2</span> are in a shared content space, <strong class="bold">C</strong>. In other words, the contents may not be exactly the same but are similar. The styles are in their respective domain-specific style spaces. Therefore, image translation from <strong class="bold">X</strong><span class="subscript">1</span> and <strong class="bold">X</strong><span class="subscript">2</span> can be formulated as generating image with content code from <strong class="bold">X</strong><span class="subscript">1</span> and style code from <strong class="bold">X</strong><span class="subscript">2</span>, or, in other words, from code (<strong class="bold">C</strong><span class="subscript">1</span>, <strong class="bold">S</strong><span class="subscript">2</span>). </p>
			<p>Previously in style transfer, we viewed styles as artistic styles with different brush strokes, colors, and textures. Now, we expand the meaning of style to beyond artistic painting. For example, tigers and lions are just cats with different styles of whiskers, skin, fur, and shapes. Next, let's look at<a id="_idIndexMarker386"/> the MUNIT model architecture.</p>
			<h3>Understanding the architecture</h3>
			<p>The MUNIT architecture is <a id="_idIndexMarker387"/>shown in the following diagram:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B14538_05_13.jpg" alt="Figure 5.13 – MUNIT model overview &#13;&#10;(Redrawn from: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13 – MUNIT model overview (Redrawn from: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)</p>
			<p>There are two autoencoders, one in each domain. The autoencoder encodes the image into its style and content codes, and then the decoder decodes them back into the original image. This is trained using adversarial loss, in other words, the model is made up of an autoencoder but is trained like a GAN. </p>
			<p>In the preceding diagram, the image reconstruction process is shown on the left. On the right is the cross-domain translation. As mentioned earlier, to translate from <strong class="bold">X</strong><span class="subscript">1</span> to <strong class="bold">X</strong><span class="subscript">2</span>, we first encode the images into their respective content and style codes, and then we do two things with it as follows:</p>
			<ol>
				<li value="1">We generate a fake image in style domain 2 with (<strong class="bold">C</strong><span class="subscript">1</span>, <strong class="bold">S</strong><span class="subscript">2</span>). This is also trained using GANs. </li>
				<li>We encode the fake image into content and style code. If the translation works well, then it should be similar to (<strong class="bold">C</strong><span class="subscript">1</span>, <strong class="bold">S</strong><span class="subscript">2</span>).</li>
			</ol>
			<p>Well, if this is sounding very <a id="_idIndexMarker388"/>familiar to you, that is because this is the <em class="italic">cycle consistency constraint</em> from CycleGAN. Except, here the cycle consistency is not applied to the image, but to the content and style codes.</p>
			<h3>Looking into autoencoder design</h3>
			<p>Finally, let's look at the detailed architecture <a id="_idIndexMarker389"/>of the autoencoder, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B14538_05_14.jpg" alt="Figure 5.14 – MUNIT model overview&#13;&#10;(Source: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14 – MUNIT model overview (Source: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)</p>
			<p>Unlike other style transfer models, MUNIT doesn't use VGG as an encoder. It uses two separate encoders, one for content and another for style. The content encoder consists of several residual blocks<a id="_idIndexMarker390"/> with instance normalization and downsampling. This is quite similar to VGG's style feature. </p>
			<p>The style encoder is different from the content encoder in two aspects:</p>
			<ul>
				<li>Firstly, there is no normalization. As we have learned, normalizing activations to zero means removing the style information. </li>
				<li>Secondly, the residual blocks are replaced with fully connected layers. This is because style is seen as spatially invariant and therefore we don't need convolutional layers to provide the spatial information. </li>
			</ul>
			<p>This is to say that the style code only contains information about the eye color and doesn't need to know where the eyes are as it is the responsibility of the content code. The style code is a low-dimensional vector and usually has the size of 8, which is in contrast to high-dimensional latent variables in GAN and VAE, and styles features in style transfer. The reason for a small style code size is so that we have a fewer number of knobs to control the styles, which make things more manageable. The following diagram shows how the content and style code feed into the decoder:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B14538_05_15.jpg" alt="Figure 5.15 – AdaIN layers within the decoder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15 – AdaIN layers within the decoder</p>
			<p>The generator within the <a id="_idIndexMarker391"/>decoder is made up of a group of residual blocks. Only residual blocks within the first group have AdaIN as the normalization layer. The equation for AdaIN, where <em class="italic">z</em> is the activation from the previous convolutional layer, is shown here:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/Formula_05_006.jpg" alt=""/>
				</div>
			</div>
			<p>In the arbitrary<a id="_idIndexMarker392"/> feed-forward neural style transfer, we use the mean and standard deviation from a single style layer as gamma and beta in AdaIN. In MUNIT, the gamma and beta are generated from the style code with a <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>). </p>
			<h3>Translating animal images</h3>
			<p>The following screenshot shows<a id="_idIndexMarker393"/> samples of <em class="italic">1-to-many</em> image translations by MUNIT. We can generate a variety of output images by using different style codes:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B14538_05_16.jpg" alt="Figure 5.16 – Animal image translation by MUNIT&#13;&#10;(Source: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16 – Animal image translation by MUNIT (Source: X. Huang et al., 2018, “Multimodal Unsupervised Image-to-Image Translation” – https://arxiv.org/abs/1804.04732)</p>
			<p>At the time of writing, MUNIT is still the state-of-the-art model for multimodal image-to-image translation, according to <a href="https://paperswithcode.com/task/multimodal-unsupervised-image-to-image">https://paperswithcode.com/task/multimodal-unsupervised-image-to-image</a>.</p>
			<p>If you are<a id="_idIndexMarker394"/> interested in the code implementation, you can refer to the official implementation by NVIDIA at <a href="https://github.com/NVlabs/MUNIT">https://github.com/NVlabs/MUNIT</a>.<a id="_idTextAnchor122"/> </p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor123"/>Summary</h1>
			<p>In this chapter, we covered the evolution of styled-based generative models. It all started with neural style transfer, where we learned that the image can be disentangled into content and style. The original algorithm was slowed and the iterative optimization process in inference time replaced with a feed-forward style transfer that could perform style transfer in real time. </p>
			<p>We then learned that the Gram matrix is not the only method for representing style, and that we could use the layers' statistics instead. As a result, normalization layers have been explored to control the style of an image, which eventually led to the creation of AdaIN. By combing a feed-forward network and AdaIN, we implemented arbitrary style transfer in real time. </p>
			<p>With the success in style transfer, AdaIN found its way into GANs. We went over the MUNIT architecture in detail in terms of how AdaIN was used for multimodal image generation. There is a style-based GAN that you should be familiar with, and it is called StyleGAN. It was made famous for its ability to generate ultra-realistic, high-fidelity face images. The implementation of StyleGAN requires pre-requisite knowledge of progressive GANs. Therefore, we will defer the detailed implementation to <a href="B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a>, <em class="italic">High Fidelity Face Generation</em>. </p>
			<p>At this point, GANs are moving away from the black box method, which uses only random noise as input, and toward the disentangled representation approach, which better exploits data properties. In the next chapter, we will look at how to use specific GAN techniques in drawing paintings.</p>
		</div>
	</body></html>