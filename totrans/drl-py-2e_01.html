<html><head></head><body>
  <div id="_idContainer143">
    <h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-14" class="chapterTitle">Fundamentals of Reinforcement Learning</h1>
    <p class="normal"><strong class="keyword">Reinforcement Learning</strong> (<strong class="keyword">RL</strong>) is one of the areas of <strong class="keyword">Machine Learning </strong>(<strong class="keyword">ML</strong>). Unlike other ML paradigms, such as <a id="_idIndexMarker000"/>supervised and unsupervised learning, RL works in a trial and error fashion by <a id="_idIndexMarker001"/>interacting with its environment.</p>
    <p class="normal">RL is one of the most active areas of research in artificial intelligence, and it is believed that RL will take us a step closer towards achieving artificial general intelligence. RL has evolved rapidly in the past few years with a wide variety of applications ranging from building a recommendation system to self-driving cars. The major reason for this evolution is the advent of deep reinforcement learning, which is a combination of deep learning and RL. With the emergence of new RL algorithms and libraries, RL is clearly one of the most promising areas of ML.</p>
    <p class="normal">In this chapter, we will build a strong foundation in RL by exploring several important and fundamental concepts involved in RL.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bullet">Key elements of RL</li>
      <li class="bullet">The basic idea of RL</li>
      <li class="bullet">The RL algorithm</li>
      <li class="bullet">How RL differs from other ML paradigms</li>
      <li class="bullet">The Markov Decision Processes</li>
      <li class="bullet">Fundamental concepts of RL</li>
      <li class="bullet">Applications of RL</li>
      <li class="bullet">RL glossary</li>
    </ul>
    <p class="normal">We will begin the chapter by defining <em class="italic">Key elements of RL</em>. This will help explain <em class="italic">The basic idea of RL</em>.</p>
    <h1 id="_idParaDest-15" class="title">Key elements of RL</h1>
    <p class="normal">Let's begin by understanding some key elements of RL.</p>
    <h2 id="_idParaDest-16" class="title">Agent</h2>
    <p class="normal">An agent<a id="_idIndexMarker002"/> is a software <a id="_idIndexMarker003"/>program that learns to make intelligent decisions. We can say that an agent is a learner in the RL setting. For instance, a chess player can be considered an agent since the player learns to make the best moves (decisions) to win the game. Similarly, Mario in a Super Mario Bros video game can be considered an agent since Mario explores the game and learns to make the best moves in the game.</p>
    <h2 id="_idParaDest-17" class="title">Environment</h2>
    <p class="normal">The environment<a id="_idIndexMarker004"/> is the world of the agent. The agent stays within the environment. For instance, coming back to our chess game, a chessboard is called the environment since the chess<a id="_idIndexMarker005"/> player (agent) learns to play the game of chess within the chessboard (environment). Similarly, in Super Mario Bros, the world of Mario is called the environment.</p>
    <h2 id="_idParaDest-18" class="title">State and action</h2>
    <p class="normal">A state<a id="_idIndexMarker006"/> is a position <a id="_idIndexMarker007"/>or a moment in the environment that the agent can be in. We learned that the agent stays within the environment, and there can be many positions in the environment that the agent can stay in, and those positions are called states. For instance, in our chess game example, each position on the chessboard is called the state. The state is usually denoted by <em class="italic">s</em>.</p>
    <p class="normal">The agent <a id="_idIndexMarker008"/>interacts with the <a id="_idIndexMarker009"/>environment and moves from one state to another by performing an action. In the chess game environment, the action is the move performed by the player (agent). The action is usually denoted by <em class="italic">a</em>.</p>
    <h2 id="_idParaDest-19" class="title">Reward</h2>
    <p class="normal">We learned that <a id="_idIndexMarker010"/>the agent<a id="_idIndexMarker011"/> interacts with an environment by performing an action and moves from one state to another. Based on the action, the agent receives a reward. A reward is nothing but a numerical value, say, +1 for a good action and -1 for a bad action. How do we decide if an action is good or bad?</p>
    <p class="normal">In our chess game example, if the agent makes a move in which it takes one of the opponent's chess pieces, then it is considered a good action and the agent receives a positive <a id="_idIndexMarker012"/>reward. Similarly, if the agent makes a move that leads to the <a id="_idIndexMarker013"/>opponent taking the agent's chess piece, then it is considered a bad action and the agent receives a negative reward. The reward is denoted by <em class="italic">r</em>.</p>
    <h1 id="_idParaDest-20" class="title">The basic idea of RL</h1>
    <p class="normal">Let's begin with an analogy. Let's suppose we are teaching a dog (agent) to catch a ball. Instead of teaching the dog explicitly to catch a ball, we just throw a ball and every time the dog catches the ball, we give the dog a cookie (reward). If the dog fails to catch the ball, then we do not give it a cookie. So, the dog will figure out what action caused it to receive a cookie and<a id="_idIndexMarker014"/> repeat that action. Thus, the dog will understand that catching the ball caused it to receive a cookie and will attempt to repeat catching the ball. Thus, in this way, the dog will learn to catch a ball while aiming to maximize the cookies it can receive.</p>
    <p class="normal">Similarly, in an RL setting, we will not teach the agent what to do or how to do it; instead, we will give a reward to the agent for every action it does. We will give a positive reward to the agent when it performs a good action and we will give a negative reward to the agent when it performs a bad action. The agent begins by performing a random action and if the action is good, we then give the agent a positive reward so that the agent understands it has performed a good action and it will repeat that action. If the action performed by the agent is bad, then we will give the agent a negative reward so that the agent will understand it has performed a bad action and it will not repeat that action.</p>
    <p class="normal">Thus, RL can be viewed as a trial and error learning process where the agent tries out different actions and learns the good action, which gives a positive reward.</p>
    <p class="normal">In the dog analogy, the dog represents the agent, and giving a cookie to the dog upon it catching the ball is a positive reward and not giving a cookie is a negative reward. So, the dog (agent) explores different actions, which are catching the ball and not catching the ball, and understands that catching the ball is a good action as it brings the dog a positive reward (getting a cookie).</p>
    <p class="normal">Let's further explore the idea of RL with one more simple example. Let's suppose we want to teach a robot (agent) to walk without hitting a mountain, as <em class="italic">Figure 1.1</em> shows: </p>
    <figure class="mediaobject"><img src="../Images/B15558_01_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.1: Robot walking</p>
    <p class="normal">We will not teach the <a id="_idIndexMarker015"/>robot explicitly to not go in the direction of the mountain. Instead, if the robot hits the mountain and gets stuck, we give the robot a negative reward, say -1. So, the robot will understand that hitting the mountain is the wrong action, and it will not repeat that action:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.2: Robot hits mountain</p>
    <p class="normal">Similarly, when the robot walks in the right direction without hitting the mountain, we give the robot a positive reward, say +1. So, the robot will understand that not hitting the mountain is a good action, and it will repeat that action:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.3: Robot avoids mountain</p>
    <p class="normal">Thus, in the RL setting, the agent explores different actions and learns the best action based on the reward it gets.</p>
    <p class="normal">Now that we have a basic idea of how RL works, in the upcoming sections, we will go into more detail and also learn the important concepts involved in RL.</p>
    <h1 id="_idParaDest-21" class="title">The RL algorithm</h1>
    <p class="normal">The steps <a id="_idIndexMarker016"/>involved in a typical RL algorithm are as follows:</p>
    <ol>
      <li class="numbered">First, the agent interacts with the environment by performing an action.</li>
      <li class="numbered">By performing an action, the agent moves from one state to another.</li>
      <li class="numbered">Then the agent will receive a reward based on the action it performed.</li>
      <li class="numbered">Based on the reward, the agent will understand whether the action is good or bad.</li>
      <li class="numbered">If the action was good, that is, if the agent received a positive reward, then the agent will prefer performing that action, else the agent will try performing other actions in search of a positive reward.</li>
    </ol>
    <p class="normal">RL is basically a trial and error learning process. Now, let's revisit our chess game example. The agent (software program) is the chess player. So, the agent interacts with the environment (chessboard) by performing an action (moves). If the agent gets a positive reward for an action, then it will prefer performing that action; else it will find a different action that gives a positive reward.</p>
    <p class="normal">Ultimately, the goal of the agent is to maximize the reward it gets. If the agent receives a good reward, then it means it has performed a good action. If the agent performs a good action, then it implies that it can win the game. Thus, the agent learns to win the game by maximizing the reward.</p>
    <h2 id="_idParaDest-22" class="title">RL agent in the grid world</h2>
    <p class="normal">Let's strengthen our<a id="_idIndexMarker017"/> understanding of RL by looking at another simple example. Consider the following grid world environment:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.4: Grid world environment</p>
    <p class="normal">The positions <strong class="keyword">A</strong> to <strong class="keyword">I</strong> in the environment are called the states of the environment. The goal of the agent is to reach state <strong class="keyword">I</strong> by starting from state <strong class="keyword">A</strong> without visiting the shaded states (<strong class="keyword">B</strong>, <strong class="keyword">C</strong>, <strong class="keyword">G</strong>, and <strong class="keyword">H</strong>). Thus, in order to achieve the goal, whenever our agent visits a shaded state, we will give a negative reward (say -1) and when it visits an unshaded state, we will give a positive reward (say +1). The actions in the environment are moving <em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">right</em> and <em class="italic">left</em>. The agent can perform any of these four actions to reach state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong>.</p>
    <p class="normal">The first time the agent<a id="_idIndexMarker018"/> interacts with the environment (the first iteration), the agent is unlikely to perform the correct action in each state, and thus it receives a negative reward. That is, in the first iteration, the agent performs a random action in each state, and this may lead the agent to receive a negative reward. But over a series of iterations, the agent learns to perform the correct action in each state through the reward it obtains, helping it achieve the goal. Let us explore this in detail.</p>
    <p class="normal"><strong class="keyword">Iteration 1</strong></p>
    <p class="normal">As we learned, in the first iteration, the agent performs a random action in each state. For instance, look at the following figure. In the first iteration, the agent moves <em class="italic">right</em> from state <strong class="keyword">A</strong> and reaches the new state <strong class="keyword">B</strong>. But since <strong class="keyword">B</strong> is the shaded state, the agent will receive a negative reward and so the agent will understand that moving <em class="italic">right</em> is not a good action in state <strong class="keyword">A</strong>. When it visits state <strong class="keyword">A</strong> next time, it will try out a different action instead of moving <em class="italic">right</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.5: Actions taken by the agent in iteration 1</p>
    <p class="normal">As <em class="italic">Figure 1.5</em> shows, from state <strong class="keyword">B</strong>, the agent moves <em class="italic">down</em> and reaches the new state <strong class="keyword">E</strong>. Since <strong class="keyword">E</strong> is an unshaded state, the agent will receive a positive reward, so the agent will understand that moving <em class="italic">down</em> from state <strong class="keyword">B</strong> is a good action.</p>
    <p class="normal">From state <strong class="keyword">E</strong>, the <a id="_idIndexMarker019"/>agent moves <em class="italic">right</em> and reaches state <strong class="keyword">F</strong>. Since <strong class="keyword">F</strong> is an unshaded state, the agent receives a positive reward, and it will understand that moving <em class="italic">right</em> from state <strong class="keyword">E</strong> is a good action. From state <strong class="keyword">F</strong>, the agent moves <em class="italic">down</em> and reaches the goal state <strong class="keyword">I</strong> and receives a positive reward, so the agent will understand that moving down from state <strong class="keyword">F</strong> is a good action.</p>
    <p class="normal"><strong class="keyword">Iteration 2</strong></p>
    <p class="normal">In the second iteration, from state <strong class="keyword">A</strong>, instead of moving <em class="italic">right</em>, the agent tries out a different action as the agent learned in the previous iteration that moving <em class="italic">right</em> is not a good action in state <strong class="keyword">A</strong>.</p>
    <p class="normal">Thus, as <em class="italic">Figure 1.6</em> shows, in this iteration the agent moves <em class="italic">down</em> from state <strong class="keyword">A</strong> and reaches state <strong class="keyword">D</strong>. Since <strong class="keyword">D</strong> is an unshaded state, the agent receives a positive reward and now the agent will understand that moving <em class="italic">down</em> is a good action in state <strong class="keyword">A</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.6: Actions taken by the agent in iteration 2</p>
    <p class="normal">As shown in the preceding figure, from state <strong class="keyword">D</strong>, the agent moves <em class="italic">down</em> and reaches state <strong class="keyword">G</strong>. But since <strong class="keyword">G</strong> is a shaded state, the agent will receive a negative reward and so the agent will understand that moving <em class="italic">down</em> is not a good action in state <strong class="keyword">D</strong>, and when it visits state <strong class="keyword">D </strong>next time, it will try out a different action instead of moving <em class="italic">down</em>.</p>
    <p class="normal">From <strong class="keyword">G</strong>, the agent moves <em class="italic">right</em> and reaches state <strong class="keyword">H</strong>. Since <strong class="keyword">H</strong> is a shaded state, it will receive a negative reward<a id="_idIndexMarker020"/> and understand that moving <em class="italic">right</em> is not a good action in state <strong class="keyword">G</strong>.</p>
    <p class="normal">From <strong class="keyword">H</strong> it moves <em class="italic">right</em> and reaches the goal state <strong class="keyword">I</strong> and receives a positive reward, so the agent will understand that moving <em class="italic">right</em> from state <strong class="keyword">H</strong> is a good action.</p>
    <p class="normal"><strong class="keyword">Iteration 3</strong></p>
    <p class="normal">In the third iteration, the agent moves <em class="italic">down</em> from state <strong class="keyword">A</strong> since, in the second iteration, our agent learned that moving <em class="italic">down</em> is a good action in state <strong class="keyword">A</strong>. So, the agent moves <em class="italic">down</em> from state <strong class="keyword">A</strong> and reaches the next state, <strong class="keyword">D</strong>,<strong class="keyword"> </strong>as <em class="italic">Figure 1.7</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.7: Actions taken by the agent in iteration 3</p>
    <p class="normal">Now, from state <strong class="keyword">D</strong>, the<strong class="keyword"> </strong>agent tries a different action instead of moving <em class="italic">down</em> since in the second iteration our agent learned that moving <em class="italic">down</em> is not a good action in state <strong class="keyword">D</strong>. So, in this iteration, the <a id="_idIndexMarker021"/>agent moves <em class="italic">right</em> from state <strong class="keyword">D</strong> and reaches state <strong class="keyword">E</strong>.</p>
    <p class="normal">From state <strong class="keyword">E</strong>, the agent moves <em class="italic">right</em> as the agent already learned in the first iteration that moving <em class="italic">right</em> from state <strong class="keyword">E</strong> is a good action<em class="italic"> </em>and reaches state <strong class="keyword">F</strong>.</p>
    <p class="normal">Now, from state <strong class="keyword">F</strong>, the<strong class="keyword"> </strong>agent moves <em class="italic">down</em> since the agent learned in the first iteration that moving <em class="italic">down</em> is a good action in state <strong class="keyword">F</strong>, and reaches the goal state <strong class="keyword">I</strong>.</p>
    <p class="normal"><em class="italic">Figure 1.8</em> shows the result of the third iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.8: The agent reaches the goal state without visiting the shaded states</p>
    <p class="normal">As we can see, our agent has successfully learned to reach the goal state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong> without visiting the shaded states based on the rewards.</p>
    <p class="normal">In this way, the agent <a id="_idIndexMarker022"/>will try out different actions in each state and understand whether an action is good or bad based on the reward it obtains. The goal of the agent is to maximize rewards. So, the agent will always try to perform good actions that give a positive reward, and when the agent performs good actions in each state, then it ultimately leads the agent to achieve the goal.</p>
    <p class="normal">Note that these iterations are called episodes in RL terminology. We will learn more about episodes later in the chapter.</p>
    <h1 id="_idParaDest-23" class="title">How RL differs from other ML paradigms</h1>
    <p class="normal">We can categorize ML into three types:</p>
    <ul>
      <li class="bullet">Supervised learning</li>
      <li class="bullet">Unsupervised learning</li>
      <li class="bullet">RL</li>
    </ul>
    <p class="normal">In supervised learning, the <a id="_idIndexMarker023"/>machine learns from training<a id="_idIndexMarker024"/> data. The training data consists of a labeled pair of inputs and outputs. So, we train the model (agent) using the training data in such a way that the model can generalize its learning to new unseen data. It is called supervised learning because the training data acts as a supervisor, since it has a labeled pair of inputs and outputs, and it guides the model in learning the given task.</p>
    <p class="normal">Now, let's understand the difference between supervised and reinforcement learning with an example. Consider the dog analogy we discussed earlier in the chapter. In supervised learning, to teach the dog to catch a ball, we will teach it explicitly by specifying turn left, go right, move forward seven steps, catch the ball, and so on in the form of training data. But in RL, we just throw a ball, and every time the dog catches the ball, we give it a cookie (reward). So, the dog will learn to catch the ball while trying to maximize the cookies (reward) it can get.</p>
    <p class="normal">Let's consider one more example. Say we want to train the model to play chess using supervised learning. In this case, we will have training data that includes all the moves a player can make in each state, along with labels indicating whether it is a good move or not. Then, we train the model to learn from this training data, whereas in the case of RL, our agent will not be given any sort of training data; instead, we just give a reward to the agent for each action it performs. Then, the agent will learn by interacting with the environment and, based on the reward it gets, it will choose its actions.</p>
    <p class="normal">Similar to supervised learning, in unsupervised learning, we train the model (agent) based on the training data. But in the case of unsupervised learning, the training data does not contain any labels; that is, it consists of only inputs and not outputs. The goal of unsupervised learning is to determine hidden patterns in the input. There is a common misconception that RL is a kind of unsupervised learning, but it is not. In unsupervised learning, the model learns the hidden structure, whereas, in RL, the model learns by maximizing the reward.</p>
    <p class="normal">For instance, consider a movie recommendation system. Say we want to recommend a new movie to the user. With unsupervised learning, the model (agent) will find movies similar to the movies the user (or users with a profile similar to the user) has viewed before and recommend new movies to the user.</p>
    <p class="normal">With RL, the <a id="_idIndexMarker025"/>agent constantly receives feedback<a id="_idIndexMarker026"/> from the user. This feedback represents rewards (a reward could be ratings the user has given for a movie they have watched, time spent watching a movie, time spent watching trailers, and so on). Based on the rewards, an RL agent will understand the movie preference of the user and then suggest new movies accordingly.</p>
    <p class="normal">Since the RL agent is learning with the aid of rewards, it can understand if the user's movie preference changes and suggest new movies according to the user's changed movie preference dynamically.</p>
    <p class="normal">Thus, we can say that in both supervised and unsupervised learning the model (agent) learns based on the given training dataset, whereas in RL the agent learns by directly interacting with the environment. Thus, RL is essentially an interaction between the agent and its environment.</p>
    <p class="normal">Before moving on to the fundamental concepts of RL, we will introduce a popular process to aid decision-making in an RL environment.</p>
    <h1 id="_idParaDest-24" class="title">Markov Decision Processes</h1>
    <p class="normal">The <strong class="keyword">Markov Decision Process</strong> (<strong class="keyword">MDP</strong>) provides a<a id="_idIndexMarker027"/> mathematical framework for solving the RL problem. Almost all RL problems can be modeled as an MDP. MDPs are widely used for solving various optimization problems. In this section, we will understand what an MDP is and how it is used in RL.</p>
    <p class="normal">To understand an MDP, first, we need to learn about the Markov property and Markov chain.</p>
    <h2 id="_idParaDest-25" class="title">The Markov property and Markov chain</h2>
    <p class="normal">The Markov property<a id="_idIndexMarker028"/> states that the future depends only on the present and not on the past. The Markov chain, also <a id="_idIndexMarker029"/>known as the Markov process, consists of a sequence of states that strictly obey the Markov property; that is, the Markov chain is the probabilistic model that solely depends on the current state to predict the next state and not the previous states, that is, the future is conditionally independent of the past.</p>
    <p class="normal">For example, if we want to predict the weather and we know that the current state is cloudy, we can predict that the next state could be rainy. We concluded that the next state is likely to be rainy only by considering the current state (cloudy) and not the previous states, which might have been sunny, windy, and so on.</p>
    <p class="normal">However, the Markov property does not hold for all processes. For instance, throwing a dice (the next state) has no dependency on the previous number that showed up on the dice (the current state).</p>
    <p class="normal">Moving from one state to another is called a transition, and its probability is called a transition probability. We denote the transition probability by <img src="../Images/B15558_01_001.png" alt="" style="height: 1.2em;"/>. It indicates the probability of moving from the state <em class="italic">s</em> to the next state <img src="../Images/B15558_01_002.png" alt="" style="height: 1.2em;"/>. Say we have three states (cloudy, rainy, and windy) in our Markov chain. Then we can represent the probability of transitioning from one state to another using a table called a Markov table, as shown in <em class="italic">Table 1.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_09.png" alt=""/></figure>
    <p class="packt_figref">Table 1.1: An example of a Markov table</p>
    <p class="normal">From <em class="italic">Table 1.1</em>, we can observe that:</p>
    <ul>
      <li class="bullet">From the state cloudy, we transition to the state rainy with 70% probability and to the state windy with 30% probability.</li>
      <li class="bullet">From the state rainy, we transition to the same state rainy with 80% probability and to the state cloudy with 20% probability.</li>
      <li class="bullet">From the state windy, we transition to the state rainy with 100% probability.</li>
    </ul>
    <p class="normal">We can also represent this transition information of the <a id="_idIndexMarker030"/>Markov chain in the form of a state diagram, as shown in <em class="italic">Figure 1.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.9: A state diagram of a Markov chain</p>
    <p class="normal">We can also formulate the transition probabilities into a matrix called the transition matrix, as shown in <em class="italic">Figure 1.10</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.10: A transition matrix</p>
    <p class="normal">Thus, to conclude, we can say that the Markov chain or Markov process consists of a set of states along with their transition probabilities.</p>
    <h2 id="_idParaDest-26" class="title">The Markov Reward Process</h2>
    <p class="normal">The<strong class="keyword"> Markov Reward Process</strong> (<strong class="keyword">MRP</strong>) is an extension of the Markov chain with the reward function. That is, we learned<a id="_idIndexMarker031"/> that the Markov chain consists of states and a transition probability. The MRP consists of states, a transition probability, and also a reward function.</p>
    <p class="normal">A reward function tells us the reward we obtain in each state. For instance, based on our previous weather example, the reward function tells us the reward we obtain in the state cloudy, the reward we obtain in the state windy, and so on. The reward function is usually denoted by <em class="italic">R</em>(<em class="italic">s</em>).</p>
    <p class="normal">Thus, the MRP consists of states <em class="italic">s</em>, a transition probability <img src="../Images/B15558_01_003.png" alt="" style="height: 1.2em;"/>, and a reward function <em class="italic">R</em>(<em class="italic">s</em>).</p>
    <h2 id="_idParaDest-27" class="title">The Markov Decision Process</h2>
    <p class="normal">The <strong class="keyword">Markov Decision Process</strong> (<strong class="keyword">MDP</strong>) is an extension of the MRP with actions. That is, we learned that the MRP consists of <a id="_idIndexMarker032"/>states, a transition probability, and a reward function. The MDP consists of states, a transition probability, a reward function, and also actions. We learned that the Markov property states that the next state is dependent only on the current state and is not based on the previous state. Is the Markov property applicable to the RL setting? Yes! In the RL environment, the agent makes decisions only based on the current state and not based on the past states. So, we can model an RL environment as an MDP.</p>
    <p class="normal">Let's understand this with an example. Given any environment, we can formulate the environment using an MDP. For instance, let's consider the same grid world environment we learned earlier. <em class="italic">Figure 1.11</em> shows the grid world environment, and the goal of the agent is to reach state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong> without visiting the shaded states:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.11: Grid world environment</p>
    <p class="normal">An agent makes a decision (action) in the environment only based on the current state the agent is in and not based on the past state. So, we can formulate our environment as an MDP. We learned that the MDP consists of states, actions, transition probabilities, and a reward function. Now, let's learn how this relates to our RL environment:</p>
    <p class="normal"><strong class="keyword">States</strong> – A set of states<a id="_idIndexMarker033"/> present in the environment. Thus, in the grid world environment, we have states <strong class="keyword">A</strong> to <strong class="keyword">I</strong>.</p>
    <p class="normal"><strong class="keyword">Actions</strong> – A set of actions<a id="_idIndexMarker034"/> that our agent can perform in each state. An agent performs an action and moves from one state to another. Thus, in the grid world environment, the set of actions is <em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left</em>, and <em class="italic">right</em>.</p>
    <p class="normal"><strong class="keyword">Transition probability</strong> – The transition probability is denoted by <img src="../Images/B15558_01_004.png" alt="" style="height: 1.2em;"/>. It implies the probability of moving from a state <em class="italic">s</em> to the<a id="_idIndexMarker035"/> next state <img src="../Images/B15558_01_005.png" alt="" style="height: 1.2em;"/> while performing an action <em class="italic">a</em>. If you observe, in the MRP, the transition probability is just <img src="../Images/B15558_01_003.png" alt="" style="height: 1.2em;"/>, that is, the probability of going from state <em class="italic">s</em> to state <img src="../Images/B15558_01_005.png" alt="" style="height: 1.2em;"/>, and it doesn't include actions. But in the MDP, we include the actions, and thus the transition probability is denoted by <img src="../Images/B15558_01_004.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">For example, in our grid world environment, say the <a id="_idIndexMarker036"/>transition probability of moving from state <strong class="keyword">A</strong> to state <strong class="keyword">B</strong> while performing an action <em class="italic">right</em> is 100%. This can be expressed as <em class="italic">P</em>(<em class="italic">B</em>|<em class="italic">A</em>, right) = 1.0. We can also view this in the state diagram, as shown in <em class="italic">Figure 1.12</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.12: Transition probability of moving right from A to B</p>
    <p class="normal">Suppose our agent is in state <strong class="keyword">C</strong> and the transition probability of moving from state <strong class="keyword">C </strong>to state <strong class="keyword">F</strong> while performing the action <em class="italic">down</em> is 90%, then it can be expressed as <em class="italic">P</em>(<em class="italic">F</em>|<em class="italic">C</em>, down) = 0.9. We can also view this in the state diagram, as shown in <em class="italic">Figure 1.13</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.13: Transition probability of moving down from C to F</p>
    <p class="normal"><strong class="keyword">Reward function</strong> - The reward function<a id="_idIndexMarker037"/> is denoted by <img src="../Images/B15558_01_009.png" alt="" style="height: 1.2em;"/>. It represents the reward our agent obtains while transitioning from state <em class="italic">s</em> to state <img src="../Images/B15558_01_005.png" alt="" style="height: 1.2em;"/> while performing an action <em class="italic">a</em>.</p>
    <p class="normal">Say the reward we obtain while transitioning from state <strong class="keyword">A</strong> to state <strong class="keyword">B</strong> while performing the action <em class="italic">right</em> is -1, then it can be expressed as <em class="italic">R</em>(<em class="italic">A</em>, right, <em class="italic">B</em>) = -1. We can also view this in the state diagram, as shown in <em class="italic">Figure 1.14</em>: </p>
    <figure class="mediaobject"><img src="../Images/B15558_01_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.14: Reward of moving right from A to B</p>
    <p class="normal">Suppose our agent is in state <strong class="keyword">C</strong> and say the <a id="_idIndexMarker038"/>reward we obtain while transitioning from state <strong class="keyword">C</strong> to state <strong class="keyword">F</strong> while performing the action <em class="italic">down</em> is +1, then it can be expressed as <em class="italic">R</em>(<em class="italic">C</em>, down, <em class="italic">F</em>) = +1. We can also view this in the state diagram, as shown in <em class="italic">Figure 1.15</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.15: Reward of moving down from C to F</p>
    <p class="normal">Thus, an RL environment can be represented as an MDP with states, actions, transition probability, and the reward function. But wait! What is the use of representing the RL environment using the MDP? We can solve the RL problem easily once we model our environment as the MDP. For instance, once we model our grid world environment using the MDP, then we can easily find how to reach the goal state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong> without visiting the shaded states. We will learn more about this in the upcoming chapters. Next, we will go through more essential concepts of RL.</p>
    <h1 id="_idParaDest-28" class="title">Fundamental concepts of RL</h1>
    <p class="normal">In this section, we will learn about several important fundamental RL concepts.</p>
    <h2 id="_idParaDest-29" class="title">Math essentials</h2>
    <p class="normal">Before going ahead, let's quickly <a id="_idIndexMarker039"/>recap expectation from our high school days, as we will be dealing with expectation throughout the book.</p>
    <h3 id="_idParaDest-30" class="title">Expectation</h3>
    <p class="normal">Let's say we have a<a id="_idIndexMarker040"/> variable <em class="italic">X</em> and it<a id="_idIndexMarker041"/> has the values 1, 2, 3, 4, 5, 6. To compute the average value of <em class="italic">X</em>, we can just sum all the values of <em class="italic">X</em> divided by the number of values of <em class="italic">X</em>. Thus, the average of <em class="italic">X</em> is (1+2+3+4+5+6)/6 = 3.5.</p>
    <p class="normal">Now, let's suppose <em class="italic">X</em> is a random variable. The random variable takes values based on a random experiment, such as throwing dice, tossing a coin, and so on. The random variable takes different values with some probabilities. Let's suppose we are throwing a fair dice, then the possible outcomes (<em class="italic">X</em>) are 1, 2, 3, 4, 5, and 6 and the probability of occurrence of each of these outcomes is 1/6, as shown in <em class="italic">Table 1.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_17.png" alt=""/></figure>
    <p class="packt_figref">Table 1.2: Probabilities of throwing a dice</p>
    <p class="normal">How can we compute the average value of the random variable <em class="italic">X</em>? Since each value has a probability of an occurrence, we can't just take the average. So, instead, we compute the weighted average, that is, the sum of values of <em class="italic">X</em> multiplied by their respective probabilities, and this is called expectation. The expectation of a random variable <em class="italic">X</em> can be defined as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_011.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Thus, the expectation of<a id="_idIndexMarker042"/> the random variable <em class="italic">X</em> is <em class="italic">E</em>(<em class="italic">X</em>) = 1(1/6) + 2(1/6) + 3(1/6) + 4(1/6) + 5 (1/6) + 6(1/6) = 3.5.</p>
    <p class="normal">The expectation is also<a id="_idIndexMarker043"/> known as the expected value. Thus, the expected value of the random variable <em class="italic">X</em> is 3.5. Thus, when we say expectation or the expected value of a random variable, it basically means the weighted average.</p>
    <p class="normal">Now, we will look into the expectation of a function of a random variable. Let <img src="../Images/B15558_01_012.png" alt="" style="height: 1.2em;"/>, then we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_18.png" alt=""/></figure>
    <p class="packt_figref">Table 1.3: Probabilities of throwing a dice</p>
    <p class="normal">The expectation of a function of a random variable can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_013.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Thus, the expected value of <em class="italic">f</em>(<em class="italic">X</em>) is given as <em class="italic">E</em>(<em class="italic">f</em>(<em class="italic">X</em>)) = 1(1/6) + 4(1/6) + 9(1/6) + 16(1/6) + 25(1/6) + 36(1/6) = 15.1.</p>
    <h2 id="_idParaDest-31" class="title">Action space</h2>
    <p class="normal">Consider <a id="_idIndexMarker044"/>the grid world <a id="_idIndexMarker045"/>environment shown in <em class="italic">Figure 1.16</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.16: Grid world environment</p>
    <p class="normal">In the preceding grid world environment, the goal of the agent is to reach state <strong class="keyword">I</strong> starting from state <strong class="keyword">A</strong> without visiting the shaded states. In each of the states, the agent can perform any of the four actions—<em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left,</em> and <em class="italic">right</em>—to achieve the goal. The set of all possible actions in the environment is called the action space. Thus, for this grid world environment, the action space will be [<em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left</em>, <em class="italic">right</em>].</p>
    <p class="normal">We can categorize action spaces into two types:</p>
    <ul>
      <li class="bullet">Discrete action space</li>
      <li class="bullet">Continuous action space</li>
    </ul>
    <p class="normal"><strong class="keyword">Discrete action space</strong>: When our<a id="_idIndexMarker046"/> action space consists of actions that are discrete, then it is called a discrete action space. For instance, in the grid world environment, our action space consists of four discrete actions, which are up, down, left, right, and so it is called a discrete action space.</p>
    <p class="normal"><strong class="keyword">Continuous action space</strong>: When our <a id="_idIndexMarker047"/>action space consists of actions that are continuous, then it is called a continuous action space. For instance, let's suppose we are training an agent to drive a car, then our action space will consist of several actions that have continuous values, such as the speed at which we need to drive the car, the number of degrees we need to rotate the wheel, and so on. In cases where our action space consists of actions that are continuous, it is called a continuous action space.</p>
    <h2 id="_idParaDest-32" class="title">Policy</h2>
    <p class="normal">A policy <a id="_idIndexMarker048"/>defines the <a id="_idIndexMarker049"/>agent's behavior in an environment. The policy tells the agent what action to perform in each state. For instance, in the grid world environment, we have states <strong class="keyword">A</strong> to <strong class="keyword">I</strong> and four possible actions. The policy may tell the agent to move <em class="italic">down</em> in state <strong class="keyword">A</strong>, move <em class="italic">right</em> in state <strong class="keyword">D</strong>, and so on.</p>
    <p class="normal">To interact with the environment for the first time, we initialize a random policy, that is, the random policy tells the agent to perform a random action in each state. Thus, in an initial iteration, the agent performs a random action in each state and tries to learn whether the action is good or bad based on the reward it obtains. Over a series of iterations, an agent will learn to perform good actions in each state, which gives a positive reward. Thus, we can say that over a series of iterations, the agent will learn a good policy that gives a positive reward.</p>
    <p class="normal">This good policy is called the optimal policy. The optimal policy is the policy that gets the agent a good reward and helps the agent to achieve the goal. For instance, in our grid world environment, the optimal policy tells the agent to perform an action in each state such that the agent can reach state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong> without visiting the shaded states.</p>
    <p class="normal">The optimal policy is shown in <em class="italic">Figure 1.17</em>. As we can observe, the agent selects the action in each state based on the optimal policy and reaches the terminal state <strong class="keyword">I</strong> from the starting state <strong class="keyword">A</strong> without visiting the shaded states:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.17: The optimal policy in the grid world environment </p>
    <p class="normal">Thus, the optimal policy tells the agent to perform the correct action in each state so that the agent can receive a good reward.</p>
    <p class="normal">A policy can be classified as the following:</p>
    <ul>
      <li class="bullet">A deterministic policy</li>
      <li class="bullet">A stochastic policy</li>
    </ul>
    <h3 id="_idParaDest-33" class="title">Deterministic policy</h3>
    <p class="normal">The policy that we just covered is called a<a id="_idIndexMarker050"/> deterministic policy. A deterministic policy<a id="_idIndexMarker051"/> tells the agent to perform one particular action in a state. Thus, the deterministic policy maps the state to one particular action and is often denoted by <img src="../Images/B15558_01_014.png" alt="" style="height: 0.84em;"/>. Given a state <em class="italic">s</em> at a time <em class="italic">t</em>, a deterministic policy tells the agent to perform one particular action <em class="italic">a</em>. It can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_015.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">For instance, consider our grid world example. Given state <strong class="keyword">A</strong>, the deterministic policy <img src="../Images/B15558_01_016.png" alt="" style="height: 0.84em;"/> tells the agent to perform the action <em class="italic">down</em>. This can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_017.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Thus, according to the deterministic policy, whenever the agent visits state <strong class="keyword">A</strong>, it performs the action <em class="italic">down</em>.</p>
    <h3 id="_idParaDest-34" class="title">Stochastic policy</h3>
    <p class="normal">Unlike a deterministic policy, a stochastic policy<a id="_idIndexMarker052"/> does not map a state directly to one <a id="_idIndexMarker053"/>particular action; instead, it maps the state to a probability distribution over an action space.</p>
    <p class="normal">That is, we learned that given a state, the deterministic policy will tell the agent to perform one particular action in the given state, so whenever the agent visits the state it always performs the same particular action. But with a stochastic policy, given a state, the stochastic policy will return a probability distribution over an action space. So instead of performing the same action every time the agent visits the state, the agent performs different actions each time based on a probability distribution returned by the stochastic policy.</p>
    <p class="normal">Let's understand this with an example; we know that our grid world environment's action space consists of four actions, which are [<em class="italic">up, down, left, right</em>]. Given a state <strong class="keyword">A</strong>, the stochastic policy returns the probability distribution over the action space as [0.10,0.70,0.10,0.10]. Now, whenever the agent visits state <strong class="keyword">A</strong>, instead of selecting the same particular action every time, the agent selects <em class="italic">up</em> 10% of the time, <em class="italic">down</em> 70% of the time, <em class="italic">left</em> 10% of the time, and <em class="italic">right</em> 10% of the time.</p>
    <p class="normal">The difference between the<a id="_idIndexMarker054"/> deterministic policy and stochastic policy is shown in <em class="italic">Figure 1.18</em>. As we<a id="_idIndexMarker055"/> can observe, the deterministic policy maps the state to one particular action, whereas the stochastic policy maps the state to the probability distribution over an action space:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.18: The difference between deterministic and stochastic policies</p>
    <p class="normal">Thus, the stochastic policy maps the state to a probability distribution over the action space and is often denoted by <img src="../Images/B15558_01_018.png" alt="" style="height: 0.84em;"/>. Say we have a state <em class="italic">s</em> and action <em class="italic">a</em> at a time <em class="italic">t</em>, then we can express the stochastic policy as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_019.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Or it can also be expressed as <img src="../Images/B15558_01_020.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">We can categorize the stochastic policy into two types:</p>
    <ul>
      <li class="bullet">Categorical policy</li>
      <li class="bullet">Gaussian policy</li>
    </ul>
    <h4 class="title">Categorical policy</h4>
    <p class="normal">A stochastic policy is called a categorical policy<a id="_idIndexMarker056"/> when the action space is discrete. That is, the <a id="_idIndexMarker057"/>stochastic policy uses a categorical probability distribution over the action space to select actions when the action space is discrete. For instance, in the grid world environment from the previous example, we select actions based on a categorical probability distribution (discrete distribution) as the action space of the environment is discrete. As <em class="italic">Figure 1.19 </em>shows, given state <strong class="keyword">A</strong>, we select an action based on the categorical probability distribution over the action space:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.19: Probability of next move from state A for a discrete action space</p>
    <h4 class="title">Gaussian policy</h4>
    <p class="normal">A stochastic policy is called a Gaussian policy<a id="_idIndexMarker058"/> when our action space is continuous. That is, the<a id="_idIndexMarker059"/> stochastic policy uses a Gaussian probability distribution over the action space to select actions when the action space is continuous. Let's understand this with a simple example. Suppose we are training an agent to drive a car and say we have one continuous action in our action space. Let the action be the speed of the car, and the value of the speed of the car ranges from 0 to 150 kmph. Then, the stochastic policy uses the Gaussian distribution over<a id="_idIndexMarker060"/> the action <a id="_idIndexMarker061"/>space to select an action, as <em class="italic">Figure 1.20</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_23.png" alt=""/> </figure>
    <p class="packt_figref">Figure 1.20: Gaussian distribution</p>
    <p class="normal">We will learn more about the Gaussian policy in the upcoming chapters.</p>
    <h2 id="_idParaDest-35" class="title">Episode</h2>
    <p class="normal">The agent interacts <a id="_idIndexMarker062"/>with the environment by performing some actions, starting from the initial state and<a id="_idIndexMarker063"/> reaches the final state. This agent-environment interaction starting from the initial state until the final state is called an episode. For instance, in a car racing video game, the agent plays the game by starting from the initial state (the starting point of the race) and reaches the final state (the endpoint of the race). This is considered an episode. An episode is also often called a trajectory (the path taken by the agent) and it is denoted by <img src="../Images/B15558_01_021.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">An agent can play the game for any number of episodes, and each episode is independent of the others. What is the use of playing the game for multiple episodes? In order to learn the optimal policy, that is, the policy that tells the agent to perform the correct action in each state, the agent plays the game for many episodes.</p>
    <p class="normal">For example, let's say we are playing a car racing game; the first time, we may not win the game, so we play the game several times to understand more about the game and discover some good strategies for winning the game. Similarly, in the first episode, the agent may not win the game and it plays the game for several episodes to understand more about the game environment and good strategies to win the game.</p>
    <p class="normal">Say we begin the game from an initial state at a time step <em class="italic">t</em> = 0 and reach the final state at a time step <em class="italic">T</em>, then the episode information consists of the agent-environment interaction, such as state, action, and reward, starting from the initial state until the final state, that is, (<em class="italic">s</em><sub class="Subscript--PACKT-">0</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">0</sub>, <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub>, <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">r</em><sub class="Subscript--PACKT-">1</sub>,…,<em class="italic">s</em><sub class="" style="font-style: italic;">T</sub>).</p>
    <p class="normal"><em class="italic">Figure 1.21</em> shows an example of an episode/trajectory:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_24.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.21: An example of an episode</p>
    <p class="normal">Let's strengthen <a id="_idIndexMarker064"/>our understanding of the episode and the optimal policy with the grid world<a id="_idIndexMarker065"/> environment. We learned that in the grid world environment, the goal of our agent is to reach the final state <strong class="keyword">I</strong> starting from the initial state <strong class="keyword">A</strong> without visiting the shaded states. An agent receives a +1 reward when it visits the unshaded states and a -1 reward when it visits the shaded states.</p>
    <p class="normal">When we say generate an episode, it means going from the initial state to the final state. The agent generates the first episode using a random policy and explores the environment and over several episodes, it will learn the optimal policy.</p>
    <p class="normal"><strong class="keyword">Episode 1</strong></p>
    <p class="normal">As the <em class="italic">Figure 1.22</em> shows, in the first episode, the agent uses a random policy and selects a random action in each state from the initial state until the final state and observes the reward:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_25.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.22: Episode 1</p>
    <p class="normal"><strong class="keyword">Episode 2</strong></p>
    <p class="normal">In the second episode, the <a id="_idIndexMarker066"/>agent tries a different policy to avoid the negative rewards it <a id="_idIndexMarker067"/>received in the previous episode. For instance, as we can observe in the previous episode, the agent selected the action <em class="italic">right</em> in state <strong class="keyword">A</strong> and received a negative reward, so in this episode, instead of selecting the action <em class="italic">right</em> in state <strong class="keyword">A</strong>, it tries a different action, say <em class="italic">down, </em>as shown in <em class="italic">Figure 1.23</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_26.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.23: Episode 2</p>
    <p class="normal"><strong class="keyword">Episode n</strong></p>
    <p class="normal">Thus, over a series of episodes, the agent learns the optimal policy, that is, the policy that takes the agent to the final state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong> without visiting the shaded states, as <em class="italic">Figure 1.24</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_27.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.24: Episode n</p>
    <h2 id="_idParaDest-36" class="title">Episodic and continuous tasks</h2>
    <p class="normal">An RL task can be categorized as:</p>
    <ul>
      <li class="bullet">An episodic task</li>
      <li class="bullet">A continuous task</li>
    </ul>
    <p class="normal"><strong class="keyword">Episodic task</strong>: As the name suggests, an episodic task is one that has a terminal/final state. That is, episodic tasks<a id="_idIndexMarker068"/> are tasks<a id="_idIndexMarker069"/> made up of episodes and thus they have a terminal state. For example, in a car racing game, we start from the starting point (initial state) and reach the destination (terminal state).</p>
    <p class="normal"><strong class="keyword">Continuous task</strong>: Unlike episodic tasks, continuous tasks<a id="_idIndexMarker070"/> do not contain any episodes and so they don't have <a id="_idIndexMarker071"/>any terminal state. For example, a personal assistance robot does not have a terminal state. </p>
    <h2 id="_idParaDest-37" class="title">Horizon</h2>
    <p class="normal">Horizon is <a id="_idIndexMarker072"/>the time step until which the agent interacts with the environment. We can classify the horizon<a id="_idIndexMarker073"/> into two categories:</p>
    <ul>
      <li class="bullet">Finite horizon</li>
      <li class="bullet">Infinite horizon</li>
    </ul>
    <p class="normal"><strong class="keyword">Finite horizon</strong>: If the<a id="_idIndexMarker074"/> agent-environment interaction stops at a particular time step, then the horizon is called a finite horizon. For instance, in episodic tasks, an agent interacts with the environment by starting from the initial state at time step <em class="italic">t</em> = 0 and reaches the final state at time step <em class="italic">T</em>. Since the agent-environment interaction stops at time step <em class="italic">T</em>, it is considered a finite horizon.</p>
    <p class="normal"><strong class="keyword">Infinite horizon</strong>: If<a id="_idIndexMarker075"/> the agent-environment interaction never stops, then it is called an infinite horizon. For instance, we learned that a continuous task has no terminal states. This means the agent-environment interaction will never stop in a continuous task and so it is considered an infinite horizon.</p>
    <h2 id="_idParaDest-38" class="title">Return and discount factor</h2>
    <p class="normal">A return<a id="_idIndexMarker076"/> can be defined as the sum of the rewards obtained by the agent in an episode. The return <a id="_idIndexMarker077"/>is often denoted by <em class="italic">R</em> or <em class="italic">G</em>. Say the agent starts from the initial state at time step <em class="italic">t</em> = 0 and reaches the final state at time step <em class="italic">T</em>, then the return obtained by the agent is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_022.png" alt="" style="height: 1.11em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_01_023.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Let's understand this with an example; consider the trajectory (episode) <img src="../Images/B15558_01_024.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_28.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.25: Trajectory/episode <img src="../Images/B15558_01_025.png" alt="" style="height: 0.84em;"/> </p>
    <p class="normal">The return of the trajectory is the sum of the rewards, that is, <img src="../Images/B15558_01_026.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Thus, we can say that the goal of our agent is to maximize the return, that is, maximize the sum of rewards (cumulative rewards) obtained over the episode. How can we maximize the <a id="_idIndexMarker078"/>return? We can maximize the return if we perform the correct action in each state. Okay, how can we perform the correct action in each state? We can<a id="_idIndexMarker079"/> perform the correct action in each state by using the optimal policy. Thus, we can maximize the return using the optimal policy. Thus, the optimal policy is the policy that gets our agent the maximum return (sum of rewards) by performing the correct action in each state.</p>
    <p class="normal">Okay, how can we define the return for continuous tasks? We learned that in continuous tasks there are no terminal states, so we can define the return as a sum of rewards up to infinity:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_027.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">But how can we maximize the return that just sums to infinity? We introduce a new term called discount factor <img src="../Images/B15558_01_028.png" alt="" style="height: 0.93em;"/> and rewrite our return as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_029.png" alt="" style="height: 1.2em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_01_030.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">Okay, but how is this<a id="_idIndexMarker080"/> discount factor <img src="../Images/B15558_01_031.png" alt="" style="height: 0.93em;"/> helping us? It helps us in <a id="_idIndexMarker081"/>preventing the return from reaching infinity by deciding how much importance we give to future rewards and immediate rewards. The value of the discount factor ranges from 0 to 1. When we set the discount factor to a small value (close to 0), it implies that we give more importance to immediate rewards than to future rewards. When we set the discount factor to a high value (close to 1), it implies that we give more importance to future rewards than to immediate rewards. Let's understand this with an example with different discount factor values.</p>
    <h3 id="_idParaDest-39" class="title">Small discount factor</h3>
    <p class="normal">Let's set the <a id="_idIndexMarker082"/>discount factor to a small value, say 0.2, that is, let's set <img src="../Images/B15558_01_032.png" alt="" style="height: 1.11em;"/>, then we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_033.png" alt="" style="height: 3.8em;"/></figure>
    <p class="normal">From this equation, we can observe that the reward at each time step is weighted by a discount factor. As the time steps increase, the discount factor (weight) decreases and thus the importance of rewards at future time steps also decreases. That is, from the equation, we can observe that:</p>
    <ul>
      <li class="bullet">At time step 0, the reward <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub> is weighted by a discount factor of 1.</li>
      <li class="bullet">At time step 1, the reward <em class="italic">r</em><sub class="Subscript--PACKT-">1</sub> is weighted by a heavily decreased discount factor of 0.2.</li>
      <li class="bullet">At time step 2, the reward <em class="italic">r</em><sub class="Subscript--PACKT-">2</sub> is weighted by a heavily decreased discount factor of 0.04.</li>
    </ul>
    <p class="normal">As we can observe, the discount factor is heavily decreased for the subsequent time steps and more importance is given to the immediate reward <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub> than the rewards obtained at the future time steps. Thus, when we set the discount factor to a small value, we give more importance to the immediate reward than future rewards.</p>
    <h3 id="_idParaDest-40" class="title">Large discount factor</h3>
    <p class="normal">Let's set the <a id="_idIndexMarker083"/>discount factor to a high value, say 0.9, that is, let's set, <img src="../Images/B15558_01_034.png" alt="" style="height: 1.11em;"/>, then we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_035.png" alt="" style="height: 3.8em;"/></figure>
    <p class="normal">From this equation, we can infer that as the time step increases the discount factor (weight) decreases; however, it is not decreasing heavily (unlike the previous case) since here we started off with <img src="../Images/B15558_01_036.png" alt="" style="height: 1.11em;"/>. So, in this case, we can say that we give more importance to future rewards. That is, from the equation, we can observe that:</p>
    <ul>
      <li class="bullet">At time step 0, the reward <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub> is weighted by a discount factor of 1.</li>
      <li class="bullet">At time step 1, the reward <em class="italic">r</em><sub class="Subscript--PACKT-">1</sub> is weighted by a slightly decreased discount factor of 0.9.</li>
      <li class="bullet">At time step 2, the reward <em class="italic">r</em><sub class="Subscript--PACKT-">2</sub> is weighted by a slightly decreased discount factor of 0.81.</li>
    </ul>
    <p class="normal">As we can observe, the<a id="_idIndexMarker084"/> discount factor is decreased for subsequent time steps but unlike the previous case, the discount factor is not decreased heavily. Thus, when we set the discount factor to a high value, we give more importance to future rewards than the immediate reward.</p>
    <h3 id="_idParaDest-41" class="title">What happens when we set the discount factor to 0?</h3>
    <p class="normal">When we <a id="_idIndexMarker085"/>set the discount factor to 0, that is <img src="../Images/B15558_01_037.png" alt="" style="height: 1.11em;"/>, it implies that we consider only the immediate reward <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub> and not the reward obtained from the future time steps. Thus, when we set the discount factor to 0, then the agent will never learn as it will consider only the immediate reward <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_038.png" alt="" style="height: 5em;"/></figure>
    <p class="normal">As we can observe, when we set <img src="../Images/B15558_01_039.png" alt="" style="height: 1.11em;"/>, our return will be just the immediate reward <em class="italic">r</em><sub class="Subscript--PACKT-">0</sub>.</p>
    <h3 id="_idParaDest-42" class="title">What happens when we set the discount factor to 1?</h3>
    <p class="normal">When<a id="_idIndexMarker086"/> we set the discount factor to 1, that is <img src="../Images/B15558_01_040.png" alt="" style="height: 1.11em;"/>, it implies that we consider all the future rewards. Thus, when we set the discount factor to 1, then the agent will learn forever, looking for all the future rewards, which may lead to infinity, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_041.png" alt="" style="height: 3.71em;"/></figure>
    <p class="normal">As we can observe, when we set <img src="../Images/B15558_01_042.png" alt="" style="height: 1.11em;"/>, then our return will be the sum of rewards up to infinity.</p>
    <p class="normal">Thus, we have learned that when we set the discount factor to 0, the agent will never learn, considering only the immediate reward, and when we set the discount factor to 1 the agent will learn forever, looking for the future rewards that lead to infinity. So, the optimal value of the discount factor lies between 0.2 and 0.8.</p>
    <p class="normal">But the question is, why<a id="_idIndexMarker087"/> should we care about immediate and future rewards? We give importance to immediate and future rewards depending on the tasks. In some tasks, future rewards are more desirable than immediate rewards, and vice versa. In a chess game, the goal is to defeat the opponent's king. If we give more importance to the immediate reward, which is acquired by actions such as our pawn defeating any opposing chessman, then the agent will learn to perform this sub-goal instead of learning the actual goal. So, in this case, we give greater importance to future rewards than the immediate reward, whereas in some cases, we prefer immediate rewards over future rewards. Would you prefer chocolates if I gave them to you today or 13 days later?</p>
    <p class="normal">In the following two sections, we'll analyze the two fundamental functions of RL.</p>
    <h2 id="_idParaDest-43" class="title">The value function</h2>
    <p class="normal">The value function, also called the <a id="_idIndexMarker088"/>state value function, denotes the value of the state. The value of a<a id="_idIndexMarker089"/> state is the return an agent would obtain starting from that state following policy <img src="../Images/B15558_01_043.png" alt="" style="height: 0.84em;"/>. The value of a state or value function is usually denoted by <em class="italic">V</em>(<em class="italic">s</em>) and it can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_044.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">where <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> = <em class="italic">s</em> implies that the starting state is <em class="italic">s</em>. The value of a state is called the state value.</p>
    <p class="normal">Let's understand the<a id="_idIndexMarker090"/> value function with an example. Let's suppose we generate the<a id="_idIndexMarker091"/> trajectory <img src="../Images/B15558_01_045.png" alt="" style="height: 0.84em;"/> following some policy <img src="../Images/B15558_01_046.png" alt="" style="height: 0.84em;"/> in our grid world environment, as shown in <em class="italic">Figure 1.26</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_29.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.26: A value function example</p>
    <p class="normal">Now, how do we compute the value of all the states in our trajectory? We learned that the value of a state is the return (sum of rewards) an agent would obtain starting from that state following policy <img src="../Images/B15558_01_046.png" alt="" style="height: 0.84em;"/>. The preceding trajectory is generated using policy <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/>, thus we can say that the value of a state is the return (sum of rewards) of the trajectory starting from that state:</p>
    <ul>
      <li class="bullet">The value of state <strong class="keyword">A</strong> is the return of the trajectory starting from state <strong class="keyword">A</strong>. Thus, <em class="italic">V</em>(<em class="italic">A</em>) = 1+1+ -1+1 = 2.</li>
      <li class="bullet">The value of state <strong class="keyword">D</strong> is the return of the trajectory starting from state <strong class="keyword">D</strong>. Thus, <em class="italic">V</em>(<em class="italic">D</em>) = 1-1+1= 1.</li>
      <li class="bullet">The value of state <strong class="keyword">E</strong> is the return of the trajectory starting from state <strong class="keyword">E</strong>. Thus, <em class="italic">V</em>(<em class="italic">E</em>) = -1+1 = 0.</li>
      <li class="bullet">The value of state <strong class="keyword">H</strong> is the return of the trajectory starting from state <strong class="keyword">H</strong>. Thus, <em class="italic">V</em>(<em class="italic">H</em>) = 1.</li>
    </ul>
    <p class="normal">What about the <a id="_idIndexMarker092"/>value of the final state <strong class="keyword">I</strong>? We learned the value of a state is the <a id="_idIndexMarker093"/>return (sum of rewards) starting from that state. We know that we obtain a reward when we transition from one state to another. Since <strong class="keyword">I</strong> is the final state, we don't make any transition from the final state, so there is no reward and thus no value for the final state <strong class="keyword">I</strong>.</p>
    <p class="normal">In a nutshell, the value of a state is the return of the trajectory starting from that state.</p>
    <p class="normal">Wait! There is a small change here: instead of taking the return directly as a value of a state, we will use the expected return. Thus, the value function or the value of state <em class="italic">s</em> can be defined as the expected return that the agent would obtain starting from state <em class="italic">s</em> following policy <img src="../Images/B15558_01_046.png" alt="" style="height: 0.84em;"/>. It can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_050.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Now, the question is why expected return? Why we can't we just compute the value of a state as a return directly? Because our return is the random variable and it takes different values with some probability. </p>
    <p class="normal">Let's understand this with a simple example. Suppose we have a stochastic policy <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/>. We learned that unlike the deterministic policy, which maps the state to the action directly, the stochastic policy maps the state to the probability distribution over the action space. Thus, the stochastic policy selects actions based on a probability distribution.</p>
    <p class="normal">Let's suppose we are in state <strong class="keyword">A</strong> and the stochastic policy returns the probability distribution over the action space as [0.0,0.80,0.00,0.20]. It implies that with the stochastic policy, in state <strong class="keyword">A</strong>, we perform the action <em class="italic">down</em> 80% of the time, that is, <img src="../Images/B15558_01_052.png" alt="" style="height: 1.11em;"/>, and the action <em class="italic">right</em> 20% of the time, that is <img src="../Images/B15558_01_053.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Thus, in state <strong class="keyword">A</strong>, our<a id="_idIndexMarker094"/> stochastic policy <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/> selects the action <em class="italic">down</em> 80% of the time and the action <em class="italic">right</em> 20% of the time, and say our stochastic policy<a id="_idIndexMarker095"/> selects the action <em class="italic">right</em> in states <strong class="keyword">D</strong> and <strong class="keyword">E</strong> and the action <em class="italic">down</em> in states <strong class="keyword">B</strong> and <strong class="keyword">F</strong> 100% of the time.</p>
    <p class="normal">First, we generate an episode <img src="../Images/B15558_01_055.png" alt="" style="height: 0.84em;"/> using our stochastic policy <img src="../Images/B15558_01_056.png" alt="" style="height: 0.84em;"/>, as shown in <em class="italic">Figure 1.27</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_30.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.27: Episode <img src="../Images/B15558_01_057.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">For better understanding, let's focus only on the value of state <strong class="keyword">A</strong>. The value of state <strong class="keyword">A</strong> is the return (sum of rewards) of the trajectory starting from state <strong class="keyword">A</strong>. Thus, <img src="../Images/B15558_01_058.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Say we generate another episode <img src="../Images/B15558_01_059.png" alt="" style="height: 0.84em;"/> using the same given stochastic policy <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/>, as shown in <em class="italic">Figure 1.28</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_31.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.28: Episode <img src="../Images/B15558_01_061.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">The value of state <strong class="keyword">A</strong> is the return (sum of rewards) of the trajectory from state <strong class="keyword">A</strong>. Thus, <img src="../Images/B15558_01_062.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">As you may<a id="_idIndexMarker096"/> observe, although we use the same policy, the values of state <strong class="keyword">A</strong> in<a id="_idIndexMarker097"/> trajectories <img src="../Images/B15558_01_063.png" alt="" style="height: 0.84em;"/> and <img src="../Images/B15558_01_064.png" alt="" style="height: 0.84em;"/> are different. This is because our policy is a stochastic policy and it performs the action <em class="italic">down</em> in state <strong class="keyword">A</strong> 80% of the time and the action <em class="italic">right</em> in state <strong class="keyword">A</strong> 20% of the time. So, when we generate a trajectory using policy <img src="../Images/B15558_01_065.png" alt="" style="height: 0.84em;"/>, the trajectory <img src="../Images/B15558_01_066.png" alt="" style="height: 0.84em;"/> will occur 80% of the time and the trajectory <img src="../Images/B15558_01_067.png" alt="" style="height: 0.84em;"/> will occur 20% of the time. Thus, the return will be 4 for 80% of the time and 2 for 20% of the time.</p>
    <p class="normal">Thus, instead of taking the value of the state as a return directly, we will take the expected return, since the return takes different values with some probability. The expected return is basically the weighted average, that is, the sum of the return multiplied by their probability. Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_068.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">The value of a state <strong class="keyword">A</strong> can be obtained as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_069.png" alt="" style="height: 2.04em;"/></figure>
    <figure class="mediaobject"> <img src="../Images/B15558_01_070.png" alt="" style="height: 2.69em;"/></figure>
    <figure class="mediaobject"> <img src="../Images/B15558_01_071.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Thus, the <a id="_idIndexMarker098"/>value of a state is the expected return of the trajectory starting from that state.</p>
    <p class="normal">Note that the<a id="_idIndexMarker099"/> value function depends on the policy, that is, the value of the state varies based on the policy we choose. There can be many different value functions according to different policies. The optimal value function <em class="italic">V</em>*(<em class="italic">s</em>) yields the maximum value compared to all the other value functions. It can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_072.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">For example, let's say we have two policies <img src="../Images/B15558_01_073.png" alt="" style="height: 0.84em;"/> and <img src="../Images/B15558_01_074.png" alt="" style="height: 0.84em;"/>. Let the value of state <em class="italic">s</em> using policy <img src="../Images/B15558_01_075.png" alt="" style="height: 0.84em;"/> be <img src="../Images/B15558_01_076.png" alt="" style="height: 1.11em;"/> and the value of state <em class="italic">s</em> using policy <img src="../Images/B15558_01_077.png" alt="" style="height: 0.84em;"/> be <img src="../Images/B15558_01_078.png" alt="" style="height: 1.11em;"/>. Then the optimal value of state <em class="italic">s</em> will be <img src="../Images/B15558_01_079.png" alt="" style="height: 1.11em;"/> as it is the maximum. The policy that gives the maximum state value is called the optimal policy <img src="../Images/B15558_01_080.png" alt="" style="height: 1.11em;"/>. Thus, in this case, <img src="../Images/B15558_01_075.png" alt="" style="height: 0.84em;"/> is the optimal policy as it gives the maximum state value.</p>
    <p class="normal">We can view the<a id="_idIndexMarker100"/> value function in a table called a value table. Let's say we have two <a id="_idIndexMarker101"/>states <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, then the value function can be represented as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_32.png" alt=""/></figure>
    <p class="packt_figref">Table 1.4: Value table</p>
    <p class="normal">From the value table, we can tell that it is better to be in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> than state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> as <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> has a higher value. Thus, we can say that state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> is the optimal state.</p>
    <h2 id="_idParaDest-44" class="title">Q function</h2>
    <p class="normal">A Q function, also<a id="_idIndexMarker102"/> called the state-action value function, denotes the<a id="_idIndexMarker103"/> value of a state-action pair. The value of a state-action pair is the return the agent would obtain starting from state <em class="italic">s</em> and performing action <em class="italic">a</em> following policy <img src="../Images/B15558_01_082.png" alt="" style="height: 0.84em;"/>. The value of a state-action pair or Q function is usually denoted by <em class="italic">Q</em>(<em class="italic">s</em>,<em class="italic">a</em>) and is known as the <em class="italic">Q</em> value or state-action value. It is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_083.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Note that the only difference between the value function and Q function is that in the value function we compute the value of a state, whereas in the Q function we compute the value of a state-action pair. Let's understand the Q function with an example. Consider the trajectory in <em class="italic">Figure 1.29</em> generated using policy <img src="../Images/B15558_01_046.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_33.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.29: A trajectory/episode example</p>
    <p class="normal">We learned that the <a id="_idIndexMarker104"/>Q function computes the value of a<a id="_idIndexMarker105"/> state-action pair. Say we need to compute the Q value of state-action pair <strong class="keyword">A</strong>-<em class="italic">down</em>. That is the Q value of moving <em class="italic">down</em> in state <strong class="keyword">A</strong>. Then the Q value will be the return of our trajectory starting from state <strong class="keyword">A</strong> and performing the action <em class="italic">down</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_085.png" alt="" style="height: 1.2em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_01_086.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Let's suppose we need to compute the Q value of the state-action pair <strong class="keyword">D</strong>-<em class="italic">right</em>. That is the Q value of moving <em class="italic">right</em> in state <strong class="keyword">D</strong>. The Q value will be the return of our trajectory starting from state <strong class="keyword">D</strong> and performing the action <em class="italic">right</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_087.png" alt="" style="height: 1.2em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_01_088.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Similarly, we can compute the Q value for all the state-action pairs. Similar to what we learned about the value function, instead of taking the return directly as the Q value of a state-action pair, we use the expected return because the return is the random variable and it takes different values with some probability. So, we can redefine our Q function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_089.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">It implies that the Q value is the expected return the agent would obtain starting from state <em class="italic">s</em> and performing action <em class="italic">a</em> following policy <img src="../Images/B15558_01_090.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">Similar to the value<a id="_idIndexMarker106"/> function, the Q function depends on the <a id="_idIndexMarker107"/>policy, that is, the Q value varies based on the policy we choose. There can be many different Q functions according to different policies. The optimal Q function is the one that has the maximum Q value over other Q functions, and it can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_091.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">The optimal policy <img src="../Images/B15558_01_092.png" alt="" style="height: 1.11em;"/> is the policy that gives the maximum Q value.</p>
    <p class="normal">Like the value function, the Q function can be viewed in a table. It is called a Q table. Let's say we have two states <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, and two actions 0 and 1; then the Q function can be represented as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_34.png" alt=""/></figure>
    <p class="packt_figref">Table 1.5: Q table</p>
    <p class="normal">As we can observe, the Q table represents the Q values of all possible state-action pairs. We learned that the optimal policy is the policy that gets our agent the maximum return (sum of rewards). We can extract the optimal policy from the Q table by just selecting the action that has the maximum <a id="_idIndexMarker108"/>Q value in each state. Thus, our optimal policy will select action 1 in <a id="_idIndexMarker109"/>state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and action 0 in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> since they have a high Q value, as shown in <em class="italic">Table 1.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_35.png" alt=""/></figure>
    <p class="packt_figref">Table 1.6: Optimal policy extracted from the Q table</p>
    <p class="normal">Thus, we can extract the optimal policy by computing the Q function.</p>
    <h2 id="_idParaDest-45" class="title">Model-based and model-free learning</h2>
    <p class="normal">Now, let's look into two different types of learning called model-based and model-free learning.</p>
    <p class="normal"><strong class="keyword">Model-based learning</strong>: In<a id="_idIndexMarker110"/> model-based learning, an agent will have a complete description of the environment. We know that the transition probability tells us the probability of moving from state <em class="italic">s</em> to the next state <img src="../Images/B15558_01_002.png" alt="" style="height: 1.2em;"/> by performing action <em class="italic">a</em>. The reward function tells us the reward we would obtain while moving from state <em class="italic">s</em> to the next state <img src="../Images/B15558_01_002.png" alt="" style="height: 1.2em;"/> by performing action <em class="italic">a</em>. When the agent knows the model dynamics of its environment, that is, when the agent knows the transition probability of its environment, then the learning is called model-based learning. Thus, in model-based learning, the agent uses the model dynamics to find the optimal policy.</p>
    <p class="normal"><strong class="keyword">Model-free learning</strong>: Model-free learning is<a id="_idIndexMarker111"/> when the agent does not know the model dynamics of its environment. That is, in model-free learning, an agent tries to find the optimal policy without the model dynamics.</p>
    <p class="normal">Next, we'll discover the different types of environment an agent works within.</p>
    <h2 id="_idParaDest-46" class="title">Different types of environments</h2>
    <p class="normal">At the beginning of the chapter, we learned <a id="_idIndexMarker112"/>that the environment is the world of the agent and the agent lives/stays within the environment. We can categorize the environment into different types.</p>
    <h3 id="_idParaDest-47" class="title">Deterministic and stochastic environments</h3>
    <p class="normal"><strong class="keyword">Deterministic environment</strong>: In a<a id="_idIndexMarker113"/> deterministic environment, we are certain that when an agent performs action <em class="italic">a</em> in state <em class="italic">s</em>, then it always reaches state <img src="../Images/B15558_01_002.png" alt="" style="height: 1.2em;"/>. For example, let's consider our grid world environment. Say the agent is in state <strong class="keyword">A</strong>, and when it moves <em class="italic">down</em> from state <strong class="keyword">A</strong>, it always reaches state <strong class="keyword">D</strong>. Hence the environment is called a deterministic environment:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_36.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.30: Deterministic environment</p>
    <p class="normal"><strong class="keyword">Stochastic environment</strong>: In a<a id="_idIndexMarker114"/> stochastic environment, we cannot say that by performing action <em class="italic">a</em> in state <em class="italic">s</em> the agent always reaches state <img src="../Images/Image59404.png" alt=""/> because there will be some randomness associated with the stochastic environment. For example, let's suppose our grid world environment is a stochastic environment. Say our agent is in state <strong class="keyword">A</strong>; now if it moves <em class="italic">down</em> from state <strong class="keyword">A</strong>, then the agent doesn't always reach state <strong class="keyword">D</strong>. Instead, it reaches state <strong class="keyword">D</strong> 70% of the time and state <strong class="keyword">B</strong> 30% of the time. That is, if the agent moves <em class="italic">down</em> in state <strong class="keyword">A</strong>, then the agent reaches state <strong class="keyword">D</strong> with 70% probability and state <strong class="keyword">B</strong> with 30% probability, as <em class="italic">Figure 1.31</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_01_37.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.31: Stochastic environment</p>
    <h3 id="_idParaDest-48" class="title">Discrete and continuous environments</h3>
    <p class="normal"><strong class="keyword">Discrete environment</strong>: A discrete environment<a id="_idIndexMarker115"/> is one where the environment's action space is discrete. For instance, in the grid world environment, we have a discrete action space, which consists of the actions [<em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left</em>, <em class="italic">right</em>] and thus our grid world environment is discrete.</p>
    <p class="normal"><strong class="keyword">Continuous environment</strong>: A continuous environment<a id="_idIndexMarker116"/> is one where the environment's action space is continuous. For instance, suppose we are training an agent to drive a car, then our action space will be continuous, with several continuous actions such as changing the car's speed, the number of degrees the agent needs to rotate the wheel, and so on. In such a case, our environment's action space is continuous.</p>
    <h3 id="_idParaDest-49" class="title">Episodic and non-episodic environments</h3>
    <p class="normal"><strong class="keyword">Episodic environment</strong>: In an episodic environment, an agent's current action will not affect future actions, and thus an <a id="_idIndexMarker117"/>episodic environment is also called a non-sequential environment.</p>
    <p class="normal"><strong class="keyword">Non-episodic environment</strong>: In a non-episodic environment, an agent's current action will affect future actions, and thus a <a id="_idIndexMarker118"/>non-episodic environment is also called a sequential environment. For example, a chessboard is a sequential environment since the agent's current action will affect future actions in a chess match.</p>
    <h3 id="_idParaDest-50" class="title">Single and multi-agent environments</h3>
    <ul>
      <li class="bullet"><strong class="keyword">Single-agent environment</strong>: When <a id="_idIndexMarker119"/>our environment consists of only a single agent, then it is called a single-agent environment.</li>
      <li class="bullet"><strong class="keyword">Multi-agent environment</strong>: When our environment consists of multiple agents, then it is called <a id="_idIndexMarker120"/>a multi-agent environment.</li>
    </ul>
    <p class="normal">We have covered a lot of concepts of RL. Now, we'll finish the chapter by looking at some exciting applications of RL.</p>
    <h1 id="_idParaDest-51" class="title">Applications of RL</h1>
    <p class="normal">RL has evolved rapidly<a id="_idIndexMarker121"/> over the past couple of years with a wide range of applications ranging from playing games to self-driving cars. One of the major reasons for this evolution is <a id="_idIndexMarker122"/>due to <strong class="keyword">Deep Reinforcement Learning</strong> (<strong class="keyword">DRL</strong>), which is a combination of RL and deep learning. We will learn about the various state-of-the-art deep RL algorithms in the upcoming chapters, so be excited! In this section, we will look at some real-life applications of RL:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Manufacturing</strong>: In manufacturing, intelligent robots are trained using RL to place objects in the right position. The use of intelligent robots reduces labor costs and increases productivity.</li>
      <li class="bullet"><strong class="keyword">Dynamic pricing</strong>: One of the popular applications of RL is dynamic pricing. Dynamic pricing implies that we change the price of products based on demand and supply. We can train the RL agent for the dynamic pricing of products with the goal of maximizing revenue.</li>
      <li class="bullet"><strong class="keyword">Inventory management</strong>: RL is used extensively in inventory management, which is a crucial business activity. Some of these activities include supply chain management, demand forecasting, and handling several warehouse operations (such as placing products in warehouses to manage space efficiently).</li>
      <li class="bullet"><strong class="keyword">Recommendation system</strong>: RL is widely used in building a recommendation system where the behavior of the user constantly changes. For instance, in music recommendation systems, the behavior or the music preferences of the user changes from time to time. So, in those cases using an RL agent can be very useful as the agent constantly learns by interacting with the environment.</li>
      <li class="bullet"><strong class="keyword">Neural architecture search</strong>: In order for a neural network to perform a given task with good accuracy, the architecture of the network is very important, and it has to be properly designed. With RL, we can automate the process of complex neural architecture search by training the agent to find the best neural architecture for a given task with the goal of maximizing the accuracy.</li>
      <li class="bullet"><strong class="keyword">Natural Language Processing (NLP)</strong>: With the increase in popularity of deep reinforcement algorithms, RL has been widely used in several NLP tasks, such as abstractive text summarization, chatbots, and more.</li>
      <li class="bullet"><strong class="keyword">Finance</strong>: RL is <a id="_idIndexMarker123"/>widely used in financial portfolio management, which is the process of constant redistribution of a fund into different financial products. RL is also used in predicting and trading in commercial transaction markets. JP Morgan has successfully used RL to provide better trade execution results for large orders.</li>
    </ul>
    <h1 id="_idParaDest-52" class="title">RL glossary</h1>
    <p class="normal">We have learned several important and fundamental concepts of RL. In this section, we revisit several important terms that are very useful for understanding the upcoming chapters.</p>
    <p class="normal"><strong class="keyword">Agent</strong>: The agent<a id="_idIndexMarker124"/> is the software program that learns to make intelligent decisions, such as a software program that plays chess intelligently.</p>
    <p class="normal"><strong class="keyword">Environment</strong>: The environment<a id="_idIndexMarker125"/> is the world of the agent. If we continue with the chess example, a chessboard is the environment where the agent plays chess.</p>
    <p class="normal"><strong class="keyword">State</strong>: A state<a id="_idIndexMarker126"/> is a position or a moment in the environment that the agent can be in. For example, all the positions on the chessboard are called states. </p>
    <p class="normal"><strong class="keyword">Action</strong>: The agent interacts with the environment by performing an action<a id="_idIndexMarker127"/> and moves from one state to another, for example, moves made by chessmen are actions.</p>
    <p class="normal"><strong class="keyword">Reward</strong>: A reward<a id="_idIndexMarker128"/> is a numerical value that the agent receives based on its action. Consider a reward as a point. For instance, an agent receives +1 point (reward) for a good action and -1 point (reward) for a bad action. </p>
    <p class="normal"><strong class="keyword">Action space</strong>: The set of all possible actions in the environment is called the action space. The action space<a id="_idIndexMarker129"/> is called a discrete action space when our action space consists of discrete actions, and the action space is called a continuous action space when our actions space consists of continuous actions.</p>
    <p class="normal"><strong class="keyword">Policy</strong>: The agent<a id="_idIndexMarker130"/> makes a decision based on the policy. A policy tells the agent what action to perform in each state. It can be considered the brain of an agent. A policy is called a deterministic policy if it exactly maps a state to a particular action. Unlike a deterministic policy, a stochastic policy maps the state to a probability distribution over the action space. The optimal policy is the one that gives the maximum reward.</p>
    <p class="normal"><strong class="keyword">Episode</strong>: The agent-environment interaction from the initial state to the terminal state is called an episode. An episode<a id="_idIndexMarker131"/> is often called a trajectory or rollout.</p>
    <p class="normal"><strong class="keyword">Episodic and continuous task</strong>: An RL task is called an episodic task<a id="_idIndexMarker132"/> if it has a terminal state, and it is called a continuous task<a id="_idIndexMarker133"/> if it does not have a terminal state.</p>
    <p class="normal"><strong class="keyword">Horizon</strong>: The horizon<a id="_idIndexMarker134"/> can be considered an agent's lifespan, that is, the time step until which the agent interacts with the environment. The horizon is called a finite horizon if the agent-environment interaction stops at a particular time step, and it is called an infinite horizon when the agent environment interaction continues forever.</p>
    <p class="normal"><strong class="keyword">Return</strong>: Return is <a id="_idIndexMarker135"/>the sum of rewards received by the agent in an episode.</p>
    <p class="normal"><strong class="keyword">Discount factor</strong>: The discount factor<a id="_idIndexMarker136"/> helps to control whether we want to give importance to the immediate reward or future rewards. The value of the discount factor ranges from 0 to 1. A discount factor close to 0 implies that we give more importance to immediate rewards, while a discount factor close to 1 implies that we give more importance to future rewards than immediate rewards.</p>
    <p class="normal"><strong class="keyword">Value function</strong>: The value function<a id="_idIndexMarker137"/> or the value of the state is the expected return that an agent would get starting from state <em class="italic">s</em> following policy <img src="../Images/B15558_01_018.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">Q function</strong>: The Q function<a id="_idIndexMarker138"/> or the value of a state-action pair implies the expected return an agent would obtain starting from state <em class="italic">s</em> and performing action <em class="italic">a</em> following policy <img src="../Images/B15558_01_046.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">Model-based and model-free learning</strong>: When the agent tries to learn the optimal policy with the model dynamics, then it is called<a id="_idIndexMarker139"/> model-based learning; and when the agent tries to learn the optimal policy without the model dynamics, then it is called<a id="_idIndexMarker140"/> model-free learning.</p>
    <p class="normal"><strong class="keyword">Deterministic and stochastic environment</strong>: When an agent performs action <em class="italic">a</em> in state <em class="italic">s</em> and it <a id="_idIndexMarker141"/>reaches state <img src="../Images/B15558_01_002.png" alt="" style="height: 1.2em;"/> every time, then the environment is called a deterministic environment. When an agent performs action <em class="italic">a</em> in state <em class="italic">s</em> and it reaches different states every time based on some probability distribution, then the environment is called a <a id="_idIndexMarker142"/>stochastic environment.</p>
    <h1 id="_idParaDest-53" class="title">Summary</h1>
    <p class="normal">We started the chapter by understanding the basic idea of RL. We learned that RL is a trial and error learning process and the learning in RL happens based on a reward. We then explored the difference between RL and the other ML paradigms, such as supervised and unsupervised learning. Going ahead, we learned about the MDP and how the RL environment can be modeled as an MDP. Next, we understood several important fundamental concepts involved in RL, and at the end of the chapter we looked into some real-life applications of RL.</p>
    <p class="normal">Thus, in this chapter, we have learned several fundamental concepts of RL. In the next chapter, we will begin our <em class="italic">Hands-on reinforcement learning</em> journey by implementing all the fundamental concepts we have learned in this chapter using the popular toolkit called Gym.</p>
    <h1 id="_idParaDest-54" class="title">Questions</h1>
    <p class="normal">Let's evaluate our newly acquired knowledge by answering these questions:</p>
    <ol>
      <li class="numbered" value="1">How does RL differ from other ML paradigms?</li>
      <li class="numbered">What is called the environment in the RL setting?</li>
      <li class="numbered">What is the difference between a deterministic and a stochastic policy?</li>
      <li class="numbered">What is an episode?</li>
      <li class="numbered">Why do we need a discount factor?</li>
      <li class="numbered">How does the value function differ from the Q function?</li>
      <li class="numbered">What is the difference between deterministic and stochastic environments?</li>
    </ol>
    <h1 id="_idParaDest-55" class="title">Further reading</h1>
    <p class="normal">For further information, refer to the following link:</p>
    <p class="normal"><strong class="keyword">Reinforcement Learning</strong>: A Survey by <em class="italic">L. P. Kaelbling, M. L. Littman, A. W. Moore</em>, available at <a href="https://arxiv.org/abs/cs/9605103"><span class="url">https://arxiv.org/abs/cs/9605103</span></a></p>
  </div>
</body></html>