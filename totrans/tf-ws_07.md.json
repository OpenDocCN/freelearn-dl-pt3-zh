["```\nmodel = models.Sequential()\nmodel.add(Dense(32, input_shape=(250,)))\n```", "```\nour_cnn_model = models.Sequential([layers.Conv2D\\\n                                   (filters = 32, \\\n                                    kernel_size = (3,3),\n                                    input_shape=(28, 28, 1)), \\\n                                   layers.Activation('relu'), \\\n                                   layers.MaxPool2D\\\n                                   (pool_size = (2, 2)), \\\n                                   layers.Conv2D\\\n                                   (filters = 64, \\\n                                    kernel_size = (3,3)), \\\n                                   layers.Activation('relu'), \\\n                                   layers.MaxPool2D\\\n                                   (pool_size = (2,2)), \\\n                                   layers.Conv2D\\\n                                   (filters = 64, \\\n                                    kernel_size = (3,3)), \\\n                                    layers.Activation('relu')])\n```", "```\n    import tensorflow as tf\n    from tensorflow.keras import models, layers\n    ```", "```\n    print(tf.__version__)\n    ```", "```\n    2.6.0\n    ```", "```\n    image_shape = (300, 300, 3)\n    our_first_layer = models.Sequential([layers.Conv2D\\\n                                        (filters = 16, \\\n                                        kernel_size = (3,3), \\\n                                        input_shape = image_shape), \\\n                                        layers.Activation('relu')])\n    ```", "```\nlayers.MaxPool2D(pool_size=(2, 2), strides=None, \\\n                 padding='valid')\n```", "```\nimage_shape = (300, 300, 3)\nour_first_model = models.Sequential([\n    layers.Conv2D(filters = 16, kernel_size = (3,3), \\\n                  input_shape = image_shape), \\\n    layers.Activation('relu'), \\\n    layers.MaxPool2D(pool_size = (2, 2)), \\\n    layers.Conv2D(filters = 32, kernel_size = (3,3)), \\\n    layers.Activation('relu')])\n```", "```\nlayers.AveragePooling2D(pool_size=(2, 2), strides=None, \\\n                        padding='valid')\n```", "```\nimage_shape = (300, 300, 3)\nour_first_model = models.Sequential([\n    layers.Conv2D(filters = 16, kernel_size = (3,3), \\\n                  input_shape = image_shape), \\\n    layers.Activation('relu'), \\\n    layers.AveragePooling2D(pool_size = (2, 2)), \\\n    layers.Conv2D(filters = 32, kernel_size = (3,3)), \\\n    layers.Activation('relu')])\n```", "```\n    import tensorflow as tf\n    from tensorflow.keras import models, layers\n    ```", "```\n    image_shape = (300, 300, 3)\n    our_first_model = models.Sequential([\n        layers.Conv2D(filters = 16, kernel_size = (3,3), \\\n                      input_shape = image_shape), \\\n        layers.Activation('relu')])\n    ```", "```\n    our_first_model.add(layers.MaxPool2D(pool_size = (2, 2))\n    ```", "```\nimage_shape = (300, 300, 3)\nour_first_model = models.Sequential([\n    layers.Conv2D(filters = 16, kernel_size = (3,3), \\\n                  input_shape = image_shape), \\\n    layers.Activation('relu'), \\\n    layers.MaxPool2D(pool_size = (2, 2)), \\\n    layers.Conv2D(filters = 32, kernel_size = (3,3)), \\\n    layers.Activation('relu'), \\\n    layers.MaxPool2D(pool_size = (2, 2)), \\\n    layers.Flatten()])\n```", "```\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    from tensorflow.keras import models, layers\n    from tensorflow.keras.optimizers import RMSprop\n    from keras_preprocessing import image as kimage\n    ```", "```\n    (our_train_dataset, our_test_dataset), \\\n    dataset_info = tfds.load('horses_or_humans',\\\n                             split = ['train', 'test'],\\\n                             data_dir = 'content/',\\\n                             shuffle_files = True,\\\n                             with_info = True)\n    assert isinstance(our_train_dataset, tf.data.Dataset)\n    ```", "```\n    image_shape = dataset_info.features[\"image\"].shape\n    print(f'Shape of Images in the Dataset: \\t{image_shape}')\n    print(f'Number of Classes in the Dataset: \\\n          \\t{dataset_info.features[\"label\"].num_classes}')\n    names_of_classes = dataset_info.features[\"label\"].names\n    for name in names_of_classes:\n        print(f'Label for class \"{name}\": \\\n              \\t\\t{dataset_info.features[\"label\"].str2int(name)}')\n    ```", "```\n    print(f'Total examples in Train Dataset: \\\n          \\t{len(our_train_dataset)}')\n    pos_tr_samples = sum(i['label'] for i in our_train_dataset)\n    print(f'Horses in Train Dataset: \\t\\t{len(our_train_dataset) \\\n                                          - pos_tr_samples}')\n    print(f'Humans in Train Dataset: \\t\\t{pos_tr_samples}')\n    print(f'\\nTotal examples in Test Dataset: \\\n          \\t{len(our_test_dataset)}')\n    pos_ts_samples = sum(i['label'] for i in our_test_dataset)\n    print(f'Horses in Test Dataset: \\t\\t{len(our_test_dataset) \\\n                                         - pos_ts_samples}')\n    print(f'Humans in Test Dataset: \\t\\t{pos_ts_samples}') \n    ```", "```\n    fig = tfds.show_examples(our_train_dataset, dataset_info)\n    ```", "```\n    fig = tfds.show_examples(our_test_dataset, dataset_info)\n    ```", "```\n    our_cnn_model = models.Sequential([\n        layers.Conv2D(filters = 16, kernel_size = (3,3), \\\n                      input_shape = image_shape),\\\n        layers.Activation('relu'),\\\n        layers.MaxPool2D(pool_size = (2, 2)),\\\n        layers.Conv2D(filters = 32, kernel_size = (3,3)),\\\n        layers.Activation('relu'),\\\n        layers.MaxPool2D(pool_size = (2, 2)),\\\n        layers.Flatten(),\\\n        layers.Dense(units = 512),\\\n        layers.Activation('relu'),\\\n        layers.Dense(units = 1),\\\n        layers.Activation('sigmoid')\n    ])\n    ```", "```\n    our_cnn_model.compile(optimizer=RMSprop(learning_rate=0.001), \\\n                          loss='binary_crossentropy',\\\n                          metrics=['acc'], loss_weights=None,\\\n                          weighted_metrics=None, run_eagerly=None,\\\n                          steps_per_execution=None)\n    print(our_cnn_model.summary())\n    ```", "```\nfrom tensorflow import image as tfimage\nfrom tensorflow.keras.preprocessing import image as kimage\n```", "```\naugment_dataset(image, label):\n    image = kimage.random_shift(image, wrg = 0.1, hrg = 0.1)\n    image = tfimage.random_flip_left_right(image)\n    return image, label\n```", "```\naugment_dataset(image, label):\n    image = kimage.random_shift(image, wrg = 0.1, hrg = 0.1)\n    image = tfimage.random_flip_left_right(image)\n    return image, label    \nour_train_dataset = our_train_dataset.map(augment_dataset)\nmodel.fit(our_train_dataset,\\\n          epochs=50,\\\n          validation_data=our_test_dataset)\n```", "```\nour_train_dataset = our_train_dataset.cache()\nour_train_dataset = our_train_dataset.map(augment_dataset)\nour_train_dataset = our_train_dataset.shuffle\\\n                    (len(our_train_dataset))\nour_train_dataset = our_train_dataset.batch(128)\nour_train_dataset = our_train_dataset.prefetch\\\n                    (tf.data.experimental.AUTOTUNE)\n```", "```\nimage = kimage.random_rotation(image, rg = 135)\n```", "```\nimage = kimage.random_shift(image, wrg = 0.15, hrg = 0) \n```", "```\nimage = kimage.random_shift(image, wrg = 0, hrg = 0.15)\n```", "```\nimage = kimage.random_brightness(image, brightness_range=(0.1,0.9))\n```", "```\nmodel.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), use_bias=False))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Activation(\"relu\"))\n```", "```\n    normalization_layer = layers.Rescaling(1./255)\n    our_train_dataset = our_train_dataset.map\\\n                        (lambda x: (normalization_layer(x['image']), \\\n                                                        x['label']), \\\n                         num_parallel_calls = \\\n                         tf.data.experimental.AUTOTUNE)\n    our_train_dataset = our_train_dataset.cache()\n    our_train_dataset = our_train_dataset.shuffle\\\n                        (len(our_train_dataset))\n    our_train_dataset = our_train_dataset.batch(128)\n    our_train_dataset = \\\n    our_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    our_test_dataset = our_test_dataset.map\\\n                       (lambda x: (normalization_layer(x['image']), \\\n                                                       x['label']),\\\n                        num_parallel_calls = \\\n                        tf.data.experimental.AUTOTUNE)\n    our_test_dataset = our_test_dataset.cache()\n    our_test_dataset = our_test_dataset.batch(32)\n    our_test_dataset = our_test_dataset.prefetch\\\n                       (tf.data.experimental.AUTOTUNE)\n    ```", "```\n    history = our_cnn_model.fit\\\n              (our_train_dataset, \\\n              validation_data = our_test_dataset, \\\n              epochs=15, \\\n              validation_steps=8, \\\n              verbose=1)\n    ```", "```\n    from matplotlib.pyplot import imshow\n    for images, lables in our_test_dataset.take(1):\n        imshow(np.asarray(images[0]))\n        image_to_test = kimage.img_to_array(images[0])\n        image_to_test = np.array([image_to_test])\n        prediction = our_cnn_model.predict(image_to_test)\n        print(prediction)\n        if prediction > 0.5:\n            print(\"Image is a human\")\n            else:\n            print(\"Image is a horse\")\n    ```", "```\n    layer_outputs = []\n    for layer in our_cnn_model.layers[1:]:\n        layer_outputs.append(layer.output)\n    layer_names = []\n    for layer in our_cnn_model.layers:\n        layer_names.append(layer.name)\n    features_model = models.Model(inputs = our_cnn_model.input, \\\n                                  outputs = layer_outputs)\n    random_sample = our_train_dataset.take(1)\n    layer_predictions = features_model.predict(random_sample)\n    for layer_name, prediction in zip(layer_names, \\\n                                      layer_predictions):\n        if len(prediction.shape) != 4:\n            continue\n        num_features = prediction.shape[-1]\n        size = prediction.shape[1]\n        grid = np.zeros((size, size * num_features))\n        for i in range(num_features):\n            img = prediction[0, :, :, i]\n            img = ((((img - img.mean()) / img.std()) * 64) + 128)\n            img = np.clip(img, 0, 255).astype('uint8')\n            grid[:, i * size : (i + 1) * size] = img\n        scale = 20\\. / num_features\n        plt.figure(figsize=(scale * num_features, scale))\n        plt.title(layer_name)\n        plt.imshow(grid)\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import tensorflow_datasets as tfds\n    from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \\\n        Dropout, GlobalMaxPooling2D, Activation, Rescaling\n    from tensorflow.keras.models import Model\n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n    import itertools\n    import matplotlib.pyplot as plt\n    ```", "```\n    (our_train_dataset, our_test_dataset), \\\n    dataset_info = tfds.load(\\\n                             'fashion_mnist'\n                              , split = ['train', 'test']\n                              , data_dir = 'content/FashionMNIST/'\n                              , shuffle_files = True\n                              , as_supervised = True\n                              , with_info = True)\n    assert isinstance(our_train_dataset, tf.data.Dataset)\n    ```", "```\n    image_shape = dataset_info.features[\"image\"].shape\n    print(f'Shape of Images in the Dataset: \\t{image_shape}')\n    num_classes = dataset_info.features[\"label\"].num_classes\n    print(f'Number of Classes in the Dataset: \\t{num_classes}')\n    names_of_classes = dataset_info.features[\"label\"].names\n    print(f'Names of Classes in the Dataset: \\t{names_of_classes}\\n')\n    for name in names_of_classes:\n        print(f'Label for class \\\n              \"{name}\":  \\t\\t{dataset_info.features[\"label\"].\\\n              str2int(name)}')\n    ```", "```\n    print(f'Total examples in Train Dataset: \\\n          \\t{len(our_train_dataset)}')\n    print(f'Total examples in Test Dataset: \\\n          \\t{len(our_test_dataset)}')\n    ```", "```\n    input_layer = Input(shape=image_shape)\n    x = Conv2D(filters = 32, kernel_size = (3, 3), \\\n               strides=2)(input_layer)\n    x = Activation('relu')(x)\n    x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x)\n    x = Activation('relu')(x)\n    x = Flatten()(x)\n    x = Dropout(rate = 0.2)(x)\n    x = Dense(units = 512)(x)\n    x = Activation('relu')(x)\n    x = Dropout(rate = 0.2)(x)\n    x = Dense(units = num_classes)(x)\n    output = Activation('softmax')(x)\n    our_classification_model = Model(input_layer, output)\n    ```", "```\n    our_classification_model.compile(\n                       optimizer='adam', \\\n                       loss='sparse_categorical_crossentropy',\n                       metrics=['accuracy'], loss_weights=None,\n                       weighted_metrics=None, run_eagerly=None,\n                       steps_per_execution=None\n    )\n    history = our_classification_model.fit(our_train_dataset, validation_data=our_test_dataset, epochs=15)\n    ```", "```\n    def plot_trend_by_epoch(tr_values, val_values, title):\n        epoch_number = range(len(tr_values))\n        plt.plot(epoch_number, tr_values, 'r')\n        plt.plot(epoch_number, val_values, 'b')\n        plt.title(title)\n        plt.xlabel('epochs')\n        plt.legend(['Training '+title, 'Validation '+title])\n        plt.figure()\n    hist_dict = history.history\n    tr_accuracy, val_accuracy = hist_dict['accuracy'], \\\n                                hist_dict['val_accuracy']\n    plot_trend_by_epoch(tr_accuracy, val_accuracy, \"Accuracy\")\n    ```", "```\n    tr_loss, val_loss = hist_dict['loss'], hist_dict['val_loss']\n    plot_trend_by_epoch(tr_loss, val_loss, \"Loss\")\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import tensorflow_datasets as tfds\n    from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \\\n        Dropout, GlobalMaxPooling2D, Activation, Rescaling\n    from tensorflow.keras.models import Model\n    from sklearn import metrics import confusion_matrix, \\\n        ConfusionMatrixDisplay\n    import itertools\n    import matplotlib.pyplot as plt\n    ```", "```\n    (our_train_dataset, our_test_dataset), \\\n    dataset_info = tfds.load('cifar10',\\\n                             split = ['train', 'test'],\\\n                             data_dir = 'content/Cifar10/',\\\n                             shuffle_files = True,\\\n                             as_supervised = True,\\\n                             with_info = True)\n    assert isinstance(our_train_dataset, tf.data.Dataset)\n    ```", "```\n    image_shape = dataset_info.features[\"image\"].shape\n    print(f'Shape of Images in the Dataset: \\t{image_shape}')\n    num_classes = dataset_info.features[\"label\"].num_classes\n    print(f'Number of Classes in the Dataset: \\t{num_classes}')\n    names_of_classes = dataset_info.features[\"label\"].names\n    print(f'Names of Classes in the Dataset: \\t{names_of_classes}\\n')\n    for name in names_of_classes:\n        print(f'Label for class \"{name}\": \\\n              \\t\\t{dataset_info.features[\"label\"].str2int(name)}')\n    print(f'Total examples in Train Dataset: \\\n          \\t{len(our_train_dataset)}')\n    print(f'Total examples in Test Dataset: \\\n          \\t{len(our_test_dataset)}')\n    ```", "```\n    normalization_layer = Rescaling(1./255)\n    our_train_dataset = our_train_dataset.map\\\n                        (lambda x, y: (normalization_layer(x), y),\\\n                         num_parallel_calls = \\\n                         tf.data.experimental.AUTOTUNE)\n    our_train_dataset = our_train_dataset.cache()\n    our_train_dataset = our_train_dataset.shuffle\\\n                        (len(our_train_dataset))\n    our_train_dataset = our_train_dataset.batch(128)\n    our_train_dataset = our_train_dataset.prefetch\\\n                        (tf.data.experimental.AUTOTUNE)\n    our_test_dataset = our_test_dataset.map\\\n                       (lambda x, y: (normalization_layer(x), y),\\\n                        num_parallel_calls = \\\n                        tf.data.experimental.AUTOTUNE)\n    our_test_dataset = our_test_dataset.cache()\n    our_test_dataset = our_test_dataset.batch(1024)\n    our_test_dataset = our_test_dataset.prefetch\\\n                       (tf.data.experimental.AUTOTUNE)\n    ```", "```\n    input_layer = Input(shape=image_shape)\n    x = Conv2D(filters = 32, \\\n               kernel_size = (3, 3), strides=2)(input_layer)\n    x = Activation('relu')(x)\n    x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x)\n    x = Activation('relu')(x)\n    x = Flatten()(x)\n    x = Dropout(rate = 0.5)(x)\n    x = Dense(units = 1024)(x)\n    x = Activation('relu')(x)\n    x = Dropout(rate = 0.2)(x)\n    x = Dense(units = num_classes)(x)\n    output = Activation('softmax')(x)\n    our_classification_model = Model(input_layer, output)\n    ```", "```\n    our_classification_model.compile(\n                          optimizer='adam', \\\n                          loss='sparse_categorical_crossentropy',\n                          metrics=['accuracy'], loss_weights=None,\n                          weighted_metrics=None, run_eagerly=None,\n                          steps_per_execution=None\n    )\n    print(our_classification_model.summary())\n    history = our_classification_model.fit(our_train_dataset, validation_data=our_test_dataset, epochs=15)\n    ```", "```\n    def plot_trend_by_epoch(tr_values, val_values, title):\n        epoch_number = range(len(tr_values))\n        plt.plot(epoch_number, tr_values, 'r')\n        plt.plot(epoch_number, val_values, 'b')\n        plt.title(title)\n        plt.xlabel('epochs')\n        plt.legend(['Training '+title, 'Validation '+title])\n        plt.figure()\n    hist_dict = history.history\n    tr_loss, val_loss = hist_dict['loss'], hist_dict['val_loss']\n    plot_trend_by_epoch(tr_loss, val_loss, \"Loss\")\n    ```", "```\n    tr_accuracy, val_accuracy = hist_dict['accuracy'], \\\n                                hist_dict['val_accuracy']\n    plot_trend_by_epoch(tr_accuracy, val_accuracy, \"Accuracy\")\n    ```", "```\n    test_labels = []\n    test_images = []\n    for image, label in tfds.as_numpy(our_test_dataset.unbatch()):\n        test_images.append(image)\n        test_labels.append(label)\n    test_labels = np.array(test_labels)\n    predictions = our_classification_model.predict(our_test_dataset).argmax(axis=1)\n    conf_matrix = confusion_matrix(test_labels, predictions)\n    disp = ConfusionMatrixDisplay(conf_matrix, \\\n                                  display_labels = names_of_classes)\n    fig = plt.figure(figsize = (12, 12))\n    axis = fig.add_subplot(111)\n    disp.plot(values_format = 'd', ax = axis)\n    ```", "```\n    conf_matrix = conf_matrix.astype\\\n                  ('float') / conf_matrix.sum(axis=1) \\\n                  [:, np.newaxis]\n    disp = ConfusionMatrixDisplay(\\\n           conf_matrix, display_labels = names_of_classes)\n    fig = plt.figure(figsize = (12, 12))\n    axis = fig.add_subplot(111)\n    disp.plot(ax = axis)\n    ```", "```\n    incorrect_predictions = np.where(predictions != test_labels)[0]\n    index = np.random.choice(incorrect_predictions)\n    plt.imshow(test_images[index])\n    print(f'True label: {names_of_classes[test_labels[index]]}')\n    print(f'Predicted label: {names_of_classes[predictions[index]]}')\n    ```"]