["```\nimport tensorflow as tf \nimport numpy as np \n```", "```\ndef layer(x, W, b):    \n    # Building the graph\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform\n    return h \n```", "```\n@tf.function\ndef layer(x, W, b):    \n    # Building the graph\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform\n    return h \n```", "```\nx = np.array([[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]],dtype=np.float32) \n```", "```\ninit_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])\nW = tf.Variable(init_w, dtype=tf.float32, name='W') \ninit_b = tf.initializers.RandomUniform()(shape=[5])\nb = tf.Variable(init_b, dtype=tf.float32, name='b') \n```", "```\ninit_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])\ninit_b = tf.initializers.RandomUniform()(shape=[5]) \n```", "```\nh = layer(x,W,b) \n```", "```\nprint(f\"h = {h.numpy()}\") \n```", "```\nh = [[0.7027744 0.687556  0.635395  0.6193934 0.6113584]] \n```", "```\n@tf.function\ndef layer(x, W, b):    \n    # Building the graph\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed\n    return h\nx = np.array([[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]], dtype=np.float32) \n# Variable\ninit_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])\nW = tf.Variable(init_w, dtype=tf.float32, name='W') \n# Variable\ninit_b = tf.initializers.RandomUniform()(shape=[5])\nb = tf.Variable(init_b, dtype=tf.float32, name='b') \nh = layer(x,W,b)\nprint(f\"h = {h.numpy()}\") \n```", "```\ngraph = tf.Graph() # Creates a graph\nsession = tf.InteractiveSession(graph=graph) # Creates a session \n```", "```\nx = tf.placeholder(shape=[1,10],dtype=tf.float32,name='x')\nW = tf.Variable(tf.random_uniform(shape=[10,5], minval=-0.1, maxval=0.1, dtype=tf.float32),name='W')\nb = tf.Variable(tf.zeros(shape=[5],dtype=tf.float32),name='b') h = tf.nn.sigmoid(tf.matmul(x,W) + b) \n```", "```\ntf.global_variables_initializer().run() \n```", "```\nh_eval = session.run(h,feed_dict={x: np.random.rand(1,10)}) \n```", "```\nsession.close() \n```", "```\nimport tensorflow as tf import numpy as np\n# Defining the graph and session graph = tf.Graph() # Creates a graph\nsession = tf.InteractiveSession(graph=graph) # Creates a session\n# Building the graph\n# A placeholder is an symbolic input\nx = tf.placeholder(shape=[1,10],dtype=tf.float32,name='x')\n# Variable\nW = tf.Variable(tf.random_uniform(shape=[10,5], minval=-0.1, maxval=0.1, dtype=tf.float32),name='W') \nb = tf.Variable(tf.zeros(shape=[5],dtype=tf.float32),name='b')\nh = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed\n# Executing operations and evaluating nodes in the graph\ntf.global_variables_initializer().run() # Initialize the variables\n# Run the operation by providing a value to the symbolic input x h_eval = session.run(h,feed_dict={x: np.random.rand(1,10)})\n# Closes the session to free any held resources by the session\nsession.close() \n```", "```\nx = np.array([[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]], dtype=np.float32) \n```", "```\nx = tf.constant(value=[[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]],\ndtype=tf.float32,name='x') \n```", "```\nimport tensorflow as tf\n@tf.function\ndef layer(x, W, b):    \n    # Building the graph\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed\n    return h\n# A pre-loaded input\nx = tf.constant(value=[[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]],dtype=tf.float32,name='x') \n# Variable\ninit_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])\nW = tf.Variable(init_w, dtype=tf.float32, name='W') \n# Variable\ninit_b = tf.initializers.RandomUniform()(shape=[5])\nb = tf.Variable(init_b, dtype=tf.float32, name='b') \nh = layer(x,W,b)\nprint(f\"h = {h}\")\nprint(f\"h is of type {type(h)}\") \n```", "```\nimport tensorflow as tf\nimport numpy as np \n```", "```\nfilenames = [f\"./iris.data.{i}\" for i in range(1,4)] \n```", "```\ndataset = tf.data.experimental.CsvDataset(filenames, [tf.float32, tf.float32, tf.float32, tf.float32, tf.string]) \n```", "```\ndataset = dataset.map(lambda x1,x2,x3,x4,y: (tf.stack([x1,x2,x3,x4]), y)) \n```", "```\nfor next_element in dataset:\n    x, y = next_element[0].numpy(), next_element[1].numpy().decode('ascii')\n    if np.min(x)<0.0:\n        print(f\"(corrupted) X => {x}\\tY => {y}\") \n```", "```\ndataset = dataset.filter(lambda x,y: tf.reduce_min(x)>0) \n```", "```\nbatch_size = 5\ndataset = dataset.batch(batch_size=batch_size) \n```", "```\nx.shape = (5, 4), y.shape = (5,) \n```", "```\n`tf.initializers.RandomUniform(minval=-`0.1`, maxval=`0.1`)(shape=[`10`,`5`])` \n```", "```\n`a = tf.Variable(tf.zeros([`5`]),name=`'b'`)` \n```", "```\nx = tf.matmul(w,A) \ny = x + B \n```", "```\n# Let's assume the following values for x and y \n# x (2-D tensor) => [[1,2],[3,4]]\n# y (2-D tensor) => [[4,3],[3,2]]\nx = tf.constant([[1,2],[3,4]], dtype=tf.int32)\ny = tf.constant([[4,3],[3,2]], dtype=tf.int32)\n# Checks if two tensors are equal element-wise and returns a boolean\n# tensor\n# x_equal_y => [[False,False],[True,False]] \nx_equal_y = tf.equal(x, y, name=None)\n# Checks if x is less than y element-wise and returns a boolean tensor\n# x_less_y => [[True,True],[False,False]]\nx_less_y = tf.less(x, y, name=None)\n# Checks if x is greater or equal than y element-wise and returns a\n# boolean tensor\n# x_great_equal_y => [[False,False],[True,True]]\nx_great_equal_y = tf.greater_equal(x, y, name=None)\n# Selects elements from x and y depending on whether, # the condition is satisfied (select elements from x) # or the condition failed (select elements from y)\ncondition = tf.constant([[True,False],[True,False]],dtype=tf.bool)\n# x_cond_y => [[1,3],[3,2]]\nx_cond_y = tf.where(condition, x, y, name=None) \n```", "```\n# Let's assume the following values for x and y\n# x (2-D tensor) => [[1,2],[3,4]]\n# y (2-D tensor) => [[4,3],[3,2]]\nx = tf.constant([[1,2],[3,4]], dtype=tf.float32)\ny = tf.constant([[4,3],[3,2]], dtype=tf.float32)\n# Add two tensors x and y in an element-wise fashion\n# x_add_y => [[5,5],[6,6]]\nx_add_y = tf.add(x, y)\n# Performs matrix multiplication (not element-wise)\n# x_mul_y => [[10,7],[24,17]]\nx_mul_y = tf.matmul(x, y)\n# Compute natural logarithm of x element-wise # equivalent to computing ln(x)\n# log_x => [[0,0.6931],[1.0986,1.3863]]\nlog_x = tf.log(x)\n# Performs reduction operation across the specified axis\n# x_sum_1 => [3,7]\nx_sum_1 = tf.reduce_sum(x, axis=[1], keepdims=False)\n# x_sum_2 => [[4,6]]\nx_sum_2 = tf.reduce_sum(x, axis=[0], keepdims=True)\n# Segments the tensor according to segment_ids (items with same id in\n# the same segment) and computes a segmented sum of the data\ndata = tf.constant([1,2,3,4,5,6,7,8,9,10], dtype=tf.float32)\nsegment_ids = tf.constant([0,0,0,1,1,2,2,2,2,2 ], dtype=tf.int32)\n# x_seg_sum => [6,9,40]\nx_seg_sum = tf.segment_sum(data, segment_ids) \n```", "```\n`v = tf.Variable(tf.constant([[`1`,`9`],[`3`,`10`],[`5`,`11`]],dtype=tf.float32),name=`'ref'`)` \n```", "```\n`v[`0`].assign([-`1`, -`9`])` \n```", "```\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\narray([[-1., -9.],\n       [ 3., 10.],\n       [ 5., 11.]], dtype=float32)> \n```", "```\n`v[`1`,`1`].assign(-`10`)` \n```", "```\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\narray([[  1.,   9.],\n       [  3., -10.],\n       [  5.,  11.]], dtype=float32)> \n```", "```\n`v[`1`:,`0`].assign([-`3`,-`5`])` \n```", "```\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\narray([[ 1.,  9.],\n       [-3., 10.],\n       [-5., 11.]], dtype=float32)> \n```", "```\n`t = tf.constant([[`1`,`9`],[`3`,`10`],[`5`,`11`]],dtype=tf.float32)` \n```", "```\n`t[`0`].numpy()` \n```", "```\n[1\\. 9.] \n```", "```\n`t[`1`:,`0`].numpy()` \n```", "```\n[3\\. 5.] \n```", "```\n# Sigmoid activation of x is given by 1 / (1 + exp(-x))\ntf.nn.sigmoid(x,name=None)\n# ReLU activation of x is given by max(0,x) \ntf.nn.relu(x, name=None) \n```", "```\nx = tf.constant(\n    [[\n        [[1],[2],[3],[4]],\n        [[4],[3],[2],[1]],\n        [[5],[6],[7],[8]],\n        [[8],[7],[6],[5]]\n    ]],\n    dtype=tf.float32)\nx_filter = tf.constant(\n    [ [ [[0.5]],[[1]] ],\n      [ [[0.5]],[[1]] ] \n    ],\n    dtype=tf.float32)\nx_stride = [1,1,1,1]\nx_padding = 'VALID'\nx_conv = tf.nn.conv2d(\n    input=x, filters=x_filter, strides=x_stride, padding=x_padding\n) \n```", "```\nx = tf.constant(\n    [[\n        [[1],[2],[3],[4]],\n        [[4],[3],[2],[1]],\n        [[5],[6],[7],[8]],\n        [[8],[7],[6],[5]]\n    ]],\n    dtype=tf.float32)\nx_ksize = [1,2,2,1]\nx_stride = [1,2,2,1]\nx_padding = 'VALID'\nx_pool = tf.nn.max_pool2d(\n    input=x, ksize=x_ksize,\n    strides=x_stride, padding=x_padding\n)\n# Returns (out) => [[[[ 4.],[ 4.]],[[ 8.],[ 8.]]]] \n```", "```\n# Returns half of L2 norm of t given by sum(t**2)/2\nx = tf.constant([[2,4],[6,8]],dtype=tf.float32)\nx_hat = tf.constant([[1,2],[3,4]],dtype=tf.float32)\n# MSE = (1**2 + 2**2 + 3**2 + 4**2)/2 = 15\nMSE = tf.nn.l2_loss(x-x_hat)\n# A common loss function used in neural networks to optimize the network\n# Calculating the cross_entropy with logits (unnormalized outputs of the last layer)\n# instead of probabilsitic outputs leads to better numerical stabilities\ny = tf.constant([[1,0],[0,1]],dtype=tf.float32)\ny_hat = tf.constant([[3,1],[2,5]],dtype=tf.float32)\n# This function alone doesn't average the cross entropy losses of all data points,\n# You need to do that manually using reduce_mean function\nCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=y)) \n```", "```\nmodel = tf.keras.Sequential([\n        tf.keras.layers.Dense(500, activation='relu', shape=(784, )),\n        tf.keras.layers.Dense(250, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ]) \n```", "```\ninp = tf.keras.layers.Input(shape=(784,))\nout_1 = tf.keras.layers.Dense(500, activation='relu')(inp)\nout_2 = tf.keras.layers.Dense(250, activation='relu')(out_1)\nout = tf.keras.layers.Dense(10, activation='softmax')(out_2)\nmodel = tf.keras.models.Model(inputs=inp, outputs=out) \n```", "```\nclass MyModel(tf.keras.Model):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.hidden1_layer = tf.keras.layers.Dense(500, activation='relu')\n        self.hidden2_layer = tf.keras.layers.Dense(250, activation='relu')\n        self.final_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n    def call(self, inputs):\n        h = self.hidden1_layer(inputs)\n        h = self.hidden2_layer(h)\n        y = self.final_layer(h)\n        return y\n\nmodel = MyModel(num_classes=10) \n```", "```\nos.makedirs('data', exist_ok=True)\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\n    path=os.path.join(os.getcwd(), 'data', 'mnist.npz')\n)\n# Reshaping x_train and x_test tensors so that each image is represented\n# as a 1D vector\nx_train = x_train.reshape(x_train.shape[0], -1)\nx_test = x_test.reshape(x_test.shape[0], -1)\n# Standardizing x_train and x_test tensors\nx_train = (\n    x_train - np.mean(x_train, axis=1, keepdims=True)\n)/np.std(x_train, axis=1, keepdims=True)\nx_test = (\n    x_test - np.mean(x_test, axis=1, keepdims=True)\n)/np.std(x_test, axis=1, keepdims=True)\n# One hot encoding y_train and y_test\ny_onehot_train = np.zeros((y_train.shape[0], num_labels), dtype=np.float32)\ny_onehot_train[np.arange(y_train.shape[0]), y_train] = 1.0\ny_onehot_test = np.zeros((y_test.shape[0], num_labels), dtype=np.float32)\ny_onehot_test[np.arange(y_test.shape[0]), y_test] = 1.0 \n```", "```\nmodel = tf.keras.Sequential([\n        tf.keras.layers.Dense(500, activation='relu'),\n        tf.keras.layers.Dense(250, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ]) \n```", "```\noptimizer = tf.keras.optimizers.RMSprop()\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc']) \n```", "```\nbatch_size = 100\nnum_epochs = 10\ntrain_history = model.fit(\n    x=x_train, \n    y=y_onehot_train, \n    batch_size=batch_size, \n    epochs= num_epochs, \n    validation_split=0.2\n) \n```", "```\ntest_res = model.evaluate(\n    x=x_test, \n    y=y_onehot_test, \n    batch_size=batch_size\n) \n```"]