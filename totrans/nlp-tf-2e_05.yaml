- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentence Classification with Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss a type of neural network known as **Convolutional
    Neural Networks** (**CNNs**). CNNs are quite different from fully connected neural
    networks and have achieved state-of-the-art performance in numerous tasks. These
    tasks include image classification, object detection, speech recognition, and
    of course, sentence classification. One of the main advantages of CNNs is that,
    compared to a fully connected layer, a convolution layer in a CNN has a much smaller
    number of parameters. This allows us to build deeper models without worrying about
    memory overflow. Also, deeper models usually lead to better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will introduce you to what a CNN is in detail by discussing different components
    found in a CNN and what makes CNNs different from their fully connected counterparts.
    Then we will discuss the various operations used in CNNs, such as the convolution
    and pooling operations, and certain hyperparameters related to these operations,
    such as filter size, padding, and stride. We will also look at some of the mathematics
    behind the actual operations. After establishing a good understanding of CNNs,
    we will look at the practical side of implementing a CNN with TensorFlow. First,
    we will implement a CNN to classify images and then use a CNN for sentence classification.
    Specifically, we’ll go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the fundamentals of CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying images with CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying sentences with CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about CNNs. Specifically, you will first get
    an understanding of the sort of operations present in a CNN, such as convolution
    layers, pooling layers, and fully connected layers. Next, we will briefly see
    how all of these are connected to form an end-to-end model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the first use case we’ll be solving with CNNs is
    an image classification task. CNNs were originally used to solve computer vision
    tasks and were adopted for NLP much later. Furthermore, CNNs have a stronger presence
    in the computer vision domain than the NLP domain, making it easier to explain
    the underlying concepts in a vision context. For this reason, we will first learn
    how CNNs are used in computer vision and then move on to NLP.
  prefs: []
  type: TYPE_NORMAL
- en: CNN fundamentals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let’s explore the fundamental ideas behind a CNN without delving into too
    much technical detail. A CNN is a stack of layers, such as convolution layers,
    pooling layers, and fully connected layers. We will discuss each of these to understand
    their role in the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, the input is connected to a set of convolution layers. These convolution
    layers slide a patch of weights (sometimes called the convolution window or filter)
    over the input and produce an output by means of the convolution operation. Convolution
    layers use a small number of weights, organized to cover only a small patch of
    input in each layer, unlike fully connected neural networks, and these weights
    are shared across certain dimensions (for example, the width and height dimensions
    of an image). Also, CNNs use the convolution operations to share the weights from
    the output by sliding this small set of weights along the desired dimension. What
    we ultimately get from this convolution operation is illustrated in *Figure 5.1*.
    If the pattern present in a convolution filter is present in a patch of image,
    the convolution will output a high value for that location; if not, it will output
    a low value. Also, by convolving the full image, we get a matrix indicating whether
    a pattern was present or not in a given location. Finally, we will get a matrix
    as the convolution output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN fundamentals](img/B14070_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: What the convolution operation does to an image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, these convolution layers are optionally interleaved with pooling/subsampling
    layers, which reduces the dimensionality of the input. While reducing the dimensionality,
    we make the translation of CNNs invariant, as well as force the CNN to learn with
    less information, leading to better generalization and regularization of the model.
    The dimensionality is reduced by dividing the input into several patches and transforming
    each patch into a single element. For example, such transformations include picking
    the maximum element of a patch or averaging all the values in a patch. We will
    illustrate how pooling can make the translation of CNNs invariant in *Figure 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN fundamentals](img/B14070_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: How the pooling operation helps to make data translation invariant'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have the original image and an image slightly translated on the *y*
    axis. We have convolution output for both images, and you can see that the value
    **10** appears at slightly different places in the convolution output. However,
    using max pooling (which takes the maximum value of each thick square), we can
    get the same output at the end. We will discuss these operations in detail later.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output is fed to a set of fully connected layers, which then forward
    the output to the final classification/regression layer (for example, sentence/image
    classification). Fully connected layers contain a significant fraction of the
    total number of weights of the CNN, as convolution layers have a small number
    of weights. However, it has been found that CNNs perform better with fully connected
    layers than without them. This could be because convolution layers learn more
    localized features due to their small size, whereas fully connected layers provide
    a global picture of how these localized features should be connected together
    to produce a desirable final output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.3* shows a typical CNN used to classify images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN fundamentals](img/B14070_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: A typical CNN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'As is evident from the figure, CNNs, by design, preserve the spatial structure
    of the inputs during learning. In other words, for a two-dimensional input, a
    CNN will mostly have two-dimensional layers, whereas it will only have fully connected
    layers close to the output layer. Preserving the spatial structure allows CNNs
    to exploit valuable spatial information of the inputs and learn about inputs with
    fewer parameters. The value of spatial information is illustrated in *Figure 5.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN fundamentals](img/B14070_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Unwrapping an image into a one-dimensional vector loses some of
    the important spatial information'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, when a two-dimensional image of a cat is unwrapped to be a one-dimensional
    vector, ears are no longer close to the eyes, and the nose is far away from the
    eyes as well. This means we have destroyed some of the useful spatial information
    during the unwrapping. This is why preserving the two-dimensional nature of the
    inputs is so important.
  prefs: []
  type: TYPE_NORMAL
- en: The power of CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs are a very versatile family of models and have shown a remarkable performance
    in many types of tasks. Such versatility is attributed to the ability of CNNs
    to perform feature extraction and learning simultaneously, leading to greater
    efficiency and generalizability. Let’s discuss a few examples of the utility of
    CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In the **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) 2020,
    which involved classifying images, detecting objects, and localizing objects in
    an image, CNNs were used to achieve incredible test accuracies. For example, for
    image-classification tasks, its top-1 test accuracy was approximately 90% for
    1,000 different object classes, which means that the CNN was able to correctly
    identify around 900 different objects correctly.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs also have been used for image segmentation. Image segmentation involves
    segmenting an image into different areas. For example, in an urbanscape image
    that includes buildings, a road, vehicles, and passengers, isolating the road
    from the buildings is a segmentation task. Moreover, CNNs have made incredible
    strides, demonstrating their performance in NLP tasks such as sentence classification,
    text generation, and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the high level concepts governing CNNs, let’s walk through
    the technical details of a CNN. First, we will discuss the convolution operation
    and introduce some terminology, such as filter size, stride, and padding. In brief,
    **filter size** refers to the window size of the convolution operation, **stride**
    refers to the distance between two movements of the convolution window, and **padding**
    refers to the way you handle the boundaries of the input. We will also discuss
    an operation that is known as deconvolution or transposed convolution. Then we
    will discuss the details of the pooling operation. Finally, we will discuss how
    to add fully connected layers, which produce the classification or regression
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss the convolution operation in detail. First,
    we will discuss the convolution operation without stride and padding, then we
    will describe the convolution operation with stride, and then we will discuss
    the convolution operation with padding. Finally, we will discuss something called
    transposed convolution. For all the operations in this chapter, we consider the
    index starting from one, and not from zero.
  prefs: []
  type: TYPE_NORMAL
- en: Standard convolution operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The convolution operation is a central part of CNNs. For an input of size ![](img/B14070_05_001.png)
    and a weight patch (also known as a *filter* or a *kernel*) of ![](img/B14070_05_002.png),
    where ![](img/B14070_05_003.png), the convolution operation slides the patch of
    weights over the input. Let’s denote the input by `X`, the patch of weights by
    `W`, and the output by `H`. Also, at each location *i, j*, the output is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *x* [i,j], *w* [i,j], and *h*[i,j] denote the value at the *(i,j)*^(th)
    location of *X*, *W*, and *H*, respectively. As already shown by the equation,
    though the input size is ![](img/B14070_05_001.png), the output in this case will
    be ![](img/B14070_05_006.png). Also, *m* is known as the filter size. This means
    the width and height of the output will be slightly less than of the original.
    Let’s look at this through a visualization (see *Figure 5.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard convolution operation](img/B14070_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: The convolution operation with a filter size (m) = 3, stride =
    1, and no padding'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: The output produced by the convolution operation (the rectangle at the top in
    *Figure 5.5*) is sometimes called a **features map**.
  prefs: []
  type: TYPE_NORMAL
- en: Next let’s discuss the stride parameter in convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Convolving with stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding example, we shifted the filter by a single step. However, this
    is not mandatory; we can take large steps or strides while convolving the input.
    Therefore, the size of the step is known as the stride.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s modify the previous equation to include the *s* [i] and *s* [j] strides:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_007.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_05_007.1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the output will be smaller as the size of *s*[i] and *s*[j] increases.
    Comparing *Figure 5.5* (*stride = 1*) and *Figure 5.6* (*stride = 2*) illustrates
    the effect of different strides:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolving with stride](img/B14070_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: The convolution operation with a filter size (m) = 2, stride =
    2, and no padding'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, doing convolution with stride helps to reduce the dimensionality
    of the input similar to a pooling layer. Therefore, sometimes convolution with
    stride is used instead of pooling in the CNNs as it reduces the computational
    complexity. Also note that the dimensionality reduction achieved by stride can
    be tuned or controlled as opposed to the inherent dimensionality reduction from
    the standard convolution operation. We will now discuss another important concept
    in convolution known as padding.
  prefs: []
  type: TYPE_NORMAL
- en: Convolving with padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The inevitable output size reduction resulting from each convolution (without
    stride) is an undesirable property. This greatly limits the number of layers we
    can have in a network. Also, it is known that deeper networks perform better than
    shallow networks. This should not be confused with the dimensionality reduction
    achieved by stride, as this is a design choice and we can decide to have a stride
    of 1 if necessary. Therefore, padding is used to circumvent this issue. This is
    achieved by padding zeros to the boundary of the input so that the output size
    and the input size are equal. Let’s assume a stride of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5.7* depicts the result of the padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolving with padding](img/B14070_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: The convolution operation with a filter size (m=3), stride (s=1),
    and zero padding'
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss the transposed convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the convolution operation looks complicated in terms of mathematics,
    it can be simplified to a matrix multiplication. For this reason, we can define
    the transpose of the convolution operation or **deconvolution**, as it is sometimes
    called. However, we will use the term **transposed convolution** as it sounds
    more natural. In addition, deconvolution refers to a different mathematical concept.
    The transposed convolution operation plays an important role in CNNs for the reverse
    accumulation of the gradients during backpropagation. Let’s go through an example.
  prefs: []
  type: TYPE_NORMAL
- en: For an input of size ![](img/B14070_05_001.png) and a weight patch, or filter,
    of ![](img/B14070_05_002.png), where ![](img/B14070_05_003.png), the convolution
    operation slides the patch of weights over the input. Let’s denote the input by
    *X*, the patch of weights by *W*, and the output by *H*. The output *H* can be
    calculated as a matrix multiplication as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume ![](img/B14070_05_013.png) and ![](img/B14070_05_014.png) for
    clarity and unwrap the input *X* from left to right, top to bottom, resulting
    in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s define a new matrix *A* from *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, if we perform the following matrix multiplication, we obtain *H*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_017.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, by reshaping the output ![](img/B14070_05_018.png) to ![](img/B14070_05_019.png)
    we obtain the convolved output. Now let’s project this result back to *n* and
    *m*.
  prefs: []
  type: TYPE_NORMAL
- en: By unwrapping the input ![](img/B14070_05_020.png) to ![](img/B14070_05_021.png)
    and by creating a matrix ![](img/B14070_05_022.png) from *w*, as we showed earlier,
    we obtain ![](img/B14070_05_023.png), which will then be reshaped to ![](img/B14070_05_024.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, to obtain the transposed convolution, we simply transpose *A* and arrive
    at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_025.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_05_026.png) is the resultant output of the transposed convolution.
  prefs: []
  type: TYPE_NORMAL
- en: We end our discussion about the convolution operation here. We discussed the
    convolution operation, convolution operation with stride, convolution operation
    with padding, and how to calculate the transposed convolution. Next, we will discuss
    the pooling operation in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pooling operation, which is sometimes known as the subsampling operation,
    was introduced to CNNs mainly for reducing the size of the intermediate outputs
    as well as for making the translation of CNNs invariant. This is preferred over
    the natural dimensionality reduction caused by convolution without padding, as
    we can decide where to reduce the size of the output with the pooling layer, in
    contrast to forcing it to happen every time. Forcing the dimensionality to decrease
    without padding would strictly limit the number of layers we can have in our CNN
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the pooling operation mathematically in the following sections. More
    precisely, we will discuss two types of pooling: max pooling and average pooling.
    First, however, we will define the notation. For an input of size ![](img/B14070_05_001.png)
    and a kernel (analogous to the filter of a convolution layer) of size ![](img/B14070_05_002.png),
    where ![](img/B14070_05_003.png), the convolution operation slides the patch of
    weights over the input. Let’s denote the input by *X*, the patch of weights by
    *W*, and the output by *H*. Then let us use, *x* [i,j], *w*[i,j], and *h*[i,j]
    to denote the value at the (*i*,*j*)^(th) location of *X*, *W*, and *H*, respectively.
    We will now look at specific implementations of pooling commonly used.'
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The max pooling operation picks the maximum element within the defined kernel
    of an input to produce the output. The max pooling operation shifts are windows
    over the input (the middle squares in *Figure 5.8*) and take the maximum at each
    time. Mathematically, we define the pooling equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_030.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_05_030.1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5.8* shows this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Max pooling](img/B14070_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: The max pooling operation with a filter size of 3, stride of 1,
    and no padding'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss how to perform max pooling with stride.
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling with stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Max pooling with stride is similar to convolution with stride. Here is the
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_031.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_05_031.1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5.9* shows the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Max pooling with stride](img/B14070_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: The max pooling operation for an input of size (n=4) with a filter
    size of (m=2), stride (s=2), and no padding'
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss another variant of pooling known as average pooling, below.
  prefs: []
  type: TYPE_NORMAL
- en: Average pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Average pooling works similar to max pooling, except that instead of only taking
    the maximum, the average of all the inputs falling within the kernel is taken.
    Consider the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The average pooling operation is shown in *Figure 5.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Average pooling](img/B14070_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: The average pooling operation for an input of size (n=4) with
    a filter size of (m=2), stride (s=1), and no padding'
  prefs: []
  type: TYPE_NORMAL
- en: We have so far discussed the operations directly performed on the two-dimensional
    inputs like images. Next we will discuss how they are connected to one-dimensional
    fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fully connected layers are a fully connected set of weights from the input to
    the output. These fully connected weights are able to learn global information
    as they are connected from each input to each output. Also, having such layers
    of full connectedness allows us to combine features learned by the convolution
    layers preceding the fully connected layers, globally, to produce meaningful outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define the output of the last convolution or pooling layer to be of size
    ![](img/B14070_05_034.png), where *p* is the height of the input, *o* is the width
    of the input, and *d* is the depth of the input. As an example, think of an RGB
    image, which will have a fixed height, fixed width, and a depth of 3 (one depth
    channel for each RGB component).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, for the initial fully connected layer found immediately after the last
    convolution or pooling layer, the weight matrix will be ![](img/B14070_05_035.png),
    where *height* x *width* x *depth* of the layer output is the number of output
    units produced by that last layer and *m* is the number of hidden units in the
    fully connected layer. Then, during inference (or prediction), we reshape the
    output of the last convolution/pooling layer to be of size ![](img/B14070_05_036.png)
    and perform the following matrix multiplication to obtain *h*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_037.png)'
  prefs: []
  type: TYPE_IMG
- en: The resultant fully connected layers will behave as in a fully connected neural
    network, where you have several fully connected layers and an output layer. The
    output layer can be a softmax classification layer for a classification problem
    or a linear layer for a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we will discuss how the convolutional, pooling, and fully connected layers
    come together to form a complete CNN.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 5.11*, the convolution, pooling, and fully connected layers
    come together to form an end-to-end learning model that takes raw data, which
    can be high-dimensional (for example, RGB images) and produce meaningful output
    (for example, the class of the object). First, the convolution layers learn the
    spatial features of the images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower convolution layers learn low-level features such as differently oriented
    edges present in the images, and the higher layers learn more high-level features
    such as shapes present in the images (for example, circles and triangles) or bigger
    parts of an object (for example, the face of a dog, tail of a dog, and front section
    of a car). The pooling layers in the middle make each of these learned features
    slightly translation invariant. This means that, in a new image, even if the feature
    appears a bit offset compared to the location in which the feature appeared in
    the learned images, the CNN will still recognize that feature. Finally, the fully
    connected layers combine the high-level features learned by the CNN to produce
    global representations that will be used by the final output layer to determine
    the class the object belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Putting everything together](img/B14070_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Combining convolution layers, pooling layers, and fully connected
    layers to form a CNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a strong conceptual understanding of a CNN, we will now get started on
    our first use case: classifying images with a CNN model.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise – image classification on Fashion-MNIST with CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This will be our first example of using a CNN for a real-world machine learning
    task. We will classify images using a CNN. The reason for not starting with an
    NLP task is that applying CNNs to NLP tasks (for example, sentence classification)
    is not very straightforward. There are several tricks involved in using CNNs for
    such a task. However, originally, CNNs were designed to cope with image data.
    Therefore, let’s start there, and then find our way through to see how CNNs apply
    to NLP tasks in the *Using CNNs for sentence classification* section.
  prefs: []
  type: TYPE_NORMAL
- en: About the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will use a dataset well-known in the computer vision community:
    the Fashion-MNIST dataset. Fashion-MNIST was inspired by the famous MNIST dataset
    ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)). MNIST
    is a database of labeled images of handwritten digits from 0 to 9 (i.e. 10 digits).
    However, due to the simplicity of the MNIST image classification task, test accuracy
    on MNIST is just shy of 100%. At the time of writing, the popular research benchmarking
    site *paperswithcode.com* has published a test accuracy of 99.87% ([https://paperswithcode.com/sota/image-classification-on-mnist](https://paperswithcode.com/sota/image-classification-on-mnist)).
    Because of this, Fashion-MNIST came to life.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fashion-MNIST consists of images of clothing garments. Our task is to classify
    each garment into a category (e.g. dress, t-shirt). The dataset contains two sets:
    the training set, and the test set. We will train on the training set and evaluate
    the performance of our model on the unseen test dataset. We will further split
    the training set into two sets: training and validation sets. We will use the
    validation dataset as a continuous performance monitoring mechanism for our model.
    We will discuss the details later, but we will see that we can reach up to approximately
    88% test accuracy without any special regularization or tricks.'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and exploring the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The very first task will be to download and explore the data. To download the
    data, we will simply tap into the `tf.keras.datasets` module, as it provides several
    datasets to be downloaded conveniently through TensorFlow. To see what other datasets
    are available, visit [https://www.tensorflow.org/api_docs/python/tf/keras/datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets).
    The full code for this chapter is available in `ch5_image_classification_fashion_mnist.ipynb`
    in the `Ch05-Sentence-Classification` folder. Simply call the following function
    to download the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The data will be downloaded to a default cache directory specified by TensorFlow
    (for example: `~/.keras/dataset/fasion_minst`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then see the sizes of the data by printing their shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have 60,000 training images, each of size 28x28, and 10,000
    testing images of the same dimensions. The labels are simple class IDs ranging
    from 0 to 9\. We will also create a variable to contain the class ID to class
    name mapping, which will help us during explorations and post-training analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot the images, which will give the following plot of images (*Figure
    5.12*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: An overview of the images found in the Fashion-MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are going to extend `train_images` and `test_images` by adding a
    new dimension (of size 1) to the end of each tensor. Standard implementation of
    the convolution operation in TensorFlow is designed to work on a four-dimensional
    input (i.e. batch, height, width, and channel dimensions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the channel dimension is omitted in the images as they are black and
    white images. Therefore, to comply with the dimensional requirement of TensorFlow’s
    convolution operation, we add this additional dimension to the images. This is
    a necessity for using the convolution operation in CNNs. You can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the indexing and slicing capabilities available in NumPy, you can simply
    add a `None` dimension to the tensor when indexing as above. Let’s now check the
    shapes of the tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s have a crack at implementing a CNN model that can learn from this data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will look at some important code snippets from the TensorFlow
    implementation of the CNN. The full code is available in `ch5_image_classification_mnist.ipynb`
    in the `Ch05-Sentence-Classification` folder. First, we will define several important
    hyperparameters. The code comments are self-explanatory for the purpose of these
    hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With that, we can start to implement the model. We will find inspiration from
    one of the earliest CNN models, known as LeNet, introduced in the paper *Gradient-Based
    Learning Applied to Document Recognition* by LeCun et al. ([http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)).
    This model will be a great start as it is a simple model yet gives a reasonably
    good performance on the dataset. We will introduce some slight modifications to
    the original model, because the original model operated on a 32x32-sized image,
    whereas in our case, the image is a 28x28-sized image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through some quick details of the model. It has the following sequence
    of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer with a 5x5 kernel, 1x1 stride, and valid padding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A convolutional layer with a 5x5 kernel, 1x1 stride, and valid pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A convolutional layer with a 4x4 kernel, 1x1 stride, and valid pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A layer that flattens the 2D output to a 1D vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with 84 nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A final softmax prediction layer with 10 nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, all the layers except the last have ReLU (Rectified Linear Unit) activation.
    A convolutional layer in a CNN model generalizes the convolution operation we
    discussed, to work on multi-channel inputs and produce multi-channel outputs.
    Let’s understand what we meant by that. The original convolution operation we
    saw operated on a simple 2D plane with a height *h* and width *w*. Next, the kernel
    moves over the plane while producing a single value at each position. This process
    produces another 2D plane. But in practice, CNN models operate on four-dimensional
    inputs, i.e. an input of size `[batch size, height, width, in channels]`, and
    produce an output that is a four-dimensional, i.e. an output of size `[batch size,
    height, width, out channels]`. To produce this output, the kernel would need to
    be a four-dimensional tensor having the dimensions `[kernel height, kernel width,
    in channels, out channels]`.
  prefs: []
  type: TYPE_NORMAL
- en: It might not be entirely clear why inputs, outputs, and kernels would be in
    this format. *Figure 5.13* clarifies this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: How input and output shapes look for a two-dimensional convolution
    layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we will outline the full model. Don’t worry if you don’t understand
    it at first glance. We will go through line by line to understand how the model
    comes to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The very first thing to notice is that we are using the Keras Sequential API.
    The CNN we are implementing here has a series of layers connected one after the
    other. Therefore, we will use the simplest API possible. We then have our first
    convolutional layer. We have already discussed the convolution operation. Let’s
    take the first line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tensorflow.keras.layers.Conv2D` layer takes the following argument values
    in that order:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filters` (`int`): This is the number of output filters (i.e. the number of
    out channels).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size` (`Tuple[int]`): This is the (height, width) of the convolution
    kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strides` (`Tuple[int]`): This denotes the stride on the height and width dimension
    of the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`str`): This denotes the type of padding (can be `''``SAME''` or
    `''VALID''`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation` (`str`): The non-linear activation used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_shape` (`Tuple[int]`): The shape of the input. When defining `input_shape`,
    we do not specify the batch dimension as it’s automatically added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we have the first max-pooling layer, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments are quite similar to the ones in `tf.keras.layers.Conv2D`. The
    `pool_size` argument corresponds to the `kernel_size` argument that specifies
    the (height, width) of the pool window. Following a similar pattern, the following
    convolutional and pooling layers are defined. The final convolution layer produces
    a `[batch size, 1, 1, 120]`-sized output. The height and width dimensions are
    equal to 1, because LeNet is designed in a way that the last convolutional kernel
    has the same height and width as the output. Before this input is fed to a fully
    connected layer, we need to flatten this output, such that it has the shape `[batch
    size, 120]`. This is because a standard Dense layer takes a two-dimensional input.
    For that, we use the `tf.keras.layers.Flatten()` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we define two Dense layers as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As the final step, we will compile the model using the sparse categorical cross-entropy
    loss and the Adam optimizer. We will also track the accuracy on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data prepared and the model defined fully, we are good to train our
    model. Model training is as simple as calling one function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tf.keras.layers.Model.fit()` takes many arguments. But let’s only discuss
    the ones we have used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x` (`np.ndarray` / `tf.Tensor` / other): Takes in a tensor that will act as
    input to the model (implemented as a NumPy array or a TensorFlow tensor). But
    the accepted values are not limited just to tensors. To see the full list, please
    refer to [https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` (`np.ndarray` / `tf.Tensor`): Takes in a tensor that will act as the labels
    (targets) for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_split` (`float`): Setting this argument means a fraction of training
    data (e.g. 0.2 translates to 20%) will be used as validation data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs` (`int`): The number of epochs to train the model for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can evaluate the trained model on the test data by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once run, you’ll see an output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The model should get up to around 88% accuracy when trained.
  prefs: []
  type: TYPE_NORMAL
- en: You just finished learning about the functions that we used to create our first
    CNN. You learned to use the functions to implement the CNN structure as well as
    define the loss, minimize the loss, and get predictions for unseen data. We used
    a simple CNN to see if it could learn to classify clothing items. Also, we were
    able to achieve an accuracy above 88% with a reasonably simple CNN. Next, we will
    analyze some of the results produced by the CNN. We will see why the CNN couldn’t
    recognize some of the images correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the predictions produced with a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we can randomly pick some correctly and incorrectly classified samples
    from the test set to evaluate the learning power of CNNs (see *Figure 5.14*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that for the correctly classified instances, the CNN is very confident
    about the output, most of the time. This is a good sign that the model is making
    very confident and accurate decisions. However, when we evaluate the incorrectly
    classified examples, we can see that some of them are in fact difficult, and even
    a human can get some of them wrong. For example, for an ankle boot that’s classified
    as a sandal, there is a large black patch that can indicate the presence of straps,
    which makes it more likely to be a sandal (the third image from the right in the
    third row). Also, in the fifth image from the right in the third row, it’s difficult
    to say whether it’s a shirt or a collared t-shirt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Fashion-MNIST correctly classified and misclassified instances'
  prefs: []
  type: TYPE_NORMAL
- en: Using CNNs for sentence classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though CNNs have mostly been used for computer vision tasks, nothing stops them
    from being used in NLP applications. But as we highlighted earlier, CNNs were
    originally designed for visual content. Therefore, using CNNs for NLP tasks requires
    somewhat more effort. This is why we started out learning about CNNs with a simple
    computer vision problem. CNNs are an attractive choice for machine learning problems
    due to the low parameter count of convolution layers. One such NLP application
    for which CNNs have been used effectively is sentence classification.
  prefs: []
  type: TYPE_NORMAL
- en: In sentence classification, a given sentence should be classified with a class.
    We will use a question database, where each question is labeled by what the question
    is about. For example, the question “Who was Abraham Lincoln?” will be a question
    and its label will be *Person*. For this we will use a sentence classification
    dataset available at [http://cogcomp.org/Data/QA/QC/](http://cogcomp.org/Data/QA/QC/);
    here you will find several datasets. We are using the set with ~5,500 training
    questions and their respective labels and 500 testing sentences.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the CNN network introduced in a paper by Yoon Kim, *Convolutional
    Neural Networks for Sentence Classification*, to understand the value of CNNs
    for NLP tasks. However, using CNNs for sentence classification is somewhat different
    from the Fashion-MNIST example we discussed, because operations (for example,
    convolution and pooling) now happen in one dimension (length) rather than two
    dimensions (height and width). Furthermore, the pooling operations will also have
    a different flavor to the normal pooling operation, as we will see soon. You can
    find the code for this exercise in the `ch5_cnn_sentence_classification.ipynb`
    file in the `Ch5-Sentence-Classification` folder. As the first step, we will understand
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: How data is transformed for sentence classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume a sentence of *p* words. First, we will pad the sentence with some
    special words (if the length of the sentence is < *n*) to set the sentence length
    to *n* words, where ![](img/B14070_05_038.png). Next, we will represent each word
    in the sentence by a vector of size *k*, where this vector can either be a one-hot-encoded
    representation, or Word2vec word vectors learned using skip-gram, CBOW, or GloVe.
    Then a batch of sentences of size *b* can be represented by a ![](img/B14070_05_039.png)
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through an example. Let’s consider the following three sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bob and Mary are friends.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bob plays soccer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mary likes to sing in the choir.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this example, the third sentence has the most words, so let’s set *n* =
    *7*, which is the number of words in the third sentence. Next, let’s look at the
    one-hot-encoded representation for each word. In this case, there are 13 distinct
    words. Therefore, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bob*: 1,0,0,0,0,0,0,0,0,0,0,0,0'
  prefs: []
  type: TYPE_NORMAL
- en: '*and*: 0,1,0,0,0,0,0,0,0,0,0,0,0'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mary*: 0,0,1,0,0,0,0,0,0,0,0,0,0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, *k* = *13* for the same reason. With this representation, we can represent
    the three sentences as a three-dimensional matrix of size *3 x 7 x 13*, as shown
    in *Figure 5.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data transformation](img/B14070_05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: A batch of sentences represented as a sentence matrix'
  prefs: []
  type: TYPE_NORMAL
- en: You could also utilize word embeddings instead of one-hot encoding here. Representing
    each word as a one-hot-encoded feature introduces sparsity and wastes computational
    memory. By using embeddings, we are enabling the model to learn more compact and
    powerful word representations than one-hot-encoded representations. This also
    means that ![](img/B14070_04_020.png) becomes a hyperparameter (i.e. the embedding
    size), as opposed to being driven by the size of the vocabulary. This means that,
    in *Figure 5.15*, each column will be a distributed continuous vector, not a combination
    of 0s and 1s.
  prefs: []
  type: TYPE_NORMAL
- en: We know that one-hot vectors lead to high-dimensional and highly sparse representations
    that are sub-optimal. On the other hand, word vectors give richer representations
    of words. However, learning word vectors is computationally costly. There is another
    alternative called the hashing trick. The beauty of the hashing trick is that
    it is extremely simple but gives a powerful and economical alternative that sits
    between one-hot vectors and word vectors. The idea behind the hashing trick is
    to use a hash function that converts a given token to an integer.
  prefs: []
  type: TYPE_NORMAL
- en: '*f(<token>)-->hash value*'
  prefs: []
  type: TYPE_NORMAL
- en: Here *f* is a chosen hash function. Some example popular hash functions are
    SHA ([https://brilliant.org/wiki/secure-hashing-algorithms/](https://brilliant.org/wiki/secure-hashing-algorithms/))
    and MD5 ([https://searchsecurity.techtarget.com/definition/MD5](https://searchsecurity.techtarget.com/definition/MD5)).
    There’s also more advanced hashing such as locality-sensitive hashing ([https://www.pinecone.io/learn/locality-sensitive-hashing/](https://www.pinecone.io/learn/locality-sensitive-hashing/))
    to give out similar IDs for morphologically similar words. You can easily use
    the hashing trick via TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick)).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation – downloading and preparing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First we will download the data from the web. The data download functions are
    provided in the notebook and are simply downloading two files: training and testing
    data (the paths to the files are retained in `train_filename` and `test_filename`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open these files you will see that they contain a collection of lines
    of text. Each line has the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<Category>: <sub-category> <question>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two pieces of meta information for each question: a category and
    a sub-category. A category is a macro-level classification, where sub-category
    is a finer grain identification of the type of the question. There are six categories
    available: `DESC` (description-related), `ENTY` (entity-related), `HUM` (human-related),
    `ABBR` (abbreviation related), `NUM` (numerical), and `LOC` (location related).
    Each category has several sub-categories associated with them. For example, the
    `ENTY` category is further broken down to animal, currency, events, food, etc.
    For our problem, we will be focusing on high-level classification (i.e. six classes),
    but you could also leverage the same model with minimal changes to classify on
    the sub-category level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the files are downloaded, we’ll read the data into the memory. For that,
    we will implement the `read_data()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This function simply goes through each line in the file and separates the question,
    category, and sub-category, using the format of each line elucidated above. After
    that, each question, category, and sub-category is written to the lists `questions`,
    `categories`, and `sub_categories` respectively. Finally, the function returns
    these lists. With the `questions`, `categories`, and `sub_categories` available
    for both training and testing data, we will create `pandas` DataFrames for training
    and testing data.
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` DataFrames are an expressive data structure for storing multi-dimensional
    data. A DataFrame can have indices, columns, and values. Each value has a specific
    index and a column. It is quite simple to create a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We call the `pd.DataFrame` construct with a dictionary. The keys of the dictionary
    represent columns of the DataFrame, and the values represent the elements in each
    column. Here we create three columns: `question`, `category`, and `sub_category`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.16* depicts what the `train_df` looks like.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: A sample of data captured in the pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do a simple shuffle of rows in the training set, to make sure we are
    not introducing any unintentional ordering in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This process will sample 100% of the data from the DataFrame randomly. In other
    words, it will shuffle the order of the rows. From this point onward, we will
    not consider the `sub_category` column. We will first map each class label to
    a class ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We first identify the unique values present in the `train_df["category"]`. Then
    we will create a dictionary by mapping from the unique values to a list of numerical
    IDs (0 to 5). The `np.arange()` function can be used to generate a series of integers
    in a specified range (here, the range is from 0 to the length of `unique_cats`).
    This process will give us the following `labels_map`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Label->ID mapping: {0: 0, 1: 1, 2: 2, 4: 3, 3: 4, 5: 5}`'
  prefs: []
  type: TYPE_NORMAL
- en: Then we simply apply this mapping to the category column of both the train and
    test DataFrames to convert string labels to numerical labels. The data would look
    as follows, after the transformation (*Figure 5.17*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: A sample of data in the DataFrame after mapping categories to
    integers'
  prefs: []
  type: TYPE_NORMAL
- en: We create a validation set, stemming from the original training set, to monitor
    model performance while it trains. We will use the `train_test_split()` function
    from the scikit-learn library. 10% of the data will be separated as validation
    data, while 90% is kept as training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can see that approximately 4,900 examples are used as training and the rest
    as validation. In the next section, we will build a tokenizer to tokenize the
    questions and assign individual tokens numerical IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation – building a tokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Moving on, now it’s time to build a tokenizer that can map words to numerical
    IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we simply create a `Tokenizer` object and use the `fit_on_texts()` function
    to train it on the training corpus. In this process, the tokenizer will map words
    in the vocabulary to IDs. We will convert all of the train, validation, and test
    inputs to sequences of word IDs. Simply call the `tokenizer.texts_to_sequences()`
    function with a list of strings, where each string represents a question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s important to understand that we are feeding our model a batch of questions
    at a given time. It is very unlikely that all of the questions have the same number
    of tokens. If all questions do not have the same number of tokens, we cannot form
    a tensor due to the uneven lengths of different questions. To solve this, we have
    to pad shorter sequences with special tokens and truncate sequences longer than
    a specified length. To achieve this we can easily use the `tf.keras.preprocessing.sequence.pad_sequences()`
    function. It would be worthwhile going through the arguments accepted by this
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences (List[List[int]])` – List of list integers; each list of integers
    is a sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxlen (int)` – The maximum padding length'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding (string)` – Whether to pad at the beginning `(pre)` or end `(post)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncating (string)` – Whether to truncate at the beginning `(pre)` or end
    `(post)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value (int)` – What value is to be used for padding (defaults to 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Below we use this function to create sequence matrices for training, validation,
    and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The reason we picked 22 as the sequence length is through a simple analysis.
    The 99% percentile of the sequence lengths of the training corpus is equal to
    22\. Therefore, we have picked that. Another important statistic is that the vocabulary
    size will be approximately 7,880 words. Now we will discuss the model.
  prefs: []
  type: TYPE_NORMAL
- en: The sentence classification CNN model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we will discuss the technical details of the CNN used for sentence classification.
    First, we will discuss how data or sentences are transformed into a preferred
    format that can easily be dealt with by CNNs. Next, we will discuss how the convolution
    and pooling operations are adapted for sentence classification, and finally, we
    will discuss how all these components are connected.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we ignore the batch size, that is, if we assume that we are only processing
    a single sentence at a time, our data is a ![](img/B14070_05_042.png) matrix,
    where *n* is the number of words per sentence after padding, and *k* is the dimension
    of a single word vector. In our example, this would be *7* x *13*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will define our convolution weight matrix to be of size ![](img/B14070_05_043.png),
    where *m* is the filter size for a one-dimensional convolution operation. By convolving
    the input *x* of size ![](img/B14070_05_042.png) with a weight matrix *W* of size
    ![](img/B14070_05_043.png), we will produce an output of *h* of size ![](img/B14070_05_046.png)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w*[i,j] is the *(i,j)*^(th) element of *W* and we will pad *x* with
    zeros so that *h* is of size ![](img/B14070_05_046.png). Also, we will define
    this operation more simply, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *** defines the convolution operation (with padding) and we will add
    an additional scalar bias *b*. *Figure 5.18* illustrates this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The convolution operation](img/B14070_05_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: A convolution operation for sentence classification. Convolution
    layers with different kernel widths are used to convolve over the sentence (i.e.
    sequence of tokens)'
  prefs: []
  type: TYPE_NORMAL
- en: Then, to learn a rich set of features, we have parallel layers with different
    convolution filter sizes. Each convolution layer outputs a hidden vector of size
    ![](img/B14070_05_046.png), and we will concatenate these outputs to form the
    input to the next layer of size ![](img/B14070_05_051.png), where *q* is the number
    of parallel layers we will use. The larger *q* is, the better the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of convolving can be understood in the following manner. Think about
    the movie rating learning problem (with two classes, positive or negative), and
    we have the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I like the movie, not too bad*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I did not like the movie, bad*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now imagine a convolution window of size 5\. Let’s bin the words according to
    the movement of the convolution window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence *I like the movie, not too bad* gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[I, like, the, movie, ‘,’]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[like, the, movie, ‘,’, not]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[the, movie, ‘,’, not, too]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[movie, ‘,’, not, too, bad]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence *I did not like the movie, bad* gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[I, did, not, like, the]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[did, not ,like, the, movie]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[not, like, the, movie, ‘,’]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[like, the, movie, ‘,’, bad]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first sentence, windows such as the following convey that the rating
    is positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[I, like, the, movie, ‘,’]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[movie, ‘,’, not, too, bad]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for the second sentence, windows such as the following convey negativity
    in the rating:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[did, not, like, the, movie]*'
  prefs: []
  type: TYPE_NORMAL
- en: We are able to see such patterns that help to classify ratings thanks to the
    preserved spatiality. For example, if you use a technique such as *bag-of-words*
    to calculate sentence representations that lose spatial information, the sentence
    representations of the above two sentences would be highly similar. The convolution
    operation plays an important role in preserving the spatial information of the
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Having *q* different layers with different filter sizes, the network learns
    to extract the rating with different size phrases, leading to an improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling over time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pooling operation is designed to subsample the outputs produced by the previously
    discussed parallel convolution layers. This is achieved as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume the output of the last layer *h* is of size ![](img/B14070_05_051.png).
    The pooling over time layer would produce an output *h’* of size ![](img/B14070_05_053.png)
    output. The precise calculation would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_05_054.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_05_055.png) and *h*^((i)) is the output produced by the
    *i*^(th) convolution layer and ![](img/B14070_05_056.png) is the set of weights
    belonging to that layer. Simply put, the pooling over time operation creates a
    vector by concatenating the maximum element of each convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will illustrate this operation in *Figure 5.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling over time](img/B14070_05_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19: The pooling over time operation for sentence classification'
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining these operations, we finally arrive at the architecture shown
    in *Figure 5.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling over time](img/B14070_05_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5\. 20: A sentence classification CNN architecture. The pool of convolution
    layers having different kernel widths produces a set of output sequences. They
    are fed into the Pooling Over Time Layer that produces a compact representation
    of that input. This is finally connected to a classification layer with softmax
    activation'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation – sentence classification with CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are off implementing the model in TensorFlow 2\. As a prerequisite, let’s
    import several necessary modules from TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Clear the running session to make sure previous runs are not interfering with
    the current run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start, we will be using the Functional API from Keras. The reason
    for this is that the model we will be building here cannot be built with the Sequential
    API, due to intricate pathways present in the model. Let’s start off by creating
    an input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The input layer simply takes a batch of `max_seq_length` word IDs. That is,
    a batch of sequences, where each sequence is padded/truncated to a max length.
    We specify the `dtype` as `int32`, since they are word IDs. Next, we define an
    embedding layer, from which we will look up embeddings corresponding to the word
    IDs coming through the `word_id_inputs` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a randomly initialized embedding layer. It contains a large matrix
    of size `[n_vocab, 64]`, where each row represents the word vector of the word
    indexed by that row number. The embeddings will be jointly learned with the model,
    while the model is trained on the supervised task. For the next part, we will
    define three different one-dimensional convolution layers with three different
    kernel (filter) sizes of `3`, `4`, and `5`, having 100 feature maps each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'An important distinction to make here is that we are using one-dimensional
    convolution as opposed to the two-dimensional convolution we used in the earlier
    exercise. However, most of the concepts remain the same. The main difference is
    that, unlike `tf.keras.layers.Conv2D,` which works on four-dimensional inputs,
    `tf.keras.layers.Conv1D` operates on three-dimensional inputs (i.e. inputs with
    shape `[batch size, width, in channels]`). In other words, the convolution kernel
    moves only in one direction over the inputs. Each of these layers produces a `[batch
    size, sentence length, 100]`-sized output. Afterward, these outputs are concatenated
    on the last axis to produce a single tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Subsequently, the new tensor of size `[batch size, sentence length, 300]` will
    be used to perform the pooling over time operation. We can implement the pooling
    over time operation by defining a one-dimensional max-pooling layer (i.e. `tf.keras.layers.MaxPool1D`)
    with a window as wide as the sequence length. This will produce a single value
    as the output, for each feature map in `conv_out`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we get a `[batch_size, 1, 300]`-sized output after performing the operation.
    Next, we will convert this output to a `[batch_size, 300]`-sized output, by using
    the `tf.keras.layers.Flatten` layer. The Flatten layer simply collapses all the
    dimensions (except the batch dimension) to a single dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, `flatten_out` is passed to a Dense layer that has `n_classes` (i.e.
    six) nodes as the output and has a softmax activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of the `kernel_regularizer` argument. We can use this argument
    to add any special regularization (e.g. L1 or L2 regularization) to a given layer.
    Finally, we define a model as,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'and compile the model with the desired loss function, an optimizer, and metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the model by running the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: which gives,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will train the model on the data we already prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we have done the hard yard at the beginning, by making sure the data
    is transformed, training the model is simple. All we need to do is call the `tf.keras.layers.Model.fit()`
    function. However, let’s leverage a few techniques to improve model performance.
    This will be done by leveraging a built-in callback of TensorFlow. The technique
    we’ll be using is known as “decaying the learning rate.” The idea is to reduce
    the learning rate (by some fraction) whenever the model has stopped to improve
    performance. The following callback assists us to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters can be set as you wish, to control the learning rate reduction.
    Let’s understand the arguments above:'
  prefs: []
  type: TYPE_NORMAL
- en: '`monitor (str)` – Which metric to monitor in order to decay the learning rate.
    We will monitor the validation loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`factor (float)` – By how much to reduce the learning rate. For example, a
    factor of 0.1 means that the learning rate will be reduced by 10 times (e.g. 0.01
    will be stepped down to 0.001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patience (int)` – How many epochs to wait without an improvement, before reducing
    the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode (string)` – Whether to look for an increase or decrease of the metric;
    ‘auto’ means that the direction will be determined by looking at the metric name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_delta (float)` – How much of an increase/decrease to consider as an improvement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_lr (float)` – Minimum learning rate (floor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the accuracy quickly going up and the validation accuracy plateauing
    around 88%. Here’s a snippet of the output produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s test the model on the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the test data as given in the exercise gives us a test accuracy of
    close to 88% (for 500 test sentences) in this sentence classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Here we end our discussion about using CNNs for sentence classification. We
    first discussed how one-dimensional convolution operations combined with a special
    pooling operation called *pooling over time* can be used to implement a sentence
    classifier based on the CNN architecture. Finally, we discussed how to use TensorFlow
    to implement such a CNN and saw that it in fact performs well in sentence classification.
  prefs: []
  type: TYPE_NORMAL
- en: It can be useful to know how the problem we just solved can be useful in the
    real world. Assume that you have a large document about the history of Rome in
    your hand, and you want to find out about Julius Caesar without reading the whole
    document. In this situation, the sentence classifier we just implemented can be
    used as a handy tool to summarize the sentences that only correspond to a person,
    so you don’t have to read the whole document.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence classification can be used for many other tasks as well; one common
    use of this is classifying movie reviews as positive or negative, which is useful
    for automating the computation of movie ratings. Another important application
    of sentence classification can be seen in the medical domain, where it is used
    to extract clinically useful sentences from large documents containing large amounts
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed CNNs and their various applications. First, we
    went through a detailed explanation of what CNNs are and their ability to excel
    at machine learning tasks. Next we decomposed the CNN into several components,
    such as convolution and pooling layers, and discussed in detail how these operators
    work. Furthermore, we discussed several hyperparameters that are related to these
    operators such as filter size, stride, and padding.
  prefs: []
  type: TYPE_NORMAL
- en: Then, to illustrate the functionality of CNNs, we walked through a simple example
    of classifying images of garments. We also did a bit of analysis to see why the
    CNN fails to recognize some images correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we started talking about how CNNs are applied for NLP tasks. Concretely,
    we discussed an altered architecture of CNNs that can be used to classify sentences.
    We then implemented this particular CNN architecture and tested it on an actual
    sentence classification task.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on to one of the most popular types of neural
    networks used for many NLP tasks – **Recurrent Neural Networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  prefs: []
  type: TYPE_IMG
