<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer124">&#13;
			<p id="_idParaDest-142" class="chapter-number"><a id="_idTextAnchor249"/>Chapter 9:</p>&#13;
			<h1 id="_idParaDest-143"><a id="_idTextAnchor250"/>Serving a TensorFlow Model </h1>&#13;
			<p>By now, after learning all the previous chapters, you have seen many facets of a model building process in <strong class="bold">TensorFlow Enterprise (TFE)</strong>. Now it is time to wrap up what we have done and look at how we can serve the model we have built. In this chapter, we are going to look at the fundamentals of serving a TensorFlow model, which is through a RESTful API in localhost. The easiest way to get started is by using <strong class="bold">TensorFlow Serving (TFS)</strong>. Out of the box, TFS is a system for serving machine learning models built with TensorFlow. Although it is not yet officially supported by TFE, you will see that it works with models built by TFE 2. It can run as either a server or as a Docker container. For our ease, we are going to use a Docker container, as it is really the easiest way to start using TFS, regardless of your local environment, as long as you have a Docker engine available. In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Running Local Serving</li>&#13;
				<li>Understanding TFS with Docker</li>&#13;
				<li>Downloading TFS Docker images</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-144"><a id="_idTextAnchor251"/>Technical requirements</h1>&#13;
			<p>To follow along with this chapter, and for trying the example code here: https://github.com/PacktPublishing/learn-tensorflow-enterprise, you will need to clone the GitHub repository for this book, and navigate to the folder in <strong class="source-inline">chapter_09</strong>. You may clone the repository with the following command: </p>&#13;
			<p class="source-code">git clone https://github.com/PacktPublishing/learn-tensorflow-enterprise.git</p>&#13;
			<p>We will work from the folder named <strong class="source-inline">chapter_09</strong>. Inside this folder, there is a Jupyter notebook containing source code. You will also find the <strong class="source-inline">flowerclassifier/001</strong> directory, which contains a <strong class="source-inline">saved_model.pb</strong> file ready for your use. In the <strong class="source-inline">raw_images</strong> directory, you will find a few raw JPG images for testing. </p>&#13;
			<h1 id="_idParaDest-145"><a id="_idTextAnchor252"/>Running Local Serving</h1>&#13;
			<p>A prerequisite to serving the model is serialization of the model structure and its assets, such as weights and biases matrices. A trained <a id="_idIndexMarker495"/>TensorFlow model is typically saved in a <strong class="source-inline">SavedModel</strong> format. A <strong class="source-inline">SavedModel</strong> format consists of the complete TensorFlow program with weights, biases, and computation ops. This is done through the low-level <strong class="source-inline">tf.saved_model</strong> API.</p>&#13;
			<p>Typically, when you execute a model training process using Fit, you end up with something like this:</p>&#13;
			<p class="source-code">mdl.fit(</p>&#13;
			<p class="source-code">    train_dataset,</p>&#13;
			<p class="source-code">    epochs=5, steps_per_epoch=steps_per_epoch,</p>&#13;
			<p class="source-code">    validation_data=valid_dataset,</p>&#13;
			<p class="source-code">    validation_steps=validation_steps)</p>&#13;
			<p>After you've executed the preceding code, you have a model object, <strong class="source-inline">mdl</strong>, that can be saved via the following syntax:</p>&#13;
			<p class="source-code">saved_model_path = ''</p>&#13;
			<p class="source-code">tf.saved_model.save(mdl, saved_model_path)</p>&#13;
			<p>If you take a look at the current directory, you will find a <strong class="source-inline">saved_model.pb</strong> file there. </p>&#13;
			<p>For your convenience, a <strong class="source-inline">saved_model</strong> file is provided for this exercise. In the <strong class="source-inline">flowerclassifier/001</strong> directory, you will find the following output:</p>&#13;
			<p class="source-code">-rw-r--r--  1 2405393 Oct 12 22:02 saved_model.pb</p>&#13;
			<p class="source-code">drwxr-xr-x@ 2      64 Oct 12 22:02 assets</p>&#13;
			<p class="source-code">drwxr-xr-x@ 4     128 Oct 12 22:02 variables</p>&#13;
			<p>Notice that <strong class="source-inline">save_model_path</strong> is defined as <strong class="source-inline">null</strong>. This indicates that the model is to be saved in the current directory. If you have another directory that you want to use, you need to specify the full or relative path for that directory.</p>&#13;
			<p><strong class="source-inline">saved_model.pb</strong> is the Protobuf format of the model structure. The <strong class="source-inline">assets</strong> folder contains objects such as a vocabulary list or any lookup table, which are necessary for model execution. It may be empty if no such objects are created or required. The <strong class="source-inline">variables</strong> folder contains the weights and bias values as the result of training. These items constitute <strong class="source-inline">SavedModel</strong>. We are going to take a look at how to invoke <strong class="source-inline">SavedModel</strong> for scoring test data. Now let's turn our attention to the Jupyter notebook in this chapter's GitHub repository:</p>&#13;
			<ol>&#13;
				<li>If you simply want to use a Python script to invoke <strong class="source-inline">SavedModel</strong>, it is very simple. All you need to do is load the model as follows:<p class="source-code">path_saved_model =  'flowerclassifier/001'</p><p class="source-code">working_model = tf.saved_model.load(path_saved_model)</p></li>&#13;
				<li>Each <strong class="source-inline">SavedModel</strong> has a default model signature that describes model inputs and outputs structures. This signature also has a name associated with it. We need to find out what this name is:<p class="source-code">print(list(working_model.signatures.keys()))</p><p>Since the signature name is not specified during the save process, the output of the signature name is as follows:</p><p class="source-code">['serving_default']</p></li>&#13;
				<li>Next, we need <a id="_idIndexMarker496"/>to create an inference object, <strong class="source-inline">infer</strong>, and then find the name for the model output as well as its shape, which are required when using this model to score test data:<p class="source-code">infer = working_model.signatures['serving_default']</p><p class="source-code">print(infer.structured_outputs)</p><p>This will output the following:</p><p class="source-code">{'custom_class': TensorSpec(shape=(None, 5), dtype=tf.float32, name='custom_class')}</p><p>The output is named <strong class="source-inline">custom_class</strong>, and it is a tensor with a floating-point NumPy array of <strong class="source-inline">shape=(None, 5)</strong>. This indicates that the output is an array of probabilities for each of the five flower types. And the position index of the array with the highest probability is what we need to map to the flower type. We have seen this map in <a href="B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Optimization</em>, where we learned how to process TFRecord to build and train this model. This is the map:</p><p class="source-code">{4: 'tulips', 3: 'dandelion', 1: 'sunflowers', 2: 'daisy', 0: 'roses'}</p><p>If the highest probability is in the first position in the <strong class="source-inline">custom_class</strong> output's array, then the prediction is mapped to <strong class="source-inline">roses</strong>. If it is in the fifth position, then the prediction is mapped to <strong class="source-inline">tulips</strong>.</p></li>&#13;
				<li>Another thing we need to confirm is the shape of the input expected by the model. We may use <strong class="source-inline">save_model_cli</strong> to give us this information. We may execute this inline command in the Jupyter notebook cell:<p class="source-code">!saved_model_cli show --dir {path_saved_model} --all</p><p>You will observe that the output of this command includes the following:</p><p class="source-code">signature_def['serving_default']:</p><p class="source-code">  The given SavedModel SignatureDef contains the following input(s):</p><p class="source-code">    inputs['input_4'] tensor_info:</p><p class="source-code">        dtype: DT_FLOAT</p><p class="source-code">        shape: (-1, 224, 224, 3)</p><p>Notice the <strong class="source-inline">shape</strong> requirement. We know that (<strong class="source-inline">224</strong>, <strong class="source-inline">224</strong>, <strong class="source-inline">3</strong>) refers to the image dimensions. <strong class="source-inline">-1</strong> in the first dimension indicates this input is set up to handle multiple (batches) of (<strong class="source-inline">224</strong>, <strong class="source-inline">224</strong>, <strong class="source-inline">3</strong>) image arrays. Therefore, if we want to score one image, we need to expand that image by a dimension.</p></li>&#13;
				<li>Let's use a test <a id="_idIndexMarker497"/>image in the <strong class="source-inline">raw_image</strong> directory and read the image with the <strong class="source-inline">nvision</strong> library's <strong class="source-inline">imread</strong>:<p class="source-code">jpg1 = 'raw_images2440874162_27a7030402_n.jpg'</p><p class="source-code">img1_np = nv.imread(jpg1, resize=(224,224),normalize=True)</p><p class="source-code">img1_np = nv.expand_dims(img1_np,axis=0)</p><p>Notice that we only need to provide the height and width for resizing images to the correct pixel count in each dimension. </p></li>&#13;
				<li>Use the <strong class="source-inline">infer</strong> object to score this image:<p class="source-code">prediction = infer(tf.constant(img1_np))</p><p>This produces the prediction for each of the five flower types, given<strong class="source-inline"> img1_np</strong>:</p><p class="source-code">prediction['custom_class'].numpy()</p><p>This generates the following output:</p><p class="source-code">array([[2.4262092e-06, 5.6151916e-06, 1.8000206e-05, 1.4342861e-05, 9.9995959e-01]], dtype=float32)</p><p>The highest probability occurs in the fifth position with a value of <strong class="source-inline">9.9995959e-01</strong>. Therefore, based on the aforementioned map in <em class="italic">step 3</em>, this image is mapped to <strong class="source-inline">tulips</strong>. </p></li>&#13;
			</ol>&#13;
			<p>We have seen how to use <strong class="source-inline">SavedModel</strong> for inference. This requires us to work in a Python runtime to load the model, read the image, and pass it to the model for scoring. In a production or application environment, however, the call to model is usually through TFS. In the next section, we are going to see how to make this model work with this approach<a id="_idTextAnchor253"/>. </p>&#13;
			<h1 id="_idParaDest-146"><a id="_idTextAnchor254"/>Understanding TensorFlow Serving with Docker</h1>&#13;
			<p>At the core of <a id="_idIndexMarker498"/>TFS is actually a TensorFlow model <a id="_idIndexMarker499"/>server that runs a model Protobuf file. Installing the model server is not straightforward, as there are many dependencies. As a convenience, the TensorFlow team also provides this model server in a Docker container, which is a platform that uses virtualization at the operating system level, and it is self-contained with all the necessary dependencies (that is, libraries or modules) to run in an isolated environment. </p>&#13;
			<p>Therefore, the easiest way to deploy a TensorFlow <strong class="source-inline">SavedModel</strong> is by means of TFS with a Docker container. To install <a id="_idIndexMarker500"/>Docker, you can refer to the Docker site (<a href="https://docs.docker.com/install/">https://docs.docker.com/install/</a>), along with the instructions for Mac, Windows, or Linux installations. For our chapter, a community <a id="_idIndexMarker501"/>version will suffice. We will be using Docker <a id="_idIndexMarker502"/>Desktop 2.4 running in macOS Catalina 10.15.6 with specs as indicated in <em class="italic">Figure 9<a id="_idTextAnchor255"/>.1</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer121" class="IMG---Figure">&#13;
					<img src="Images/image0014.jpg" alt="Figure 9.1 – The Docker version used for this chapter&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 9.1 – The Docker version used for this chapter</p>&#13;
			<p>It is assumed that you have installed Docker Desktop properly and that it is running. At a high level, we are going to download a TFS Docker image, add our model to it, and build a new Docker image on top of the base image, which is TFS. The final image is exposed through a TCP/IP port, which handles a RESTful API call from a client. </p>&#13;
			<h1 id="_idParaDest-147"><a id="_idTextAnchor256"/>Downloading TensorFlow Serving Docker images</h1>&#13;
			<p>Once the <a id="_idIndexMarker503"/>Docker engine is up and running, you are ready to perform the following steps:</p>&#13;
			<ol>&#13;
				<li value="1">You may pull the latest TFS Docker image with this Docker command:<p class="source-code"><strong class="bold">docker pull tensorflow/serving</strong></p></li>&#13;
				<li>This is now our base image. In order to add our model on top of this image, we need to run this base image first: <p class="source-code"><strong class="bold">docker run -d --name serv_base_img tensorflow/serving</strong></p></li>&#13;
			</ol>&#13;
			<p>In the <a id="_idIndexMarker504"/>preceding command, we invoked the <strong class="source-inline">tensorflow/serving</strong> image and now it is running as a Docker container. We also name this container <strong class="source-inline">serv_base_img</strong>. </p>&#13;
			<h2 id="_idParaDest-148"><a id="_idTextAnchor257"/>Creating a new image with the model and serving it</h2>&#13;
			<p>Let's now <a id="_idIndexMarker505"/>take a look at the file directory here. For <a id="_idIndexMarker506"/>this example, the directory structure is as shown in the following <a id="_idTextAnchor258"/>figure:</p>&#13;
			<div>&#13;
				<div id="_idContainer122" class="IMG---Figure">&#13;
					<img src="Images/image0021.jpg" alt="Figure 9.2 – Directory structure for creating a custom Docker container&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 9.2 – Directory structure for creating a custom Docker container</p>&#13;
			<p>We will execute the following commands from the same directory as <strong class="source-inline">Tensorflow_Serving.ipynb</strong>. </p>&#13;
			<p>After we have the TFS base Docker image up and running as a container, we are ready to put our own <strong class="source-inline">SavedModel</strong> into this container:</p>&#13;
			<ol>&#13;
				<li value="1">Basically, we have to copy our model into the TFS container's <strong class="source-inline">model</strong> folder:<p class="source-code"><strong class="bold">docker cp ${PWD}/flowerclassifier serv_base_img:/models/flowerclassifier</strong></p><p><strong class="source-inline">flowerclassifier</strong> is the directory name two levels up from the <strong class="source-inline">saved_model.pb</strong> file. In between the two, you will notice that there is a directory, <strong class="source-inline">001</strong>. This hierarchy is required by TFS, and so is the naming convention for the middle directory, which has to be an integer. It doesn't have to be <strong class="source-inline">001</strong>, as long as it is all integers.</p><p>The preceding command copies our model into the base image's <strong class="source-inline">/model</strong> directory.</p></li>&#13;
				<li>Now we <a id="_idIndexMarker507"/>commit our <a id="_idIndexMarker508"/>change to the base image and give the container a name that matches our model directory:<p class="source-code"><strong class="bold">docker commit --change "ENV MODEL_NAME flowermodel" serv_base_img flowermodel</strong></p></li>&#13;
				<li>We no longer require the base image to be running. Now we can just kill it:<p class="source-code"><strong class="bold">docker kill serv_base_img</strong></p><p>What we have done so far is create a Docker image of our model, <strong class="source-inline">flowermodel</strong>, which is deployed in a TFS container. Once we launch the TFS container, it brings our model up for serving.</p></li>&#13;
				<li>To serve the image and score on our test image, we will run the following command:<p class="source-code"><strong class="bold">docker run -p 8501:8501 \</strong></p><p class="source-code"><strong class="bold">  --mount type=bind,\</strong></p><p class="source-code"><strong class="bold">source=$PWD/flowerclassifier,\</strong></p><p class="source-code"><strong class="bold">target=/models/flowerclassifier \</strong></p><p class="source-code"><strong class="bold">  -e MODEL_NAME=flowerclassifier -t tensorflow/serving &amp;</strong></p><p>We first open a local TC/PIP port, <strong class="source-inline">8501</strong>, and map it to the Docker container's port, <strong class="source-inline">8501</strong>. If your local port <strong class="source-inline">8501</strong> is not available, you may try another local port, say <strong class="source-inline">8502</strong>. Then the command would take on <strong class="source-inline">-p 8502:8501</strong>. </p><p>The source <a id="_idIndexMarker509"/>of our model is in the current <a id="_idIndexMarker510"/>directory (as indicated by the inline <strong class="source-inline">$PWD</strong> command) and followed by <strong class="source-inline">flowerclassifier</strong>. This folder also defines an environment variable, <strong class="source-inline">MODEL_NAME</strong>. <strong class="source-inline">-t tensorflow/serving</strong> indicates we want the container to be ready for <strong class="source-inline">STDIN</strong> from <strong class="source-inline">tensorflow/serving</strong>. </p></li>&#13;
			</ol>&#13;
			<p>At your command terminal, you will observe output such as that shown in the followi<a id="_idTextAnchor259"/>ng figure:</p>&#13;
			<div>&#13;
				<div id="_idContainer123" class="IMG---Figure">&#13;
					<img src="Images/image0033.jpg" alt="Figure 9.3 – Docker container running with a custom-built model&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 9.3 – Docker container running with a custom-built model</p>&#13;
			<p>Notice the following in the preceding screenshot: <strong class="bold">Successfully loaded servable version {name: flowerclassifier version: 1}</strong>.</p>&#13;
			<p>This line <a id="_idIndexMarker511"/>indicates that the container successfully <a id="_idIndexMarker512"/>found our model, <strong class="source-inline">flowerclassifier</strong>, and the next directory in the naming hierarchy, <strong class="source-inline">001</strong>. This is a general pattern that TFS needs in order to make TFS work with a custom model that you have built. </p>&#13;
			<p>Now the model is served. In the next section, we will see how to build our client that calls this model using the Python<a id="_idTextAnchor260"/> JSON library.</p>&#13;
			<h2 id="_idParaDest-149"><a id="_idTextAnchor261"/>Scoring through the RESTful API</h2>&#13;
			<p>Now let's return to our Jupyter environment. We will see how to pick up from what we did in Local Serving and <a id="_idIndexMarker513"/>continue from there. Recall that we used the <strong class="source-inline">nvision</strong> library to normalize and standardize our test image. We also need to expand the image dimension because the model expects to have a batch dimension in our input. After we have performed these steps as in Local Serving, we will have <strong class="source-inline">img1_np</strong> as the properly shaped NumPy array. Let's build this array into a JSON payload, and pass the payload to our model through the RESTful API with the help of the following steps:</p>&#13;
			<ol>&#13;
				<li value="1">We will build the JSON payload with the following command:<p class="source-code">data = json.dumps({ </p><p class="source-code">    "instances": img1_np.tolist()</p><p class="source-code">})</p><p class="source-code">headers = {"content-type": "application/json"}</p><p>In the preceding code, we converted the test image into a JSON payload format and we defined a header for our RESTful API call to indicate that the payload is in JSON format for the application to consume.</p><p>As per TFS, the payload must encode the scoring data with a key-value pair by the key name of the instances and the NumPy array to be converted to a list as the input. We also need a header to be defined for the JSON payload as well. </p></li>&#13;
				<li>We will score our test image, <strong class="source-inline">img1_np</strong>, for this data with the following command:<p class="source-code">response = requests.post('http://localhost:8501/v1/models/flowerclassifier:predict', data=data, headers=headers)</p><p>The preceding command will produce a <strong class="source-inline">response</strong> payload back from our TFS container. </p></li>&#13;
				<li>We will examine the <strong class="source-inline">response</strong> payload by using the following command:<p class="source-code">response.json()</p><p>The following is the output of the preceding command:</p><p class="source-code">{'predictions': [[2.42621149e-06,</p><p class="source-code">   5.61519164e-06,</p><p class="source-code">   1.80002226e-05,</p><p class="source-code">   1.43428879e-05,</p><p class="source-code">   0.9999595<a id="_idTextAnchor262"/>88]]}</p><p>This is a <a id="_idIndexMarker514"/>dictionary containing the key prediction and the probability values for each of the five flower types. These values are identical to what we saw in Local Serving. Therefore, we know that the model is correctly served via TFS using a Docker container. </p></li>&#13;
			</ol>&#13;
			<h1 id="_idParaDest-150"><a id="_idTextAnchor263"/>Summary</h1>&#13;
			<p>In this chapter, you learned how to deploy a TensorFlow <strong class="source-inline">SavedModel</strong>. This is by no means the most common method to use in enterprise deployment. In an enterprise deployment scenario, many factors determine how the deployment pipeline should be built, and depending on the use cases, it can quickly diverge in terms of deployment patterns and choices from there. For example, some organizations use AirFlow as their orchestration tool, and some may prefer KubeFlow, while many others still use Jenkins. </p>&#13;
			<p>The goal of this book is to show you how to leverage the latest and most reliable implementation of TensorFlow Enterprise from a data scientist/machine learning model builder's perspective.</p>&#13;
			<p>From here, depending on your interests or priorities, you may take up what you learned in this book and pursue many other topics, such as MLOps, model orchestration, drift monitoring, and redeployment. These are some of the important topics in any enterprise machine learning discussions from a use case perspective. Use cases, IT infrastructure, and business considerations typically determine how a model is actually served. Further considerations include what kind of service-level agreement the serving pipeline has to meet, and the security and compliance issues related to authentication and model safety.</p>&#13;
		</div>&#13;
	</div></body></html>