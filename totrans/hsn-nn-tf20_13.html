<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Bringing a Model to Production</h1>
                </header>
            
            <article>
                
<p>In this chapter, the ultimate goal of any real-life machine learning application will be presented—<span>the deployment and inference of a trained model</span>. As we saw in the previous chapters, TensorFlow allows us to train models and save their parameters in checkpoint files, making it possible to restore the model's status and continue with the training process, while also running the inference from Python.</p>
<p>The checkpoint files, however, are not in the right file format when the goal is to use a trained machine learning model with low latency and a low memory footprint. In fact, the checkpoint files only contain the models' parameters value, without any description of the computation; this forces the program to define the model structure first and then restore the model parameters. Moreover, the checkpoint files contain variable values that are only useful during the training process. However, they are a complete waste of resources during inference (for instance, all the variables created by the optimizers). The correct representation to use is the SavedModel serialization format, which is described in the next section. After analyzing the SavedModel serialization format, and seeing how a <kbd>tf.function</kbd> decorated function can be graph-converted and serialized, we will deep dive into the TensorFlow deployment ecosystem to see how TensorFlow 2.0 speeds up the deployment of a graph on a wide number of platforms and how it is designed for serving at scale.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>The SavedModel serialization format</li>
<li>Python deployment</li>
<li>Supported deployment platforms</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The SavedModel serialization format</h1>
                </header>
            
            <article>
                
<p><span>As we explained in <a href="f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml">Chapter 3</a>,</span><em><span> </span>TensorFlow Graph Architecture</em>, representing computations using DataFlow graphs has several advantages in terms of model portability since a graph is a language-agnostic representation of the computation.</p>
<p>SavedModel is a universal serialization format for TensorFlow models that extends the TensorFlow standard graph representation by creating a language-agnostic representation for the computation that is recoverable and hermetic. This representation has been designed not only to carry the graph description and values (like the standard graph) but also to offer additional features that were designed to simplify the usage of the trained models in heterogeneous production environments.</p>
<p>TensorFlow 2.0 has been designed with simplicity in mind. This design choice is visible in the following diagram, where it is possible to appreciate how the SavedModel format is the only bridge between the research and development phases (on the left) and the deployment phase (on the right):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/74244cce-6674-4e44-9fa0-a47aee4adaf9.png" style="width:41.75em;height:23.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The TensorFlow 2.0 training and deployment ecosystem. Image source: <a href="https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8">https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8</a>—the TensorFlow Team</div>
<p>Being the bridge between the model's training and its deployment, the SavedModel format must offer a broad set of features to satisfy the wide spectrum of deployment platforms available, thereby providing excellent support for different software and hardware platforms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Features</h1>
                </header>
            
            <article>
                
<p>A SavedModel contains a complete computational graph, including model parameters and everything else that's specified during its creation. SavedModel objects that are created using the TensorFlow 1.x API only contain a flat graph representation of the computation; in TensorFlow 2.0, a SavedModel contains a serialized representation of <kbd>tf.function</kbd> objects.</p>
<p>Creating a SavedModel is straightforward when you're using the TensorFlow Python API (as shown in the next section), but its configuration requires that you understand its main features, which are as follows:</p>
<ul>
<li><strong>Graph tagging</strong>: In a production environment, you often need to put a model into production, while at the same time continuing the development of the same model after getting new data. Another possible scenario is the parallel training of two or more identical models, trained with different techniques or with different data, with the desire to put them all in production to test which performs better.<br/>
The SavedModel format allows you to have multiple graphs that share the same set of variables and assets in the same file. Each graph is associated with one or more tags (user-defined <span>strings</span>) that allow us to identify it during the load operation.</li>
<li><strong>SignatureDefs</strong>: When defining a computational graph, we are aware of the model's inputs and outputs; this is called a <strong>Model Signature</strong>. The SavedModel serialization format uses <kbd>SignatureDefs</kbd> to allow generic support for signatures that may need to be saved within <kbd>graph.SignatureDefs</kbd> are nothing but a set of named<em> </em>Model Signatures that defines from which nodes the model can be called and which is the output node, given a certain input.</li>
<li><strong>Assets</strong>: To allow the models to rely upon external files for initialization, SavedModel supports the concept of assets. The assets are copied to the SavedModel location during its creation, and they can be read by the model initialization procedure safely.</li>
<li><strong>Device cleanup</strong>: The computational graph, which we looked at in <a href="f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml">Chapter 3</a>, <em>TensorFlow Graph Architecture</em>, contains the device name of where the computation must be executed. To generate generic graphs that can run on any hardware platform, SavedModel supports clearing devices before its generation.</li>
</ul>
<p>These features allow you to create hardware that's independent and self-contained objects that specify how the model should be called, the output nodes, given a specific input, and which particular model to use among the ones available (via tags).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a SavedModel from a Keras model</h1>
                </header>
            
            <article>
                
<p>In TensorFlow 1.x, creating a SavedModel requires that we know what the input nodes are, what the output nodes are, and that we have successfully loaded the graph representation of the model we want to save inside a <kbd>tf.Session</kbd> function.</p>
<p>TensorFlow 2.0 simplified the way of creating a SavedModel a lot. Since Keras is the only way of defining models, and there are no more sessions, the process of creation of SavedModel consists of a single line of code:</p>
<p><kbd>(tf2)</kbd></p>
<p>This is as follows:</p>
<pre># model is a tf.keras.Model model<br/>path = "/tmp/model/1"<br/>tf.saved_model.save(model, path)</pre>
<p>The <kbd>path</kbd> variable follows a good practice that consists of adding a version number to the model directly in the export path (<kbd>/1</kbd>). The only tag associated with the model is the default <kbd>tag: "serve"</kbd>.</p>
<p>The <kbd>tf.saved_model.save</kbd> call creates the following directory structure in the specified <kbd>path </kbd>variable</p>
<pre>assets/<br/>variables/<br/>    variables.data-?????-of-?????<br/>    variables.index<br/>saved_model.pb</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The directory contains the following:</p>
<ul>
<li><kbd>assets</kbd> contains auxiliary files. These files were described in the previous section.</li>
<li><kbd>variables</kbd> contains the model variables. These variables are created from a TensorFlow Saver object in the same way they are created for the checkpoint files.</li>
<li><kbd>saved_model.pb</kbd> is the compiled Protobuf. This is a binary representation of the computation the Keras model describes.</li>
</ul>
<p><span>The Keras model already specifies what the model input and outputs are; therefore, there is no need to worry about which is which. The SignatureDef that's exported by a Keras model (it is worth recalling from the previous section that they are just named functions that describe how to call the model) is the invocation of the </span><kbd>call</kbd><span> method of the Keras model (its forward pass), and it is exported under the </span><kbd>serving_default</kbd><span> signature key.</span></p>
<p>Creating a SavedModel from a Keras model is straightforward since the description of the forward pass is contained in its <kbd>call</kbd> method. This function is then automatically converted by TensorFlow into its graph equivalent using AutoGraph. The input parameters of the <kbd>call</kbd> method become the input signature of the graph and the outputs of the Keras model.</p>
<p>However, we may not be interested in exporting a Keras model. What if we just want to deploy and serve a generic computational graph?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting a SavedModel from a generic function</h1>
                </header>
            
            <article>
                
<p>In TensorFlow 1.x, there is no difference between exporting a generic graph and a model: select the input and output nodes, create a session, define the signature, and save it.</p>
<p>In TensorFlow 2.0, since graphs are hidden, the conversion of a generic TensorFlow computation to a SavedModel (graph) requires some additional attention.</p>
<p>The description of the first parameter of the <kbd>tf.saved_model.save(obj, export_dir, signatures=None)</kbd> function clearly states that <kbd>obj</kbd> must be a trackable <em>object</em>.</p>
<p>A trackable object is an object derived from the <kbd>TrackableBase</kbd> <span>class (private, which means it's not visible in the <kbd>tensorflow</kbd> package)—</span>almost every object in TensorFlow 2.0 derives from this class. These objects are the objects that can be stored inside a checkpoint file, and among them, we find the Keras models, the optimizers, and so on.</p>
<p>For this reason, it is not possible to export a function like the following one without creating an object that inherits from a <kbd>TrackableBase</kbd> object:</p>
<p><kbd>(tf2)</kbd></p>
<pre>def pow(x, y):<br/>    return tf.math.pow(x, y)</pre>
<p>The most generic class in the TensorFlow API that, once instantiated, creates a trackable object is the <kbd>tf.Module</kbd> class. A module is a named container for <kbd>tf.Variable</kbd> objects, other modules, and functions that apply to user input. Subclassing <kbd>tf.Module</kbd> is a straightforward way to create a trackable object and satisfying the requirement of the <kbd>tf.saved_model.save</kbd> function:</p>
<p><kbd>(tf2)</kbd></p>
<pre>class Wrapper(tf.Module):<br/><br/>    def pow(self, x, y):<br/>        return tf.math.pow(x, y)</pre>
<p>Not being a Keras model, <kbd>tf.saved_model.save</kbd> doesn't know which one of the <kbd>Wrapper</kbd> class methods applies to graph conversion. There are two different ways of instructing the <kbd>save</kbd> function to convert only the methods we are interested in. They are as follows:</p>
<ul>
<li><strong>Specify the signature</strong>: The third parameter of the <kbd>save</kbd> function optionally accepts a dictionary. The dictionary must contain the name of the method to export and the input description. It does so by using the <kbd>tf.TensorSpec</kbd> object.</li>
<li><strong>Use</strong> <kbd>tf.function</kbd>: The <kbd>save</kbd> mode, when the <kbd>signature</kbd> parameter is omitted, searches inside the <kbd>obj</kbd> for a <kbd>@tf.function</kbd> decorated method. If exactly one method is found, that method will be used as the default signature for the SavedModel. Also, in this case, we have to describe the input type and shape by using <kbd>tf.TensorSpec</kbd> objects that are manually passed to the <kbd>tf.function</kbd> <kbd>input_signature</kbd> parameter.</li>
</ul>
<p class="mce-root"/>
<p>The second method is the handiest, and it also brings the advantage of having defined and converted to graph the current Python program. When used, this could speed up computation.</p>
<p><kbd>(tf2)</kbd></p>
<pre>class Wrapper(tf.Module):<br/><br/>    @tf.function(<br/>        input_signature=[<br/>            tf.TensorSpec(shape=None, dtype=tf.float32),<br/>            tf.TensorSpec(shape=None, dtype=tf.float32),<br/>        ]<br/>    )<br/>    def pow(self, x, y):<br/>        return tf.math.pow(x, y)<br/><br/><br/>obj = Wrapper()<br/>tf.saved_model.save(obj, "/tmp/pow/1")</pre>
<p>Therefore, the way of exporting a generic function to its SavedModel representation is to wrap the function into a trackable object, decorate the method with <kbd>tf.function</kbd>, and specify the input signature to use during the conversion.</p>
<p>This is all we need to do to export a generic function, that is, a generic computational graph, or a Keras model to its self-contained and language-agnostic representation, so that it's ready to use in every programming language.</p>
<p>The easiest way to use a SavedModel object is to use the TensorFlow Python API, since it's the more complete high-level API for TensorFlow and offers convenient methods to load and use a SavedModel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python deployment</h1>
                </header>
            
            <article>
                
<p><span>Using Python, it is straightforward to load the computational graphs stored inside a SavedModel and use them as native Python functions. This is all thanks to the TensorFlow Python API. </span>The <kbd>tf.saved_model.load(path)</kbd> method deserializes the SavedModel located in <kbd>path</kbd> and returns a trackable object with a <kbd>signatures</kbd> attribute that contains the mapping from the signature keys to Python functions that are ready to be used.</p>
<p>The <kbd>load</kbd> method is capable of deserializing the following:</p>
<ul>
<li>Generic computational graphs, such as the ones we created in the previous section</li>
<li>Keras models</li>
<li>SavedModel created using TensorFlow 1.x or the Estimator API</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generic computational graph</h1>
                </header>
            
            <article>
                
<p>Let's say we are interested in loading the computational graph of the <kbd>pow</kbd> function we created in the previous section and using it inside a Python program. Doing this is straightforward in TensorFlow 2.0. Follow these steps to do so:</p>
<ol>
<li>Import the model:</li>
</ol>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre style="padding-left: 60px">path = "/tmp/pow/1"<br/>imported = tf.saved_model.load(path)</pre>
<ol start="2">
<li>The <kbd>imported</kbd> object has a <kbd>signatures</kbd> attribute we can inspect to see the available functions. In this case, since we didn't specify a signature when we exported the model, we expect to find only the default signature, <kbd>"serving_default"</kbd>:</li>
</ol>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre style="padding-left: 60px">assert "serving_default" == list(imported.signatures)[0]<br/>assert len(imported.signatures) == 1</pre>
<p style="padding-left: 60px">The computational graph of the power function can be made available by accessing <kbd>imported.signatures["serving_default"]</kbd>. Then, it is ready to be used.</p>
<div class="packt_infobox">Using the imported computational graphs requires you to have good understanding of the TensorFlow graph structure, as explained in <a href="https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=308&amp;action=edit#post_26">Chapter 3</a><span>, </span><em>TensorFlow Graph Architecture</em>. In fact, the <kbd><span>imported.signatures["serving_default"]</span></kbd> function is a static graph, and as such, it requires some additional attention to be used.</div>
<ol start="3">
<li>Calling the graph but passing a wrong input type will make it raise an exception since the static graph is strictly statically typed. Moreover, the object returned by the <kbd>tf.saved_model.load</kbd> function forces the usage of named parameters only, and not positional ones (which is different to the <kbd>pow</kbd> function's original definition, which used only positional arguments). Thus, once the inputs with the correct shape and input type are defined, it is possible to invoke the function easily:</li>
</ol>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre style="padding-left: 60px">pow = imported.signatures["serving_default"]<br/>result = pow(x=tf.constant(2.0), y=tf.constant(5.0))</pre>
<p style="padding-left: 60px">The <kbd>result</kbd> variable, as opposed to what you might expect, does not contain a <kbd>tf.Tensor</kbd> object with a value of <kbd>32.0</kbd>; it is a dictionary. Using a dictionary to return the result of a computation is a good design choice. In fact, this forces the caller (the Python program using the imported computational graph) to explicitly access a key that indicates the desired return value.</p>
<ol start="4">
<li>In the case of the <kbd>pow</kbd> function, where the return value is a <kbd>tf.Tensor</kbd> and not a Python dictionary, the returned dictionary has keys that follow a naming convention—the key name is always the<kbd>"output_"</kbd> string, followed by the position (starting from zero) of the returned argument. The <span>following </span>code snippet clarifies this concept:</li>
</ol>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre style="padding-left: 60px">assert result["output_0"].numpy() == 32</pre>
<p style="padding-left: 60px">If the <kbd>pow</kbd> function is updated as follows, the dictionary keys will be <kbd>"output_0", "output_1"</kbd>:</p>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre>    def pow(self, x, y):<br/>        return tf.math.pow(x, y), tf.math.pow(y, x)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">Of course, falling back on the default naming convention is not a good or maintainable solution (what does <kbd>output_0</kbd> represent?). Therefore, when designing functions that will be exported in a SavedModel, it's good practice to make the function return a dictionary so that the exported SavedModel will use the same dictionary as the return value when invoked. Thus, a better design of the <kbd>pow</kbd> function could be as follows:</p>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre style="padding-left: 60px">class Wrapper(tf.Module):<br/><br/>class Wrapper(tf.Module):<br/>    @tf.function(<br/>        input_signature=[<br/>            tf.TensorSpec(shape=None, dtype=tf.float32),<br/>            tf.TensorSpec(shape=None, dtype=tf.float32),<br/>        ]<br/>    )<br/>    def pow(self, x, y):<br/>        return {"pow_x_y":tf.math.pow(x, y), "pow_y_x": tf.math.pow(y, x)}<br/><br/><br/>obj = Wrapper()<br/>tf.saved_model.save(obj, "/tmp/pow/1")</pre>
<p style="padding-left: 60px">Once imported and executed, the following code will produce a dictionary with meaningful names:</p>
<p style="padding-left: 60px"><kbd>(tf2)</kbd></p>
<pre style="padding-left: 60px">path = "/tmp/pow/1"<br/><br/>imported = tf.saved_model.load(path)<br/>print(imported.signatures["serving_default"](<br/>        x=tf.constant(2.0),y=tf.constant(5.0)))</pre>
<p style="padding-left: 60px">The resultant output is the following dictionary:</p>
<pre style="padding-left: 60px"><span>{<br/>  'pow_x_y': &lt;tf.Tensor: id=468, shape=(), dtype=float32, numpy=32.0&gt;,<br/>  'pow_y_x': &lt;tf.Tensor: id=469, shape=(), dtype=float32, numpy=25.0&gt;<br/>}</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The TensorFlow Python API simplifies not only the loading of a generic computational graph, but also the usage of a trained Keras model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keras models</h1>
                </header>
            
            <article>
                
<p>Being the official TensorFlow 2.0 way of defining machine learning models, the Keras models, when serialized, contain more than just the serialized <kbd>call</kbd> method. The object returned by the <kbd>load</kbd> function is similar to the object that's returned when you're restoring a generic computational graph, but with more attributes and peculiarities:</p>
<ul>
<li>The <kbd>.variables</kbd> a<span>ttribute</span>: The non-trainable variables attached to the original Keras model have been serialized and stored inside the SavedModel.</li>
<li>The <kbd>.trainable_variables</kbd> a<span>ttribute</span>: In the same manner as the <kbd>.variables</kbd> attribute, the trainable variables of the model have also been serialized and stored inside the SavedModel.</li>
<li>The <kbd>__call__</kbd> method: Instead of exposing a <kbd>signatures</kbd> attribute with a single key, <kbd>"serving_default"</kbd>, the returned object exposes a <kbd>__call__</kbd> method that accepts inputs just like the original Keras model.</li>
</ul>
<p>All of these features allow not only the use of the SavedModel as a standalone computational graph, as shown in the following code snippet, but they also allow you to completely restore the Keras model and continue to train it:</p>
<p><kbd>(tf2)</kbd></p>
<pre>imported = tf.saved_model.load(path)<br/># inputs is a input compatible with the serialized model<br/>outputs = imported(inputs)</pre>
<p>As we mentioned previously, all these additional features (variables that are trainable and not trainable, plus the serialized representation of the computation) allow for a complete restore of a Keras model object from a SavedModel, making it possible to use them as checkpoint files. The Python API offers the <kbd>tf.keras.models.load_model</kbd> function to do that, and, as usual, in TensorFlow 2.0, it is really handy:</p>
<p><kbd>(tf2)</kbd></p>
<pre>model = tf.keras.models.load_model(path)<br/># models is now a tf.keras.Model object!</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here, <kbd>path</kbd> is the path of the SavedModel, or the <kbd>h5py</kbd> file. The <kbd>h5py</kbd> serialization format is not considered in this book since it is a Keras representation and has no additional advantages with respect to the SavedModel serialization format.</p>
<p>The Python API is also backward-compatible with the TensorFlow 1.x SavedModel format, and so you can restore flat graphs instead of <kbd>tf.function</kbd> objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flat graphs</h1>
                </header>
            
            <article>
                
<p>The SavedModel objects created by the <kbd>tf.estimator</kbd> API or using the SavedModel 1.x API contain a rawer representation of the computation. This representation is known as <strong>flat graph</strong>.</p>
<p>In this representation, the flat graph inherits no signatures from a <kbd>tf.function</kbd> object in order to simplify the restoration process. It only takes the computational graph as is, along with its node names and variables (see <a href="https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=308&amp;action=edit#post_26">Chapter 3</a><span>, </span><em>TensorFlow Graph Architecture</em>, for details).</p>
<p>These SavedModels have functions that correspond to their signatures (defined manually before the serialization process) in the <kbd>.signatures</kbd> attribute, but more importantly, the restored SavedModel that uses the new TensorFlow 2.0 API has a <kbd>.prune</kbd> method that allows you to extract functions from arbitrary subgraphs just by knowing the input and output node names.</p>
<p>Using the <kbd>.prune</kbd> method is the equivalent of restoring the SavedModel in the default graph and putting it in a TensorFlow 1.x Session; then, the input and output nodes can be accessed by using the <kbd>tf.Graph.get_tensor_by_name</kbd> method.</p>
<p>TensorFlow 2.0, through the <kbd>.prune</kbd> method, simplified this process, making it just as easy, as shown in the following code snippet:</p>
<p><kbd>(tf2)</kbd></p>
<pre>imported = tf.saved_model.load(v1savedmodel_path)<br/>pruned = imported.prune("input_:0", "cnn/out/identity:0")<br/># inputs is an input compatible with the flat graph<br/>out = pruned(inputs)</pre>
<p>Here, <kbd>input_</kbd> is a placeholder of any possible input node, and <kbd>"cnn/out/identity:0"</kbd> is the output node.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>After the SavedModel has been loaded inside the Python program, it is possible to use the trained model (or the generic computational graph) as a building block for any standard Python application. For instance, once you've trained a face detection model, it is straightforward to use OpenCV (the most famous open source computer vision library) to open the webcam stream and feed it to the face detection model. The applications of trained models are countless and you can develop your own Python application that uses a trained machine learning model as a building block.</p>
<p>Although Python is the language of data science, it isn't the perfect candidate for the deployment of machine learning models on different platforms. There are programming languages that are the<span> de facto </span>standard for certain tasks or environments; for example, Javascript for client-side web development, C++ and Go for data centers and cloud services, and so on.</p>
<p>Being a language-agnostic representation, it is, in theory, possible to load and execute (deploy) a SavedModel using every programming language; this is a huge advantage since there are cases in which Python is not usable, or it is not the best choice.</p>
<p>TensorFlow supports many different deployment platforms: it offers tools and frameworks in many different languages in order to satisfy a wide range of use cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supported deployment platforms</h1>
                </header>
            
            <article>
                
<p>As shown in the diagram at the beginning of this chapter, SavedModel is the input for a vast ecosystem of deployment platforms, with each one being created to satisfy a different range of use cases:</p>
<ul>
<li><strong>TensorFlow Serving</strong>: This is the official Google solution for serving machine learning models. It supports model versioning, multiple models can be deployed in parallel, and it ensures that concurrent models achieve high throughput with low latency thanks to its complete support for hardware accelerators (GPUs and TPUs). TensorFlow Serving is not merely a deployment platform, but an entire ecosystem built around TensorFlow and written in highly efficient C++ code. Currently, this is the solution Google itself uses to run tens of millions of inferences per second on Google Cloud's ML platform.</li>
<li><strong>TensorFlow Lite</strong>:<em> </em>This<em> </em>is the deployment platform of choice for running machine learning models on mobile and embedded devices. TensorFlow Lite is a whole new ecosystem and has its own training and deployment tools. It is designed to optimize the trained models for size, thereby creating a small binary representation of the original model that's optimized for fast inference and low power consumption. Moreover, the TensorFlow Lite framework also offers the tools to build a new model and retrain an existing one (thus it allows you to do transfer learning/fine-tuning) directly from the embedded device or smartphone.<br/>
TensorFlow Lite comes with a Python toolchain that's used to convert the SavedModel into its optimized representation, the <kbd>.tflite</kbd> file.</li>
<li><strong>TensorFlow.js</strong>: This is a framework similar to TensorFlow Lite but designed to train and deploy TensorFlow models in the browser and Node.js. Like TensorFlow Lite, the framework comes with a Python toolchain that can be used to convert a SavedModel into a JSON readable format by the TensorFlow Javascript library. TensorFlow.js can be used to fine-tune or train models from scratch, which it does by using sensor data coming from the browser or any other client-side data.</li>
<li><strong>Other language bindings</strong>: TensorFlow Core is written in C++, and there are bindings for many different programming languages, most of which are automatically generated. The structure of the binding is often very low-level and similar to the TensorFlow Graph structure used in the TensorFlow 1.x Python API and under the hood of the TensorFlow C++ API. </li>
</ul>
<p>Supporting many different deployment platforms, TensorFlow is ready to deploy on a broad range of platforms and devices. In the following sections, you will learn how to deploy a trained model on a browser using TensorFlow.js and how to run inferences using the Go programming language.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow.js</h1>
                </header>
            
            <article>
                
<p>TensorFlow.js (<a href="https://www.tensorflow.org/js/">https://www.tensorflow.org/js/</a>) is a library that's used for developing and training machine learning models on JavaScript and deploying them in browsers or in Node.js.</p>
<p>To be used inside TensorFlow.js, a trained model must be converted into a format TensorFlow.js can load. The target format is a directory containing a <kbd>model.json</kbd> file and a set of binary files containing the model parameters. The <kbd>model.json</kbd> file contains the graph description and information about the binary files, to make it possible to restore the trained model successfully.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<div class="packt_infobox">Although it is fully compatible with TensorFlow 2.0, it is good practice to create an isolated environment for TensorFlow.js, as explained in the <em>Environment setup</em> section of <a href="https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=308&amp;action=edit#post_26">Chapter 3</a><span>, </span><em>TensorFlow Graph Architecture</em>. The TensorFlow.js dedicated environment is, from now on, displayed using the <kbd>(tfjs)</kbd> notation, before the code snippets.</div>
<p>The first step in developing a TensorFlow.js application is to install TensorFlow.js inside the isolated environment. You need to do this so that you can use all the provided command-line tools and the library itself via Python:</p>
<p><kbd>(tfjs)</kbd></p>
<pre>pip install tensorflowjs</pre>
<p>TensorFlow.js has tight integration with TensorFlow 2.0. In fact, it is possible to convert a Keras model into a TensorFlow.js representation directly using Python. Moreover, it offers a command-line interface for converting a generic SavedModel that could contain any computational graph into its supported representation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting a SavedModel into model.json format</h1>
                </header>
            
            <article>
                
<p>Since it is not possible to use a SavedModel directly from TensorFlow.js, we need to convert it into a compatible version and then load it in the TensorFlow.js runtime. The <kbd>tensorflowjs_converter</kbd> command-line application makes the conversion process straightforward. This tool not only performs the conversion between the SavedModel and the TensorFlow.js representation but also automatically quantizes the model, thereby reducing its dimensions when necessary.</p>
<p>Let's say we are interested in converting the SavedModel of the computational graph we exported in the previous section into TensorFlow format via the serialized <kbd>pow</kbd> function. Using <kbd>tensorflowjs_converter</kbd>, we only need to specify the input and output file formats (in this case, the input is a SavedModel, and the output is a TensorFlow.js graph model) and location, and then we are ready to go:</p>
<p><kbd>(tfjs)</kbd></p>
<pre>tensorflowjs_converter \<br/>    --input_format "tf_saved_model" \<br/>    --output_format "tfjs_graph_model" \<br/>    /tmp/pow/1 \<br/>    exported_js</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding command reads the SavedModel present in <kbd>/tmp/pow/1</kbd> and places the result of the conversion in the current directory, <kbd>exported_js</kbd> (creating it if it doesn't exist). Since the SavedModel has no parameters, in the <kbd>exported_js</kbd> folder, we only find the <kbd>model.json</kbd> file that contains the description of the computation.</p>
<p>We are now ready to go <span>– </span>we can define a simple web page or a simple Node.js application that imports the TensorFlow.js runtime and then successfully import and use the converted SavedModel. The following code creates a one-page application with a form inside it; by using the click event of the <strong>pow</strong> button, the exported graph is loaded, and the computation is executed:</p>
<pre> &lt;html&gt;<br/>    &lt;head&gt;<br/>        &lt;title&gt;Power&lt;/title&gt;<br/>        &lt;!-- Include the latest TensorFlow.js runtime --&gt;<br/>        &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"&gt;&lt;/script&gt;<br/>    &lt;/head&gt;<br/>    &lt;body&gt;<br/>        x: &lt;input type="number" step="0.01" id="x"&gt;&lt;br&gt;<br/>        y: &lt;input type="number" step="0.01" id="y"&gt;&lt;br&gt;<br/>        &lt;button id="pow" name="pow"&gt;pow&lt;/button&gt;&lt;br&gt;<br/>        &lt;div&gt;<br/>            x&lt;sup&gt;y&lt;/sup&gt;: &lt;span id="x_to_y"&gt;&lt;/span&gt;<br/>        &lt;/div&gt;<br/>        &lt;div&gt;<br/>            y&lt;sup&gt;x&lt;/sup&gt;: &lt;span id="y_to_x"&gt;&lt;/span&gt;<br/>        &lt;/div&gt;<br/><br/>        &lt;script&gt;<br/>            document.getElementById("pow").addEventListener("click", async function() {<br/>                // Load the model<br/>                const model = await tf.loadGraphModel("exported_js/model.json")<br/>                // Input Tensors<br/>                let x = tf.tensor1d([document.getElementById("x").value], dtype='float32')<br/>                let y = tf.tensor1d([document.getElementById("y").value], dtype='float32')<br/>                let results = model.execute({"x": x, "y": y})<br/>                let x_to_y = results[0].dataSync()<br/>                let y_to_x = results[1].dataSync()<br/><br/>                document.getElementById("x_to_y").innerHTML = x_to_y<br/>                document.getElementById("y_to_x").innerHTML = y_to_x<br/>            });<br/>        &lt;/script&gt;<br/>    &lt;/body&gt;<br/>&lt;/html&gt;</pre>
<p>TensorFlow.js follows different conventions in regards to how to use a loaded SavedModel. As we can see in the preceding code snippet, the signature defined inside the SavedModel has been preserved, and the function is being invoked by passing the named parameters <kbd>"x"</kbd> and <kbd>"y"</kbd>. Instead, the return value format has been changed: the <kbd>pow_x_y</kbd> and <kbd>pow_y_x</kbd> keys have been discarded, and the return values are now positional; in the first position (<kbd>results[0]</kbd>), we found the value of the <kbd>pow_x_y</kbd> key, and in the second position, the value of the <kbd>pow_y_x</kbd> key.</p>
<p>Moreover, with JavaScript <span>being </span>a language with strong support for asynchronous operations, the TensorFlow.js API uses it a lot—the model loading is asynchronous and defined inside an <kbd>async</kbd> function. Even fetching the results from the model is asynchronous by default. But in this case, we forced the call to be synchronous using the <kbd>dataSync</kbd> method.</p>
<p>Using Python, we can now launch a simple HTTP server and see the application inside the browser:</p>
<p><kbd>(tfjs)</kbd></p>
<pre>python -m http.server</pre>
<p>By visiting the <kbd>http://localhost:8000/</kbd> address using a web browser and opening the HTML page containing the previously written code, we can see and use the deployed graph, directly in the browser:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-938 image-border" src="assets/5fd26d0a-d14f-4070-a6f5-c3bcd2e2f26c.png" style="width:14.50em;height:7.33em;"/></p>
<p>The TensorFlow.js API, although similar to the Python one, is different and follows different rules; a complete analysis of TensorFlow.js is beyond the scope of this book, and so you should have a look at the official documentation to gain a better understanding of the TensorFlow.js API.</p>
<p><span>Compared to the preceding procedure, which involves the usage of <kbd>tensorflowjs_converter</kbd>, the deployment of a Keras model is simplified, and it is possible to integrate the conversion from a Keras model to a <kbd>model.json</kbd> file directly in the TensorFlow 2.0 Python script that's used to train the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting a Keras Model into model.json format</h1>
                </header>
            
            <article>
                
<p>As shown at the beginning of this chapter, a Keras model can be exported as a SavedModel, and therefore, the procedure explained earlier to convert a SavedModel into a <kbd>model.json</kbd> file can still be used. However, since the Keras models are particular objects in the TensorFlow 2.0 framework, it is possible to directly embed the deployment into TensorFlow.js at the end of the training pipeline:</p>
<p><kbd>(tfjs)</kbd></p>
<pre>import tensorflowjs as tfjs<br/>from tensorflow import keras<br/><br/>model = keras.models.Sequential() # for example<br/># create the model by adding layers<br/><br/># Standard Keras way of defining and executing the training loop<br/># (this can be replaced by a custom training loop)<br/>model.compile(...)<br/>model.fit(...)<br/><br/># Convert the model to the model.json in the exported_js dir<br/>tfjs_target_dir = "exported_js"<br/>tfjs.converters.save_keras_model(model, tfjs_target_dir)</pre>
<p>The conversion is straightforward since it only consists of a single line, <kbd>tfjs.<span>converters.save_keras_model(model, tfjs_target_dir)</span></kbd>. For this reason, the practical application is left as an exercise to you (see the <em>Exercises</em> section for more information).</p>
<p>Among the available deployment platforms, there is a long list of programming languages whose support to TensorFlow is given by bindings, which are usually automatically generated.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Supporting different programming languages is a great advantage since it allows developers to embed machine learning models that have been developed and trained using Python in their applications. If, for instance, we are Go developers and we want to embed a machine learning model in our application, we can use the TensorFlow Go bindings or a simplified interface built upon them called <strong>tfgo</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Go Bindings and tfgo</h1>
                </header>
            
            <article>
                
<p>The TensorFlow bindings for the Go programming language are almost entirely automatically generated from the C++ API, and as such, they implement only primitive operations. There's no Keras models, no eager execution, nor any other TensorFlow 2.0 new features; in fact, almost no changes were made to the Python API. Moreover, the Go API is not covered by the TensorFlow API satabilty guarantee, which means that everything can change between minor releases. However, this API is particularly useful for loading models that are created with Python and running them within a Go application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setup</h1>
                </header>
            
            <article>
                
<p>Setting up the environment is more complex compared to Python since it is necessary to download and install the TensorFlow C library and clone the whole TensorFlow repository to create the Go TensorFlow package at the correct version.</p>
<p class="mce-root">The following <kbd>bash</kbd> script shows how to download, configure, and install the TensorFlow Go API, with no GPU, at version 1.13:</p>
<pre>#!/usr/bin/env bash<br/><br/># variables<br/>TF_VERSION_MAJOR=1<br/>TF_VERSION_MINOR=13<br/>TF_VERSION_PATCH=1<br/><br/>curl -L "https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-""$TF_VERSION_MAJOR"."$TF_VERSION_MINOR"."$TF_VERSION_PATCH"".tar.gz" | sudo tar -C /usr/local -xz<br/>sudo ldconfig<br/>git clone https://github.com/tensorflow/tensorflow $GOPATH/src/github.com/tensorflow/tensorflow/<br/>pushd $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/go<br/>git checkout r"$TF_VERSION_MAJOR"."$TF_VERSION_MINOR"<br/>go build</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Once installed, it is possible to build and run an example program that only uses the Go bindings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Go bindings</h1>
                </header>
            
            <article>
                
<p>Refer to the example program available at <a href="https://www.tensorflow.org/install/lang_go">https://www.tensorflow.org/install/lang_go</a> for this section.</p>
<p>As you will see from the code, using TensorFlow in Go is very different compared to Python or even JavaScript. In particular, the operations that are available are really low-level and there is still the graph definition and session execution pattern to follow. A detailed explanation of the TensorFlow Go API is beyond the scope of this book; however, you can read the <em>Understanding TensorFlow using GO</em> article (<a href="https://pgaleone.eu/tensorflow/go/2017/05/29/understanding-tensorflow-using-go/">https://pgaleone.eu/tensorflow/go/2017/05/29/understanding-tensorflow-using-go/</a>), which explains the basics of the Go API.</p>
<p>A <kbd>Go</kbd> package that simplifies the usage of Go bindings is <kbd>tfgo</kbd>. In the following section, we are going to use it to restore and execute the computational graph of the <kbd>pow</kbd> operation from the previously exported SavedModel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with tfgo</h1>
                </header>
            
            <article>
                
<p>Installing <kbd>tfgo</kbd> is straightforward; just use the following code after installing the TensorFlow Go package:</p>
<pre>go get -u github.com/galeone/tfgo</pre>
<p>Since the goal is to use Go to deploy the SavedModel of the previously defined <kbd>pow</kbd> function, we are going to use the <kbd>tfgo</kbd> <kbd>LoadModel</kbd> function, which was created to load a SavedModel given the path and the desired tag.</p>
<p>TensorFlow 2.0 comes with the <kbd>saved_model_cli</kbd> tool, which can be used to inspect a SavedModel file. This tool is fundamental to correctly using a SavedModel using the Go bindings or <kbd>tfgo</kbd>. In fact, contrary to Python or TensorFlow.js, the Go API requires the name of the operations of input and output, and not the high-level names given during the SavedModel's creation.</p>
<p>By using <kbd>saved_model_cli show</kbd>, it is possible to have all the information about the inspect SavedModel and thus be able to use them in Go:</p>
<pre>saved_model_cli show --all --dir /tmp/pow/1</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This produces the following list of information:</p>
<pre>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:<br/><br/>signature_def['__saved_model_init_op']:<br/>  The given SavedModel SignatureDef contains the following input(s):<br/>  The given SavedModel SignatureDef contains the following output(s):<br/>    outputs['__saved_model_init_op'] tensor_info:<br/>        dtype: DT_INVALID<br/>        shape: unknown_rank<br/>        name: NoOp<br/>  Method name is: <br/><br/>signature_def['serving_default']:<br/>  The given SavedModel SignatureDef contains the following input(s):<br/>    inputs['x'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: unknown_rank<br/>        name: serving_default_x:0<br/>    inputs['y'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: unknown_rank<br/>        name: serving_default_y:0<br/>  The given SavedModel SignatureDef contains the following output(s):<br/>    outputs['pow_x_y'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: unknown_rank<br/>        name: PartitionedCall:0<br/>    outputs['pow_y_x'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: unknown_rank<br/>        name: PartitionedCall:1<br/>  Method name is: tensorflow/serving/predict</pre>
<p>The most important parts are as follows:</p>
<ul>
<li><strong>The tag name</strong>: <kbd>serve</kbd> is the only tag present in this SavedModel object.</li>
<li><strong>The SignatureDefs</strong>: There are two different SignatureDefs in this SavedModel: <kbd>__saved_model_init_op</kbd> which, in this case, does nothing; and <kbd>serving_default</kbd>, which contains all the necessary information about the input and output nodes of the exported computational graph.</li>
<li><strong>The inputs and outputs</strong>: Every SignatureDef section contains a list of input and outputs. As we can see, for every node, the dtype, shape, and name of the operation that generates the output Tensor are available.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Since the Go bindings support the flat graph structure, we have to use the operation names and not the names that were given during the SavedModel's creation to access the input/output nodes.</p>
<p>Now that we have all this information, it is easy to use <kbd>tfgo</kbd> to load and execute the model. The following code contains information about how the model is loaded and its usage so that it only executes the output node that computes <sub><img class="fm-editor-equation" src="assets/4e15f405-513a-440e-ba07-b2a5a1f12d53.png" style="width:1.58em;height:1.42em;"/></sub>:</p>
<p><kbd>(go)</kbd></p>
<pre>package main<br/> <br/> import (<br/> "fmt"<br/> tg "github.com/galeone/tfgo"<br/> tf "github.com/tensorflow/tensorflow/tensorflow/go"<br/> )</pre>
<p><span>In the following code snippet, you restore the model from the SavedModel tag, <kbd>"serve"</kbd>. Define the input tensors, that is, <em>x=2</em>, <em>y=5</em>. Then, compute the result. The output is the first node, <kbd>"PartitionedCall:0"</kbd>, which corresponds to <em>x_to_y</em>. The input names are <kbd>"serving_default_{x,y}"</kbd> and correspond to <kbd>x</kbd> and <kbd>y</kbd>. The predictions need to be converted back into the correct type, which is <kbd>float32</kbd> in this case:</span></p>
<pre>func main() {<br/> model := tg.LoadModel("/tmp/pow/1", []string{"serve"}, nil)<br/> x, _ := tf.NewTensor(float32(2.0))<br/> y, _ := tf.NewTensor(float32(5.0))<br/><br/>results := model.Exec([]tf.Output{<br/> model.Op("PartitionedCall", 0),<br/> }, map[tf.Output]*tf.Tensor{<br/> model.Op("serving_default_x", 0): x,<br/> model.Op("serving_default_y", 0): y,<br/> })<br/><br/> predictions := results[0].Value().(float32)<br/> fmt.Println(predictions)<br/> }</pre>
<p>As expected, the program produces <em>32</em> as the output.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The process of inspecting a SavedModel using <kbd>saved_model_cli</kbd> and using it in a Go program or in any other supported deployment platform is always the same, no matter what the content of the SavedModel is. This is one of the greatest advantages of using the standardized SavedModel serialization format as the unique connection point between the training/graph definition and the deployment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the SavedModel serialization format. This standardized serialization format was designed with the goal of simplifying the deployment of machine learning models on many different platforms.</p>
<p>SavedModel is a language-agnostic, self-contained representation of the computation, and the whole TensorFlow ecosystem supports it. Deploying a trained machine learning model on embedded devices, smartphones, browsers, or using many different languages is possible thanks to the conversion tools based on the SavedModel format or the native support offered by the TensorFlow bindings for other languages.</p>
<p>The easiest way to deploy a model is by using Python since the TensorFlow 2.0 API has complete support for the creation, restoration, and manipulation of SavedModel objects. Moreover, the Python API offers additional features and integrations between the Keras models and the SavedModel objects, making it possible to use them as checkpoints.</p>
<p>We saw how all the other deployment platforms supported by the TensorFlow ecosystem are based on the SavedModel file format or on some of its transformations. We used TensorFlow.js to deploy a model in a browser and in Node.js. We learned that we require an additional conversion step, but doing this is straightforward thanks to the Python TensorFlow.js package and the native support for Keras models. The automatically generated language bindings are close to the C++ API, and so they are more low-level and difficult to use. We also learned about Go bindings and <kbd>tfgo</kbd>, which is a simplified interface for the TensorFlow Go API. Together with the command-line tools that are used to analyze a SavedModel object, you've seen how to read the information contained inside a SavedModel and use it to deploy a SavedModel in Go.</p>
<p class="mce-root"/>
<p>We've reached the end of this book. By looking back at the previous chapters, we can see all the progress that we've made. Your journey into the world of neural networks shouldn't end here; in fact, this should be a starting point so that you can create your own neural network applications in TensorFlow 2.0. Throughout this journey, we learned about the basics of machine learning and deep learning while emphasizing the graph representation of the computation. In particular, we learned about the following:</p>
<ul>
<li>Machine learning basics, from the dataset's importance to the most common machine learning algorithm families (supervised, unsupervised, and semi-supervised).</li>
<li>The most common neural network architectures, how to train a machine learning model, and how to fight the overfitting problem through regularization.</li>
<li>The TensorFlow graph architecture that's explicitly used in TensorFlow 1.x and still present in TensorFlow 2.0. In this chapter, we started to write TensorFlow 1.x code, which we found to be extremely useful when working with <kbd>tf.function</kbd>.</li>
<li>The TensorFlow 2.0 architecture with its new way of programming, the TensorFlow 2.0 Keras implementation, eager execution, and many other new features, which were also explained in previous chapters.</li>
<li>How to create efficient data input pipelines and how to use the new <strong>TensorFlow datasets</strong> (<strong>tfds</strong>) project to quickly get a common benchmark dataset. Moreover, the Estimator API was presented, although it still uses the old graph representation.</li>
<li>How to use TensorFlow Hub and Keras to fine-tune a pre-trained model or do transfer learning. By doing this, we learned how to quickly prototype a classification network, thereby speeding up the training time by reusing the work made by the tech giant.</li>
<li>How to define a simple classification and regression network, with the goal of introducing the topic of object detection and showing how easy it is to train a multi-headed network using TensorFlow eager execution.</li>
<li>After object detection, we focused on the more difficult task (but easier to implement) of performing semantic segmentation on images, and we developed our own version of U-Net to solve it. Since a dataset of semantic segmentation is not presented in TensorFlow datasets (tfds), we also learned how to add a custom DatasetBuilder to add a new datset.</li>
<li>The <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) theory and how to implement the adversarial training loop using TensorFlow 2.0. Moreover, by using the fashion-MNIST dataset, we also learned how to define and train a conditional GAN.</li>
<li>Finally, in this chapter, we learned how to bring a trained model (or a generic computational graph) to production by leveraging the SavedModel serialization format and the TensorFlow 2.0 Serving ecosystem.</li>
</ul>
<p>Although this is the last chapter, there are exercises to do and, as usual, you shouldn't skip them!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p><span>The following exercises are programming challenges, combining the expressive power of the TensorFlow Python API and the advantages brought by other programming languages:</span></p>
<ol>
<li>What is a checkpoint file?</li>
<li>What is a SavedModel file?</li>
<li>What are the differences between a checkpoint and a SavedModel?</li>
<li>What is a SignatureDef?</li>
<li>Can a checkpoint have a SignatureDef?</li>
<li><span>Can a SavedModel have more than one SignatureDef?</span></li>
<li>Export a computational graph as a SavedModel that computes the batch matrix multiplication; the returned dictionary must have a meaningful key value.</li>
<li>Convert the SavedModel defined in the previous exercise into its TensorFlow.js representation.</li>
<li>Use the <kbd>model.json</kbd><span> file we </span>created in the previous exercise to develop a simple web page that computes the multiplication of matrices chosen by the user.</li>
<li>Restore the semantic segmentation model defined in <a href="51f4dcda-add6-4e58-a660-75f34a7e5593.xhtml">Chapter 8</a>, <em>Semantic Segmentation and Custom Dataset Builder</em>, from its latest checkpoint and use <kbd><span>tfjs.converters.save_keras_model</span></kbd><span> </span>to convert it into a <kbd>model.json</kbd><span> </span>file.</li>
<li>Use the semantic segmentation model we exported in the previous exercise to develop a simple web page that, given an image, performs semantic segmentation. Use the <kbd>tf.fromPixels</kbd><span> </span>method to get the input model. A complete reference for the TensorFlow.js API is available at <a href="https://js.tensorflow.org/api/latest/">https://js.tensorflow.org/api/latest/</a>.</li>
<li>Write a Go application using the TensorFlow Go bindings that computes the convolution between one image and a 3 x 3 kernel.</li>
</ol>
<ol start="13">
<li>Rewrite the Go application that you wrote in the previous exercise using tfgo. Use the "image" package. Read the documentation at <a href="https://github.com/galeone/tfgo">https://github.com/galeone/tfgo</a> for more information.</li>
<li><span>Restore the semantic segmentation model we defined in <a href="https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=308&amp;action=edit#post_32">Chapter 8</a>, <em>Semantic Segmentation and Custom Dataset Builder</em></span><span>, to its latest checkpoint and export it as a SavedModel object.</span></li>
<li>Use <kbd>tg.LoadModel</kbd><span> </span>to load the Semantic Segmentation model into a Go program and use it to produce a segmentation map for an input image whose path is passed as a command-line parameter.</li>
</ol>


            </article>

            
        </section>
    </body></html>