- en: GANs - Attacks and Defenses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**) represent the most advanced
    example of neural networks that deep learning makes available to us in the context
    of cybersecurity. GANs can be used for legitimate purposes, such as authentication
    procedures, but they can also be exploited to violate these procedures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental concepts of GANs and their use in attack and defense scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main libraries and tools for developing adversarial examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attacks against **deep neural networks** (**DNNs**) via model substitution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attacks against **intrusion detection systems** (**IDS**) via GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attacks against facial recognition procedures using adversarial examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now begin the chapter by introducing the basic concepts of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: GANs in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs were theorized in a famous paper that dates back to 2014 ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)),
    written by a team of researchers including Ian Goodfellow and Yoshua Bengio, which
    described the potential and characteristics of a special category of adversarial
    processes, called GANs.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind GANs is simple, as they consist of putting two neural
    networks in competition with one another, until a balanced condition of results
    is achieved; however at the same time, the possibilities of using these intuitions
    are almost unlimited, since GANs are able to learn how to imitate and artificially
    reproduce any data distribution, whether it represents faces, voices, texts, or
    even works of art.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will extend the use of GANs in the field of cybersecurity,
    learning how it is possible to use them to both carry out attacks (such as attacks
    against security procedures based on the recognition of biometric evidences) and
    to defend neural networks from attacks conducted through GANs. In order to fully
    understand the characteristics and potential of GANs, we need to introduce a number
    of fundamental concepts concerning **neural networks** (**NNs**) and **deep learning**
    (**DL**).
  prefs: []
  type: TYPE_NORMAL
- en: A glimpse into deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already encountered NNs in [Chapter 4](3311e837-18a2-4a50-8322-f7b9c12bcbc8.xhtml),
    *Malware Threat Detection*, and [Chapter 6](f467340a-244d-4714-8a39-68b230db2404.xhtml),
    *Securing User Authentication*, and now, we will extend the topic further by treating
    DL in a more systematic way. DL is a branch of **machine learning** (**ML**) that
    aims to emulate the cognitive abilities of the human brain in an attempt to perform
    those typically higher-level human tasks characterized by high complexity, such
    as facial recognition and speech recognition. DL therefore seeks to emulate the
    behavior of the human brain by introducing networks based on artificial neurons
    that are stratified on multiple levels and connected to one another, and that
    are characterized by a more or less high degree of depth which is where the **deep**
    adjective in the phrase deep learning has its origins.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts of DL and NNs are not new, but only in recent years have they found
    concrete practical, as well as theoretical, application, thanks to the progress
    achieved in the field of digital architectures, which have benefited from increased
    computational capacity, as well as the possibility of fully exploiting distributed
    computing through cloud computing, together with the almost unlimited availability
    of training data made possible by big data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: The potential of DL has been recognized not only in the research and business
    sector, but also in the field of cybersecurity, where it is increasingly essential
    to use solutions capable of dynamically adapting to changes in context, adopting
    not only static detection tools, but algorithms that are able to dynamically learn
    how to recognize new types of attacks autonomously, finding possible threats by
    analyzing the most representative features within the often noisy datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to traditional ML, DL is also characterized by a greater complexity
    from a mathematical point of view, especially regarding its widespread use of
    calculus and linear algebra. However, compared to ML, DL is able to achieve much
    better results in terms of accuracy and the potential reuse of algorithms in different
    application sectors.
  prefs: []
  type: TYPE_NORMAL
- en: Through the use of layers of NNs that are connected to one another, DL does
    not limit itself to analyzing the features of the original datasets, but is also
    able to recombine them by creating new ones, thereby adapting to the complexity
    of the analysis that is to be conducted.
  prefs: []
  type: TYPE_NORMAL
- en: The layers of artificial neurons that constitute DL analyze the data and features
    received as input and share them with the various inner layers, and these, in
    turn, process the output data of the outer layers. In this way, the original features
    extracted from the datasets are recombined, giving rise to new features that are
    optimized for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The greater the number of internal layers that are interconnected, the greater
    the depth and ability to recombine the features and adapt to the complexity of
    the problem, thereby reducing it to more specific and more manageable subtasks.
  prefs: []
  type: TYPE_NORMAL
- en: We have already mentioned that the constitutive elements of DL are the layers
    of NNs composed of artificial neurons. Now, we will examine the characteristics
    of these constituent elements in greater detail, starting with artificial neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons and activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already encountered (in [Chapter 3](aaf59353-00b3-4625-8732-63aad02cc8e5.xhtml),
    *Ham or Spam? Detecting Email Cybersecurity Threats with AI*) a particular type
    of artificial neuron, Rosenblatt's Perceptron, and we have seen that this artificial
    neuron emulates the behavior of neurons in the human brain by activating itself
    in the presence of a positive signal beyond a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify the presence of a positive signal beyond a threshold, a special function
    is used, known as the **activation** function, which, in the case of a Perceptron,
    has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/893080af-61b0-4cd9-9b91-d306614030ee.png)'
  prefs: []
  type: TYPE_IMG
- en: In practice, if the product of the *wx* values—consisting of the input data
    multiplied by the corresponding weights—exceeds a certain threshold ![](img/ab446c36-e490-4d55-b41c-08b080a54fef.png),
    then the Perceptron is activated; otherwise, it remains inert. Therefore, the
    task of the activation function is precisely to activate or not activate the artificial
    neuron following the verification of certain conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of activation functions are possible, but perhaps the most common
    is the **rectified linear unit** (**ReLU**), which, in its simplest version, entails
    assuming, as the activation value, the result obtained by applying the function
    *max(0, wx)* to the input values (multiplied by the respective weights).
  prefs: []
  type: TYPE_NORMAL
- en: 'In formal terms, this can be expressed as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d82dfa4-04d7-4444-b1e5-8140e51eb28a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is also a variant known as *LeakyReLU,* as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7f0552e-90c4-4272-ac4e-d4f70f2b0947.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike the plain ReLU, the leaky version of the activation function returns
    a softened value of the product *wx* (instead of *0*, for negative values ​​of
    *wx*), determined by the application of a multiplicative constant, ![](img/666e3062-4473-4fc6-99e6-256c9b4cd643.png),
    which usually assumes reduced values ​​close to *0* (but not equal to *0*).
  prefs: []
  type: TYPE_NORMAL
- en: From a mathematical point of view, the ReLU activation function represents a
    nonlinear transformation of a linear relationship consisting of the product of
    the input values ​​for their respective weights.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we are able to approximate any kind of behavior without having
    to limit ourselves to simple linear relationships. We mentioned this in [Chapter
    6](f467340a-244d-4714-8a39-68b230db2404.xhtml), *Securing User Authentication*,when
    we introduced the section titled *User detection with multilayer perceptrons*,
    showing how a **multilayer perceptron** (**MLP**), being made up of multiple layers
    of artificial neurons implemented by Perceptrons, is able to overcome the limitations
    of the single Perceptron, approximating any continuous mathematical function by
    introducing an adequate number of neurons in the neural network. This ability
    to approximate any continuous mathematical function is what characterizes neural
    networks, and this determines their power in terms of learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we get to neural networks from individual artificial neurons.
  prefs: []
  type: TYPE_NORMAL
- en: From artificial neurons to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the characteristics of artificial neurons and the tasks performed
    by the activation functions. Now let's look more closely at the characteristics
    of NNs. NNs are made up of layers of neurons, which together form a network. NNs
    can also be interpreted as artificial neuron graphs in which a weight is associated
    with each connection.
  prefs: []
  type: TYPE_NORMAL
- en: We have said that by adding an adequate number of neurons to the NNs, it is
    possible to emulate the behavior of any continuous mathematical function. In practice,
    NNs are nothing but an alternative way of representing mathematical functions
    of arbitrary complexity. The power of NNs manifests itself in their ability to
    assemble the original features extracted from the datasets by creating new ones.
  prefs: []
  type: TYPE_NORMAL
- en: Layers (hidden layers) are added to a neural network in order to perform such
    a combination of features. More layers are added, thereby enhancing the power
    of the network to generate new features. Particular attention must be given to
    the training procedure for NNs.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common approaches to training NNs is **forward propagation.**
    Training data is fed as input to the outer layers of the network, which, in turn,
    pass on their own partial processing output to the inner layers, and so on. The
    inner layers will carry out their elaborations on the input data received from
    the external layers, propagating the partial output returned from their processing
    forward to successive layers
  prefs: []
  type: TYPE_NORMAL
- en: The processing carried out by the various layers usually entails evaluating
    the goodness of the weights associated with the individual predictions, based
    on the anticipated values. In the case of supervised learning, for example, we
    already know the expected values ​​of the labeled samples in advance, and adjust
    the weights accordingly, based on the chosen learning algorithm. This results
    in a series of calculations, usually represented by the partial derivatives of
    the parameters associated with the individual neurons of which the different layers
    are composed, to be performed iteratively within the individual layers, thus resulting
    in a considerable load in computational terms.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of layers in the NNs increases, the number of steps that the data
    must make within the network increases exponentially. To get an idea of this,
    just think of the number of paths taken by the output of a neuron that gets forwarded
    to an inner layer consisting of 100 neurons, whose output is then, in turn, propagated
    to another layer composed of as many as 100 neurons, and so on, until it reaches
    the outer layers of neurons that return the final network output.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative training strategy, which significantly reduces the computational
    load, involves **backpropagation**. Instead of propagating the partial outputs
    obtained from the single layers toward the subsequent layers, the final outputs
    are computed at the level of the individual layers by consolidating the values
    ​​obtained, by memorizing the outputs obtained at the individual layers. In this
    way, training is affected by propagating back the output of the entire network.
    Instead of the single outputs returned by the individual layers, the weights are
    updated accordingly to minimize the error rate.
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, **backpropagation** is as a product of the **matrices**
    and **vectors** (which is less demanding in computational terms), rather than
    of a **matrix**–**matrix multiplication**, as in the case of forward propagation
    (for further details, refer to *Python Machine Learning – Second Edition*, by
    Sebastian Raschka, Packt Publishing).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at some of the most common types of NNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedforward neural networks (FFNNs)**: FFNNs represent the basic typology
    of NNs. The individual layers of neurons are connected to some (or all) of the
    neurons present in the next layer. The peculiarity of FFNNs is that the connections
    between the neurons of the individual layers go only in one direction, and there
    are no cyclical or backward connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent neural network (RNNs)**: These networks are characterized by the
    fact that the connections between neurons take the form of directed cycles in
    which the inputs and outputs consist of time series. RNNs facilitate the identification
    of patterns within the data as the data is accumulated and analyzed, and are therefore
    particularly useful for performing dynamic tasks such as speech recognition and
    language translation. In cybersecurity, RNNs are widely used in network traffic
    analysis, in static analysis, and so on. One example of an RNN is **long short-term
    memory** (**LSTM**) networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**): These networks are particularly used
    to perform image-recognition tasks. CNNs are characterized by their ability to
    identify the presence of specific features within the data. The layers that make
    up the CNNs are associated with specific filters that represent the features of
    interest (such as, for example, a set of pixels representing digits within an
    image). These filters have the characteristic of being **invariant** with respect
    to the translations in space, thereby enabling the presence of features of interest
    in different areas of the search space to be detected (for example, the presence
    of the same digit in different areas of the image). The typical architecture of
    a CNN includes a series of **convolution layers**, **activation layers**, **pooling
    layers**, and **fully connected layers**. The pooling layer has the function of
    reducing the size of the features of interest to facilitate the search for the
    presence of the features within the search space. The following diagram shows
    CNN filters in action, within the different layers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d71eec3a-9d49-4bce-b136-d5c5f45cfc91.png)'
  prefs: []
  type: TYPE_IMG
- en: '(*Image credits: https://commons.wikimedia.org/wiki/File:3_filters_in_a_Convolutional_Neural_Network.gif*)'
  prefs: []
  type: TYPE_NORMAL
- en: Following this quick review of NNs, we are now ready to get acquainted with
    GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have said that the intuition on which GANs are based entails putting two
    NNs in competition with one another in order to improve the overall results. The
    term **adversarial** refers specifically to the fact that the two NNs compete
    between themselves in completing their respective tasks. The outcome of this competition
    is an overall result that cannot be further improved, thereby attaining an equilibrium
    condition.
  prefs: []
  type: TYPE_NORMAL
- en: A typical example of using GANs is the implementation of a particular NN, called
    a **generative network**, with the task of creating an artificial image that simulates
    the characteristics of a real image. A second NN, called the **discriminator network**,
    is placed in competition with the first one (the generator) in order to distinguish
    the artificially simulated image from the real one.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting aspect is the fact that the two networks collaborate in achieving
    a situation of equilibrium (condition of indifference), putting in competition
    with one another the optimization of their respective objective functions. The
    generator network bases its optimization process on its ability to deceive the
    discriminator network.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator network, in turn, carries out its optimization process, based
    on the accuracy achieved in distinguishing the real image from the artificially
    generated image from the generator network. Now, let's look at the differences
    between the two NNs in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Generative versus discriminative networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to intuitively understand the different tasks associated with individual
    NNs involved in a GAN is to consider the scenario in which the discriminator network
    tries to correctly classify spam messages artificially generated by the generator
    network. To demonstrate the different objective functions that the individual
    NNs must optimize, we will resort to conditional probabilities (which are the
    basis of the Bayes' rule), which we have already encountered in [Chapter 3](aaf59353-00b3-4625-8732-63aad02cc8e5.xhtml), *Ham
    or Spam? Detecting Email Cybersecurity Threats with AI*, in the section *Spam
    detection with Naive Bayes*.
  prefs: []
  type: TYPE_NORMAL
- en: We define ***P***(***S***|***W***) as the probability that a given email message
    represents spam (***S***), based on the presence within the text of occurrences
    of suspect words (***W***). The task of the discriminator network therefore entails
    correctly estimating the probability ***P***(***S***|***W***) associated with
    each single email analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Symmetrically, the task of the generative network is the opposite: namely,
    to estimate the probability ***P***(***W***|***S***)—that is, given a spam message,
    how conceivable it is that the text contains the occurrences of the suspect words
    (***W***). You will recall from the theory of conditional probabilities that the
    value ***P***(***S***|***W***) is different from the value ***P***(***W***|***S***),
    so the two neural networks have different objective functions to optimize, even
    if they are correlated.'
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator network will therefore seek to optimize its objective function,
    which involves estimating appropriately the probability ***P***(***S***|***W***) by
    correctly classifying the spam messages artificially generated by the generative
    network, which in turn optimizes its objective function by generating spam email messages based
    on the probability ***P***(***W***|***S***) associated with each message. The
    generator network will then try to simulate spam messages, trying to deceive the
    discriminator network. At the same time, the discriminator network tries to correctly
    identify authentic spam messages, distinguishing them from those artificially
    created by the generator network by comparing them against the samples of genuine
    spam messages previously classified.
  prefs: []
  type: TYPE_NORMAL
- en: Both networks learn from mutual interaction. The fake spam messages generated
    by the generative network are passed as input to the discriminative network, which
    analyzes them together with real spam messages, progressively refining the estimate
    of the probability constituted by the ***P***(***S***|***W***)  estimated value.
    This establishes a symbiotic relationship  between the two neural networks, in
    which both networks try to optimize their opposite objective functions.
  prefs: []
  type: TYPE_NORMAL
- en: This situation is defined by game theory as a **zero-sum game**, and the dynamic
    equilibrium that is progressively reached, which puts an end to the optimization
    process of both networks, is known as the **Nash equilibrium**.
  prefs: []
  type: TYPE_NORMAL
- en: The Nash equilibrium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the mathematical theory of games, the Nash equilibrium is defined as the
    condition in which two competing players consider their respective game strategies
    as the best possible options available to them. This condition of equilibrium
    is the result of the learning performed by the players by iteratively repeating
    playing sessions.
  prefs: []
  type: TYPE_NORMAL
- en: In a Nash equilibrium condition, each player will then choose to perform the
    same action without modifying it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conditions under which this balance is determined are particularly restrictive.
    In fact, they imply the following:'
  prefs: []
  type: TYPE_NORMAL
- en: All players are rational (that is, they must maximize their own objective function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the players know that the other players are, in turn, rational, and know
    the respective objective functions to be maximized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All players play their game simultaneously, without being aware of the choices
    made by the others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at how to represent GANs in mathematical terms.
  prefs: []
  type: TYPE_NORMAL
- en: The math behind GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have said that the purpose of a GAN is to achieve a condition of equilibrium
    between the two NNs. The search for this equilibrium involves solving the following
    equation, a minimax condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/086f1ba1-e063-4bd4-850e-0ee0d62e0194.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding formula, you can see the antagonistic goal that characterizes
    the two neural networks. We try to maximize *D* while minimizing *G*. In other
    words, the neural network *D*, which represents the discriminator, aims to maximize
    the equation, which translates into maximizing the output associated with real
    samples while minimizing the output associated with fake samples. On the other
    side, the neural network *G*, which represents the generator, has the inverse
    goal, which is to minimize the number of failures of *G*, which results in maximizing
    the output returned by *D* when it is put in front of the fake samples.
  prefs: []
  type: TYPE_NORMAL
- en: The overall objective of the GAN is to achieve a balance in a zero-sum game
    (Nash equilibrium), characterized by a condition of indifference in which the
    output of *D* will consist of a probability estimate of 50% assigned to each categorized
    sample. In other words, the discriminator cannot reliably distinguish between
    genuine samples and fake samples.
  prefs: []
  type: TYPE_NORMAL
- en: How to train a GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a GAN may require high computational capacity; otherwise, the time
    required to carry out the training may vary from a few hours to a few days. Given
    the mutual dependency that is established between the two NNs, it is advisable
    to keep the values returned by the generator network constant while training the
    discriminator network. At the same time, it can be useful to perform the pretraining
    of the discriminator network using the training data available, before training
    the generator network.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to adequately set the learning rates of the two NNs, so
    as to avoid a situation where the learning rate of the discriminator network exceeds
    that of the generator network and vice versa, thereby preventing the respective
    NNs from achieving their optimization goals.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a GAN–emulating MNIST handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following example, adapted from the original code available at [https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py](https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py)
    (released under the MIT license at [https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE](https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE)),
    we see an example of a GAN that is able to artificially generate, from some input
    noise, the reproduction of handwritten digit images by comparing them against
    the MNIST dataset (available for download at[ http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
  prefs: []
  type: TYPE_NORMAL
- en: The activation functions of the GAN's NNs, implemented by the `build_generator()`
    and `build_discriminator()` functions, are both based on Leaky ReLU (in order
    to improve the stability of the GAN, which can be affected by the presence of
    sparse gradients).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will make use of sample noise as generator input by leveraging the `normal()`
    function from the `random` library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The training phase of both NNs is implemented by the `train()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the `train()` method, the link between the two NNs is evident:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following image, we see the progressive learning of GANs in relation
    to the different epochs. The progress achieved by the GAN in generating the representative
    images of the numbers is clearly visible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee6d925d-0113-4f8d-987d-380e93f968d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the code example, adapted from the original, that is available
    at [https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py](https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py)
    (released under the MIT license at [https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE](https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing the necessary libraries, we are now ready to address the `GAN`
    class definition, which implements our GAN, deploying deep, fully-connected neural
    networks in the form of generator and discriminator components, instantiated in
    the class constructor (the `__init__()` method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The generator and discriminator components are defined in the `build_generator()` and
    the `build_discriminator()` class methods, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the GAN, we define the `train()` class method, which takes care of
    training both the generator and discriminator components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the GAN, we can save the newly created adversarial sample images
    with the `save_imgs()` class method, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To launch the script, we just need to define the `__main__` entry point as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's move on and have a look at the GAN tools and libraries developed
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: GAN Python tools and libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of tools and libraries (both to carry out attacks and to defend from
    attacks) for developing adversarial examples is constantly growing. We will look
    at some of the most common examples of these. In this section, we will consolidate
    the general-use libraries and tools, and in the following sections, we will deal
    with libraries and specific tools based on the individual strategies and scenarios
    of attack and defense.
  prefs: []
  type: TYPE_NORMAL
- en: To fully understand the usefulness of these tools and libraries, we need to
    analyze the vulnerabilities of the cybersecurity solutions based on neural networks,
    the possibilities involved in the implementation of the attacks, and the difficulties
    in preparing an appropriate defense.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network vulnerabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the fact, as we have seen previously, that NNs have acquired particular
    relevance in recent times (as we have seen previously), due to their significant
    potential when it comes to resolving more complex problems that are usually the
    prerogative of human cognitive abilities, such as facial recognition and speech
    recognition, NNs, especially DNNs, suffer from a number of rather important vulnerabilities,
    which can be exploited through the use of GANs. This implies the possibility,
    for example, of deceiving biometric authentication procedures based on facial
    recognition or other biometric evidence made possible by the artificial creation
    of adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apparently harmless devices such as 3D medical imagery scanners have been exploited
    as attack vectors, as shown in a recent paper, *CT-GAN: Malicious Tampering of
    3D Medical Imagery using Deep Learning*, by Yisroel Mirsky, Tom Mahler, Ilan Shelef,
    and Yuval Elovici of the Department of Information Systems Engineering, Ben-Gurion
    University, Israel Soroka University Medical Center, arXiv: 1901.03597v2).'
  prefs: []
  type: TYPE_NORMAL
- en: In the study, the authors focused on the possibility of injecting and removing
    cancer images from CT scans, demonstrating how DNNs are highly susceptible to
    attack.
  prefs: []
  type: TYPE_NORMAL
- en: By adding fake evidence or removing some genuine evidence of a medical condition,
    an attacker with access to medical imagery can change the outcome of a patient's
    diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an attacker can add or remove evidence of aneurysms, tumors in
    the brain, and other forms of pathological evidences, such as heart disease. This
    type of threat shows how the use of DNNs to manage sensitive information, such
    as those pertaining to health conditions, can determine the expansion of the potential
    attack surface by several orders of magnitude, up to the possibility of committing
    crimes such as murder, which could involve politicians, heads of state, and so
    on as potential victims, without the need for the attacker to get their hands
    dirty, simply by exploiting the vulnerabilities of digital devices and procedures
    as **aseptic** attack vectors.
  prefs: []
  type: TYPE_NORMAL
- en: From what we have said, we can easily understand the severity level caused by
    the lack of robustness to adversarial attacks by DNNs, which can determine the
    compromising of the procedures and the applications they rely on.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, among the applications that exploit DNNs, there are also mission-critical
    applications (such as those that manage the functions of self-driving cars).
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural network attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are basically two main ways to carry out an attack against DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**White-box attacks**: This type of attack presupposes the model transparency
    of the DNN''s target, which grants the ability to directly verify the sensitivity
    of the response to the adversarial examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Black-box attacks**: Unlike the previous case, the sensitivity check of the
    adversarial example is implemented indirectly, not having available the configuration
    details of the targeted neural network; the only information available is the
    output values ​​returned by the neural networks to the respective inputs sent
    to them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irrespective of the type of attack, the attacker is, in any case, able to exploit
    some general characteristics concerning neural networks. As we have seen, among
    the most widespread adversarial attacks are those that aim to deceive the image
    classification algorithms, exploiting artificially created image samples. Therefore,
    knowing that image classification applications prefer to use **convolutional neural
    networks** (**CNNs**), an attacker will focus more on the vulnerabilities of such
    neural networks to conduct their own attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Even the learning strategies used by DNNs can indirectly constitute vectors
    of attack. We have previously seen how the use of the backpropagation technique
    is preferred in carrying out the training of the algorithms by virtue of its greater
    efficiency in computational terms. Being aware of this preferential learning choice,
    an attacker can, in turn, exploit algorithms such as gradient descent to attack
    DNNs, trusting that the backpropagation strategy allows the gradient computation
    of the output returned by the entire DNN.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attack methodologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the most commonly used methods to develop adversarial
    attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast gradient sign method** (**FGSM**): To generate adversarial examples,
    this method exploits the sign of the gradient associated with the backpropagation
    method used by the DNN''s victim.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jacobian-based saliency map attack** (**JSMA**): This attack methodology
    iteratively modifies information (such as the most significant pixel of an image)
    to create adversarial examples, based on a JSMA that characterizes the existing
    relationship between the input and output returned by the target neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Carlini and Wagner** (**C and W**): This adversarial attack methodology is
    perhaps the most reliable, and the most difficult to detect. The adversarial attack
    is treated as an optimization problem that uses a predefined measure (such as
    the Euclidean distance) to determine the gap between the original and the adversarial
    examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, adversarial examples also show an interesting feature: **attack transferability**.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attack transferability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical feature of adversarial attacks has to do with their **transferability**.
  prefs: []
  type: TYPE_NORMAL
- en: This feature refers to the possibility that the adversarial examples generated
    for a given DNN can also be transferred to another DNN, due to the high generalization
    capacity that characterizes the neural networks, and that constitutes their power
    (but also their fragility).
  prefs: []
  type: TYPE_NORMAL
- en: Taking advantage of the transferability of adversarial attacks, an attacker
    is able to create reusable adversarial examples without needing to know the exact
    parameters of the individual configurations of the neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: It is therefore very likely that a set of adversarial examples developed to
    successfully deceive a specific DNN for image classification, for example, can
    be exploited to deceive other neural networks with similar classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Defending against adversarial attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following the growing diffusion of adversarial attacks, many attempts have
    been made to provide adequate defense measures, based mainly on the following
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical-based detection defense**: This method tries to detect the presence
    of adversarial examples by exploiting statistical tests and outlier detection.
    It assumes that the statistical distributions characterizing the real examples
    and the adversarial examples are fundamentally distinct from one another. However,
    the effectiveness of the C and W attack methodology shows that this assumption
    is not at all obvious or reliable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient masking defense**: We have seen how adversarial attacks exploit
    the backpropagation optimization strategy adopted by most DNNs to their advantage,
    relying on information pertaining to gradient calculations performed by the target
    neural network. One form of defense, gradient masking, therefore involves hiding
    information specifically pertaining to gradients during neural network training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial training defense**: This method of defense aims to make the learning
    algorithm more robust with regard to possible perturbations present in the training
    data by inserting the adversarial samples, as well as the genuine samples, in
    the training dataset. This defense methodology also appears to be the most promising
    against C and W adversarial attacks. However, it does have a cost associated with
    it, involving the increased complexity of the network and of the increase in model
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that the vulnerabilities of the DNNs—along with the adversarial attacks
    and defense methodologies—have been introduced, we can now analyze the main libraries
    used to develop the adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: CleverHans library of adversarial examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the Python libraries that is garnering the most attention is definitely
    the CleverHans library, which is often the basis of other libraries and tools
    for developing adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: The CleverHans library is available at [https://github.com/tensorflow/cleverhans](https://github.com/tensorflow/cleverhans),
    and is released under the MIT license ([https://github.com/tensorflow/cleverhans/blob/master/LICENSE](https://github.com/tensorflow/cleverhans/blob/master/LICENSE)).
  prefs: []
  type: TYPE_NORMAL
- en: This library is particularly suitable for constructing attacks, building defenses,
    and benchmarking machine learning systems' vulnerability to adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: To install the CleverHans library, we must first proceed with the installation
    of the TensorFlow library ([https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)),
    which is used to perform the graph computations necessary in the implementation
    of learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing TensorFlow, we can proceed with the installation of CleverHans
    using the usual command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: One of the many advantages of the CleverHans library is that it offers several
    examples and tutorials in which the many different methods of using the models
    for the development of adversarial examples are shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the CleverHans library provides us with the following tutorials
    (based on the MNIST training handwritten digits dataset, available for download
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) ):'
  prefs: []
  type: TYPE_NORMAL
- en: '**MNIST with FGSM**: This tutorial covers how to train a MNIST model to craft
    adversarial examples using the FGSM and make the model more robust to adversarial
    examples using adversarial training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MNIST with JSMA**: This tutorial covers how to define a MNIST model to craft
    adversarial examples using the JSMA approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MNIST using a black-box attack**: This tutorial implements a black-box attack
    based on the adversarial training of a substitute model (that is, a copy that
    imitates the black-box model by observing the labels that the black-box model
    assigns to inputs chosen carefully by the adversary). The adversary then uses
    the substitute model''s gradients to find adversarial examples that are incorrectly
    classified by the black-box model as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During this chapter, we will encounter some examples that use the CleverHans
    library to develop adversarial attack and defense scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: EvadeML-Zoo library of adversarial examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another library of particular interest is EvadeML-Zoo. EvadeML-Zoo is a benchmarking
    and visualization tool for adversarial machine learning, developed by the machine
    learning group and the security research group at the University of Virginia.
  prefs: []
  type: TYPE_NORMAL
- en: EvadeML-Zoo is released under the MIT license ([https://github.com/mzweilin/EvadeML-Zoo/blob/master/LICENSE](https://github.com/mzweilin/EvadeML-Zoo/blob/master/LICENSE))
    and is freely available for download at [https://github.com/mzweilin/EvadeML-Zoo](https://github.com/mzweilin/EvadeML-Zoo).
  prefs: []
  type: TYPE_NORMAL
- en: 'The EvadeML-Zoo library provides a series of tools and models, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Attacking methods such as FGSM, BIM, JSMA, Deepfool, Universal Perturbations,
    and Carlini/Wagner-L2/Li/L0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretrained state-of-the-art models to attack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization of adversarial examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defense methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several ready-to-use datasets, such as, MNIST, CIFAR-10, and ImageNet-ILSVRC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the package has been downloaded, you can install the EvadeML-Zoo library
    on a machine that only uses a CPU with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, if you have a compatible GPU available, you can execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have seen that the features offered by the EvadeML-Zoo library also include
    the pretrained models, particularly useful for accelerating the development process
    of adversarial examples, which are notoriously rather heavy in computational terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the pretrained models, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Another interesting feature of the EvadeML-Zoo library is that it can be executed
    by running the `main.py` utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, you can see the usage menu of `main.py`, along
    with an example of execution of the tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The EvadeML-Zoo library is executed using the Carlini model and an FGSM adversarial
    attack on the MNIST dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will learn how to develop defense models against adversarial attacks
    using the `Defense-GAN` library.
  prefs: []
  type: TYPE_NORMAL
- en: Before analyzing the details of the Defense-GAN library, let's try to understand
    the assumptions that it is based on, along with the features it offers to implement
    an adequate defense against adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, adversarial attacks are categorized as either white-box attacks
    or black-box attacks; in the case of white-box attacks, the attacker has complete
    access to the model architecture and parameters, while in the case of black-box
    attacks, the attacker does not have access to the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We also know that many methods of defense against adversarial attacks have been
    proposed that are essentially based on the ability to distinguish the statistical
    distributions of adversarial examples from genuine samples (statistical detection),
    on the ability to hide sensitive information relating to the neural learning phase
    network (gradient masking), or on the possibility of training the learning algorithm
    using the adversarial examples together with the other training samples (adversarial
    training).
  prefs: []
  type: TYPE_NORMAL
- en: All these defense methods present limitations, as they are effective against
    either white-box attacks or black-box attacks, but not both.
  prefs: []
  type: TYPE_NORMAL
- en: Defense-GAN can instead be used as a defense against any attack, since it does
    not assume an attack model, but simply leverages the generative power of GANs
    to reconstruct adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: Defense-GAN proposes a new defense strategy based on a GAN trained in an unsupervised
    manner on legitimate (unperturbed) training samples in order to denoise adversarial
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: The Defense-GAN library is released under the Apache 2.0 license ([https://github.com/kabkabm/defensegan/blob/master/LICENSE](https://github.com/kabkabm/defensegan/blob/master/LICENSE)),
    and is freely available for download at [https://github.com/kabkabm/defensegan](https://github.com/kabkabm/defensegan).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the library is downloaded, you can install it by launching the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To download the dataset and prepare the data directory, launch the `download_dataset.py` Python
    script with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a GAN model by launching `train.py script`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The script execution will create:'
  prefs: []
  type: TYPE_NORMAL
- en: A directory in the output directory for each experiment with the same name as
    the directory where the model checkpoints are saved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A configuration file is saved in each experiment directory so that it can be
    loaded as the address to that directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A training directory in the output directory for each experiment with the same
    name as the directory where the model checkpoints are saved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A training configuration file is saved in each experiment directory so that
    it can be loaded as the address to that directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Defense-GAN library also offers tools that you can use to experiment with
    the different attack modes, thereby allowing the effectiveness of defense models
    to be verified.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform black-box attacks, we can launch the `blackbox.py` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at each parameter here:'
  prefs: []
  type: TYPE_NORMAL
- en: The `--cfg` parameter is the path to the configuration file for training the
    iWGAN. This can also be the path to the output directory of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--results_dir` parameter is the path where the final results are saved
    in text files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--bb_model` parameter represents the black-box model architectures that
    are used in tables 1 and 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--sub_model` parameter represents the substitute model architectures that
    are used in tables 1 and 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--defense_type` parameter specifies the type of defense to protect the
    classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `--train_on_recs` and `--online_training` parameters are optional. If they
    are set, the classifier will be trained on the reconstructions of Defense-GAN
    (for example, in the `Defense-GAN-Rec` column of tables 1 and 2); otherwise, the
    results are for `Defense-GAN-Orig`. Note that `--online_training` will take a
    while if `--rec_iters`, or `L` in the paper, is set to a large value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also a list of `--` that are the same as the hyperparameters that
    are defined in configuration files (all lowercase), along with a list of flags
    in `blackbox.py`. The most important ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--rec_iters`: The number of **gradient descent** (**GD**) reconstruction iterations
    for Defense-GAN, or `L` in the paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rec_lr`: The learning rate of the reconstruction step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rec_rr`: The number of random restarts for the reconstruction step, or `R`
    in the paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_train`: The number of images on which to train the black-box model.
    For debugging purposes, set this to a small value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_test`: The number of images to test on. For debugging purposes, set
    this to a small value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--debug`: This will save qualitative attack and reconstruction results in
    the debug directory and will not run the adversarial attack part of the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of `blackbox.py` execution with parameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can, of course, test Defense-GAN for white-box attacks by launching the
    `whitebox.py` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of `whitebox.py` execution with parameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As for `blackbox.py` , there is also a list of `--` that are the same as the
    hyperparameters that are defined in the configuration files (all lowercase), along
    with a list of flags in `whitebox.py`. The most important ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--rec_iters`: The number of GD reconstruction iterations for Defense-GAN,
    or `L` in the paper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rec_lr`: The learning rate of the reconstruction step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rec_rr`: The number of random restarts for the reconstruction step, or `R`
    in the paper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_test`: The number of images to test on. For debugging purposes, set
    this to a small value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now move on and see how attacks against neural networks can be performed
    via model substitution.
  prefs: []
  type: TYPE_NORMAL
- en: Network attack via model substitution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An interesting demonstration of the potential offered by adversarial attacks
    conducted in black-box mode is the one described in the paper *Practical Black-Box
    Attacks against Machine Learning* (arXiv: 1602.02697v4), in which the possibility
    of carrying out an attack against remotely hosted DNNs is demonstrated, without
    the attacker being aware of the configuration characteristics of the target NN.'
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the only information available to the attacker is that of the
    output returned by the neural network based on the type of input provided by the
    attacker. In practice, the attacker observes the classification labels returned
    by the DNN in relation to the attacking inputs. And it is here that an attack
    strategy becomes interesting. A local substitute model is, in fact, trained in
    place of the remotely hosted NN, using inputs synthetically generated by an adversary
    model and labeled by the target NN.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network hosted by MetaMind is used as a remote hosted network target,
    which exposes a DL API on the internet. By submitting to the hosted network, the
    adversarial examples trained on the local substitute, the authors verify that
    the RNN wrongly classifies over 80% of the adversarial examples. Furthermore,
    this attack strategy is also verified against similar services made available
    online by Amazon and Google, with even worse results in terms of the misclassification
    rate, which goes up to 96%.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the authors demonstrate that their black-box adversarial attacks
    strategy is of general validity, and not limited to the specific target neural
    network chosen. The result obtained also demonstrates the validity of the principle
    of the **transferability of adversarial attacks,** using the synthetic dataset
    tested on the local model. The attacker is actually replacing the local model
    with the target model by approximating the characteristics sufficiently to be
    able to exploit the vulnerabilities identified on the local model to the target
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the critical elements of the model-substitution-based adversarial
    attack methodology are substitute model training and synthetic dataset generation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look at both features.
  prefs: []
  type: TYPE_NORMAL
- en: Substitute model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said previously, the model-substitution-based adversarial attack methodology
    is aimed at training a **substitute model** that resembles the original target
    NN in order to find viable vulnerabilities on the target NN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training phase of the substitute model is therefore characterized by a
    number of important peculiarities, which involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an architecture for the substitute model without knowledge of the
    targeted model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting the number of queries made to the targeted model in order to ensure
    that the approach is tractable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to address these difficult tasks, the proposed attack strategy is based
    on the generation of synthetic data (using the technique known as **Jacobian-based
    dataset augmentation**).
  prefs: []
  type: TYPE_NORMAL
- en: Generating the synthetic dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach followed in the generation of the synthetic dataset is of central
    importance in the attack strategy based on model substitution.
  prefs: []
  type: TYPE_NORMAL
- en: To understand it, you only need to consider the fact that, although, in principle,
    it is possible to carry out an indefinite (even infinite) number of different
    queries toward the targeted model (to verify the output that the target model
    generates in relation to the input contained in the individual queries), this
    approach is not viable from a practical point of view.
  prefs: []
  type: TYPE_NORMAL
- en: It is unsustainable in the first place because the high number of queries would
    make the adversarial attack easily detectable, but it is also unsustainable because
    we would increase the number of requests to be sent to the target model in proportion
    to the number of potential input components of the target neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative solution involves using an appropriate heuristic to generate
    the synthetic dataset, based on identifying how the directions in the target model's
    output vary around an initial set of training points. These directions are identified
    with the substitute model's Jacobian matrix to accurately approximate the target
    model's decision boundaries by prioritizing the samples when querying the target
    model for labels.
  prefs: []
  type: TYPE_NORMAL
- en: Fooling malware detectors with MalGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The black-box adversarial attack strategy can also be validly used to deceive
    the next-generation antimalware systems, based on NNs.
  prefs: []
  type: TYPE_NORMAL
- en: A useful library for developing black-box adversarial attacks with malware examples
    is MalGAN, available for download at [https://github.com/yanminglai/Malware-GAN/](https://github.com/yanminglai/Malware-GAN/),
    and released under the GPL 3.0 license ([https://github.com/yanminglai/Malware-GAN/blob/master/LICENSE](https://github.com/yanminglai/Malware-GAN/blob/master/LICENSE)).
    The fundamental idea behind MalGAN is to use a GAN to generate adversarial malware
    examples, which are able to bypass black-box machine-learning-based detection
    models. To install the MalGAN library, you need to install the TensorFlow 1.80,
    Keras 2.0, and Cuckoo Sandbox 2.03 ([https://cuckoo.readthedocs.io/en/2.0.3/](https://cuckoo.readthedocs.io/en/2.0.3/))
    libraries. Cuckoo Sandbox is used to extract API features from malware samples
    acquired from [https://virusshare.com/](https://virusshare.com/) (128 API features
    are selected as dimensional vectors to be input to the NN).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code of the main MalGAN class (version 2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing the necessary libraries, let''s look at the `MalGAN()` class
    definition, beginning with its constructor (the `__init__()` method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MalGAN` class then provides the methods for building the generator component
    and the substitute detector, along with the `blackbox_detector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The training of the generator component, along with the training of the `blackbox`
    and substitute detectors, is implemented in the `train()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll train the generator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To launch the script, we just need to define the `__main__` entry point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let's now continue illustrating the IDS evasion techniques that leverage the
    GANs.
  prefs: []
  type: TYPE_NORMAL
- en: IDS evasion via GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have dealt extensively with IDS in [Chapter 5](a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml), *Network
    Anomaly Detection with AI*, where we learned about the delicate role played by
    these devices in a context like the current one, characterized by a growing explosion
    of malware threats spread through network attacks.
  prefs: []
  type: TYPE_NORMAL
- en: It is therefore necessary to introduce tools capable of promptly detecting possible
    malware threats, preventing them from spreading across the entire corporate network,
    and thereby compromising both the software and the integrity of the data (just
    think, for example, of the growing diffusion of ransomware attacks).
  prefs: []
  type: TYPE_NORMAL
- en: In order to be able to promptly and effectively carry out—that is, reduce—the
    number of false positives, it is therefore necessary to equip IDS systems with
    automated procedures capable of adequately classifying the traffic analyzed. It
    is no coincidence, therefore, that modern IDSes employ machine learning algorithms,
    also increasingly resorting to DNNs (such as CNNs, and RNNs) to improve intrusion
    detection accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, not even **intrusion detection systems** (**IDSes**) can be considered
    immune to adversarial attacks, generated specifically to deceive the underlying
    models of the IDS, thereby reducing (or even eliminating) the ability to correctly
    classify the anomalous traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, to date, there are still few theoretical studies and software
    implementations that use adversarial examples to carry out attacks against IDSes.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the demonstrations of the possibility of evading IDS detection using
    GANs is described in the paper *IDSGAN: Generative Adversarial Networks for Attack
    Generation against Intrusion Detection* ([https://arxiv.org/pdf/1809.02077](https://arxiv.org/pdf/1809.02077)).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing IDSGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also, in the case of IDSGAN, the type of attack is based on a black-box strategy,
    in which the implementation details and configuration of the target IDS are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying GAN of IDSGAN usually includes two antagonistic neural networks
    in which the generator component takes care of transforming the original network
    traffic into malicious traffic through the crafting of adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator component of IDSGAN, on the other hand, deals with correctly
    classifying the traffic, simulating the black-box detection system, thereby providing
    the necessary feedback to the generator component for the creation of adversarial
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Even in the case of IDSGAN, the adversarial examples generated using the NSL-KDD
    dataset ([http://www.unb.ca/cic/datasets/nsl.html](http://www.unb.ca/cic/datasets/nsl.html))
    show the characteristics of **attack transportability**; that is, they can be
    reused to attack many detection systems, thereby demonstrating the robustness
    of the underlying model.
  prefs: []
  type: TYPE_NORMAL
- en: Features of IDSGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main features offered by IDSGAN are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to develop attacks against IDS by emulating their behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to take advantage of adversarial examples to make attacks against
    IDS in black-box mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to reduce the detection rate of artificially produced traffic to
    zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to reuse the adversarial examples generated to attack different
    types of IDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at the structure of IDSGAN.
  prefs: []
  type: TYPE_NORMAL
- en: The IDSGAN training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, IDSGAN uses the NSL-KDD dataset ([http://www.unb.ca/cic/datasets/nsl.html](http://www.unb.ca/cic/datasets/nsl.html)),
    which contains both malicious and genuine traffic samples. These samples are particularly
    useful for checking the performance of IDSGAN, as they are also used by common
    IDS.
  prefs: []
  type: TYPE_NORMAL
- en: The NSL-KDD dataset is then used as a benchmark both to verify the effectiveness
    of the generator component and to allow the discriminator component to return
    the feedback required to create the adversarial examples. Therefore, the choice
    of the NSL-KDD dataset is not by chance, as the traffic data samples contain both
    normal and malicious traffic, subdivided into four main categories, such as probing
    (probe), **denial of service** (**DoS**), **user to root** (**U2R**), and **root
    to local** (**R2L**).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the dataset exposes the traffic according to 41 complex features,
    of which 9 are characterized by discrete values, while the remaining 32 features
    take continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'These features, in turn, can be divided into the following four types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intrinsic**: The features reflect the inherent characteristics of a single
    connection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content**: The features mark the content of connections that relate to possible
    attacks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time based**: The features examine the connections established in the past
    2 seconds that have the same destination host or the same service as the current
    connection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hosted based**: The features monitor the connections in the past 100 connections
    that have the same destination host or the same service as the current connection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the data preprocessing phase, particular attention is given to the dimensional
    impact reduction between feature values. A normalization method based on the min–max
    criterion is used to convert the input data and make it fall within the interval
    [0, 1], thereby being able to manage both the discrete features and the continuous
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formula used to carry out this normalization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/518c36f9-c301-4cb8-a21a-3c0d01e002be.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* represents the feature value before normalization, and *x′* is the
    feature value after normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have analyzed the training dataset and data normalization we can move
    on to examine the characteristics of the IDSGAN components.
  prefs: []
  type: TYPE_NORMAL
- en: Generator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in all GANs, the generator network is the component responsible for generating
    the adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: In IDSGAN, the generator transforms an original sample of the input traffic,
    associated with the vector of size *m*, which represents the characteristics of
    the original sample, a vector of dimension *n*, containing noise—that is, random
    numbers extracted from a uniform distribution whose values fall within the range
    [***0***, ***1***].
  prefs: []
  type: TYPE_NORMAL
- en: The generator network consists of five layers (with which the ReLU activation
    function is associated) to manage the output of the internal layers, while the
    output layer has sufficient units to meet the original *m*-dimensional sample
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: As we anticipated, the generator network adjusts its parameters based on the
    feedback received from the discriminator network (that emulates the behavior of
    IDS in black-box mode).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the features of IDSGAN's discriminator component in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have said that the attack strategy implemented by IDSGAN follows the black-box
    mode, which means that it is assumed that the attacker has no knowledge of the
    implementations of the target IDS. In this sense, the discriminator component
    of IDSGAN tries to mimic the attacked IDS, classifying the output generated by
    the generator component by comparing it with the normal traffic examples.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the discriminator is able to provide the necessary feedback to
    the generator in order to craft the adversarial examples. Therefore, the discriminator
    component consists of a multilayer neural network whose training dataset contains
    both the normal traffic and the adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training phases of the discriminator network are therefore as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The normal samples and the adversarial examples are classified by the IDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of the IDS are used as the target labels of the discriminator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discriminator mimics the IDS classification using the resulting training
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithms used to train the generator and discriminator components are
    outlined in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding IDSGAN's algorithm training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train the generator network, the gradients of the results obtained from
    the classification of adversarial examples by the discriminator network are used.
    The objective function, also known as the **loss function**—represented by, *L*
    in the following equation that the generator network must minimize—consists of
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/199fefbe-6687-418c-a9c1-c32c1a30b175.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *G* and *D* represent the generator and the discriminator networks, respectively,
    while *S**[attack]* represents the original malicious samples, with *M* and *N*
    representing the *m*-dimensional vector that matches the original traffic sample
    and the *n*-dimensional vector matching the noisy part, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the discriminator network, training takes place by optimizing
    the objective function represented by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0be233a6-d951-40e8-b61e-12024feb99b9.png)'
  prefs: []
  type: TYPE_IMG
- en: As we have seen, the training dataset of the discriminator network consists
    of both normal samples and adversarial examples, while the target labels are represented
    by the outputs returned by the IDS.
  prefs: []
  type: TYPE_NORMAL
- en: In the objective function, therefore, *s* represents the traffic examples used
    for the discriminator's training, while *B[normal]* and *B*[*attack* ]represent
    the normal examples and the adversarial examples correctly predicted by the IDS,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition attacks with GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a last example of the use of GANs, we will look at what is perhaps the most
    symptomatic and well-known case, which involves generating adversarial examples
    representative of human faces.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the surprising effect that this technique can have on those who examine
    the results, which are often very realistic, this technique, when used as an attack
    tool, constitutes a serious threat to all those cybersecurity procedures based
    on the verification of biometric evidence (often used to access, for example,
    online banking services, or, more recently, to log in to social networks, and
    even access your own smartphone).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it can be used to deceive even the AI-empowered facial-recognition
    tools used by the police to identify suspects, consequently reducing their overall
    reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'As demonstrated in the paper *Explaining and Harnessing Adversarial Examples* (arxiv:
    1412.6572, whose authors include Ian Goodfellow, who first introduced GANs to
    the world), you only need to introduces small perturbation (imperceptible to the
    human eye) to build artificial images that can fool neural network classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is reproduced from a famous image in which a panda is erroneously
    classified as a gibbon, due to the effect of the small perturbation injected into
    the original sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2f8fe03-3026-448f-a10b-23b7297278b8.png)'
  prefs: []
  type: TYPE_IMG
- en: (Image taken from the paper entitled *Explaining and Harnessing Adversarial
    Examples* – 1412.6572)
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition vulnerability to adversarial attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reason why common facial-recognition models are vulnerable to adversarial
    attacks is that two identical CNNs are used, which together constitute a **Siamese
    network**. In an attempt to calculate the distance between two representative
    images of the faces to be compared, a CNN is combined with the first image and
    another CNN is combined with the second image.
  prefs: []
  type: TYPE_NORMAL
- en: The distance calculated between the representations—also known as **output embeddings**,
    formulated by the CNNs in relation to the respective images—is evaluated based
    on the exceedance of a given threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: The weak link of this facial recognition method is constituted precisely by
    a correct evaluation of the distance existing between the embedding outputs associated
    with the individual images, in order to verify the exceedance of the threshold
    that determines, consequently, the failed matching of the images.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, an attacker who wants to be recognized in place of the legitimate
    user, for example, in order to log in to an online banking website or a social
    network should try to obtain CNN output embeddings by performing an unauthorized
    access of the database where they are stored. Alternatively, the attacker can
    identify themselves as any user, fooling the Siamese network by leveraging an
    adversarial example attack.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial examples against FaceNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An example of an attack that uses adversarial examples to deceive a CNN implementing
    a facial-recognition model is contained in the CleverHans library (under the examples
    directory; it is freely available for download at [https://github.com/tensorflow/cleverhans/blob/master/examples/facenet_adversarial_faces/facenet_fgsm.py](https://github.com/tensorflow/cleverhans/blob/master/examples/facenet_adversarial_faces/facenet_fgsm.py).
    The example code is released under the MIT license at [https://github.com/tensorflow/cleverhans/blob/master/LICENSE](https://github.com/tensorflow/cleverhans/blob/master/LICENSE)).
  prefs: []
  type: TYPE_NORMAL
- en: The example code shows how to perform an adversarial attack against the FaceNet
    library, using the `FGSM` method, obtaining an accuracy in excess of 99%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for the adversarial attack example against the facial-recognition
    model implemented by the FaceNet library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the necessary libraries, we can delve deeper with the `InceptionResnetV1Model`
    class definition, which provides us with all the requested methods we need to
    perform the adversarial attack against the FaceNet library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to perform our attack, leveraging the FGSM method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the FGSM will follow two different attack strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Impersonation attack (the attack is aimed at impersonating a specific user),
    using pairs of faces belonging to different individuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dodging attack (the attack is aimed at being identified as any possible user),
    using pairs of faces belonging to the same person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now look at how to launch the adversarial attack against FaceNet's CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Launching the adversarial attack against FaceNet's CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to run the adversarial attack example against FaceNet''s CNN, go through
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the FaceNet library, download and align the LFW faces, and download
    a pretrained FaceNet model as described in the FaceNet tutorial available at [https://github.com/davidsandberg/facenet/wiki/Validate-on-LFW](https://github.com/davidsandberg/facenet/wiki/Validate-on-LFW).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the downloaded datasets and the models' folders are in the same
    folder of the example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the following line in the example code, verifying that the name and path
    of the `.pb` file match the path and the filename of the FaceNet model downloaded
    previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the Python script with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the attack and defense techniques that exploit
    the adversarial examples created with GANs.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at the concrete threats that can arise from the use of GANs against
    DNNs that are increasingly at the heart of cybersecurity procedures, such as malware-detection
    tools, and biometric authentication. In addition to the risks associated with
    the widespread use of NNs in the management of sensitive data, such as health
    data, these threats lead to new forms of GAN-based attacks that can compromise
    even the health and physical safety of citizens.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to evaluate algorithms with the help
    of several examples.
  prefs: []
  type: TYPE_NORMAL
