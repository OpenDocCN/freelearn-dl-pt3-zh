<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Fraud Prevention with Cloud AI Solutions</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The objective of many security attacks and data breaches that corporations suffer from is the violation of sensitive information, such as customers' credit card details. </span><span class="koboSpan" id="kobo.2.2">Such attacks are often conducted in stealth mode, and so it is difficult to detect such threats using traditional methods. </span><span class="koboSpan" id="kobo.2.3">In addition, the amount of data to be monitored often assumes dimensions that cannot be effectively analyzed with just traditional </span><strong><span class="koboSpan" id="kobo.3.1">extract, transform, and load</span></strong> <span><span class="koboSpan" id="kobo.4.1">(</span></span><strong><span class="koboSpan" id="kobo.5.1">ETL</span></strong><span class="koboSpan" id="kobo.6.1">) procedures that are executed on relational databases, which is why it is important to adopt </span><strong><span class="koboSpan" id="kobo.7.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">AI</span></strong><span class="koboSpan" id="kobo.10.1">) solutions that are scalable. </span><span class="koboSpan" id="kobo.10.2">By doing this, companies can take advantage of cloud architectures in order to manage big data and leverage predictive analytics methodology.</span></p>
<p><span class="koboSpan" id="kobo.11.1">Credit card fraud represents an important test for the application of AI solutions in the field of cybersecurity since it requires the development of predictive analytics models that exploit big data analytics through the use of cloud computing platforms.</span></p>
<p><span class="koboSpan" id="kobo.12.1">In this chapter, you will learn about the following topics:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">How to leverage </span><strong><span class="koboSpan" id="kobo.14.1">machine learning</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong><span class="koboSpan" id="kobo.16.1">ML</span></strong><span class="koboSpan" id="kobo.17.1">) algorithms for fraud detection</span></li>
<li><span class="koboSpan" id="kobo.18.1">How bagging and boosting techniques can improve an algorithm's effectiveness</span></li>
<li><span class="koboSpan" id="kobo.19.1">How to analyze data with IBM Watson and Jupyter Notebook</span></li>
<li><span class="koboSpan" id="kobo.20.1">How to resort to statistical metrics for results evaluation</span></li>
</ul>
<p><span class="koboSpan" id="kobo.21.1">Let's introduce the role that's played by algorithms in credit card fraud detection.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introducing fraud detection algorithms</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In recent years, we have witnessed an increase in fraudulent activities in the financial sector, and particularly in the area of ​​credit card frauds. </span><span class="koboSpan" id="kobo.2.2">This is due to the fact that it is rather easy for cybercriminals to set up credit card fraud, and it has, therefore, become important for financial institutions and organizations to be able to promptly identify fraud attempts.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Furthermore, the activity of fraud detection and prevention in the context of credit card fraud is complicated by the fact that this type of fraud assumes global characteristics; that is, it involves different geographical areas as well as a variety of financial institutions and organizations.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Therefore, it is essential to be able to share the information sources that are available within different organizations around the world.</span></p>
<p><span class="koboSpan" id="kobo.5.1">These sources of information are heterogeneous and characterized by explosive growth in data generations, which need to be analyzed in real time.</span></p>
<p><span class="koboSpan" id="kobo.6.1">This resembles a typical big data analytics scenario, which requires analysis tools and appropriate software and hardware platforms, such as those offered by cloud computing.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The complexity of the scenario is aggravated by the fact that </span><span><span class="koboSpan" id="kobo.8.1">we are more likely than ever to find money laundering and illegal activities, such as i</span></span><span class="koboSpan" id="kobo.9.1">nternational terrorism financing, to be ass</span><span><span class="koboSpan" id="kobo.10.1">ociated with credit card fraud.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Illicit activities that are conducted by cybercriminals, therefore, takes on a transnational dimension that involves different sectors of organized crime.</span></p>
<p><span class="koboSpan" id="kobo.12.1">All organizations, both in public and private sectors, are called upon to cooperate and counter these illicit activities on the basis of regulatory laws such as anti-money laundering legislation.</span></p>
<p><span class="koboSpan" id="kobo.13.1">The growing interest of cybercriminals toward credit card fraud is due to distorted economic incentives; the expected payout of credit card fraud is considerably higher than alternative illegal activities, combined with the fact that the risk of being caught by the police is much lower than other forms of traditional crimes.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.14.1">Moreover, if individual financial fraud involves amounts of money and values ​​that do not exceed certain thresholds, financial institutions themselves are discouraged from pursuing illegal activities because investigation activities can prove to be uneconomical (just think, for example, of fraud that's  carried out through fake e-commerce websites located in different countries and geographic areas, which entail the need for investigative activities involving different legal jurisdictions, with an increase in costs and implementation times of law enforcement).</span></p>
<p><span class="koboSpan" id="kobo.15.1">Financial losses due to credit card fraud are not the only problem that financial institutions must face; there are also reputational damages that are caused by the loss of credibility and reliability.</span></p>
<p><span class="koboSpan" id="kobo.16.1">Furthermore, credit card fraud can also be a threat to customers; one of the most disturbing aspects of credit card fraud is related to the growing phenomenon of identity theft, which can be easily achieved by creating counterfeit documents or through the appropriation of digital copies of identity documents (found, for example, through data breaches, phishing emails, and other sources).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dealing with credit card fraud</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">However, in light of the preceding discussion, financial institutions have introduced fraud prevention measures over time: in fact, financial institutions have introduced security measures based on two-factor authentication, which integrates traditional authentication procedures by sending </span><span><span class="koboSpan" id="kobo.3.1">an OTP code</span></span><span><span class="koboSpan" id="kobo.4.1"> </span></span><span><span class="koboSpan" id="kobo.5.1">via SMS to the customer's mobile phone number to prevent abuse in the use of payment instruments.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">However, the fact remains that such measures are not sufficient and the monetary losses that financial institutions suffer as a result of credit card frauds are still in the order of billions of dollars; therefore, the most effective prevention activities to reduce these losses are procedures based on fraud detection and prevention.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The field of analysis associated with credit card fraud detection and prevention is rather complex and will offer us the opportunity to see, in action, different analysis approaches that make use of the techniques of predictive analytics, ML, and big data analytics.</span></p>
<p><span class="koboSpan" id="kobo.8.1">In this chapter, we will look the advantages of using cloud computing platforms (using the tools provided by the IBM Watson platform) in light of the fact that fraud detection and prevention requires the integration of different activity analysis, as well as the integration of heterogeneous data sources.</span></p>
<p><span class="koboSpan" id="kobo.9.1">This will lead us to the adoption of a detection approach that leverages predictive analytics, including innovative approaches such as cognitive computing.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Machine learning for fraud detection</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">The introduction of algorithmic procedures for fraud detection in the credit card sector represents an important test bench in the field of predictive analytics (as we will see shortly). </span><span class="koboSpan" id="kobo.2.2">Among the first examples of scientific research that were conducted in this field, we must mention </span><em><span class="koboSpan" id="kobo.3.1">Adaptive Machine Learning for Credit Card Fraud Detection</span></em><span class="koboSpan" id="kobo.4.1"> by Andrea Dal Pozzolo available at </span><a href="https://dalpozz.github.io/static/pdf/Dalpozzolo2015PhD.pdf"><span class="koboSpan" id="kobo.5.1">https://dalpozz.github.io/static/pdf/Dalpozzolo2015PhD.pdf</span></a><span class="koboSpan" id="kobo.6.1">), one of the most thorough pieces of scientific research, which widely exposed how to effectively leverage ML algorithms in credit card fraud detection.</span><br/></span></p>
<p><span class="koboSpan" id="kobo.7.1">The choice and design of appropriate algorithms for credit card fraud detection are characterized by the following:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.8.1">Data concerning fraud transactions is not commonly available as financial institutions are reluctant to disseminate such information for fear of reputational damage, as well as confidentiality compliance requirements.</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.9.1">From a technical point of view, the data on fraud usually represents non-stationary distributions, that is to say, they undergo changes over time; this is also due to the change in customers' spending behaviors.</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.10.1">Transaction distributions are heavily unbalanced as fraud usually represents a small percentage of overall transactions; therefore, the distributions show a high skewness toward genuine transactions. </span><span class="koboSpan" id="kobo.10.2">In fact, we are usually only able to measure fraud that has actually been detected, while it is much more difficult to estimate the number of fraud instances that haven't been detected at all (false negatives). </span><span class="koboSpan" id="kobo.10.3">Furthermore, fraud is usually recorded long after it actually occurred.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.11.1">These intrinsic characteristics of misrepresentations concerning fraud transactions result in challenges in the selection and design of detection and prevention algorithms, such as:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.12.1">The use of sampling strategies in data analysis; in the presence of unbalanced distributions the choice of an undersampling/oversampling strategy can be more useful.</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.13.1">Integration of feedback generated by human operators in identifying fraud alerts. </span><span class="koboSpan" id="kobo.13.2">This aspect is particularly important for improving the learning process of algorithms in the presence of non-stationary data, which evolves over time.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.14.1">All this translates into the development of a fraud detection and prevention system, able to integrate big data analytics, ML algorithms, and human operator's feedback. </span><span class="koboSpan" id="kobo.14.2">Therefore, it is clear that the use of cloud computing architectures is the obligatory implementation of choice.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Fraud detection and prevention systems</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">There are various possible credit card fraud scenarios, including the following:</span></p>
<ul>
<li class="mce-root"><span><strong><span class="koboSpan" id="kobo.3.1">Theft of credit cards</span></strong><span class="koboSpan" id="kobo.4.1">: This is the most frequent case in practice; criminals steal or spend as much money as possible in a short time span. </span><span class="koboSpan" id="kobo.4.2">This activity is noisy and can be identified by means of anomalous or unusual pattern detection that's carried out with respect to the spending habits of the legitimate credit card holder.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.5.1">Credit card abuse</span></strong><span class="koboSpan" id="kobo.6.1">: Unlike the previous case, fraudsters don't need to physically hold the credit card, but it is sufficient that they know the relevant information associated with the card (identification codes, PIN, personal identifier number, card number, device code, and so on). </span><span class="koboSpan" id="kobo.6.2">This is represented by one of the most insidious fraud scenarios as it is conducted in stealth mode (it isn't noisy, compared to the previous scenario) and the legitimate owner of the card is often unaware of the ongoing fraud taking place behind his/her back.</span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.7.1">Identity theft</span></strong><span class="koboSpan" id="kobo.8.1">: In this case, the credit card is issued on the basis of false personal information, or by exploiting the personal information of unsuspecting third parties, who find themselves charged for service costs and withdrawals and payments that have been made in their name.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.9.1">We should bear in mind that fraud scenarios evolve over time in relation to process and product innovations concerning financial services and technologies that are adopted by financial institutions.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Similarly, fraudsters adapt their behavior based on the technical measures that are adopted by credit card issuers to prevent and combat fraud.</span></p>
<p><span class="koboSpan" id="kobo.11.1">To correctly implement a </span><strong><span class="koboSpan" id="kobo.12.1">fraud detection and prevention system</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">FDPS</span></strong><span class="koboSpan" id="kobo.15.1">), it is necessary to distinguish between the two activities related to the management of credit card fraud:</span></p>
<ul>
<li class="mce-root"><span><strong><span class="koboSpan" id="kobo.16.1">Fraud detection</span></strong><span class="koboSpan" id="kobo.17.1">: This constitutes the set of procedures aimed at correctly and reliably identifying cases of fraud; it is put in place after the fraud occurs.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.18.1">Fraud prevention</span></strong><span class="koboSpan" id="kobo.19.1">: This constitutes the set of procedures aimed at effectively preventing the realization of the fraud; it is put in place before the fraud occurs.</span></li>
</ul>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.20.1">The two activities are characterized by the different types of procedures that are implemented, as well as by the timing with which they are introduced. </span><span class="koboSpan" id="kobo.20.2">These are as follows:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.21.1">In the case of fraud prevention, analysis procedures can exploit rule-based alarm systems that are processed by experts in the field (and, as such, require constant fine-tuning by human operators), or leverage advanced analysis techniques based on data mining, machine learning, neural networks, and more, through which it is possible to automatically discover the presence of patterns within the data distribution.</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.22.1">In the case of fraud detection, the analysis procedures are aimed at correctly classifying fraud based on the available data, thereby distinguishing it from genuine transactions.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.23.1">An important aspect of the implementation of an FDPS is not only the reliability of the results that it allows us to achieve but also its cost-effectiveness. </span><span class="koboSpan" id="kobo.23.2">It wouldn't make sense to adopt an FDPS if the implementation costs proved to be greater than the losses suffered as a result of fraud!</span></p>
<p><span class="koboSpan" id="kobo.24.1">There is an obvious trade-off between the two considered activities; in the event that an attempt at fraud cannot be prevented, then it must be detected as quickly as possible.</span></p>
<p><span class="koboSpan" id="kobo.25.1">In the same way, the two activities share the need to minimize the number of false positives (that is, the number of transactions that are treated as fraudulent when, in reality, they are legitimate) and avoid the possible denial of service caused to the customer in consequence of automated reactions resulting from false positives (such as the automatic blocking of credit cards, despite the transactions being legitimate).</span></p>
<p><span class="koboSpan" id="kobo.26.1">Compounding the management of false positives is the poor scalability of the checks carried out by human operators; if the use of controls carried out by human operators is often decisive in the correct identification of real fraud, systematically recurring human control of all transactions is, indeed, overkill.</span></p>
<p><span class="koboSpan" id="kobo.27.1">This is why it has become crucial to correctly implement automated detection and prevention procedures to support the analysis carried out by the operators.</span></p>
<p><span class="koboSpan" id="kobo.28.1">In this chapter, we will see how to take the difficulties involved in managing large data, which are often unbalanced and subject to continuous changes due to customers' changing buying habits, into account, in terms of the algorithms that are available.</span></p>
<p><span class="koboSpan" id="kobo.29.1">In the following sections, we will examine the possible strategies that we can adopt in the implementation of automated predictive models, analyzing the differences existing between expert- and data-driven strategies.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Expert-driven predictive models</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The expert-driven approach consists of implementing predictive models based on rules that have been established by experts in the sector (not by chance that the expert-driven approach is also defined as a rule-based approach).</span></p>
<p><span class="koboSpan" id="kobo.3.1">The rules follow logical conditions of the </span><kbd><span class="koboSpan" id="kobo.4.1">if...then..else</span></kbd><span class="koboSpan" id="kobo.5.1"> form, and are aimed at representing the different fraud scenarios and the related countermeasures to be adopted automatically following the checks carried out on the transaction data.</span></p>
<p><span class="koboSpan" id="kobo.6.1">Therefore, a possible rule that identifies  all credit card transactions </span><span><span class="koboSpan" id="kobo.7.1">as fraudulent if they </span></span><span><span class="koboSpan" id="kobo.8.1">exceed a certain amount of money, related to purchases made with a certain daily frequency (also compared with the historical series resembling a customer's buying habits), could be the following:</span></span></p>
<pre><span class="koboSpan" id="kobo.9.1">IF amount &gt; $1,000 AND buying_frequency &gt; historical_buying_frequency THEN fraud_likelihood = 90%</span></pre>
<p><span><span class="koboSpan" id="kobo.10.1">In the case of subsequent transactions that are executed in places that are geographically very distant from one another, it might look like this:</span></span></p>
<pre><span class="koboSpan" id="kobo.11.1">IF distance(new_transaction, last_transaction) &gt; 1000 km AND time_range &lt; 30 min THEN block_transaction</span></pre>
<p><span class="koboSpan" id="kobo.12.1">In the first case, we will look at an example of a scoring rule, while in the second case, we will talk about a blocking rule.</span></p>
<p><span class="koboSpan" id="kobo.13.1">The scoring rules are aimed to estimate the probability of fraud associated with a transaction based on rules of common experience, and also by classifying the events upon exceeding a specific threshold that's been assigned to the score.</span></p>
<p><span class="koboSpan" id="kobo.14.1">The blocking rules are more restrictive as they do not limit themselves to estimating the probabilities of fraud. </span><span class="koboSpan" id="kobo.14.2">Instead they are aimed at denying the authorization of the transaction before it is completed; therefore, blocking rules must be based on more stringent logical conditions (as in our example, in which a transaction, issued in less than half an hour from the previous one, is denied if the distance between the places of execution is greater than 1,000 km. </span><span class="koboSpan" id="kobo.14.3">It is reasonable to presume that the same customer cannot physically move to places that are so distant from each other in such a short period of time).</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.15.1">The advantages associated with rule-based predictive models are as follows:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.16.1">Ease of alerts implementation</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.17.1">Ease of alerts understanding</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.18.1">Greater alerts explicability</span></li>
</ul>
<p><span class="koboSpan" id="kobo.19.1">Equally obvious are the disadvantages of expert-driven predictive models:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.20.1">They express subjective judgments and may differ according to the experts who implement them</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.21.1">They are able to handle only a few significant variables and their mutual correlations</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.22.1">They are based on past experiences and are not able to automatically identify new fraud patterns</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.23.1">A constant, manual fine-tuning of the rules needs to be carried out manually by the experts in order to take into account the evolution of the fraud strategies that are adopted by fraudsters</span></li>
</ul>
<p><span class="koboSpan" id="kobo.24.1">These disadvantages, therefore, favor the adoption of data-driven predictive models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Data-driven predictive models</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Data-driven predictive models exploit automated learning algorithms in an attempt to adapt their prediction based on data-driven learning approaches, constantly updating detection and prevention procedures, and based on dynamically identified behavior patterns.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The algorithms that are used in data-driven predictive models are derived from distinct fields of quantitative analysis, starting from statistics, ending in data mining and ML, and having an objective of learning about hidden or latent patterns within the data.</span></p>
<p><span class="koboSpan" id="kobo.4.1">The privileged role of ML algorithms in the implementation of data-driven predictive models is immediately evident; ML makes it possible to identify predictive models based on the training that's been performed on the data.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.5.1">Furthermore, the use of ML in the field of fraud detection has several advantages:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.6.1">The ability to analyze multidimensional datasets (characterized by a high number of features, representative of the possible explanatory variables of fraud)</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.7.1">The ability to correlate the various identified features between them</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.8.1">The ability to dynamically update models, adapting them to changes in strategies adopted by fraudsters</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.9.1">ML adopts the data-driven approach, which makes use of large amounts of data (big data) in real time</span></li>
</ul>
<p><span class="koboSpan" id="kobo.10.1">In light of this, data-driven predictive models usually prove to be more robust and scalable than rule-based models.</span></p>
<p><span class="koboSpan" id="kobo.11.1">However, unlike rule-based models, data-driven predictive models often behave like black boxes, meaning that the alerts they generate are difficult to interpret and justify (for example, in the face of requests for clarification that have been issued by customers whose transactions were denied based on automated decisions made by the algorithms).</span></p>
<p><span class="koboSpan" id="kobo.12.1">In the same way, the very nature of the data can lead to difficulties in the correct implementation of the algorithms; in the case of credit cards, transaction distributions present important irregularities, such as being unbalanced, non-stationary, and skewed. </span><span class="koboSpan" id="kobo.12.2">Therefore, it is</span><span><span class="koboSpan" id="kobo.13.1"> necessary to careful</span></span><span><span class="koboSpan" id="kobo.14.1">ly choose machine learning algorithms that are capable of adequately dealing with these irregularities.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">In the case of non-stationary data in particular (that is, data that changes its characteristics over time in relation to changes in customers' buying behaviors), the algorithms must carefully update their own learning parameters, weigh the most recent data, or neglect outdated samples.</span></p>
<p><span class="koboSpan" id="kobo.16.1">An undoubted advantage of data-driven predictive models consists of the ability to integrate the operators' feedback within the predictions, thus improving the accuracy of the procedures.</span></p>
<p><span class="koboSpan" id="kobo.17.1">The operator's feedback is, in fact, characterized by greater reliability in the correct classification of fraud cases, consequently reducing the number of false negatives (that is, frauds that may go undetected), and can be automatically integrated within data-driven predictive models.</span></p>
<p><span class="koboSpan" id="kobo.18.1">Instead, rule-based models require manual revisions, to take account of operators' feedback.</span></p>
<p><span class="koboSpan" id="kobo.19.1">The ability to combine the advantages deriving from both expert-driven and data-driven predictive models constitutes the strength of the FDPS, as we will see shortly.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">FDPS – the best of both worlds</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Expert-driven and data-driven predictive models can, therefore, be combined in an FDPS in order to exploit the benefits of both approaches to improve the accuracy of the forecasts by reducing both false negatives and false positives.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The rules-based models usually reduce the number of false negatives, though this is at the cost of an increase in false positives; in combination with data-driven models, it is possible to improve forecasts by reducing false positives.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Furthermore, as we have seen, data-driven models allow operators' feedback to be integrated with other big data sources, thus contributing to dynamically updating the FDPS.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The FDPS automated maintenance and fine-tuning activities require the implementation of machine learning algorithms that can autonomously learn new forecasting patterns start from huge amounts of data.</span></p>
<p><span class="koboSpan" id="kobo.6.1">As we saw earlier, the statistical distributions related to credit card transactions are characterized by non-stationary data (which changes its characteristics in relation to changes in spending habits), which also tend to be skewed toward the bigger class of data that's representative of legitimate transactions rather than toward the smaller class representing fraud.</span></p>
<p><span class="koboSpan" id="kobo.7.1">This is due to the fact that the number of fraud cases is minimal with respect to the total number of overall transactions (furthermore, the detection of fraud transactions often takes longer, so the class of fraud transactions is systematically smaller).</span></p>
<p><span class="koboSpan" id="kobo.8.1">Not all ML algorithms can adequately manage data that simultaneously has the characteristic of being non-stationary and unbalanced. </span><span class="koboSpan" id="kobo.8.2">Due to this, it is necessary to adequately select algorithms to obtain reliable and precise predictions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Learning from unbalanced and non-stationary data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In </span><a href="752acbc8-bc15-44cc-ae83-d023265eeb84.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 1</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Introduction to AI for Cybersecurity Professionals</span></em><span class="koboSpan" id="kobo.6.1">, we saw how machine learning algorithms are divided into supervised and unsupervised learning; this subdivision is also valid in regards to credit card fraud detection, although attention must be paid to the different assumptions that inspire the two categories of algorithms. </span><span class="koboSpan" id="kobo.6.2">This is because they have important consequences on the reliability and accuracy of the predictions.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span class="koboSpan" id="kobo.7.1">In the case of supervised learning algorithms, it is assumed that a dataset of already categorized samples (labeled samples) is available; that is, each sample was previously associated with one of the two possible categories (legitimate or fraud).</span></p>
<p><span class="koboSpan" id="kobo.8.1">The supervised algorithms are, therefore, trained on the basis of this information, and the predictions they make are conditioned by the previous categorization that was carried out on the training samples, which can lead to an increase in false negatives.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Unsupervised algorithms, on the other hand, do not benefit from any previous information on the possible categorization of the sample data (unlabeled samples) and must, therefore, independently infer the possible classes of membership to be attributed to the data in order to generate false positives more easily.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dealing with unbalanced datasets</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the case of credit card transactions, we said that the distribution of data is both unbalanced and non-stationary.</span></p>
<p><span class="koboSpan" id="kobo.3.1">A solution to the unbalanced data distribution problem consists of rebalancing the classes before proceeding with training the algorithm.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Among the strategies that are commonly used to rebalance the sample classes includes undersampling and oversampling the dataset.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In essence, undersampling consists of removing some observations that belongs to a certain class at random, in order to reduce its relative consistency.</span></p>
<p><span class="koboSpan" id="kobo.6.1">In the case of unbalanced distributions, such as those relating to transactions with credit cards, if we exclude random samples from the main class (which is representative of legitimate transactions), we can reasonably expect that the distribution of data will not change substantially due to the removal of data (which can be reliably considered redundant).</span></p>
<p><span class="koboSpan" id="kobo.7.1">However, we can always incur the risk of eliminating data that contains relevant information. </span><span class="koboSpan" id="kobo.7.2">Therefore determining the correct sampling level is not always immediate as it depends on the specific characteristics of the dataset, and therefore requires the use of adaptive strategies.</span></p>
<p><span class="koboSpan" id="kobo.8.1">Another data sampling strategy consists of oversampling, that is to say, to increase the size of the smaller classes by generating synthetic samples within them.</span></p>
<p><span class="koboSpan" id="kobo.9.1">The disadvantages associated with oversampling techniques consist of the risk of introducing overfitting, and of increasing the training time of the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dealing with non-stationary datasets</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In order to manage the non-stationary characteristic of the distribution, it may be useful to overweigh the feedback that was obtained by human operators, which contributes to improving the classification of supervised samples.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Therefore, in the presence of non-stationary data, it may be useful to use an ensemble of classifiers (ensemble learning), whose training is carried out on different samples, to improve the overall prediction accuracy.</span></p>
<p><span class="koboSpan" id="kobo.4.1">By integrating different classifiers, it is possible to combine the knowledge that was obtained on the basis of the new observations with the knowledge that was previously acquired, weighing each classifier on the basis of its classification capability, and excluding those classifiers that are no longer capable of representing changes in data distribution over time.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predictive analytics for credit card fraud detection</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To adequately address the problem of fraud detection, it is necessary to develop predictive analytics models, that is, mathematical models that can identify trends within the data, using a data-driven approach.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Unlike descriptive analytics (whose paradigm is constituted by </span><strong><span class="koboSpan" id="kobo.4.1">business intelligence</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong><span class="koboSpan" id="kobo.6.1">BI</span></strong><span class="koboSpan" id="kobo.7.1">)), which limits itself to classifying the past data on the basis of measures deriving from the application of descriptive statistics (such as sums, averages, variances, and so on), precisely describe the characteristics of the data being analyzed; instead, by looking at the present and past situation, predictive analytics tries to project itself in order to predict future events with a certain degree of probability. </span><span class="koboSpan" id="kobo.7.2">It does this by extrapolating hidden patterns within the analyzed data.</span></p>
<p><span class="koboSpan" id="kobo.8.1">Being data-driven, predictive analytics makes use of data mining and ML techniques to make its predictions, and is based on the analysis of large amounts of available data (big data analytics).</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.9.1">In the following sections, we will discover how to develop predictive analytics models for the analysis of credit card fraud. </span><span class="koboSpan" id="kobo.9.2">We will learn to do the following:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.10.1">Take advantage of big data analytics to integrate information from different sources</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.11.1">Combine different classifiers (ensemble learning) to improve the performance of our predictions</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.12.1">Use bagging and boosting algorithms to develop predictive models</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.13.1">Use sampling techniques to rebalance datasets, thereby improving prediction accuracy</span></li>
</ul>
<p><span class="koboSpan" id="kobo.14.1">Let's start by discovering the advantages of leveraging big data analytics in developing predictive models in order to manage credit card fraud detection.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Embracing big data analytics in fraud detection</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">The traditional ETL solutions that are commonly adopted by organizations, which make use of data architectures based on relational databases and data warehouses, are undoubtedly adequate to perform reports according to descriptive analytics BI reporting) but not to manage large amounts of data following a data-driven approach, which is typical of predictive analytics.</span></span></p>
<p><span class="koboSpan" id="kobo.3.1">Therefore, it is necessary to adopt data architectures that allow the achievement of processing scalability through the use of functional programming paradigms (such as MapReduce, NoSQL primitives, and so on).</span></p>
<p><span class="koboSpan" id="kobo.4.1">It is possible to exploit the techniques of big data analytics and combine them with ML and data mining algorithms in order to automate fraud detection.</span></p>
<p><span class="koboSpan" id="kobo.5.1">Embracing the paradigm of big data analytics helps organizations make the most of their information assets, which come disseminated from different (often heterogeneous) data sources. </span><span class="koboSpan" id="kobo.5.2">This allows for the implementation of advanced forms of contextual awareness, which can be used to adapt detection procedures to context changes in real time.</span></p>
<p><span class="koboSpan" id="kobo.6.1">It is well-known that illegal activities are often linked to each other, and being able to construct an overall picture of the ongoing fraudulent activities presupposes constantly monitoring the different sources of available information.</span></p>
<p><span><span class="koboSpan" id="kobo.7.1">The real-time monitoring and analysis of data are facilitated by the adoption of cloud computing platforms, which also make it possible to aggregate the various data sources.</span></span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.8.1">Just think, for example, of the integration of data and information that's produced within the organization with publicly available data on websites, social media, and other platforms. By integrating these different sources of information, it is possible to reconstruct the context of the financial transactions to be monitored (for example, via social media, you may discover that the credit card holder is currently in a geographical location far from the one in which a credit card transaction is in progress).</span></p>
<p><span><span class="koboSpan" id="kobo.9.1">In the same way, the integration of different data sources allows you to feature augment the datasets; that is, the introduction of new variables starting from those existing within the datasets, which can describe the behavior of legitimate card holders and compare it with fraudsters' behavior.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">For example, we can add new variables to the existing ones, which contain recalculated values ​​such as the average expenditure level in the last time period, the number of purchases made on a daily basis, and in which shops (including e-commerce websites) the purchases </span><span><span class="koboSpan" id="kobo.11.1">usually</span></span><span><span class="koboSpan" id="kobo.12.1"> </span></span><span><span class="koboSpan" id="kobo.13.1">take place.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">In this way, it is possible to keep customer profiles constantly updated, and we can promptly detect possible anomalies in behavior and consolidated spending habits.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Ensemble learning</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Moving on from data to algorithms, earlier, we mentioned how, in the presence of non-stationary data, it may be useful to introduce an ensemble of classifiers, rather than simply using individual classifiers to improve overall prediction accuracy.</span></span></p>
<p><span><span class="koboSpan" id="kobo.3.1">Therefore, the purpose of ensemble learning is to combine different classification algorithms in order to obtain a classifier that allows you to get better predictions than those that can be obtained by using individual classifiers.</span></span></p>
<p><span><span class="koboSpan" id="kobo.4.1">To understand why the ensemble classifier behaves better than individual classifiers, we need to imagine that we have a certain number of binary classifiers, all of the same type, characterized by the ability to make correct predictions in 75% of cases and erroneous forecasts in the remaining 25% of cases.</span></span></p>
<p><span><span class="koboSpan" id="kobo.5.1">By using combinatorics analysis and binomial probability distribution (since we are considering binary classifiers), it is possible to demonstrate that, by using the ensemble classifier rather than individual classifiers, the probability of obtaining correct predictions improves (while the probability of errors decreases).</span></span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.6.1">If we had, for example, 11 binary classifiers taken together (ensemble learning), the error rate would be reduced to 3.4% (compared to the 25% error rate of individual classifiers).</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.7.1">For a formal demonstration, refer to </span><em><span class="koboSpan" id="kobo.8.1">Python Machine Learning – Second Edition</span></em><span class="koboSpan" id="kobo.9.1">, by Sebastian Raschka, Packt Publishing.</span></div>
<p><span class="koboSpan" id="kobo.10.1">There are several methods that you can use to combine classifiers; one of these is the use of majority voting (also known as the </span><strong><span class="koboSpan" id="kobo.11.1">majority voting principle</span></strong><span class="koboSpan" id="kobo.12.1">).</span></p>
<p><span class="koboSpan" id="kobo.13.1">The term majority voting principle refers to the fact that, among the predictions made by individual classifiers, we select the one that shows the highest frequency.</span></p>
<p><span><span class="koboSpan" id="kobo.14.1">In formal terms, this translates to calculating one of the statistical measures of position, known as </span><strong><span class="koboSpan" id="kobo.15.1">mode</span></strong><span class="koboSpan" id="kobo.16.1">, that is, the class that has achieved the highest frequency.</span></span></p>
<p><span><span class="koboSpan" id="kobo.17.1">Imagine we have </span><em><span class="koboSpan" id="kobo.18.1">n</span></em><span class="koboSpan" id="kobo.19.1"> classifiers, </span><em><span class="koboSpan" id="kobo.20.1">C</span></em></span><em><sub><span class="koboSpan" id="kobo.21.1">i</span></sub></em><span><em><span class="koboSpan" id="kobo.22.1">(x)</span></em><span class="koboSpan" id="kobo.23.1">, and have to determine the prediction,</span></span> <em><span><span class="koboSpan" id="kobo.24.1">y</span></span></em><span class="koboSpan" id="kobo.25.1">, </span><span><span class="koboSpan" id="kobo.26.1">most voted, that is, the prediction that has been confirmed by most of the individual classifiers. </span><span class="koboSpan" id="kobo.26.2">We can write the following formula:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.27.1"><img class="fm-editor-equation" src="assets/5c20fa09-a8ba-4e68-b888-4c3e41ead895.png" style="width:19.42em;height:1.50em;"/></span></p>
<p><span class="koboSpan" id="kobo.28.1">Obviously, we can choose individual classifiers among the different types of algorithms that are available (such as decision trees, random forest, </span><strong><span class="koboSpan" id="kobo.29.1">support vector machines</span></strong><span class="koboSpan" id="kobo.30.1"> (</span><strong><span class="koboSpan" id="kobo.31.1">SVMs</span></strong><span class="koboSpan" id="kobo.32.1">), and others).</span></p>
<p><span class="koboSpan" id="kobo.33.1">At the same time, there are several ways to create an ensemble classifier, as follows:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.34.1">Bagging (bootstrap aggregating)</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.35.1">Boosting</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.36.1">Stacking</span></li>
</ul>
<p><span class="koboSpan" id="kobo.37.1">Using the bagging method, it is possible to reduce the variance of individual estimators by selecting different training sets and applying the bootstrap resampling technique to them.</span></p>
<p><span class="koboSpan" id="kobo.38.1">Through boosting, we can create an ensemble estimator that reduces the bias of the individual classifiers. </span><span class="koboSpan" id="kobo.38.2">Finally, with stacking, the different predictions that have been obtained by heterogeneous estimators are combined.</span></p>
<p><span class="koboSpan" id="kobo.39.1">We will analyze the different methods of creating ensemble estimators in the following sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Bagging (bootstrap aggregating)</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The term </span><strong><span class="koboSpan" id="kobo.3.1">bootstrap</span></strong><span class="koboSpan" id="kobo.4.1"> refers to the operation of sampling with a replacement that's been applied to a dataset. </span><span class="koboSpan" id="kobo.4.2">The bagging method, therefore, associates an individual estimator with each bootstrap; the ensemble estimator is implemented by applying the majority voting method to individual classifiers.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The number of bootstraps to be taken into consideration can be predetermined or adjusted using a validation dataset.</span></p>
<p><span class="koboSpan" id="kobo.6.1">The bagging method is particularly useful in the case where sampling with replacement helps to rebalance the original dataset, thus reducing total variance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Boosting algorithms</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The boosting method, on the other hand, uses weighed samples that have been extracted from the data, whose weights are readjusted iteratively based on the classification errors that have been reported by the individual classifiers to reduce their bias.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Greater importance (weight) is given to the most difficult classification observations.</span></p>
<p><span class="koboSpan" id="kobo.4.1">One of the best-known boosting algorithms is </span><span><strong><span class="koboSpan" id="kobo.5.1">Adaptive Boosting</span></strong><span class="koboSpan" id="kobo.6.1"> (</span></span><strong><span class="koboSpan" id="kobo.7.1">AdaBoost</span></strong><span class="koboSpan" id="kobo.8.1">), in which a first classifier is trained on the training set.</span></p>
<p><span class="koboSpan" id="kobo.9.1">The weight associated with the samples that are incorrectly classified by the first classifier is then incremented, a second classifier is trained on the dataset containing the updated weights, and so on. </span><span class="koboSpan" id="kobo.9.2">The iterative process ends when the predetermined number of estimators is reached, or when an optimal predictor is found.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Among the main disadvantages of AdaBoost is the fact that the algorithm cannot be executed in parallel due to its sequential learning strategy.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Stacking</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The stacking method owes its name to the fact that the ensemble estimator is constructed by superimposing two layers, in which the first consists of single estimators, whose predi</span><span><span class="koboSpan" id="kobo.3.1">ctions are forwarded to the underlying layer, in</span></span><span><span class="koboSpan" id="kobo.4.1"> which another estimator has the task of classifying the predictions that are received.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Unlike the bagging and boosting methods, stacking can use different types of basic estimators, whose predictions can, in turn, be classified by a different type of algorithm than the previous ones.</span></p>
<p><span class="koboSpan" id="kobo.6.1">Let's look at some examples of ensemble estimators.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Bagging example</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we will use the Python scikit-learn library to instantiate an object of the </span><kbd><span class="koboSpan" id="kobo.3.1">BaggingClassifier</span></kbd><span class="koboSpan" id="kobo.4.1"> class, which is passed as a parameter and basic classifier of the </span><kbd><span class="koboSpan" id="kobo.5.1">DecisionTreeClassifier</span></kbd><span class="koboSpan" id="kobo.6.1"> </span><span><span class="koboSpan" id="kobo.7.1">type;</span></span><span><span class="koboSpan" id="kobo.8.1"> </span></span><span><span class="koboSpan" id="kobo.9.1">the number of basic estimators of the  </span></span><kbd><span class="koboSpan" id="kobo.10.1">DecisionTreeClassifier</span></kbd><span><span class="koboSpan" id="kobo.11.1"> </span></span><span><span class="koboSpan" id="kobo.12.1">type </span></span><span><span class="koboSpan" id="kobo.13.1">to be instantiated is set with the</span></span> <kbd><span class="koboSpan" id="kobo.14.1">n_estimators</span></kbd> <span><span class="koboSpan" id="kobo.15.1">parameter.</span></span></p>
<p><span class="koboSpan" id="kobo.16.1">It is possible to invoke on the </span><strong><span class="koboSpan" id="kobo.17.1">bagging</span></strong><span class="koboSpan" id="kobo.18.1"> instance of the </span><kbd><span class="koboSpan" id="kobo.19.1">BaggingClassifier</span></kbd><span class="koboSpan" id="kobo.20.1"> type and the </span><kbd><span class="koboSpan" id="kobo.21.1">fit()</span></kbd><span class="koboSpan" id="kobo.22.1"> and </span><kbd><span class="koboSpan" id="kobo.23.1">predict()</span></kbd><span class="koboSpan" id="kobo.24.1"> methods, which are usually invoked on the common classifiers.</span></p>
<p><span class="koboSpan" id="kobo.25.1">As we already know, the bagging method uses sampling replacement. </span><span class="koboSpan" id="kobo.25.2">Due to this, we can set the maximum number of samples to associate with each basic estimator (using the </span><kbd><span class="koboSpan" id="kobo.26.1">max_samples</span></kbd><span class="koboSpan" id="kobo.27.1"> parameter and activate the bootstrap mechanism by setting the homonymous </span><kbd><span class="koboSpan" id="kobo.28.1">bootstrap</span></kbd><span class="koboSpan" id="kobo.29.1"> parameter to </span><kbd><span class="koboSpan" id="kobo.30.1">True</span></kbd><span class="koboSpan" id="kobo.31.1">, as shown in the following example:</span></p>
<pre><span class="koboSpan" id="kobo.32.1">from sklearn.tree import DecisionTreeClassifier</span><br/><br/><span class="koboSpan" id="kobo.33.1">from sklearn.ensemble import BaggingClassifier</span><br/><br/><span class="koboSpan" id="kobo.34.1">bagging = BaggingClassifier(</span><br/><span class="koboSpan" id="kobo.35.1">            DecisionTreeClassifier(), </span><br/><span class="koboSpan" id="kobo.36.1">            n_estimators=300,</span><br/><span class="koboSpan" id="kobo.37.1">            max_samples=100, </span><br/><span class="koboSpan" id="kobo.38.1">            bootstrap=True</span><br/><span class="koboSpan" id="kobo.39.1">          )</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Boosting with AdaBoost</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As an example of the boosting method, we will instantiate an object of the </span><kbd><span class="koboSpan" id="kobo.3.1">AdaBoostClassifier</span></kbd><span class="koboSpan" id="kobo.4.1"> </span><span><span class="koboSpan" id="kobo.5.1">type </span></span><span><span class="koboSpan" id="kobo.6.1">of the</span></span> <kbd><span class="koboSpan" id="kobo.7.1">scikit-learn</span></kbd> <span><span class="koboSpan" id="kobo.8.1">library, which provides us with the implementation of the AdaBoost algorithm; as a base estimator, we will also use an instance of the</span></span> <kbd><span class="koboSpan" id="kobo.9.1">DecisionTreeClassifier</span></kbd> <span><span class="koboSpan" id="kobo.10.1">class in this example and set the number of base estimators with the</span></span> <kbd><span class="koboSpan" id="kobo.11.1">n_estimators</span></kbd> <span><span class="koboSpan" id="kobo.12.1">parameter</span></span><em><span class="koboSpan" id="kobo.13.1">:</span></em></p>
<pre><span class="koboSpan" id="kobo.14.1">from sklearn.tree import DecisionTreeClassifier</span><br/><br/><span class="koboSpan" id="kobo.15.1">from sklearn.ensemble import AdaBoostClassifier</span><br/><br/><span class="koboSpan" id="kobo.16.1">adaboost = AdaBoostClassifier(</span><br/><span class="koboSpan" id="kobo.17.1">              DecisionTreeClassifier(),</span><br/><span class="koboSpan" id="kobo.18.1">              n_estimators=300</span><br/><span class="koboSpan" id="kobo.19.1">           )</span></pre>
<p><span class="koboSpan" id="kobo.20.1">Another widely used boosting algorithm is the </span><strong><span class="koboSpan" id="kobo.21.1">gradient boosting</span></strong><span class="koboSpan" id="kobo.22.1"> algorithm. </span><span class="koboSpan" id="kobo.22.2">To understand the characteristics of the gradient boosting algorithm, we must first introduce the concept of the gradient.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introducing the gradient</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In mathematical terms, a gradient represents the partial derivative that's calculated on a given point in the n-dimensional space; it also represents the tangent line (slope) of the point that's being considered.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The gradient is used in machine learning as a cost function to be minimized in order to reduce the prediction errors that are produced by the algorithms. </span><span class="koboSpan" id="kobo.3.2">This consists of minimizing the difference between the value estimated by the algorithm and the observed value.</span></p>
<p><span class="koboSpan" id="kobo.4.1">The minimization method that's used is known as gradient descent, which is a method of optimizing the combination of weights to be assigned to the input data in order to obtain the minimum difference between the values estimated and the values observed.</span></p>
<p><span class="koboSpan" id="kobo.5.1">Therefore, the gradient descent method calculates the partial derivatives of the individual weights, updating the weights themselves on the basis of these partial derivatives until it reaches a stationary value of the partial derivatives corresponding to the minimum value sought.</span></p>
<p><span class="koboSpan" id="kobo.6.1">The gradient descent formula, along with its graphical representation, is shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="aligncenter size-full wp-image-546 image-border" src="assets/32760f0f-58f1-4fb4-86bf-21e2fd8a6aa5.png" style="width:18.25em;height:20.00em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1">(Image credit: Wikipedia, at https://commons.wikimedia.org/wiki/File:Gradient_descent.jpg )</span></div>
<p><span><span class="koboSpan" id="kobo.9.1">The problem is that the minimum value returned by the gradient descent method can correspond to a global minimum (that is, not further minimizable), but it is more likely to corresponds with a local minimum; the problem is that the gradient descent method is unable to establish whether a local minimum has been reached because the optimization process stops when it reaches a stationary value.</span></span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.10.1">The gradient descent optimization method is shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.11.1"><img class="aligncenter size-full wp-image-637 image-border" src="assets/1c065f93-a617-4f63-a071-f426d6b38a27.png" style="width:33.50em;height:26.25em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1">(Image credit: Wikipedia, at https://commons.wikimedia.org/wiki/File:Gradient_descent_method.png )</span></div>
<p><span class="koboSpan" id="kobo.13.1">Now let's look at the features of the gradient boosting algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Gradient boosting</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Similar to the AdaBoost algorithm, gradient boosting also iteratively corrects the estimators on the basis of the values ​​returned by them; in the case of gradient boosting, the adjustment takes place on the basis of the residual error generated by the previous estimators, rather than on the weights to be assigned (as in the case of AdaBoost).</span></p>
<p><span class="koboSpan" id="kobo.3.1">Next, we will show an example that uses the </span><kbd><span class="koboSpan" id="kobo.4.1">GradientBoostingClassifier</span></kbd><span class="koboSpan" id="kobo.5.1"> class of the </span><kbd><span class="koboSpan" id="kobo.6.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.7.1"> library. </span></p>
<p><span class="koboSpan" id="kobo.8.1">The default estimators are represented by decision trees, whose characteristics are specified in the parameters (such as </span><kbd><span class="koboSpan" id="kobo.9.1">max_depth</span></kbd><span class="koboSpan" id="kobo.10.1">, which establishes the growth of decision trees).</span></p>
<p><span class="koboSpan" id="kobo.11.1">Also, note the </span><kbd><span class="koboSpan" id="kobo.12.1">learning_rate</span></kbd><span class="koboSpan" id="kobo.13.1"> parameter, which must be considered together with the </span><kbd><span class="koboSpan" id="kobo.14.1">warm_start</span></kbd><span class="koboSpan" id="kobo.15.1"> parameter.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.16.1">The value assigned to the </span><kbd><span class="koboSpan" id="kobo.17.1">learning_rate</span></kbd><span class="koboSpan" id="kobo.18.1"> parameter determines the contribution that each estimator provides to the ensemble classifier; if the assigned value is </span><kbd><span class="koboSpan" id="kobo.19.1">low</span></kbd><span class="koboSpan" id="kobo.20.1">, a greater number of estimators will be needed (to be set with the </span><kbd><span class="koboSpan" id="kobo.21.1">n_estimators</span></kbd><span class="koboSpan" id="kobo.22.1"> parameter) to proceed with the fitting of the ensemble on the training set.</span></p>
<p><span class="koboSpan" id="kobo.23.1">The decision on the optimal value to be assigned to the </span><kbd><span class="koboSpan" id="kobo.24.1">learning_rate</span></kbd><span class="koboSpan" id="kobo.25.1"> and </span><kbd><span class="koboSpan" id="kobo.26.1">n_estimators</span></kbd><span class="koboSpan" id="kobo.27.1"> parameters must take into account the problem related to overfitting (that is, the possible generalization errors deriving from the excessive fitting of the model on training data). </span><span class="koboSpan" id="kobo.27.2">One way to overcome these problems is to set the </span><kbd><span class="koboSpan" id="kobo.28.1">warm_start=True</span></kbd><span class="koboSpan" id="kobo.29.1"> parameter, which determines the early stopping in the training phase, as shown in the following snippet: </span></p>
<pre><span class="koboSpan" id="kobo.30.1">from sklearn.ensemble import GradientBoostingClassifier</span><br/><br/><span class="koboSpan" id="kobo.31.1">gradient_boost = GradientBoostingClassifier(</span><br/><span class="koboSpan" id="kobo.32.1">                   max_depth=2, </span><br/><span class="koboSpan" id="kobo.33.1">                   n_estimators=100, </span><br/><span class="koboSpan" id="kobo.34.1">                   learning_rate=1.0,</span><br/><span class="koboSpan" id="kobo.35.1">                   warm_start=True</span><br/><span class="koboSpan" id="kobo.36.1">                 )</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">eXtreme Gradient Boosting (XGBoost)</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An algorithm that's similar to gradient boosting is the XGBoost algorithm.</span></p>
<p><span class="koboSpan" id="kobo.3.1">It represents an extension of gradient boosting that proves to be more suitable in managing large amounts of data since it is more scalable.</span></p>
<p><span class="koboSpan" id="kobo.4.1">XGBoost also uses the gradient descent method to minimize the residual error of the estimators, and is particularly suitable for parallel computing (a feature that makes it more suitable for cloud computing).</span></p>
<p><span class="koboSpan" id="kobo.5.1">We will see the XGBoost algorithm in action shortly when we use IBM Watson to implement credit card fraud detection on the IBM Cloud platform.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Sampling methods for unbalanced datasets</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A final aspect to consider before moving on to the operational phase of fraud detection relates to the management of unbalanced data.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.3.1">We have already said that one of the characteristics of credit card transactions is to show unbalanced distributions toward genuine transactions.</span></p>
<p><span class="koboSpan" id="kobo.4.1">To manage this asymmetry in the data, we can use different sampling methods that intend to rebalance the transaction dataset, thereby allowing the classifier to perform better.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The two most adopted sampling modes are undersampling and oversampling. </span><span class="koboSpan" id="kobo.5.2">Through undersampling, some random samples are removed from the most numerous class (in our case, the class of legitimate transactions); with oversampling, synthetic samples are added to the class with the lowest occurrences.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Oversampling with SMOTE</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Among the oversampling methods, we have the </span><strong><span class="koboSpan" id="kobo.3.1">Synthetic Minority Over-sampling Technique</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">SMOTE</span></strong><span class="koboSpan" id="kobo.6.1">); this allows for the generation of synthetic samples by interpolating the values that are present within the class subjected to oversampling.</span></p>
<p><span class="koboSpan" id="kobo.7.1">In practice, synthetic samples are generated based on the clusters that are identified around the observations present in the class, therefore calculating the </span><strong><span class="koboSpan" id="kobo.8.1">k-Nearest Neighbors</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">k-NNs</span></strong><span class="koboSpan" id="kobo.11.1">).</span></p>
<p><span class="koboSpan" id="kobo.12.1">Based on the number of synthetic samples that are needed to rebalance the class, a number of k-NN clusters are randomly chosen, around which </span><span><span class="koboSpan" id="kobo.13.1">synthetic examples are</span></span><span><span class="koboSpan" id="kobo.14.1"> generated by interpolating the values that fall within the selected clusters.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Sampling examples</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following examples are taken from the official Python library imbalanced-learn documentation, which implements undersampling and oversampling algorithms, among others.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Let's look at an example of the undersampling technique by using the </span><kbd><span class="koboSpan" id="kobo.4.1">RandomUnderSampler</span></kbd><span class="koboSpan" id="kobo.5.1"> class:</span></p>
<pre><span class="koboSpan" id="kobo.6.1"># From the Imbalanced-Learn library documentation:</span><br/><span class="koboSpan" id="kobo.7.1"># https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html</span><br/><br/><span class="koboSpan" id="kobo.8.1">from collections import Counter</span><br/><span class="koboSpan" id="kobo.9.1">from sklearn.datasets import make_classification</span><br/><span class="koboSpan" id="kobo.10.1">from imblearn.under_sampling import RandomUnderSampler </span><br/><br/><span class="koboSpan" id="kobo.11.1">X, y = make_classification(n_classes=2, class_sep=2,</span><br/><span class="koboSpan" id="kobo.12.1"> weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,</span><br/><span class="koboSpan" id="kobo.13.1">n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)</span><br/><span class="koboSpan" id="kobo.14.1">print('Original dataset shape %s' % Counter(y))</span><br/><br/><span class="koboSpan" id="kobo.15.1">rus = RandomUnderSampler(random_state=42)</span><br/><span class="koboSpan" id="kobo.16.1">X_res, y_res = rus.fit_resample(X, y)</span><br/><span class="koboSpan" id="kobo.17.1">print('Resampled dataset shape %s' % Counter(y_res))</span></pre>
<p><span><span class="koboSpan" id="kobo.18.1">Here is an example of the oversampling technique using the SMOTE class:</span></span></p>
<pre><span class="koboSpan" id="kobo.19.1"># From the Imbalanced-Learn library documentation:</span><br/><span class="koboSpan" id="kobo.20.1"># https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html</span><br/><br/><span class="koboSpan" id="kobo.21.1">from collections import Counter</span><br/><span class="koboSpan" id="kobo.22.1">from sklearn.datasets import make_classification</span><br/><span class="koboSpan" id="kobo.23.1">from imblearn.over_sampling import SMOTE </span><br/><br/><span class="koboSpan" id="kobo.24.1">X, y = make_classification(n_classes=2, class_sep=2,</span><br/><span class="koboSpan" id="kobo.25.1">   weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,</span><br/><span class="koboSpan" id="kobo.26.1">   n_features=20, n_clusters_per_class=1, n_samples=1000,    </span><br/><span class="koboSpan" id="kobo.27.1">   random_state=10)</span><br/><br/><span class="koboSpan" id="kobo.28.1">print('Original dataset shape %s' % Counter(y))</span><br/><span class="koboSpan" id="kobo.29.1">Original dataset shape Counter({1: 900, 0: 100})</span><br/><br/><span class="koboSpan" id="kobo.30.1">sm = SMOTE(random_state=42)</span><br/><span class="koboSpan" id="kobo.31.1">X_res, y_res = sm.fit_resample(X, y)</span><br/><span class="koboSpan" id="kobo.32.1">print('Resampled dataset shape %s' % Counter(y_res))</span><br/><span class="koboSpan" id="kobo.33.1">Resampled dataset shape Counter({0: 900, 1: 900})</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting to know IBM Watson Cloud solutions</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The time has come to get to know one of the most interesting cloud-based solutions available on the market, and will allow us to look at a concrete example of credit card fraud detection in action: we are talking about the IBM Watson Cloud solution, which introduces, among the other things, the innovative concept of cognitive computing.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Through cognitive computing, it is possible to emulate the typically human ability of pattern recognition, which allows adequate contextual awareness to be obtained for decision-making.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.4.1">IBM Watson can be successfully used in various real scenarios; here are few:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.5.1">Augmented reality</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.6.1">Crime prevention</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.7.1">Customer support</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.8.1">Facial recognition</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.9.1">Fraud prevention</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.10.1">Healthcare and medical diagnosis</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.11.1">IoT</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.12.1">Language translation and </span><strong><span class="koboSpan" id="kobo.13.1">natural language processing</span></strong><span class="koboSpan" id="kobo.14.1"> (</span><strong><span class="koboSpan" id="kobo.15.1">NLP</span></strong><span class="koboSpan" id="kobo.16.1">)</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.17.1">Malware detection</span></li>
</ul>
<p class="mce-root"><span><span class="koboSpan" id="kobo.18.1">Before going into detail about the IBM Watson Cloud platform, let's see the advantages associated with cloud computing and cognitive computing.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cloud computing advantages</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">With the spread of higher bandwidth networks, combined with the availability of low-cost computers and storage, the architectural model of cloud computing has rapidly taken hold thanks to the availability of virtualization solutions, both on the software and hardware side.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.3.1">The central element that characterizes cloud computing is the scalability of the architecture, which has determined its commercial success.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.4.1">Organizations that have adopted cloud computing solutions have succeeded in optimizing investments in the IT sector, thereby improving their profit margins; instead of being forced to dimension their technological infrastructure based on the worst scenario (that is, the one that takes into account the peaks of workload, even if only temporary), the organizations that have embraced cloud solutions have benefited from an on-demand model, thereby reducing fixed costs and turning them into variable costs.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.5.1">This improvement in the quality of technological investments has allowed organizations to focus on the management and analysis of data constituting corporate information assets.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.6.1">In fact, cloud computing allows for the storage and management of large amounts of data efficiently, guaranteeing high performance, high availability, and low latency; to offer these guarantees of access and performance, the data is stored and replicated on servers that are distributed in various geographical areas. </span><span class="koboSpan" id="kobo.6.2">Furthermore, by partitioning the data, it is possible to obtain the advantages connected to the scalability of the architecture.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.7.1">More specifically, scalability is related to the ability to manage increasing workloads by adding resources to the architecture—increasing costs in a linear manner, proportional to the number of resources being added.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Achieving data scalability</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">One of the main problems of traditional architectures based on relational databases and the data warehouse is that these solutions do not scale well compared to the explosive growth of data. </span><span class="koboSpan" id="kobo.2.2">Such architectures need to be adequately sized, even in the design phase.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.3.1">With the spread of big data analytics, it was, therefore, necessary to move on to other paradigms for data storage, known as </span><strong><span class="koboSpan" id="kobo.4.1">distributed storage systems</span></strong><span class="koboSpan" id="kobo.5.1">, which allow for the precise prevention of bottlenecks in the management and storage of data.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.6.1">Cloud computing makes extensive use of such distributed storage systems to enable the analysis of large amounts of data (big data analytics), even in streaming mode.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.7.1">Distributed storage systems consist of non-relational databases and are defined as NoSQL databases, which store data in key-value pairs. </span><span class="koboSpan" id="kobo.7.2">This allows for the management of data in a distributed mode on multiple servers by following functional programming paradigms such as MapReduce. </span><span class="koboSpan" id="kobo.7.3">This, in turn, allows for the execution of data processing in parallel, takes full advantage of the distributed computing capabilities offered by the Cloud.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.8.1">The use of NoSQL databases also allows data to be managed in a flexible manner, without the need to reorganize its overall structure as the analysis changes.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.9.1">However, traditional solutions based on relational databases require reconfiguration of almost the entire structure of the archives, which makes data unavailable for long periods of time. </span><span class="koboSpan" id="kobo.9.2">This is no longer acceptable in a context that's characterized by the need to verify </span><span><span class="koboSpan" id="kobo.10.1">predictive model accuracy </span></span><span><span class="koboSpan" id="kobo.11.1">in real time that's based on which business decisions </span></span><span><span class="koboSpan" id="kobo.12.1">to take</span></span><span><span class="koboSpan" id="kobo.13.1">; this aspect is also of particular relevance for decision-making in the area of cybersecurity.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cloud delivery models</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">The scalability of the architecture, combined with the ability to manage resources in on-demand mode, allows providers to offer different cloud delivery models:</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.3.1">Infrastructure as a Service</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">IaaS</span></strong><span class="koboSpan" id="kobo.6.1">): The provider deploys an IT infrastructure, such as storage capabilities and networking equipment</span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.7.1">Platform as a Service</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">PaaS</span></strong><span class="koboSpan" id="kobo.10.1">): The provider deploys middleware, a database, and more</span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.11.1">Software as a Service</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong><span class="koboSpan" id="kobo.13.1">SaaS</span></strong><span class="koboSpan" id="kobo.14.1">): The provider deploys complete applications</span></li>
</ul>
<p class="mce-root"><span class="koboSpan" id="kobo.15.1">The IBM Cloud platform offers a delivery model that includes IaaS and PaaS, as well as a series of cloud services that can be integrated into applications that are developed by organizations, such as the following:</span></p>
<ul>
<li>
<p><strong><span class="koboSpan" id="kobo.16.1">Visual recognition</span></strong><span class="koboSpan" id="kobo.17.1">: This enables apps to locate information such as objects, faces, and text contained within images and videos; the services that are offered by the platform include checking the availability of pre-trained models, as well as the opportunity to train using corporate datasets.</span></p>
</li>
<li>
<p><strong><span class="koboSpan" id="kobo.18.1">Natural language understanding</span></strong><span class="koboSpan" id="kobo.19.1">: This service can extract information about sentiment based on the analysis of a text; it is particularly useful if you want to extract information from social media (to understand, for example, whether the credit card holder is actually on vacation in a foreign state when a transaction is made with their credit card). </span><span class="koboSpan" id="kobo.19.2">The service can identify information regarding people, places, organizations, concepts, and categories, and is adaptable on the basis of specific application domains of interest to the company via Watson Knowledge Studio.</span></p>
</li>
</ul>
<p><span class="koboSpan" id="kobo.20.1">The IBM Cloud platform also offers a series of advanced tools for application development:</span></p>
<ul>
<li class="mce-root"><span><strong><span class="koboSpan" id="kobo.21.1">Watson Studio</span></strong><span class="koboSpan" id="kobo.22.1">: This allows the management of projects and offers tools for collaboration between team members. </span><span class="koboSpan" id="kobo.22.2">With Watson Studio, it is possible to add data sources, create Jupyter Notebooks, train models, and use many other features that facilitate data analysis, such as data cleansing functions. </span><span class="koboSpan" id="kobo.22.3">W</span></span><span><span class="koboSpan" id="kobo.23.1">e will have the opportunity to deepen our knowledge of Watson Studio soon.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.24.1">Knowledge Studio</span></strong><span class="koboSpan" id="kobo.25.1">: This allows the development of customized models on the specific needs of the company; once developed, the models can be used by Watson services, in addition to, or in place of, the predefined models.</span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.26.1">Knowledge Catalog</span></strong><span class="koboSpan" id="kobo.27.1">: This allows the management and sharing of company data; the tool also makes it possible to perform data cleaning and wrangling operations, thereby profiling data access permissions through security policies.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.28.1">Among the major advantages offered by the IBM Cloud platform, there is the undoubted possibility of implementing advanced solutions that exploit cognitive computing. </span><span class="koboSpan" id="kobo.28.2">Let's look at what this is.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Empowering cognitive computing</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The spread of AI has been accompanied since the beginning by often excessive and unjustified concerns; many authors and commentators have foreseen apocalyptic scenarios in which machines (in the not too distant future) take precedence over humans. </span><span class="koboSpan" id="kobo.2.2">The cause of such disasters would have to be found precisely in the rise of AI.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The reality is that, despite the amazing successes achieved by computers, they still continue to be idiot savants.</span></p>
<p><span class="koboSpan" id="kobo.4.1">There is no doubt that the computational capacities reached by computers exceed those of human beings by several orders of magnitude; the victory achieved by IBM Watson in the match of the century, which saw the computer beating the then world chess champion, Garry Kasparov, seemed to have decreed the final overcoming of human cognitive faculties using AI.</span></p>
<p><span class="koboSpan" id="kobo.5.1">However, despite their computational limitations, humans are still unbeaten in relation to a whole range of skills, such as the ability to adapt, interact, make judgments, and more.</span></p>
<p><span class="koboSpan" id="kobo.6.1">We human beings can recognize, for example, a person (or an object) at a glance, without the need to be trained with large amounts of sample data; just one photograph (or an identikit) is enough to recognize the depicted person amid a crowd of people. </span><span class="koboSpan" id="kobo.6.2">Computers are far from reaching such levels of expertise.</span></p>
<p><span class="koboSpan" id="kobo.7.1">It is not a question, then, of replacing humans with machines; on the contrary, the most likely scenario before us is one in which humans and machines work together ever more closely, integrating their mutual skills in an increasingly pervasive way.</span></p>
<p><span class="koboSpan" id="kobo.8.1">This is the meaning of cognitive computing: integrating human abilities with the computational abilities of computers, combining forces to face the growing complexity that characterizes contemporary society.</span></p>
<p><span class="koboSpan" id="kobo.9.1">In this symbiotic relationship, machines make their enormous computational capabilities and inexhaustible memory available to human beings, which allows them to amplify their capacity for judgment, intuition, empathy, and creativity.</span></p>
<p><span class="koboSpan" id="kobo.10.1">In a sense, through cognitive computing, machines allow us not only to amplify our five natural senses but to add a sixth </span><em><span class="koboSpan" id="kobo.11.1">artificial</span></em><span class="koboSpan" id="kobo.12.1"> sense: contextual awareness.</span></p>
<p><span class="koboSpan" id="kobo.13.1">We have said several times that one of the major difficulties that's encountered, especially in the field of cybersecurity, is that of being able to reconstruct a precise overall picture, starting from the multiple, dispersed, and fragmented information at hand.</span></p>
<p><span class="koboSpan" id="kobo.14.1">Human abilities are at a loss in the face of the overabundance of data and information that we constantly receive from various data sources; big data analytics is beyond the capabilities of human analysis, precisely because of the countless dimensions (consisting of the many different features, as well as the amount of data) that characterize big data.</span></p>
<p><span class="koboSpan" id="kobo.15.1">However, big data allows us to define the semantic context within which we can carry out our analysis; it is as if they increased our perceptive capacity, adding an indefinite number of artificial sensors.</span></p>
<p><span class="koboSpan" id="kobo.16.1">Only the computational capacity of machines can filter the numerous pieces of information we receive in a constant and incessant way from artificial sensors to human judgment skills and human intuition to give us an overall meaning and allows us to make sense of such information.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Importing sample data and running Jupyter Notebook in the cloud</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Now, let's learn how to use the IBM Watson platform. </span><span class="koboSpan" id="kobo.2.2">The first thing we need to do is create an account, if we don't have one already; just connect to the IBM Cloud platform home link provided her</span></span><span class="koboSpan" id="kobo.3.1">e at</span><a href="https://dataplatform.cloud.ibm.com/"><span class="koboSpan" id="kobo.4.1">https://dataplatform.cloud.ibm.com/</span></a><a href="https://dataplatform.cloud.ibm.com/"><span class="koboSpan" id="kobo.5.1">. </span><span class="koboSpan" id="kobo.5.2">You will see the following screen:</span></a><a href="https://dataplatform.cloud.ibm.com/"/></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="aligncenter size-full wp-image-548 image-border" src="assets/77d67c6e-61f9-49cd-9926-9775187cba76.png" style="width:50.00em;height:16.08em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1">IBM Watson home page</span></div>
<p><span><span class="koboSpan" id="kobo.8.1">To proceed with the registration, select </span><span class="packt_screen"><span class="koboSpan" id="kobo.9.1">Try it for Free</span></span><span class="koboSpan" id="kobo.10.1"> (register) as shown in the preceding screenshot. </span><span class="koboSpan" id="kobo.10.2">We will be automatically redirected to the registration form, as shown in the following screenshot:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.11.1"><img class="aligncenter size-full wp-image-549 image-border" src="assets/28f7d868-1a85-4d61-8a4b-7b29ec83d3b1.png" style="width:70.83em;height:60.75em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1">IBM Watson Registration page</span></div>
<p><span><span class="koboSpan" id="kobo.13.1">Once registration is complete, we can log in again from the home page:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.14.1"><img class="aligncenter size-full wp-image-550 image-border" src="assets/09dd24fc-1b5b-47b3-b12c-ea0dd3726b4e.png" style="width:24.75em;height:21.33em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><span class="koboSpan" id="kobo.15.1">IBM Watson login form</span></span></div>
<p><span><span class="koboSpan" id="kobo.16.1">After logging in, we can create a new project:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.17.1"><img class="aligncenter size-full wp-image-551 image-border" src="assets/24b3e857-f5a1-4d71-9413-438a4e9798f3.png" style="width:41.25em;height:18.25em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span><span class="koboSpan" id="kobo.18.1">IBM Watson start by creating a project screen</span></span></span></div>
<p><span><span class="koboSpan" id="kobo.19.1">We can select the type of project we want to create:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.20.1"><img class="aligncenter size-full wp-image-552 image-border" src="assets/01f0b6f4-2388-4ede-b814-ad4e20cd59bf.png" style="width:50.33em;height:30.58em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span class="koboSpan" id="kobo.21.1">IBM Watson project selection</span></span></div>
<p><span class="koboSpan" id="kobo.22.1">In our case, we will choose </span><span class="packt_screen"><span class="koboSpan" id="kobo.23.1">Data Science</span></span><span class="koboSpan" id="kobo.24.1">, as shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.25.1"><img class="aligncenter size-full wp-image-553 image-border" src="assets/9c721daa-a337-4814-a130-cb849720a158.png" style="width:17.25em;height:12.92em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span><span class="koboSpan" id="kobo.26.1">IBM Watson Data Science project</span></span></span></div>
<p><span class="koboSpan" id="kobo.27.1">We assign the </span><span><span class="koboSpan" id="kobo.28.1">name</span></span><span><span class="koboSpan" id="kobo.29.1">  </span></span><kbd><span class="koboSpan" id="kobo.30.1">CREDIT CARD FRAUD DETECTION</span></kbd><span><span class="koboSpan" id="kobo.31.1"> </span></span><span><span class="koboSpan" id="kobo.32.1">to the project</span></span><span><span class="koboSpan" id="kobo.33.1"> </span></span><span><span class="koboSpan" id="kobo.34.1">(or another name of our choice):</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1"><img class="aligncenter size-full wp-image-638 image-border" src="assets/6c102afe-f26f-42b8-9452-d570f9eb5ace.png" style="width:46.25em;height:32.58em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><span><span class="koboSpan" id="kobo.36.1">IBM Watson new project screen</span></span></span></div>
<p><span class="koboSpan" id="kobo.37.1">We can now add a dataset to our project by selecting </span><span class="packt_screen"><span class="koboSpan" id="kobo.38.1">Add to project</span></span><span class="koboSpan" id="kobo.39.1"> | </span><span class="packt_screen"><span class="koboSpan" id="kobo.40.1">Data</span></span><span class="koboSpan" id="kobo.41.1">:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.42.1"><img class="aligncenter size-full wp-image-639 image-border" src="assets/ee7b3139-5238-4f54-836a-9529550d11cb.png" style="width:55.42em;height:28.17em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span class="koboSpan" id="kobo.43.1">IBM Watson-Add Data</span></span></div>
<p><span class="koboSpan" id="kobo.44.1">To add the dataset, just click on </span><span class="packt_screen"><span class="koboSpan" id="kobo.45.1">Find and Add Data</span></span><span class="koboSpan" id="kobo.46.1"> and go to the </span><span class="packt_screen"><span class="koboSpan" id="kobo.47.1">Files</span></span><span class="koboSpan" id="kobo.48.1"> tab. </span><span class="koboSpan" id="kobo.48.2">From there, you can click on and add data files from your computer.</span></p>
<p><span class="koboSpan" id="kobo.49.1">The dataset that we will use is the credit card dataset, available for download in </span><kbd><span class="koboSpan" id="kobo.50.1">.csv</span></kbd><span class="koboSpan" id="kobo.51.1"> format at</span><a href="https://www.openml.org/data/get_csv/1673544/phpKo8OWT"><span class="koboSpan" id="kobo.52.1"> </span></a><a href="https://www.openml.org/data/get_csv/1673544/phpKo8OWT"><span class="koboSpan" id="kobo.53.1">https://www.openml.org/data/get_csv/1673544/phpKo8OWT</span></a><a href="https://www.openml.org/data/get_csv/1673544/phpKo8OWT"><span class="koboSpan" id="kobo.54.1">.</span></a><a href="https://www.openml.org/data/get_csv/1673544/phpKo8OWT"/></p>
<p><span class="koboSpan" id="kobo.55.1">The credit card dataset has been released under the public domain (</span><a href="https://creativecommons.org/publicdomain/mark/1.0/"><span class="koboSpan" id="kobo.56.1">https://creativecommons.org/publicdomain/mark/1.0/</span></a><span class="koboSpan" id="kobo.57.1">) license (</span><a href="https://www.openml.org/d/1597"><span class="koboSpan" id="kobo.58.1">https://www.openml.org/d/1597</span></a><span class="koboSpan" id="kobo.59.1">) and is credited to Andrea Dal Pozzolo, Olivier Caelen, Reid A. </span><span class="koboSpan" id="kobo.59.2">Johnson, and Gianluca Bontempi for their paper </span><em><span class="koboSpan" id="kobo.60.1">Calibrating Probability with Undersampling for Unbalanced Classification</span></em><span class="koboSpan" id="kobo.61.1">, in Symposium on </span><strong><span class="koboSpan" id="kobo.62.1">Computational Intelligence and Data Mining</span></strong><span class="koboSpan" id="kobo.63.1"> (</span><strong><span class="koboSpan" id="kobo.64.1">CIDM</span></strong><span class="koboSpan" id="kobo.65.1">), IEEE, 2015.</span></p>
<p><span class="koboSpan" id="kobo.66.1">The dataset contains 31 numerical input variables, such as time (representing the time that had elapsed between each transaction), the transaction amount, and the class feature.</span></p>
<p><span class="koboSpan" id="kobo.67.1">The class feature is a binary variable that takes only the values 1 and 0 (indicating a fraudulent or legitimate transaction, respectively).</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.68.1">The main characteristic of the dataset is that it is highly unbalanced, with frauds accounting for just 0.172% of all transactions.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.69.1">Having added the dataset, we can add a Jupyter notebook to the project by selecting </span><span class="packt_screen"><span class="koboSpan" id="kobo.70.1">Add to project</span></span><span class="koboSpan" id="kobo.71.1"> | </span><span class="packt_screen"><span class="koboSpan" id="kobo.72.1">Notebook</span></span><span class="koboSpan" id="kobo.73.1">, as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.74.1"><img class="aligncenter size-full wp-image-640 image-border" src="assets/896b3a18-70ee-408f-925d-e6c7f9dc3e72.png" style="width:134.58em;height:68.33em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span class="koboSpan" id="kobo.75.1">IBM Watson-Add Notebook</span></span></div>
<p><span class="koboSpan" id="kobo.76.1">To create a Jupyter Notebook, perform the following steps:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.77.1">Click on create a notebook</span></li>
<li><span class="koboSpan" id="kobo.78.1">Select the tab</span></li>
<li><span class="koboSpan" id="kobo.79.1">Enter a name for the notebook</span></li>
<li><span class="koboSpan" id="kobo.80.1">Optionally, enter a description for the notebook</span></li>
<li><span class="koboSpan" id="kobo.81.1">Enter the notebook URL: </span><a href="https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb"><span class="koboSpan" id="kobo.82.1">https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb</span></a></li>
<li><span class="koboSpan" id="kobo.83.1">Choose the </span><span class="packt_screen"><span class="koboSpan" id="kobo.84.1">Runtime</span></span></li>
<li><span class="koboSpan" id="kobo.85.1">Click on </span><span class="packt_screen"><span class="koboSpan" id="kobo.86.1">Create</span></span></li>
</ol>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.87.1">Congratulations! </span><span class="koboSpan" id="kobo.87.2">You have successfully completed configured your project and are ready to see the credit card fraud detection model with IBM Watson Studio on the IBM Cloud platform in action.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Credit card fraud detection with IBM Watson Studio</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Let's see the fraud detection predictive model that we loaded into the IBM Watson Studio Jupyter notebook (the complete code, released by IBM with the Apache 2.0 license, is available at the link:</span></span> <a href="https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb"><span class="koboSpan" id="kobo.3.1">https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb</span></a> <span><span class="koboSpan" id="kobo.4.1">) in action.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">The first operation to perform is to convert the credit card dataset, loaded in </span><kbd><span class="koboSpan" id="kobo.6.1">.csv</span></kbd><span class="koboSpan" id="kobo.7.1"> format, into a </span><kbd><span class="koboSpan" id="kobo.8.1">pandas</span></kbd><span class="koboSpan" id="kobo.9.1"> DataFrame;. </span><span class="koboSpan" id="kobo.9.2">This operation can be performed as follows.</span></p>
<p><span><span class="koboSpan" id="kobo.10.1">Select the cell below </span><span class="packt_screen"><span class="koboSpan" id="kobo.11.1">Read the Data and convert it to the DataFrame</span></span><span class="koboSpan" id="kobo.12.1"> section in the notebook and perform the following steps:</span></span></p>
<ol>
<li class="mce-root"><span class="koboSpan" id="kobo.13.1">Use </span><span class="packt_screen"><span class="koboSpan" id="kobo.14.1">Find and Add Data</span></span><span class="koboSpan" id="kobo.15.1"> and its </span><span class="packt_screen"><span class="koboSpan" id="kobo.16.1">Files</span></span><span class="koboSpan" id="kobo.17.1"> tab. </span><span class="koboSpan" id="kobo.17.2">You should see the file names that we uploaded earlier.</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.18.1">Select </span><span class="packt_screen"><span class="koboSpan" id="kobo.19.1">Insert to Code</span></span><span class="koboSpan" id="kobo.20.1">.</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.21.1">Click on </span><span class="packt_screen"><span class="koboSpan" id="kobo.22.1">Insert Pandas DataFrame</span></span><span class="koboSpan" id="kobo.23.1">.</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.24.1">Once the dataset has been converted into a </span><kbd><span class="koboSpan" id="kobo.25.1">pandas</span></kbd><span class="koboSpan" id="kobo.26.1"> DataFrame, we can rename it by replacing the name that was automatically assigned by Watson Studio with a name of our choice, as shown in the following snippet:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><span class="koboSpan" id="kobo.27.1"># Rename the dataframe to df</span><br/><br/><span class="koboSpan" id="kobo.28.1">df = df_data_2</span></pre>
<p><span><span class="koboSpan" id="kobo.29.1">At this point, we can proceed to subdivide the dataset into train and test data using the</span></span> <kbd><span class="koboSpan" id="kobo.30.1">train_test_split</span></kbd> <span><span class="koboSpan" id="kobo.31.1">method; this is done by utilizing the usual split rate (30% for the test and the remaining 70% for training), as shown in the following example:</span></span></p>
<pre><br/><span class="koboSpan" id="kobo.32.1">from sklearn.model_selection import train_test_split</span><br/><br/><span class="koboSpan" id="kobo.33.1">x = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',</span><br/><span class="koboSpan" id="kobo.34.1">       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',</span><br/><span class="koboSpan" id="kobo.35.1">       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]</span><br/><span class="koboSpan" id="kobo.36.1">y = df['Class']</span><br/><br/><span class="koboSpan" id="kobo.37.1">xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.30,</span><br/><span class="koboSpan" id="kobo.38.1">random_state=0)</span></pre>
<p><span class="koboSpan" id="kobo.39.1">Remember that the dataset contains 31 numerical input variables, in which the </span><kbd><span class="koboSpan" id="kobo.40.1">Time</span></kbd><span class="koboSpan" id="kobo.41.1"> feature denotes</span><span><span class="koboSpan" id="kobo.42.1"> the seconds elapsed between each transaction and the first transaction in the dataset, and the</span></span> <kbd><span><span class="koboSpan" id="kobo.43.1">Amount</span></span></kbd><span><span class="koboSpan" id="kobo.44.1"> </span></span><span><span class="koboSpan" id="kobo.45.1">feature, which represents the transaction amount.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">The </span><kbd><span class="koboSpan" id="kobo.47.1">Class</span></kbd><span class="koboSpan" id="kobo.48.1"> feature is the response variable and it takes a value of </span><kbd><span class="koboSpan" id="kobo.49.1">1</span></kbd><span class="koboSpan" id="kobo.50.1"> in cases of fraud and </span><kbd><span class="koboSpan" id="kobo.51.1">0</span></kbd><span class="koboSpan" id="kobo.52.1"> otherwise.</span></p>
<p><span class="koboSpan" id="kobo.53.1">For confidentiality reasons, the meaning of most variables (indicated with </span><kbd><span class="koboSpan" id="kobo.54.1">V1</span></kbd><span class="koboSpan" id="kobo.55.1">, </span><kbd><span class="koboSpan" id="kobo.56.1">V2</span></kbd><span class="koboSpan" id="kobo.57.1">, …, </span><kbd><span class="koboSpan" id="kobo.58.1">V28</span></kbd><span class="koboSpan" id="kobo.59.1">) is not revealed and the features have been transformed by means of principal components.</span></p>
<p><span class="koboSpan" id="kobo.60.1">At this point, we can introduce our first ensemble classifier in order to test the quality of its classification on the dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predicting with RandomForestClassifier</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The choice falls on one of the most used among the ensemble algorithms, that is, the random forest algorithm.</span></p>
<p><span class="koboSpan" id="kobo.3.1">This type of algorithm is used by the </span><kbd><span class="koboSpan" id="kobo.4.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.5.1"> class of </span><kbd><span class="koboSpan" id="kobo.6.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.7.1"> to create a set of decision trees from a subset extracted at random from a training set.</span></p>
<p><span class="koboSpan" id="kobo.8.1">The algorithm represents an example of a learning ensemble that uses the bagging technique and is, therefore, particularly suitable for reducing the overfitting of the model.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Let's look at an example of </span><kbd><span class="koboSpan" id="kobo.10.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.11.1"> and its accuracy score:</span></p>
<pre><span class="koboSpan" id="kobo.12.1">from sklearn.ensemble import RandomForestClassifier</span><br/><span class="koboSpan" id="kobo.13.1">from sklearn import metrics</span><br/><br/><span class="koboSpan" id="kobo.14.1">rfmodel = RandomForestClassifier()</span><br/><span class="koboSpan" id="kobo.15.1">rfmodel.fit(xtrain,ytrain)</span><br/><span class="koboSpan" id="kobo.16.1">ypredrf = rfmodel.predict(xtest)</span><br/><br/><span class="koboSpan" id="kobo.17.1">print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))</span><br/><br/><span class="koboSpan" id="kobo.18.1">Accuracy : 0.999414</span></pre>
<p><span class="koboSpan" id="kobo.19.1">The accuracy of the model is rather high (99.9414%), demonstrating the effectiveness of the ensemble learning.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.20.1">Let's see if we can improve the predictions obtained by using another classifier ensemble, this time taking advantage of the boosting technique.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predicting with GradientBoostingClassifier</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Now we will use </span><kbd><span class="koboSpan" id="kobo.3.1">GradientBoostingClassifier</span></kbd><span class="koboSpan" id="kobo.4.1">, which is based on AlgaBoost.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The algorithm used by the ensemble classifier adopts the boosting technique; it also uses gradient descent to minimize the cost function (represented by the residual error returned by the individual base classifiers, also constituted by decision trees).</span></p>
<p><span class="koboSpan" id="kobo.6.1">In the following code, we can see the gradient-boosting ensemble classifier in action:</span></p>
<pre><span class="koboSpan" id="kobo.7.1">from sklearn import ensemble</span><br/><br/><span class="koboSpan" id="kobo.8.1">params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,</span><br/><span class="koboSpan" id="kobo.9.1">          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}</span><br/><br/><span class="koboSpan" id="kobo.10.1">clf = ensemble.GradientBoostingClassifier(**params)</span><br/><span class="koboSpan" id="kobo.11.1">clf.fit(xtrain, ytrain) </span><br/><br/><span class="koboSpan" id="kobo.12.1">y_pred = clf.predict(xtest) </span><br/><br/><span class="koboSpan" id="kobo.13.1">print("Accuracy is :")</span><br/><span class="koboSpan" id="kobo.14.1">print(metrics.accuracy_score(ytest, y_pred))</span><br/><br/><span class="koboSpan" id="kobo.15.1">Accuracy is : 0.998945085858</span></pre>
<p><span class="koboSpan" id="kobo.16.1">The accuracy of the model is still high, but it hasn't improved any further than </span><kbd><span class="koboSpan" id="kobo.17.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.18.1">; we have, in fact, reached just 99.89% accuracy in the predictions, but we can do better.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predicting with XGBoost</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We will now try to further improve our predictions by using </span><strong><span class="koboSpan" id="kobo.3.1">XGBoost</span></strong><span class="koboSpan" id="kobo.4.1">, which represents an improved version of the gradient boosting algorithm since it was designed to optimize performance (using parallel computing), thus reducing overfitting.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.5.1">We will use the </span><kbd><span class="koboSpan" id="kobo.6.1">XGBClassifier</span></kbd><span class="koboSpan" id="kobo.7.1"> class of the </span><kbd><span class="koboSpan" id="kobo.8.1">xgboost</span></kbd><span class="koboSpan" id="kobo.9.1"> library, which implements the eXtreme Gradient Boosting Classifier, as shown in the following code: </span></p>
<pre><span class="koboSpan" id="kobo.10.1">from sklearn import metrics</span><br/><span class="koboSpan" id="kobo.11.1">from xgboost.sklearn import XGBClassifier</span><br/><br/><span class="koboSpan" id="kobo.12.1">xgb_model = XGBClassifier()</span><br/><br/><span class="koboSpan" id="kobo.13.1">xgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])</span><br/><br/><span class="koboSpan" id="kobo.14.1">y_pred = xgb_model.predict(xtest)  </span><br/><br/><span class="koboSpan" id="kobo.15.1">print("Accuracy is :")</span><br/><span class="koboSpan" id="kobo.16.1">print(metrics.accuracy_score(ytest, y_pred))</span><br/><br/><span class="koboSpan" id="kobo.17.1">Accuracy is : 0.999472542929</span></pre>
<p><span><span class="koboSpan" id="kobo.18.1">The accuracy has improved even further; we have reached a percentage equal to 99.9472% (higher, albeit slightly, than the accuracy of </span><kbd><span class="koboSpan" id="kobo.19.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.20.1">, which is equal to 99.9414%). </span><span class="koboSpan" id="kobo.20.2">This isn't bad, but now we must carefully evaluate the quality of our predictions.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Evaluating the quality of our predictions</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To correctly evaluate the quality of the predictions that were obtained by our classifiers, we cannot be satisfied with just </span><kbd><span class="koboSpan" id="kobo.3.1">accuracy_score</span></kbd><span class="koboSpan" id="kobo.4.1">, but must also use other measures, such as the </span><strong><span class="koboSpan" id="kobo.5.1">F1 score</span></strong><span class="koboSpan" id="kobo.6.1"> and the </span><strong><span class="koboSpan" id="kobo.7.1">ROC curve</span></strong><span class="koboSpan" id="kobo.8.1">, which we previously encountered in </span><a href="a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml"><span class="koboSpan" id="kobo.9.1">Chapter 5</span></a><span class="koboSpan" id="kobo.10.1">, </span><em><span class="koboSpan" id="kobo.11.1">Network Anomalies Detection with AI</span></em><span class="koboSpan" id="kobo.12.1">, dealing with the topic related to anomaly detection.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">F1 value</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">For the convenience, let's briefly go over the metrics that were previously introduced and their definitions:</span></span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em><span class="koboSpan" id="kobo.3.1">Sensitivity or True Positive Rate (TPR) = True Positive / (True Positive + False Negative);</span></em></p>
<p><span class="koboSpan" id="kobo.4.1">Here, sensitivity is also known as the recall rate:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em><span class="koboSpan" id="kobo.5.1">False Positive Rate (FPR) = False Positive / (False Positive + True Negative);</span></em><br/>
<em><span class="koboSpan" id="kobo.6.1">Precision = True Positive / (True Positive + False Positive)</span></em></p>
<p><span><span class="koboSpan" id="kobo.7.1">On the basis of these metrics, it is possible to estimate the F1 score, which represents the harmonic average between precision and sensitivity:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><em><span class="koboSpan" id="kobo.8.1">F1 = 2 * Precision * Sensitivity / (Precision + Sensitivity)</span></em></p>
<p><span class="koboSpan" id="kobo.9.1">The F1 score can be used to evaluate the results that were obtained from the predictions; the best estimates are obtained with F1 values close to 1, while the worst estimates correspond to F1 values close to 0.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">ROC curve</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Often, between false positives and false negatives, there is a trade-off; </span></span><span><span class="koboSpan" id="kobo.3.1">reducing the number of false negatives leads to an increase in false positives and to detect the existence of this trade-off, a particular curve is used, known as the ROC curve. </span><span class="koboSpan" id="kobo.3.2">This is as shown in the following image:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.4.1"><img class="aligncenter size-full wp-image-557 image-border" src="assets/39ea1875-2a82-4ba1-8969-149be037b7e7.png" style="width:27.58em;height:18.08em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.5.1">(Image Credit: Wikipedia, at https://commons.wikimedia.org/wiki/File:ROC_curve.svg )</span></div>
<p><span><span class="koboSpan" id="kobo.6.1">The ROC curve is calculated using </span></span><kbd><span class="koboSpan" id="kobo.7.1">roc_curve()</span></kbd> <span><span class="koboSpan" id="kobo.8.1">of </span><kbd><span class="koboSpan" id="kobo.9.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.10.1">, which takes the target values and the corresponding probabilities as parameters as shown in the following code:</span></span></p>
<pre><span class="koboSpan" id="kobo.11.1">from sklearn.metrics import roc_curve</span><br/><br/><span class="koboSpan" id="kobo.12.1">FPR, TPR, OPC = roc_curve(targets, probs)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span><span class="koboSpan" id="kobo.13.1">We should notice the existing link between the True Positive Rate (</span><kbd><span class="koboSpan" id="kobo.14.1">TPR</span></kbd><span class="koboSpan" id="kobo.15.1"> or sensitivity), the False Positive Rate (</span><kbd><span class="koboSpan" id="kobo.16.1">FPR</span></kbd><span class="koboSpan" id="kobo.17.1">), and the ROC curve (instead, the </span><kbd><span class="koboSpan" id="kobo.18.1">OPC</span></kbd><span class="koboSpan" id="kobo.19.1"> parameter represents a control coefficient, known as an </span><strong><span class="koboSpan" id="kobo.20.1">operating characteristic</span></strong><span class="koboSpan" id="kobo.21.1">, which identifies the possible classification thresholds on the curve). </span><span class="koboSpan" id="kobo.21.2">We can, therefore, represent the sensitivity by plotting the </span><kbd><span class="koboSpan" id="kobo.22.1">TPR</span></kbd><span class="koboSpan" id="kobo.23.1"> value with respect to the value of the </span><kbd><span class="koboSpan" id="kobo.24.1">OPC</span></kbd><span class="koboSpan" id="kobo.25.1"> control coefficient:</span></span></p>
<pre><span class="koboSpan" id="kobo.26.1"># Plotting Sensitivity</span><br/><span class="koboSpan" id="kobo.27.1">plt.plot(OPC,TPR)</span></pre>
<p><span><span class="koboSpan" id="kobo.28.1">We can see how sensitivity (</span><kbd><span class="koboSpan" id="kobo.29.1">TPR</span></kbd><span class="koboSpan" id="kobo.30.1">) decreases as the value of </span><kbd><span class="koboSpan" id="kobo.31.1">OPC</span></kbd><span class="koboSpan" id="kobo.32.1"> increases; in the same way, we can draw the ROC curve by comparing </span><kbd><span class="koboSpan" id="kobo.33.1">TPR</span></kbd><span class="koboSpan" id="kobo.34.1"> with </span><kbd><span class="koboSpan" id="kobo.35.1">FPR</span></kbd><span class="koboSpan" id="kobo.36.1">:</span></span></p>
<pre><span class="koboSpan" id="kobo.37.1"># Plotting ROC curve</span><br/><span class="koboSpan" id="kobo.38.1">plt.plot(FPR,TPR)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">AUC (Area Under the ROC curve)</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The ROC curve allows us to evaluate the performance of a classifier by plotting </span><kbd><span class="koboSpan" id="kobo.3.1">TPR</span></kbd><span class="koboSpan" id="kobo.4.1"> against </span><kbd><span class="koboSpan" id="kobo.5.1">FPR</span></kbd><span class="koboSpan" id="kobo.6.1"> (where each point of the curve corresponds to a different classification threshold).</span></p>
<p><span class="koboSpan" id="kobo.7.1">We can also compare different classifiers to find out which one is more accurate, using the area under the ROC curve.</span></p>
<p><span class="koboSpan" id="kobo.8.1">To understand the logic of this comparison, we must consider that the optimal classifier within the ROC space is identified by the coordinates of points </span><em><span class="koboSpan" id="kobo.9.1">x</span></em><span class="koboSpan" id="kobo.10.1"> = 0 and </span><em><span class="koboSpan" id="kobo.11.1">y</span></em><span class="koboSpan" id="kobo.12.1"> = 1 (which correspond to the limit case of no false negatives and no false positives).</span></p>
<p><span class="koboSpan" id="kobo.13.1">To compare different classifiers, we can calculate the value of the </span><strong><span class="koboSpan" id="kobo.14.1">Area Under the ROC Curve</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong><span class="koboSpan" id="kobo.16.1">AUC</span></strong><span class="koboSpan" id="kobo.17.1">) associated with each classifier; the classifier that obtains the highest AUC value is the most accurate.</span></p>
<p><span class="koboSpan" id="kobo.18.1">We can also take into account the AUC values of two peculiar classifiers:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.19.1">AUC of the best classifier is 1 x 1 = 1</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.20.1">AUC of the worst classifier is 0.5</span></li>
</ul>
<p><span class="koboSpan" id="kobo.21.1">Furthermore, AUC also represents a measure for unbalanced datasets.</span></p>
<p><span class="koboSpan" id="kobo.22.1">We can calculate </span><kbd><span class="koboSpan" id="kobo.23.1">AUC</span></kbd><span class="koboSpan" id="kobo.24.1"> using </span><kbd><span class="koboSpan" id="kobo.25.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.26.1">, as shown in the following code:</span></p>
<pre><span class="koboSpan" id="kobo.27.1">from sklearn.metrics import auc</span><br/><br/><span class="koboSpan" id="kobo.28.1">AUC = auc(FPR, TPR)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span><span class="koboSpan" id="kobo.29.1">Based on what we have said, we can now proceed to a more accurate evaluation of the predictions that were obtained by our classifiers, and compare them with each other.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Comparing ensemble classifiers</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We can now proceed to calculate the main accuracy measurements for each classifier by comparing them.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The RandomForestClassifier report</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The classification report for the </span><kbd><span class="koboSpan" id="kobo.3.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.4.1"> metrics is shown in the following code:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">print('classification report')</span><br/><span class="koboSpan" id="kobo.6.1">print(metrics.classification_report(ytest, ypredrf))</span><br/><span class="koboSpan" id="kobo.7.1">print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))</span><br/><span class="koboSpan" id="kobo.8.1">print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))</span><br/><br/><span class="koboSpan" id="kobo.9.1">classification report</span><br/><span class="koboSpan" id="kobo.10.1">             precision    recall  f1-score   support</span><br/><br/><span class="koboSpan" id="kobo.11.1">          0       1.00      1.00      1.00     17030</span><br/><span class="koboSpan" id="kobo.12.1">          1       0.96      0.73      0.83        33</span><br/><br/><span class="koboSpan" id="kobo.13.1">avg / total       1.00      1.00      1.00     17063</span><br/><br/><span class="koboSpan" id="kobo.14.1">Accuracy : 0.999414</span><br/><span class="koboSpan" id="kobo.15.1">Area under the curve : 0.863607</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The GradientBoostingClassifier report</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The classification report for the </span><kbd><span class="koboSpan" id="kobo.3.1">GradientBoostingClassifier</span></kbd><span class="koboSpan" id="kobo.4.1"> metrics is shown in the following code:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">print('classification report')</span><br/><span class="koboSpan" id="kobo.6.1">print(metrics.classification_report(ytest, y_pred))</span><br/><span class="koboSpan" id="kobo.7.1">print("Accuracy is :")</span><br/><span class="koboSpan" id="kobo.8.1">print(metrics.accuracy_score(ytest, y_pred))</span><br/><span class="koboSpan" id="kobo.9.1">print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))</span><br/><br/><span class="koboSpan" id="kobo.10.1">classification report</span><br/><span class="koboSpan" id="kobo.11.1">             precision    recall  f1-score   support</span><br/><br/><span class="koboSpan" id="kobo.12.1">          0       1.00      1.00      1.00     17030</span><br/><span class="koboSpan" id="kobo.13.1">          1       0.74      0.70      0.72        33</span><br/><br/><span class="koboSpan" id="kobo.14.1">avg / total       1.00      1.00      1.00     17063</span><br/><br/><span class="koboSpan" id="kobo.15.1">Accuracy is : 0.998945085858</span><br/><span class="koboSpan" id="kobo.16.1">Area under the curve : 0.848250</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The XGBClassifier report</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The classification report for the </span><kbd><span class="koboSpan" id="kobo.3.1">XGBClassifier</span></kbd><span class="koboSpan" id="kobo.4.1"> metrics is shown in the following code:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">print('classification report')</span><br/><span class="koboSpan" id="kobo.6.1">print(metrics.classification_report(ytest, y_pred))</span><br/><span class="koboSpan" id="kobo.7.1">print("Accuracy is :")</span><br/><span class="koboSpan" id="kobo.8.1">print(metrics.accuracy_score(ytest, y_pred))</span><br/><span class="koboSpan" id="kobo.9.1">print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))</span><br/><br/><span class="koboSpan" id="kobo.10.1">classification report</span><br/><span class="koboSpan" id="kobo.11.1">             precision    recall  f1-score   support</span><br/><br/><span class="koboSpan" id="kobo.12.1">          0       1.00      1.00      1.00     17030</span><br/><span class="koboSpan" id="kobo.13.1">          1       0.93      0.79      0.85        33</span><br/><br/><span class="koboSpan" id="kobo.14.1">avg / total       1.00      1.00      1.00     17063</span><br/><br/><span class="koboSpan" id="kobo.15.1">Accuracy is : 0.999472542929</span><br/><span class="koboSpan" id="kobo.16.1">Area under the curve : 0.893881</span></pre>
<p><span><span class="koboSpan" id="kobo.17.1">By comparing the AUC and F1 score values, which are calculated using the individual classifiers, </span><kbd><span class="koboSpan" id="kobo.18.1">XGBClassifier</span></kbd><span class="koboSpan" id="kobo.19.1"> remains the most accurate classifier and </span><kbd><span class="koboSpan" id="kobo.20.1">GradientBoostingClassifier</span></kbd><span class="koboSpan" id="kobo.21.1"> is the least accurate of the three.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Improving predictions accuracy with SMOTE</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We conclude our considerations by showing you how the use of a rebalancing technique based on oversampling contributes to improving the accuracy of predictions.</span></p>
<p><span class="koboSpan" id="kobo.3.1">We will use the implementation of the SMOTE oversampling algorithm offered by the imbalanced-learn library, increasing the fraud samples from 102 to 500 and reusing </span><kbd><span class="koboSpan" id="kobo.4.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.5.1"> on resampled data, as shown in the following example:</span></p>
<pre><span class="koboSpan" id="kobo.6.1">from collections import Counter</span><br/><span class="koboSpan" id="kobo.7.1">from imblearn.over_sampling import SMOTE </span><br/><br/><span class="koboSpan" id="kobo.8.1">x = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',</span><br/><span class="koboSpan" id="kobo.9.1">       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',</span><br/><span class="koboSpan" id="kobo.10.1">       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]</span><br/><br/><span class="koboSpan" id="kobo.11.1">y = df['Class']</span><br/><br/><span class="koboSpan" id="kobo.12.1"># Increase the fraud samples from 102 to 500</span><br/><br/><span class="koboSpan" id="kobo.13.1">sm = SMOTE(random_state=42,ratio={1:500})</span><br/><span class="koboSpan" id="kobo.14.1">X_res, y_res = sm.fit_sample(x, y)</span><br/><span class="koboSpan" id="kobo.15.1">print('Resampled dataset shape {}'.format(Counter(y_res)))</span><br/><br/><span class="koboSpan" id="kobo.16.1">Resampled dataset shape Counter({0: 56772, 1: 500})</span><br/><br/><span class="koboSpan" id="kobo.17.1"># Split the resampled data into train &amp; test data with 70:30 mix</span><br/><br/><span class="koboSpan" id="kobo.18.1">xtrain, xtest, ytrain, ytest = train_test_split(X_res, y_res, test_size=0.30, random_state=0)</span><br/><br/><span class="koboSpan" id="kobo.19.1"># Random Forest Classifier on resampled data</span><br/><br/><span class="koboSpan" id="kobo.20.1">from sklearn.ensemble import RandomForestClassifier</span><br/><span class="koboSpan" id="kobo.21.1">from sklearn import metrics</span><br/><br/><span class="koboSpan" id="kobo.22.1">rfmodel = RandomForestClassifier()</span><br/><span class="koboSpan" id="kobo.23.1">rfmodel.fit(xtrain,ytrain)</span><br/><br/><span class="koboSpan" id="kobo.24.1">ypredrf = rfmodel.predict(xtest)</span><br/><br/><span class="koboSpan" id="kobo.25.1">print('classification report')</span><br/><span class="koboSpan" id="kobo.26.1">print(metrics.classification_report(ytest, ypredrf))</span><br/><span class="koboSpan" id="kobo.27.1">print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))</span><br/><span class="koboSpan" id="kobo.28.1">print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))</span><br/><br/><span class="koboSpan" id="kobo.29.1">classification report</span><br/><span class="koboSpan" id="kobo.30.1">             precision    recall  f1-score   support</span><br/><br/><span class="koboSpan" id="kobo.31.1">          0       1.00      1.00      1.00     17023</span><br/><span class="koboSpan" id="kobo.32.1">          1       0.97      0.91      0.94       159</span><br/><br/><span class="koboSpan" id="kobo.33.1">avg / total       1.00      1.00      1.00     17182</span><br/><br/><span class="koboSpan" id="kobo.34.1">Accuracy : 0.998952</span><br/><span class="koboSpan" id="kobo.35.1">Area under the curve : 0.955857</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span><span class="koboSpan" id="kobo.36.1">We can see an increase in both the F1 score and the AUC due to the application of a synthetic oversampling technique.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have learned how to develop a predictive model for credit card fraud detection, exploiting the IBM Cloud platform with IBM Watson Studio.</span></p>
<p><span class="koboSpan" id="kobo.3.1">By leveraging the IBM Cloud platform, we have also learned how to address the issues related to the presence of unbalanced and non-stationary data within the dataset concerning credit card transactions and made full use of ensemble learning and data sampling techniques.</span></p>
<p><span class="koboSpan" id="kobo.4.1">In the next chapter, we will delve deep into </span><strong><span class="koboSpan" id="kobo.5.1">generative adversarial networks</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong><span class="koboSpan" id="kobo.7.1">GANs</span></strong><span class="koboSpan" id="kobo.8.1">).</span></p>


            </article>

            
        </section>
    </body></html>