- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '20'
- en: Advanced Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级卷积神经网络
- en: 'In this chapter, we will see some more advanced uses for **Convolutional Neural
    Networks** (**CNNs**). We will explore:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将看到 CNN 的一些更高级的应用。我们将探索：
- en: How CNNs can be applied within the areas of computer vision, video, textual
    documents, audio, and music
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 如何应用于计算机视觉、视频、文本文件、音频和音乐等领域
- en: How to use CNNs for text processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 CNN 进行文本处理
- en: What capsule networks are
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胶囊网络是什么
- en: Computer vision
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp20](https://packt.link/dltfchp20).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 [https://packt.link/dltfchp20](https://packt.link/dltfchp20) 找到。
- en: Let’s start by using CNNs for complex tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用 CNN 进行复杂任务开始。
- en: Composing CNNs for complex tasks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合 CNN 进行复杂任务
- en: We have discussed CNNs quite extensively in *Chapter 3*, *Convolutional Neural
    Networks*, and at this point, you are probably convinced about the effectiveness
    of the CNN architecture for image classification tasks. What you may find surprising,
    however, is that the basic CNN architecture can be composed and extended in various
    ways to solve a variety of more complex tasks. In this section, we will look at
    the computer vision tasks mentioned in *Figure 20.1* and show how they can be
    solved by turning CNNs into larger and more complex architectures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第 3 章*《卷积神经网络》中已经详细讨论了 CNN，现在你可能已经深信 CNN 架构对于图像分类任务的有效性。然而，你可能会惊讶地发现，基本的
    CNN 架构可以通过组合和扩展的方式，解决各种更复杂的任务。在本节中，我们将查看*图 20.1*中提到的计算机视觉任务，并展示如何通过将 CNN 转变为更大、更复杂的架构来解决它们。
- en: '![](img/B18331_20_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_01.png)'
- en: 'Figure 20.1: Different Computer Vision Tasks – source: Introduction to Artificial
    Intelligence and Computer Vision Revolution (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-and-computer-vision-revolution)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.1：不同的计算机视觉任务 – 来源：人工智能与计算机视觉革命介绍 (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-and-computer-vision-revolution)
- en: Classification and localization
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类与定位
- en: In the classification and localization task, not only do you have to report
    the class of object found in the image, but also the coordinates of the bounding
    box where the object appears in the image. This type of task assumes that there
    is only one instance of the object in an image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类和定位任务中，除了需要报告图像中物体的类别，还需要给出物体在图像中出现的边界框坐标。这类任务假设图像中只有一个物体实例。
- en: This can be achieved by attaching a “regression head” in addition to the “classification
    head” in a typical classification network. Recall that in a classification network,
    the final output of convolution and pooling operations, called the feature map,
    is fed into a fully connected network that produces a vector of class probabilities.
    This fully connected network is called the classification head, and it is tuned
    using a categorical loss function (*L*[c]) such as categorical cross-entropy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过在典型的分类网络中，除了“分类头”外再附加一个“回归头”来实现。回想一下，在一个分类网络中，卷积和池化操作的最终输出，称为特征图，将被送入一个全连接网络，生成一个类别概率向量。这个全连接网络被称为分类头，并通过使用分类损失函数
    (*L*[c])，如类别交叉熵，进行调优。
- en: Similarly, a regression head is another fully connected network that takes the
    feature map and produces a vector (*x*, *y*, *w*, *h*) representing the top left
    *x* and *y* coordinates, and the width and height of the bounding box. It is tuned
    using a continuous loss function (*L*[R]) such as mean squared error. The entire
    network is tuned using a linear combination of the two losses, i.e.,
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，回归头是另一个全连接网络，它接收特征图并生成一个向量 (*x*, *y*, *w*, *h*)，表示左上角的 *x* 和 *y* 坐标，以及边界框的宽度和高度。它通过使用连续损失函数
    (*L*[R])，如均方误差，进行调优。整个网络通过这两种损失的线性组合进行调优，即：
- en: '![](img/B18331_20_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_001.png)'
- en: Here, ![](img/B18331_11_021.png) is a hyperparameter and can take a value between
    0 and 1\. Unless the value is determined by some domain knowledge about the problem,
    it can be set to 0.5.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B18331_11_021.png) 是一个超参数，可以取值在 0 到 1 之间。除非该值由一些关于问题的领域知识决定，否则可以设置为
    0.5。
- en: '*Figure 20.2* shows a typical classification and localization network architecture:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20.2* 显示了一个典型的分类与定位网络架构：'
- en: '![](img/B18331_20_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_02.png)'
- en: 'Figure 20.2: Network architecture for image classification and localization'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2：图像分类与定位的网络架构
- en: As you can see, the only difference with respect to a typical CNN classification
    network is the additional regression head at the top right-hand side.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，与典型的 CNN 分类网络的唯一区别是右上角额外的回归头。
- en: Semantic segmentation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分割
- en: Another class of problem that builds on the basic classification idea is “semantic
    segmentation.” Here the aim is to classify every single pixel on the image as
    belonging to a single class.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于基本分类思想的另一类问题是“语义分割”。其目标是将图像中的每一个像素分类为属于某一类。
- en: An initial method of implementation could be to build a classifier network for
    each pixel, where the input is a small neighborhood around each pixel. In practice,
    this approach is not very performant, so an improvement over this implementation
    might be to run the image through convolutions that will increase the feature
    depth, while keeping the image width and height constant. Each pixel then has
    a feature map that can be sent through a fully connected network that predicts
    the class of the pixel. However, in practice, this is also quite expensive and
    is not normally used.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种初步的实现方法可能是为每个像素构建一个分类网络，其中输入是每个像素周围的小邻域。实际上，这种方法的性能并不好，因此对该实现的改进可能是通过卷积操作处理图像，增加特征深度，同时保持图像的宽度和高度不变。每个像素会有一个特征图，可以通过一个全连接网络来预测该像素的类别。然而，实际上，这也相当昂贵，通常不被使用。
- en: A third approach is to use a CNN encoder-decoder network, where the encoder
    decreases the width and height of the image but increases its depth (number of
    features), while the decoder uses transposed convolution operations to increase
    its size and decrease its depth. Transposed convolution (or upsampling) is the
    process of going in the opposite direction of a normal convolution. Input to this
    network is the image and the output is the segmentation map. A popular implementation
    of this encoder-decoder architecture is the U-Net (a good implementation is available
    at [https://github.com/jakeret/tf_unet](https://github.com/jakeret/tf_unet)),
    originally developed for biomedical image segmentation, which has additional skip
    connections between corresponding layers of the encoder and decoder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是使用 CNN 编码器-解码器网络，其中编码器减小图像的宽度和高度，但增加其深度（特征数量），而解码器使用反卷积操作增加图像的尺寸并减少其深度。反卷积（或上采样）是进行与普通卷积相反的过程。该网络的输入是图像，输出是分割图。该编码器-解码器架构的一个流行实现是
    U-Net（一个很好的实现可以在[https://github.com/jakeret/tf_unet](https://github.com/jakeret/tf_unet)找到），最初为生物医学图像分割开发，具有编码器和解码器之间额外的跳跃连接。
- en: '*Figure 20.3* shows the U-Net architecture:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20.3* 展示了 U-Net 架构：'
- en: '![Chart  Description automatically generated](img/B18331_20_03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Chart  Description automatically generated](img/B18331_20_03.png)'
- en: 'Figure 20.3: U-Net architecture'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.3：U-Net 架构
- en: Object detection
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测
- en: The object detection task is similar to the classification and localization
    task. The big difference is that now there are multiple objects in the image,
    and for each one of them, we need to find the class and the bounding box coordinates.
    In addition, neither the number of objects nor their size is known in advance.
    As you can imagine, this is a difficult problem, and a fair amount of research
    has gone into it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测任务与分类和定位任务类似。最大的区别在于现在图像中有多个对象，并且对于每个对象，我们需要找到其类别和边界框坐标。此外，事先无法知道对象的数量或大小。正如你所想，这个问题非常复杂，且已经有大量的研究投入其中。
- en: A first approach to the problem might be to create many random crops of the
    input image and, for each crop, apply the classification and localization network
    we described earlier. However, such an approach is very wasteful in terms of computing
    and unlikely to be very successful.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的第一种方法可能是创建许多输入图像的随机裁剪，对于每个裁剪，应用我们之前描述的分类和定位网络。然而，这种方法在计算上非常浪费，且不太可能非常成功。
- en: 'A more practical approach would be to use a tool such as Selective Search (*Selective
    Search for Object Recognition*, by Uijlings et al., [http://www.huppelen.nl/publications/selectiveSearchDraft.pdf](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)),
    which uses traditional computer vision techniques to find areas in the image that
    might contain objects. These regions are called “region proposals,” and the network
    to detect them is called **Region-based CNN**, or **R-CNN**. In the original R-CNN,
    the regions were resized and fed into a network to yield image vectors. These
    vectors were then classified with an SVM-based classifier (see [https://en.wikipedia.org/wiki/Support-vector_machine](https://en.wikipedia.org/wiki/Support-vector_machine)),
    and the bounding boxes proposed by the external tool were corrected using a linear
    regression network over the image vectors. An R-CNN network can be represented
    conceptually as shown in *Figure 20.4*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更实用的方法是使用如选择性搜索（*Selective Search for Object Recognition*，由 Uijlings 等人编写，[http://www.huppelen.nl/publications/selectiveSearchDraft.pdf](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)）等工具，利用传统的计算机视觉技术来查找图像中可能包含物体的区域。这些区域被称为“区域建议”，用于检测这些区域的网络称为**基于区域的
    CNN**，或称**R-CNN**。在原始的 R-CNN 中，区域被调整大小后输入网络，产生图像向量。然后，使用基于 SVM 的分类器对这些向量进行分类（见[https://en.wikipedia.org/wiki/Support-vector_machine](https://en.wikipedia.org/wiki/Support-vector_machine)），外部工具提出的边界框通过图像向量上的线性回归网络进行修正。R-CNN
    网络的概念表示如 *图 20.4* 所示：
- en: '![Chart, diagram  Description automatically generated](img/B18331_20_04.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图表，描述自动生成](img/B18331_20_04.png)'
- en: 'Figure 20.4: R-CNN network'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.4：R-CNN 网络
- en: The next iteration of the R-CNN network is called the Fast R-CNN. The Fast R-CNN
    still gets its region proposals from an external tool, but instead of feeding
    each region proposal through the CNN, the entire image is fed through the CNN
    and the region proposals are projected onto the resulting feature map. Each region
    of interest is fed through a **Region Of Interest** (**ROI**) pooling layer and
    then to a fully connected network, which produces a feature vector for the ROI.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 网络的下一个迭代版本叫做 Fast R-CNN。Fast R-CNN 仍然从外部工具获取区域建议，但不再将每个区域建议分别输入 CNN，而是将整个图像输入
    CNN，并将区域建议投影到生成的特征图上。每个感兴趣的区域会通过**感兴趣区域**（**ROI**）池化层，然后传递到一个全连接网络，生成该 ROI 的特征向量。
- en: 'ROI pooling is a widely used operation in object detection tasks using CNNs.
    The ROI pooling layer uses max pooling to convert the features inside any valid
    region of interest into a small feature map with a fixed spatial extent of *H*
    x *W* (where *H* and *W* are two hyperparameters). The feature vector is then
    fed into two fully connected networks, one to predict the class of the ROI and
    the other to correct the bounding box coordinates for the proposal. This is illustrated
    in *Figure 20.5*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ROI 池化是使用 CNN 进行物体检测任务时广泛应用的一种操作。ROI 池化层使用最大池化将任何有效感兴趣区域内的特征转换为一个具有固定空间大小 *H*
    x *W*（其中 *H* 和 *W* 是两个超参数）的较小特征图。然后，将该特征向量输入两个全连接网络，一个用于预测 ROI 的类别，另一个用于修正区域建议的边界框坐标。如
    *图 20.5* 所示：
- en: '![Diagram  Description automatically generated](img/B18331_20_05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图表，描述自动生成](img/B18331_20_05.png)'
- en: 'Figure 20.5: Fast R-CNN network architecture'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.5：Fast R-CNN 网络架构
- en: The Fast R-CNN is about 25x faster than the R-CNN. The next improvement, called
    the Faster R-CNN (an implementation is at [https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)),
    removes the external region proposal mechanism and replaces it with a trainable
    component, called the **Region Proposal Network** (**RPN**), within the network
    itself. The output of this network is combined with the feature map and passed
    in through a similar pipeline to the Fast R-CNN network, as shown in *Figure 20.6*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN 比 R-CNN 快约 25 倍。下一步的改进，称为 Faster R-CNN（实现代码可见 [https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)），移除了外部区域建议机制，并用一个可训练组件
    —— **区域建议网络**（**RPN**） —— 替代，嵌入到网络本身。该网络的输出与特征图结合，并通过与 Fast R-CNN 网络类似的管道传递，如
    *图 20.6* 所示。
- en: 'The Faster R-CNN network is about 10x faster than the Fast R-CNN network, making
    it approximately 250x faster than an R-CNN network:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 网络的速度是 Fast R-CNN 网络的约 10 倍，使其比 R-CNN 网络快约 250 倍：
- en: '![Diagram  Description automatically generated](img/B18331_20_06.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图表，描述自动生成](img/B18331_20_06.png)'
- en: 'Figure 20.6: Faster R-CNN network architecture'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.6：Faster R-CNN 网络架构
- en: Another somewhat different class of object detection networks are **Single Shot
    Detectors** (**SSD**) such as **YOLO** (**You Only Look Once**). In these cases,
    each image is split into a predefined number of parts using a grid. In the case
    of YOLO, a 7 x 7 grid is used, resulting in 49 sub-images. A predetermined set
    of crops with different aspect ratios are applied to each sub-image. Given *B*
    bounding boxes and *C* object classes, the output for each image is a vector of
    size ![](img/B18331_20_003.png). Each bounding box has a confidence and coordinates
    (*x*, *y*, *w*, *h*), and each grid has prediction probabilities for the different
    objects detected within them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类稍微不同的物体检测网络是 **单次检测器**（**SSD**），例如 **YOLO**（**You Only Look Once**）。在这些情况下，每张图片都会被使用网格分割成预定义数量的部分。对于
    YOLO 来说，使用的是一个 7 x 7 的网格，结果是 49 个子图像。每个子图像都会应用一组预定的不同纵横比的裁剪。给定 *B* 个边界框和 *C* 个物体类别，每张图片的输出是一个大小为
    ![](img/B18331_20_003.png) 的向量。每个边界框都有一个置信度和坐标（*x*，*y*，*w*，*h*），每个网格会有一个预测概率，表示在其中检测到的不同物体。
- en: The YOLO network is a CNN, which does this transformation. The final predictions
    and bounding boxes are found by aggregating the findings from this vector. In
    YOLO, a single convolutional network predicts the bounding boxes and the related
    class probabilities. YOLO is the faster solution for object detection. An implementation
    is at [https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow](https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 网络是一个 CNN，它执行这种转换。最终的预测和边界框通过聚合此向量中的结果来获得。在 YOLO 中，单个卷积网络预测边界框及相关类别概率。YOLO
    是物体检测的更快解决方案。实现可以在 [https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow](https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow)
    找到。
- en: Instance segmentation
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例分割
- en: Instance segmentation is similar to semantic segmentation – the process of associating
    each pixel of an image with a class label – with a few important distinctions.
    First, it needs to distinguish between different instances of the same class in
    an image. Second, it is not required to label every single pixel in the image.
    In some respects, instance segmentation is also similar to object detection, except
    that instead of bounding boxes, we want to find a binary mask that covers each
    object.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割与语义分割相似——即将图像的每个像素与一个类别标签关联——但有一些重要的区别。首先，它需要区分图像中同一类别的不同实例。其次，它不要求标记图像中的每一个像素。在某些方面，实例分割也类似于物体检测，不同之处在于我们不使用边界框，而是需要找到覆盖每个物体的二进制掩码。
- en: 'The second definition leads to the intuition behind the Mask R-CNN network.
    The Mask R-CNN is a Faster R-CNN with an additional CNN in front of its regression
    head, which takes as input the bounding box coordinates reported for each ROI
    and converts it to a binary mask [11]:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个定义揭示了 Mask R-CNN 网络背后的直觉。Mask R-CNN 是一个带有额外 CNN 的 Faster R-CNN，该 CNN 位于回归头部之前，输入为为每个
    ROI 报告的边界框坐标，并将其转换为二进制掩码 [11]：
- en: '![Diagram  Description automatically generated](img/B18331_20_07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_20_07.png)'
- en: 'Figure 20.7: Mask R-CNN architecture'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.7：Mask R-CNN 架构
- en: In April 2019, Google released Mask R-CNN in open source, pretrained with TPUs.
    This is available at
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2019 年 4 月，Google 开源发布了 Mask R-CNN，并且用 TPUs 进行了预训练。你可以在以下链接找到该模型：
- en: '[https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb)。'
- en: 'I suggest playing with the Colab notebook to see what the results are. In *Figure
    20.8*, we see an example of image segmentation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你尝试一下 Colab 笔记本，看看结果如何。在 *图 20.8* 中，我们看到了一个图像分割的示例：
- en: '![](img/B18331_20_08.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_08.png)'
- en: 'Figure 20.8: An example of image segmentation'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.8：图像分割的一个示例
- en: Google also released another model trained on TPUs called DeepLab, and you can
    see an image (*Figure 20.9*) from the demo. This is available at
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Google 还发布了另一个基于 TPUs 训练的模型，名为 DeepLab，你可以从演示中看到一张图片（*图 20.9*）。这个模型可以在以下链接找到：
- en: '[https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr](https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr](https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr)：'
- en: '![](img/B18331_20_09.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_09.png)'
- en: 'Figure 20.9: An example of image segmentation'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.9：图像分割的示例
- en: In this section, we have covered, at a somewhat high level, various network
    architectures that are popular in computer vision. Note that all of them are composed
    by the same basic CNN and fully connected architectures. This composability is
    one of the most powerful features of deep learning. Hopefully, this has given
    you some ideas for networks that could be adapted for your own computer vision
    use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们大致介绍了几种在计算机视觉领域流行的网络架构。请注意，所有这些架构都由相同的基本 CNN 和全连接架构组成。这种可组合性是深度学习最强大的特性之一。希望这能给你一些启示，帮助你设计适合自己计算机视觉应用的网络。
- en: Application zoos with tf.Keras and TensorFlow Hub
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 tf.Keras 和 TensorFlow Hub 的应用程序
- en: One of the nice things about transfer learning is that it is possible to reuse
    pretrained networks to save time and resources. There are many collections of
    ready-to-use networks out there, but the following two are the most used.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习的一个好处是可以重用预训练网络，从而节省时间和资源。市面上有许多现成的网络集合，但以下两个是最常用的。
- en: Keras Applications
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 应用程序
- en: Keras Applications (Keras Applications are available at [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications))
    includes models for image classification with weights trained on ImageNet (Xception,
    VGG16, VGG19, ResNet, ResNetV2, ResNeXt, InceptionV3, InceptionResNetV2, MobileNet,
    MobileNetV2, DenseNet, and NASNet). In addition, there are a few other reference
    implementations from the community for object detection and segmentation, sequence
    learning, reinforcement learning (see *Chapter 11*), and GANs (see *Chapter 9*).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 应用程序（Keras 应用程序可以在 [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)
    找到）包括了用于图像分类的模型，这些模型在 ImageNet 上训练过（Xception、VGG16、VGG19、ResNet、ResNetV2、ResNeXt、InceptionV3、InceptionResNetV2、MobileNet、MobileNetV2、DenseNet
    和 NASNet）。此外，还有一些来自社区的其他参考实现，涉及目标检测和分割、序列学习、强化学习（见 *第 11 章*）以及 GANs（见 *第 9 章*）。
- en: TensorFlow Hub
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Hub
- en: TensorFlow Hub (available at [https://www.tensorflow.org/hub](https://www.tensorflow.org/hub))
    is an alternative collection of pretrained models. TensorFlow Hub includes modules
    for text classification, sentence encoding (see *Chapter 4*), image classification,
    feature extraction, image generation with GANs, and video classification. Currently,
    both Google and DeepMind contribute to TensorFlow Hub.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub（可访问 [https://www.tensorflow.org/hub](https://www.tensorflow.org/hub)）是一个预训练模型的替代集合。TensorFlow
    Hub 包含了文本分类、句子编码（见 *第 4 章*）、图像分类、特征提取、使用 GAN 生成图像以及视频分类的模块。目前，Google 和 DeepMind
    都在为 TensorFlow Hub 做贡献。
- en: 'Let’s look at an example of using `TF.Hub`. In this case, we have a simple
    image classifier using MobileNetv2:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用 `TF.Hub` 的示例。在这个例子中，我们有一个使用 MobileNetv2 的简单图像分类器：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pretty simple indeed. Just remember to use `hub.KerasLayer()` for wrapping any
    Hub layer. In this section, we have discussed how to use TensorFlow Hub.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 确实很简单。只需记得使用 `hub.KerasLayer()` 来包装任何 Hub 层。在本节中，我们讨论了如何使用 TensorFlow Hub。
- en: Next, we will focus on other CNN architectures.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点介绍其他 CNN 架构。
- en: Answering questions about images (visual Q&A)
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回答关于图像的问题（视觉问答）
- en: One of the nice things about neural networks is that different media types can
    be combined together to provide a unified interpretation. For instance, **Visual
    Question Answering** (**VQA**) combines image recognition and text natural language
    processing. Training can use VQA (VQA is available at [https://visualqa.org/](https://visualqa.org/)),
    a dataset containing open-ended questions about images. These questions require
    an understanding of vision, language, and common knowledge to be answered. The
    following images are taken from a demo available at [https://visualqa.org/](https://visualqa.org/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个优点是可以将不同类型的媒体结合在一起，以提供统一的解释。例如，**视觉问答**（**VQA**）结合了图像识别和文本自然语言处理。训练可以使用VQA（VQA
    数据集可以在[https://visualqa.org/](https://visualqa.org/)获取），它包含有关图像的开放式问题。这些问题需要理解视觉、语言和常识才能回答。以下图像来自于[https://visualqa.org/](https://visualqa.org/)上的一个演示。
- en: 'Note the question at the top of the image, and the subsequent answers:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意图像顶部的问题，以及随后的答案：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_20_10.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18331_20_10.png)'
- en: 'Figure 20.10: Examples of visual question and answers'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.10：视觉问答示例
- en: 'If you want to start playing with VQA, the first thing is to get appropriate
    training datasets such as the VQA dataset, the CLEVR dataset (available at [https://cs.stanford.edu/people/jcjohns/clevr/](https://cs.stanford.edu/people/jcjohns/clevr/)),
    or the FigureQA dataset (available at [https://datasets.maluuba.com/FigureQA](https://datasets.maluuba.com/FigureQA));
    alternatively, you can participate in a Kaggle VQA challenge (available at [https://www.kaggle.com/c/visual-question-answering](https://www.kaggle.com/c/visual-question-answering)).
    Then you can build a model that is the combination of a CNN and an RNN and start
    experimenting. For instance, a CNN can be something like this code fragment, which
    takes an image with three channels (224 x 224) as input and produces a feature
    vector for the image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想开始玩VQA，首先需要获取适当的训练数据集，如VQA数据集、CLEVR数据集（可在[https://cs.stanford.edu/people/jcjohns/clevr/](https://cs.stanford.edu/people/jcjohns/clevr/)获取）或FigureQA数据集（可在[https://datasets.maluuba.com/FigureQA](https://datasets.maluuba.com/FigureQA)获取）；或者，你可以参与Kaggle的VQA挑战（可在[https://www.kaggle.com/c/visual-question-answering](https://www.kaggle.com/c/visual-question-answering)参与）。然后，你可以构建一个结合CNN和RNN的模型并开始实验。例如，CNN可以是这样的代码片段，它接受一个具有三个通道（224
    x 224）的图像作为输入，并为图像生成一个特征向量：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Text can be encoded with an RNN; for now, think of it as a black box taking
    a text fragment (the question) in input and producing a feature vector for the
    text:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以通过RNN进行编码；目前，可以将其视为一个黑盒，它接受一个文本片段（问题）作为输入，并为文本生成一个特征向量：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then the two feature vectors (one for the image, and one for the text) are
    combined into one joint vector, which is provided as input to a dense network
    to produce the combined network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将两个特征向量（一个是图像的，另一个是文本的）合并为一个联合向量，该向量作为输入提供给密集网络，以生成组合网络：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For instance, if we have a set of labeled images, then we can learn what the
    best questions and answers are for describing an image. The number of options
    is enormous! If you want to know more, I suggest that you investigate Maluuba,
    a start-up providing the FigureQA dataset with 100,000 figure images and 1,327,368
    question-answer pairs in the training set. Maluuba has been recently acquired
    by Microsoft, and the lab is advised by Yoshua Bengio, one of the fathers of deep
    learning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一组标记的图像，那么我们可以学习描述图像的最佳问题和答案。选择的数量非常庞大！如果你想了解更多，我建议你调查Maluuba，一家提供FigureQA数据集的初创公司，该数据集包含100,000个图像和1,327,368对问答。Maluuba最近被微软收购，实验室由深度学习的奠基人之一Yoshua
    Bengio担任顾问。
- en: In this section, we have discussed how to implement visual Q&A. The next section
    is about style transfer, a deep learning technique used for training neural networks
    to create art.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何实现视觉问答。下一节将介绍风格迁移，这是一种用于训练神经网络创作艺术的深度学习技术。
- en: Creating a DeepDream network
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个DeepDream网络
- en: Another interesting application of CNNs is DeepDream, a computer vision program
    created by Google [8] that uses a CNN to find and enhance patterns in images.
    The result is a dream-like hallucinogenic effect. Similar to the previous example,
    we are going to use a pretrained network to extract features. However, in this
    case, we want to “enhance” patterns in images, meaning that we need to maximize
    some functions. This tells us that we need to use a gradient ascent and not a
    descent. First, let’s see an example from Google gallery (available at [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb))
    where the classic Seattle landscape is “incepted” with hallucinogenic dreams such
    as birds, cards, and strange flying objects.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的另一个有趣应用是DeepDream，一个由Google [8] 创建的计算机视觉程序，它利用CNN在图像中寻找并增强模式。结果是梦幻般的迷幻效果。与之前的示例类似，我们将使用一个预训练的网络来提取特征。然而，在这种情况下，我们希望“增强”图像中的模式，这意味着我们需要最大化一些函数。这告诉我们需要使用梯度上升，而不是梯度下降。首先，让我们看一个来自Google画廊的示例（可在[https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb)获取），其中经典的西雅图景观被“接纳”了梦幻般的幻觉效果，如鸟类、卡片和奇怪的飞行物体。
- en: 'Google released the DeepDream code as open source (available at [https://github.com/google/deepdream](https://github.com/google/deepdream)),
    but we will use a simplified example made by a random forest (available at [https://www.tensorflow.org/tutorials/generative/deepdream](https://www.tensorflow.org/tutorials/generative/deepdream)):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Google发布了DeepDream的开源代码（可在[https://github.com/google/deepdream](https://github.com/google/deepdream)获取），但我们将使用一个由随机森林生成的简化示例（可在[https://www.tensorflow.org/tutorials/generative/deepdream](https://www.tensorflow.org/tutorials/generative/deepdream)获取）：
- en: '![](img/B18331_20_11.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_11.png)'
- en: 'Figure 20.11: DeepDreaming Seattle'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.11：深度梦境：西雅图
- en: 'Let’s start with some image preprocessing:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些图像预处理开始：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let’s use the Inception pretrained network to extract features. We use
    several layers, and the goal is to maximize their activations. The `tf.keras`
    functional API is our friend here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用预训练的Inception网络来提取特征。我们使用多个层，目标是最大化它们的激活值。`tf.keras`函数式API在这里对我们非常有用：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The loss function is the mean of all the activation layers considered, normalized
    by the number of units in the layer itself:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是所有激活层的平均值，通过该层自身单元的数量进行归一化：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s run the gradient ascent:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行梯度上升：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This transforms the image on the left into the psychedelic image on the right:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把左侧的图像转换成右侧的迷幻图像：
- en: '![](img/B18331_20_12.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_12.png)'
- en: 'Figure 20.12: DeepDreaming of a green field with clouds'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.12：深度梦境：绿地与云彩
- en: Inspecting what a network has learned
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查网络学到了什么
- en: 'A particularly interesting research effort is being devoted to understand what
    neural networks are actually learning in order to be able to recognize images
    so well. This is called neural network “interpretability.” Activation atlases
    is a promising recent technique that aims to show the feature visualizations of
    averaged activation functions. In this way, activation atlases produce a global
    map seen through the eyes of the network. Let’s look at a demo available at [https://distill.pub/2019/activation-atlas/](https://distill.pub/2019/activation-atlas/):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别有趣的研究方向是了解神经网络到底在学习什么，从而能够如此精准地识别图像。这被称为神经网络的“可解释性”。激活图谱是一种有前景的近期技术，旨在展示平均激活函数的特征可视化。通过这种方式，激活图谱生成了通过网络“眼睛”看到的全球地图。让我们来看一个可用的演示：[https://distill.pub/2019/activation-atlas/](https://distill.pub/2019/activation-atlas/)：
- en: '![](img/B18331_20_13.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_13.png)'
- en: 'Figure 20.13: Examples of inspections'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.13：检查示例
- en: In this image, an InceptionV1 network used for vision classification reveals
    many fully realized features, such as electronics, screens, a Polaroid camera,
    buildings, food, animal ears, plants, and watery backgrounds. Note that grid cells
    are labeled with the classification they give the most support for. Grid cells
    are also sized according to the number of activations that are averaged within.
    This representation is very powerful because it allows us to inspect the different
    layers of a network and how the activation functions fire in response to the input.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图片中，使用用于视觉分类的InceptionV1网络展示了许多完全实现的特征，例如电子产品、屏幕、宝丽来相机、建筑物、食物、动物耳朵、植物和水域背景。请注意，网格单元标注了它们给出最多支持的分类。网格单元的大小也根据其中平均激活的次数进行调整。这种表示方法非常强大，因为它允许我们检查网络的不同层以及激活函数如何响应输入进行激活。
- en: In this section, we have seen many techniques to process images with CNNs. Next,
    we’ll move on to video processing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经看到了许多用CNN处理图像的技术。接下来，我们将转向视频处理。
- en: Video
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频
- en: In this section, we are going to discuss how to use CNNs with videos and the
    different techniques that we can use.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何将CNN与视频结合使用，以及我们可以使用的不同技术。
- en: Classifying videos with pretrained nets in six different ways
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用预训练网络以六种不同方式分类视频
- en: 'Classifying videos is an area of active research because of the large amount
    of data needed for processing this type of media. Memory requirements are frequently
    reaching the limits of modern GPUs and a distributed form of training on multiple
    machines might be required. Researchers are currently exploring different directions
    of investigation, with increasing levels of complexity from the first approach
    to the sixth, as described below. Let’s review them:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分类是一个活跃的研究领域，因为处理这种类型的媒体需要大量的数据。内存需求通常会达到现代GPU的极限，可能需要在多台机器上进行分布式训练。目前，研究人员正在探索不同的研究方向，从第一种方法到第六种方法的复杂度逐步增加，具体如下所述。让我们来回顾一下：
- en: The **first approach** consists of classifying one video frame at a time by
    considering each one of them as a separate image processed with a 2D CNN. This
    approach simply reduces the video classification problem to an image classification
    problem. Each video frame “emits” a classification output, and the video is classified
    by taking into account the more frequently chosen category for each frame.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一种方法**是逐帧对视频进行分类，将每一帧视为一个单独的图像，并用2D卷积神经网络（CNN）处理。这种方法简单地将视频分类问题简化为图像分类问题。每一帧视频“发出”一个分类输出，视频的分类通过考虑每一帧最常选择的类别来确定。'
- en: The **second approach** consists of creating one single network where a 2D CNN
    is combined with an RNN (see *Chapter 9*, *Generative Models*). The idea is that
    the CNN will take into account the image components and the RNN will take into
    account the sequence information for each video. This type of network can be very
    difficult to train because of the very high number of parameters to optimize.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二种方法**是创建一个单一的网络，将2D卷积神经网络（CNN）与循环神经网络（RNN）结合起来（参见*第9章*，*生成模型*）。其思想是，CNN将考虑图像的组成部分，而RNN则考虑每个视频的序列信息。这种类型的网络可能非常难以训练，因为它有大量需要优化的参数。'
- en: The **third approach** is to use a 3D ConvNet, where 3D ConvNets are an extension
    of 2D ConvNets operating on a 3D tensor (time, image width, and image height).
    This approach is another natural extension of image classification. Again, 3D
    ConvNets can be hard to train.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三种方法**是使用3D卷积网络（3D ConvNet），其中3D卷积网络是2D卷积网络的扩展，操作于3D张量（时间、图像宽度和图像高度）。这种方法是图像分类的另一种自然扩展。同样，3D卷积网络也可能很难训练。'
- en: 'The **fourth approach** is based on a clever idea: instead of using CNNs directly
    for classification, they can be used for storing offline features for each frame
    in the video. The idea is that feature extraction can be made very efficient with
    transfer learning, as shown in a previous recipe. After all the features are extracted,
    they can be passed as a set of inputs into an RNN, which will learn sequences
    across multiple frames and emit the final classification.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四种方法**基于一个巧妙的想法：不是直接使用CNN进行分类，而是将它们用于存储每一帧视频的离线特征。其思想是，特征提取可以通过迁移学习变得非常高效，如之前的食谱所示。在提取所有特征后，它们可以作为输入集传递给RNN，RNN将学习跨多个帧的序列并输出最终分类。'
- en: The **fifth approach** is a simple variant of the fourth, where the final layer
    is an MLP instead of an RNN. In certain situations, this approach can be simpler
    and less expensive in terms of computational requirements.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第五种方法**是第四种方法的一种简单变体，其中最后一层是MLP，而不是RNN。在某些情况下，这种方法可能更简单，且在计算需求上更低。'
- en: The **sixth approach** is a variant of the fourth, where the phase of feature
    extraction is realized with a 3D CNN that extracts spatial and visual features.
    These features are then passed into either an RNN or an MLP.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第六种方法**是第四种方法的一种变体，其中特征提取阶段是通过一个3D卷积神经网络（CNN）来实现的，该网络提取空间和视觉特征。这些特征随后传递给一个RNN或MLP。'
- en: Deciding upon the best approach is strictly dependent on your specific application,
    and there is no definitive answer. The first three approaches are generally more
    computationally expensive and less clever, while the last three approaches are
    less expensive, and they frequently achieve better performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 确定最佳方法完全取决于你的具体应用，并没有明确的答案。前三种方法通常计算开销较大且较为笨重，而后三种方法则开销较小，且经常能够取得更好的性能。
- en: So far, we have explored how CNNs can be used for image and video applications.
    In the next section, we will apply these ideas within a text-based context.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了CNN如何用于图像和视频应用。接下来的部分，我们将把这些思想应用于基于文本的上下文中。
- en: Text documents
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本文档
- en: What do text and images have in common? At first glance, very little. However,
    if we represent a sentence or a document as a matrix, then this matrix is not
    much different from an image matrix where each cell is a pixel. So, the next question
    is, how can we represent a piece of text as a matrix?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和图像有什么共同点？乍一看，似乎没有什么共同点。然而，如果我们将一个句子或文档表示为一个矩阵，那么这个矩阵与图像矩阵没有太大区别，因为每个单元格就像图像中的一个像素。那么，下一个问题是，我们如何将一段文本表示为一个矩阵呢？
- en: 'Well, it is pretty simple: each row of a matrix is a vector that represents
    a basic unit for the text. Of course, now we need to define what a basic unit
    is. A simple choice could be to say that the basic unit is a character. Another
    choice would be to say that a basic unit is a word; yet another choice is to aggregate
    similar words together and then denote each aggregation (sometimes called cluster
    or embedding) with a representative symbol.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其实很简单：矩阵的每一行是一个表示文本基本单元的向量。当然，现在我们需要定义什么是基本单元。一个简单的选择是将基本单元定义为一个字符。另一个选择是将基本单元定义为一个单词；还有一种选择是将相似的单词聚合在一起，然后用一个代表性的符号来表示每个聚合（有时称为簇或嵌入）。
- en: Note that regardless of the specific choice adopted for our basic units, we
    need to have a 1:1 mapping from basic units into integer IDs so that the text
    can be seen as a matrix. For instance, if we have a document with 10 lines of
    text and each line is a 100-dimensional embedding, then we will represent our
    text with a matrix of 10 x 100\. In this very particular “image,” a “pixel” is
    turned on if that sentence, *X*, contains the embedding, represented by position
    *Y*. You might also notice that a text is not really a matrix but more a vector
    because two words located in adjacent rows of text have very little in common.
    Indeed, this is a major difference when compared with images, where two pixels
    located in adjacent columns are likely to have some degree of correlation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无论我们为基本单元选择什么，必须保证从基本单元到整数ID的1:1映射，以便将文本视为矩阵。例如，如果我们有一个包含10行文本的文档，每行是一个100维的嵌入，那么我们将用一个10
    x 100的矩阵来表示文本。在这个非常特殊的“图像”中，只有当某个句子*X*包含位置*Y*表示的嵌入时，那个“像素”才会被点亮。你可能还会注意到，文本并不是真正的矩阵，更像是一个向量，因为位于相邻行的两个单词几乎没有什么关联。实际上，这与图像有很大的区别，因为图像中位于相邻列的两个像素可能会有某种程度的相关性。
- en: 'Now you might wonder: *I understand that we represent the text as a vector
    but, in doing so, we lose the position of the words. This position should be important,
    shouldn’t it?* Well, it turns out that in many real applications, knowing whether
    a sentence contains a particular basic unit (a char, a word, or an aggregate)
    or not is pretty useful information even if we don’t keep track of where exactly
    in the sentence this basic unit is located.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会想：*我理解我们将文本表示为一个向量，但这样做的话，我们失去了单词的位置。这个位置应该很重要，不是吗？* 结果证明，在许多实际应用中，知道一个句子是否包含某个特定的基本单元（字符、单词或聚合）是非常有用的信息，即使我们没有追踪这个基本单元在句子中的确切位置。
- en: For instance, CNNs achieve pretty good results for **sentiment analysis**, where
    we need to understand if a piece of text has a positive or a negative sentiment;
    for **spam detection**, where we need to understand if a piece of text is useful
    information or spam; and for **topic categorization**, where we need to understand
    what a piece of text is all about. However, CNNs are not well suited for a **Part
    of Speech** (**POS**) analysis, where the goal is to understand what the logical
    role of every single word is (for example, a verb, an adverb, a subject, and so
    on). CNNs are also not well suited for **entity extraction**, where we need to
    understand where relevant entities are located in sentences.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it turns out that a position is pretty useful information for the last
    two use cases. 1D ConvNets are very similar to 2D ConvNets. However, the former
    operates on a single vector, while the latter operates on matrices.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Using a CNN for sentiment analysis
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s have a look at the code. First of all, we load the dataset with `tensorflow_datasets`.
    In this case we use IMDB, a collection of movie reviews:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we build a suitable CNN model. We use embeddings (see *Chapter 4*, *Word
    Embeddings*) to map the sparse vocabulary typically observed in documents into
    a dense feature space of dimensions `dim_embedding`. Then we use `Conv1D`, followed
    by a `GlobalMaxPooling1D` for averaging, and two `Dense` layers – the last one
    has only one neuron firing binary choices (positive or negative reviews):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model has more than 2,700,000 parameters, and it is summarized as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we compile and fit the model with the Adam optimizer and binary cross-entropy
    loss:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The final accuracy is 88.21%, showing that it is possible to successfully use
    CNNs for textual processing:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that many other non-image applications can also be converted to an image
    and classified using CNNs (see, for instance, [https://becominghuman.ai/sound-classification-using-images-68d4770df426](https://becominghuman.ai/sound-classification-using-images-68d4770df426)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Audio and music
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used CNNs for images, videos, and texts. Now let’s have a look at how
    variants of CNNs can be used for audio.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: So, you might wonder why learning to synthesize audio is so difficult. Well,
    each digital sound we hear is based on 16,000 samples per second (sometimes 48K
    or more), and building a predictive model where we learn to reproduce a sample
    based on all the previous ones is a very difficult challenge.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Dilated ConvNets, WaveNet, and NSynth
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WaveNet is a deep generative model for producing raw audio waveforms. This breakthrough
    technology was introduced (available at [https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/](https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/))
    by Google DeepMind for teaching computers how to speak. The results are truly
    impressive, and online you can find examples of synthetic voices where the computer
    learns how to talk with the voice of celebrities such as Matt Damon. There are
    experiments showing that WaveNet improved the current state-of-the-art **Text-to-Speech**
    (**TTS**) systems, reducing the difference with respect to human voices by 50%
    for both US English and Mandarin Chinese. The metric used for comparison is called
    **Mean Opinion Score** (**MOS**), a subjective paired comparison test. In the
    MOS tests, after listening to each sound stimulus, the subjects were asked to
    rate the naturalness of the stimulus on a five-point scale from “Bad” (1) to “Excellent”
    (5).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: What is even cooler is that DeepMind demonstrated that WaveNet can be also used
    to teach computers how to generate the sound of musical instruments such as piano
    music.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Now some definitions. TTS systems are typically divided into two different
    classes: concatenative and parametric.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Concatenative TTS is where single speech voice fragments are first memorized
    and then recombined when the voice has to be reproduced. However, this approach
    does not scale because it is possible to reproduce only the memorized voice fragments,
    and it is not possible to reproduce new speakers or different types of audio without
    memorizing the fragments from the beginning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Parametric TTS is where a model is created to store all the characteristic features
    of the audio to be synthesized. Before WaveNet, the audio generated with parametric
    TTS was less natural than concatenative TTS. WaveNet enabled significant improvement
    by modeling directly the production of audio sounds, instead of using intermediate
    signal processing algorithms as in the past.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In principle, WaveNet can be seen as a stack of 1D convolutional layers with
    a constant stride of one and with no pooling layers. Note that the input and the
    output have by construction the same dimension, so ConvNets are well suited to
    modeling sequential data such as audio sounds. However, it has been shown that
    in order to reach a large size for the receptive field in the output neuron, it
    is necessary to either use a massive number of large filters or increase the network
    depth prohibitively. Remember that the receptive field of a neuron in a layer
    is the cross-section of the previous layer from which neurons provide inputs.
    For this reason, pure ConvNets are not so effective in learning how to synthesize
    audio.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'The key intuition behind WaveNet is the so-called **Dilated Causal Convolutions**
    [5] (sometimes called **atrous convolution**), which simply means that some input
    values are skipped when the filter of a convolutional layer is applied. “Atrous”
    is a “bastardization” of the French expression “à trous,” meaning “with holes.”
    So an atrous convolution is a convolution with holes. As an example, in one dimension,
    a filter *w* of size 3 with a dilation of 1 would compute the following sum: *w*[0]
    *x*[0] + *w*[1] *x*[2] + *w*[3] *x*[4].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, in D-dilated convolution, usually the stride is 1, but nothing prevents
    you from using other strides. An example is given in *Figure 20.14* with increased
    dilatation (hole) sizes = 0, 1, 2:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_20_14.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.14: Dilatation with increased sizes'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to this simple idea of introducing *holes*, it is possible to stack multiple
    dilated convolutional layers with exponentially increasing filters and learn long-range
    input dependencies without having an excessively deep network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: A WaveNet is therefore a ConvNet where the convolutional layers have various
    dilation factors, allowing the receptive field to grow exponentially with depth
    and therefore efficiently cover thousands of audio timesteps.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: When we train, the inputs are sounds recorded from human speakers. The waveforms
    are quantized to a fixed integer range. A WaveNet defines an initial convolutional
    layer accessing only the current and previous input. Then, there is a stack of
    dilated ConvNet layers, still accessing only current and previous inputs. At the
    end, there is a series of dense layers combining previous results, followed by
    a softmax activation function for categorical outputs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step, a value is predicted from the network and fed back into the input.
    At the same time, a new prediction for the next step is computed. The loss function
    is the cross-entropy between the output for the current step and the input at
    the next step. *Figure 20.15* shows the visualization of a WaveNet stack and its
    receptive field as introduced in Aaron van den Oord [9]. Note that generation
    can be slow because the waveform has to be synthesized in a sequential fashion,
    as *x*[t] must be sampled first in order to obtain ![](img/B18331_20_004.png)
    where *x* is the input:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B18331_20_15.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.15: WaveNet internal connections'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'A method for performing a sampling in parallel has been proposed in Parallel
    WaveNet [10], which achieves a three orders-of-magnitude speedup. This uses two
    networks as a WaveNet teacher network, which is slow but ensures a correct result,
    and a WaveNet student network, which tries to mimic the behavior of the teacher;
    this can prove to be less accurate but is faster. This approach is similar to
    the one used for GANs (see *Chapter 9*, *Generative Models*) but the student does
    not try to fool the teacher, as typically happens in GANs. In fact, the model
    is not just quicker but also of higher fidelity, capable of creating waveforms
    with 24,000 samples per second:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, shape  Description automatically generated](img/B18331_20_16.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.16: Examples of WaveNet Student and Teacher'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: This model has been deployed in production at Google, and is currently being
    used to serve Google Assistant queries in real time to millions of users. At the
    annual I/O developer conference in May 2018, it was announced that new Google
    Assistant voices were available thanks to WaveNet.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Two implementations of WaveNet models for TensorFlow are currently available.
    One is the original implementation of DeepMind’s WaveNet, and the other is called
    Magenta NSynth. The original WaveNet version is available at [https://github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet).
    NSynth is an evolution of WaveNet recently released by the Google Brain group,
    which, instead of being causal, aims at seeing the entire context of the input
    chunk. Magenta is available at [https://magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network is truly complex, as depicted in the image below, but for
    the sake of this introductory discussion, it is sufficient to know that the network
    learns how to reproduce its input by using an approach based on reducing the error
    during the encoding/decoding phases:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![NSynth_blog_figs_WaveNetAE_diagram.png](img/B18331_20_17.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.17: Magenta internal architecture'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in understanding more, I would suggest having a look at
    the online Colab notebook where you can play with models generated with NSynth.
    NSynth Colab is available at [https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: MuseNet is a very recent and impressive cool audio generation tool developed
    by OpenAI. MuseNet uses a sparse transformer to train a 72-layer network with
    24 attention heads. MuseNet is available at [https://openai.com/blog/musenet/](https://openai.com/blog/musenet/).
    Transformers, discussed in *Chapter 6*, are very good at predicting what comes
    next in a sequence – whether text, images, or sound.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'In transformers, every output element is connected to every input element,
    and the weightings between them are dynamically calculated according to a process
    called attention. MuseNet can produce up to 4-minute musical compositions with
    10 different instruments, and can combine styles from country, to Mozart, to the
    Beatles. For instance, I generated a remake of Beethoven’s “Für Elise” in the
    style of Lady Gaga with piano, drums, guitar, and bass. You can try this for yourself
    at the link provided under the section **Try MuseNet**:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18331_20_18.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.18: An example of using MuseNet'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: A summary of convolution operations
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present a summary of different convolution operations. A
    convolutional layer has *I* input channels and produces *O* output channels. *I*
    x *O* x *K* parameters are used, where *K* is the number of values in the kernel.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Basic CNNs
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s remind ourselves briefly what a CNN is. CNNs take in an input image (two
    dimensions), text (two dimensions), or video (three dimensions) and apply multiple
    filters to the input. Each filter is like a flashlight sliding across the areas
    of the input, and the areas that it is shining over are called the receptive field.
    Each filter is a tensor of the same depth of the input (for instance, if the image
    has a depth of three, then the filter must also have a depth of three).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: When the filter is sliding, or convolving, around the input image, the values
    in the filter are multiplied by the values of the input. The multiplications are
    then summarized into one single value. This process is repeated for each location,
    producing an activation map (a.k.a. a feature map). Of course, it is possible
    to use multiple filters where each filter will act as a feature identifier. For
    instance, for images, the filter can identify edges, colors, lines, and curves.
    The key intuition is to treat the filter values as weights and fine-tune them
    during training via backpropagation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'A convolution layer can be configured by using the following config parameters:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel size**: This is the field of view of the convolution.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stride**: This is the step size of the kernel when we traverse the image.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Padding**: Defines how the border of our sample is handled.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dilated convolution
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dilated convolutions (or atrous convolutions) introduce another config parameter:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**Dilation rate**: This is the spacing between the values in a kernel.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dilated convolutions are used in many contexts including audio processing with
    WaveNet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolution
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transposed convolution is a transformation going in the opposite direction of
    a normal convolution. For instance, this can be useful to project feature maps
    into a higher dimensional space or for building convolutional autoencoders (see
    *Chapter 8*, *Autoencoders*). One way to think about transposed convolution is
    to compute the output shape of a normal CNN for a given input shape first. Then
    we invert input and output shapes with the transposed convolution. TensorFlow
    2.0 supports transposed convolutions with Conv2DTranspose layers, which can be
    used, for instance, in GANs (see *Chapter 9*, *Generative Models*) for generating
    images.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Separable convolution
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Separable convolution aims at separating the kernel in multiple steps. Let the
    convolution be *y* = *conv*(*x*, *k*) where *y* is the output, *x* is the input,
    and *k* is the kernel. Let’s assume the kernel is separable, *k* = *k*1.*k*2 where
    . is the dot product – in this case, instead of doing a 2-dimension convolution
    with *k*, we can get to the same result by doing two 1-dimension convolutions
    with *k*1 and *k*2\. Separable convolutions are frequently used to save on computation
    resources.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise convolution
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider an image with multiple channels. In the normal 2D convolution,
    the filter is as deep as the input, and it allows us to mix channels for generating
    each element of the output. In depthwise convolutions, each channel is kept separate,
    the filter is split into channels, each convolution is applied separately, and
    the results are stacked back together into one tensor.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise separable convolution
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This convolution should not be confused with the separable convolution. After
    completing the depthwise convolution, an additional step is performed: a 1x1 convolution
    across channels. Depthwise separable convolutions are used in Xception. They are
    also used in MobileNet, a model particularly useful for mobile and embedded vision
    applications because of its reduced model size and complexity.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have discussed all the major forms of convolution. The next
    section will discuss capsule networks, a new form of learning introduced in 2017.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Capsule networks
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capsule networks (or CapsNets) are a very recent and innovative type of deep
    learning network. This technique was introduced at the end of October 2017 in
    a seminal paper titled *Dynamic Routing Between Capsules* by Sara Sabour, Nicholas
    Frost, and Geoffrey Hinton ([https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829))
    [14]. Hinton is the father of deep learning and, therefore, the whole deep learning
    community is excited to see the progress made with Capsules. Indeed, CapsNets
    are already beating the best CNN on MNIST classification, which is... well, impressive!!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem with CNNs?
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In CNNs, each layer “understands” an image at a progressive level of granularity.
    As we discussed in multiple sections, the first layer will most likely recognize
    straight lines or simple curves and edges, while subsequent layers will start
    to understand more complex shapes such as rectangles up to complex forms such
    as human faces.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, one critical operation used for CNNs is pooling. Pooling aims at creating
    positional invariance and it is used after each CNN layer to make any problem
    computationally tractable. However, pooling introduces a significant problem because
    it forces us to lose all the positional data. This is not good. Think about a
    face: it consists of two eyes, a mouth, and a nose, and what is important is that
    there is a spatial relationship between these parts (for example, the mouth is
    below the nose, which is typically below the eyes). Indeed, Hinton said: *The
    pooling operation used in convolutional neural networks is a big mistake and the
    fact that it works so well is a disaster*. Technically, we do not need positional
    invariance but instead we need equivariance. Equivariance is a fancy term for
    indicating that we want to understand the rotation or proportion change in an
    image, and we want to adapt the network accordingly. In this way, the spatial
    positioning among the different components in an image is not lost.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: What is new with capsule networks?
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to Hinton et al., our brain has modules called “capsules,” and each
    capsule is specialized in handling a particular type of information. In particular,
    there are capsules that work well for “understanding” the concept of position,
    the concept of size, the concept of orientation, the concept of deformation, textures,
    and so on. In addition to that, the authors suggest that our brain has particularly
    efficient mechanisms for dynamically routing each piece of information to the
    capsule that is considered best suited for handling a particular type of information.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: So, the main difference between CNN and CapsNets is that with a CNN, we keep
    adding layers for creating a deep network, while with CapsNet, we nest a neural
    layer inside another. A capsule is a group of neurons that introduces more structure
    to a network, and it produces a vector to signal the existence of an entity in
    an image. In particular, Hinton uses the length of the activity vector to represent
    the probability that the entity exists and its orientation to represent the instantiation
    parameters. When multiple predictions agree, a higher-level capsule becomes active.
    For each possible parent, the capsule produces an additional prediction vector.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Now a second innovation comes in place: we will use dynamic routing across
    capsules and will no longer use the raw idea of pooling. A lower-level capsule
    prefers to send its output to higher-level capsules for which the activity vectors
    have a big scalar product, with the prediction coming from the lower-level capsule.
    The parent with the largest scalar prediction vector product increases the capsule
    bond. All the other parents decrease their bond. In other words, the idea is that
    if a higher-level capsule agrees with a lower-level one, then it will ask to send
    more information of that type. If there is no agreement, it will ask to send fewer
    of them. This dynamic routing by the agreement method is superior to the current
    mechanism like max pooling and, according to Hinton, routing is ultimately a way
    to parse the image. Indeed, max pooling is ignoring anything but the largest value,
    while dynamic routing selectively propagates information according to the agreement
    between lower layers and upper layers.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'A third difference is that a new nonlinear activation function has been introduced.
    Instead of adding a squashing function to each layer as in CNN, CapsNet adds a
    squashing function to a nested set of layers. The nonlinear activation function
    is represented in *Equation 1*, and it is called the squashing function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_20_005.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: where v[j] is the vector output of capsule *j* and s[j] is its total input.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Hinton and others show that a discriminatively trained, multi-layer
    capsule system achieves state-of-the-art performances on MNIST and is considerably
    better than a convolutional net at recognizing highly overlapping digits.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the paper *Dynamic Routing Between Capsules*, a simple CapsNet architecture
    looks as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2017-11-03 at 7.22.09 PM.png](img/B18331_20_19_new.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.19: An example of CapsNet'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The architecture is shallow with only two convolutional layers and one fully
    connected layer. Conv1 has 256 9 x 9 convolution kernels with a stride of 1 and
    ReLU activation. The role of this layer is to convert pixel intensities to the
    activities of local feature detectors that are then used as inputs to the PrimaryCapsules
    layer. PrimaryCapsules is a convolutional capsule layer with 32 channels; each
    primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride
    of 2\. In total, PrimaryCapsules has [32, 6, 6] capsule outputs (each output is
    an 8D vector) and each capsule in the [6, 6] grid shares its weights with each
    other. The final layer (DigitCaps) has one 16D capsule per digit class and each
    one of these capsules receives input from all the other capsules in the layer
    below. Routing happens only between two consecutive capsule layers (for example,
    PrimaryCapsules and DigitCaps).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen many applications of CNNs across very different
    domains, from traditional image processing and computer vision to close-enough
    video processing, not-so-close audio processing, and text processing. In just
    a few years, CNNs have taken machine learning by storm.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, it is not uncommon to see multimodal processing, where text, images,
    audio, and videos are considered together to achieve better performance, frequently
    by means of combining CNNs together with a bunch of other techniques such as RNNs
    and reinforcement learning. Of course, there is much more to consider, and CNNs
    have recently been applied to many other domains such as genetic inference [13],
    which are, at least at first glance, far away from the original scope of their
    design.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yosinski, J. and Clune, Y. B. J. *How transferable are features in deep neural
    networks*. Advances in Neural Information Processing Systems 27, pp. 3320–3328.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). *Rethinking
    the Inception Architecture for Computer Vision*. 2016 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2818–2826.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sandler, M., Howard, A., Zhu, M., Zhmonginov, A., and Chen, L. C. (2019). *MobileNetV2:
    Inverted Residuals and Linear Bottlenecks*. Google Inc.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Krizhevsky, A., Sutskever, I., Hinton, G. E., (2012). *ImageNet classification
    with deep convolutional neural networks*.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (28 Jan 2018).
    *Densely Connected Convolutional Networks*. [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993)
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chollet, F. (2017). *Xception: Deep Learning with Depthwise Separable Convolutions*.
    [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). *A Neural Algorithm of Artistic
    Style*. [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mordvintsev, A., Olah, C., and Tyka, M. ( 2015). *DeepDream - a code example
    for visualizing Neural Networks*. Google Research.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). *WaveNet: A generative
    model for raw audio*. arXiv preprint.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu,
    K., van den Driessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande,
    N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H.,
    Graves, A., King, H., Walters, T., Belov, D., and Hassabis, D. (2017). *Parallel
    WaveNet: Fast High-Fidelity Speech Synthesis*.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). *Mask R-CNN*.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chen, L-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. (2018). *Encoder-Decoder
    with Atrous Separable Convolution for Semantic Image Segmentation*.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flagel, L., Brandvain, Y., and Schrider, D.R. (2018). *The Unreasonable Effectiveness
    of Convolutional Neural Networks in Population Genetic Inference*.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sabour, S., Frosst, N., and Hinton, G. E. (2017). *Dynamic Routing Between Capsules*
    [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
