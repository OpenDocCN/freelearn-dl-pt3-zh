<html><head></head><body>
  <div id="_idContainer089">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-166" class="chapterTitle">Neural Networks</h1>
    <p class="normal">In this chapter, we will introduce neural networks and how to implement them in TensorFlow. Most of the subsequent chapters will be based on neural networks, so learning how to use them in TensorFlow is very important.</p>
    <p class="normal">Neural networks are currently breaking records in tasks such as image and speech recognition, reading handwriting, understanding text, image segmentation, dialog systems, autonomous car driving, and so much more. While some of these tasks will be covered in later chapters, it is important to introduce neural networks as a general-purpose, easy-to-implement machine learning algorithm, so that we can expand on it later.</p>
    <p class="normal">The concept of a neural network has been around for decades. However, it only recently gained traction because we now have the computational power to train large networks because of advances in processing power, algorithm efficiency, and data sizes.</p>
    <p class="normal">A neural network is, fundamentally, a sequence of operations applied to a matrix of input data. These operations are usually collections of additions and multiplications followed by the application of non-linear functions. One example that we have already seen is logistic regression, which we looked at in <em class="chapterRef">Chapter 4</em>, <em class="italic">Linear Regression</em>. Logistic regression is the sum of partial slope-feature products followed by the application of the sigmoid function, which is non-linear. Neural networks generalize this a little more by allowing any combination of operations and non-linear functions, which includes the application of absolute values, maximums, minimums, and so on.</p>
    <p class="normal">The most important trick to neural networks is called <strong class="keyword">backpropagation</strong>. Backpropagation is a <a id="_idIndexMarker297"/>procedure that allows us to update model variables based on the learning rate and the output of the loss function. We used backpropagation to update our model variables in <em class="chapterRef">Chapter 3</em>, <em class="italic">Keras</em>, and <em class="chapterRef">Chapter 4</em>, <em class="italic">Linear Regression</em>.</p>
    <p class="normal">Another important feature to take note of regarding neural networks is the non-linear activation function. Since most neural networks are just combinations of addition and multiplication operations, they will not be able to model non-linear datasets. To address this issue, we will use non-linear activation functions in our neural networks. This will allow the neural network to adapt to most non-linear situations.</p>
    <p class="normal">It is important to remember that, as we have seen in many of the algorithms covered, neural networks are sensitive to the hyperparameters we choose. In this chapter, we will explore the impact of different learning rates, loss functions, and optimization procedures.</p>
    <p class="normal">There are a few more resources I would recommend to you for learning about neural networks that cover the topic in greater depth and more detail:</p>
    <ul>
      <li class="bullet">The seminal paper describing backpropagation is <em class="italic">Efficient Back Prop</em> by Yann LeCun et al. The PDF is located here: <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"><span class="url">http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</span></a>.</li>
      <li class="bullet">CS231, <em class="italic">Convolutional Neural Networks for Visual Recognition</em>, by Stanford University. Class resources are available here: <a href="http://cs231n.stanford.edu/"><span class="url">http://cs231n.stanford.edu/</span></a>.</li>
      <li class="bullet">CS224d, <em class="italic">Deep Learning for Natural Language Processing</em>, by Stanford University. Class resources are available here: <a href="http://cs224d.stanford.edu/"><span class="url">http://cs224d.stanford.edu/</span></a>.</li>
      <li class="bullet"><em class="italic">Deep Learning</em>, a book by the MIT Press. Goodfellow, et al. 2016. The book is located here: <a href="http://www.deeplearningbook.org"><span class="url">http://www.deeplearningbook.org</span></a>.</li>
      <li class="bullet">The online book <em class="italic">Neural Networks and Deep Learning</em> by Michael Nielsen, which is located here: <a href="http://neuralnetworksanddeeplearning.com/"><span class="url">http://neuralnetworksanddeeplearning.com/</span></a>.</li>
      <li class="bullet">For a more pragmatic approach and introduction to neural networks, Andrej Karpathy has written a great summary with JavaScript examples called <em class="italic">A Hacker's Guide to Neural Networks</em>. The write-up is located here: <a href="http://karpathy.github.io/neuralnets/"><span class="url">http://karpathy.github.io/neuralnets/</span></a>.</li>
      <li class="bullet">Another site that summarizes deep learning well is called <em class="italic">Deep Learning for Beginners</em> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. The web page can be found here: <a href="http://randomekek.github.io/deep/deeplearning.html"><span class="url">http://randomekek.github.io/deep/deeplearning.html</span></a>.</li>
    </ul>
    <p class="normal">We will start by introducing the basic concepts of neural networking before working up to multilayer networks. In the last section, we will create a neural network that will learn how to play Tic-Tac-Toe.</p>
    <p class="normal">In this chapter, we'll cover the following recipes:</p>
    <ul>
      <li class="bullet">Implementing operational gates</li>
      <li class="bullet">Working with gates and activation functions</li>
      <li class="bullet">Implementing a one-layer neural network</li>
      <li class="bullet">Implementing different layers</li>
      <li class="bullet">Using a multilayer neural network</li>
      <li class="bullet">Improving the predictions of linear models</li>
      <li class="bullet">Learning to play Tic-Tac-Toe</li>
    </ul>
    <p class="normal">The reader can find all of the code from this chapter online at <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a>, and on the Packt repository at <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a>.</p>
    <h1 id="_idParaDest-167" class="title">Implementing operational gates</h1>
    <p class="normal">One of the most <a id="_idIndexMarker298"/>fundamental concepts of neural networks is its functioning as an operational gate. In this section, we will start with a multiplication operation as a gate, before moving on to consider nested gate operations.</p>
    <h2 id="_idParaDest-168" class="title">Getting ready</h2>
    <p class="normal">The first operational gate we will implement is <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">a</em> · <em class="italic">x</em>: </p>
    <figure class="mediaobject"><img src="../Images/B16254_06_01.png" alt=""/></figure>
    <p class="normal">To optimize this gate, we declare the <em class="italic">a</em> input as a variable and <em class="italic">x</em> as the input tensor of our model. This means that TensorFlow will try to change the <em class="italic">a</em> value and not the <em class="italic">x</em> value. We will create the loss function as the difference between the output and the target value, which is 50.</p>
    <p class="normal">The second, nested, operational gate will be <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">a</em> · <em class="italic">x</em> + <em class="italic">b</em>:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_02.png" alt=""/></figure>
    <p class="normal">Again, we will declare <em class="italic">a</em> and <em class="italic">b</em> as variables and <em class="italic">x</em> as the input tensor of our model. We optimize the output toward the <a id="_idIndexMarker299"/>target value of 50 again. The interesting thing to note is that the solution for this second example is not unique. There are many combinations of model variables that will allow the output to be 50. With neural networks, we do not care so much about the values of the intermediate model variables, but instead place more emphasis on the desired output.</p>
    <h2 id="_idParaDest-169" class="title">How to do it...</h2>
    <p class="normal">To implement the first operational gate, <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">a</em> · <em class="italic">x</em>, in TensorFlow and train the output toward the value of 50, follow these steps:</p>
    <ol>
      <li class="numbered">Start off by loading TensorFlow as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf  
</code></pre>
      </li>
      <li class="numbered">Now we will need to declare our model variable and input data. We make our input data equal to the value 5, so that the multiplication factor to get 50 will be 10 (that is, 5*10=50), as follows:
        <pre class="programlisting code"><code class="hljs-code">a = tf.Variable(<span class="hljs-number">4.</span>)
x_data = tf.keras.Input(shape=(<span class="hljs-number">1</span>,))
x_val = <span class="hljs-number">5.</span>
</code></pre>
      </li>
      <li class="numbered">Next, we create a lambda layer that computes the operation, and we create a functional Keras model with the following input:
        <pre class="programlisting code"><code class="hljs-code">multiply_layer = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x:tf.multiply(a, x))
outputs = multiply_layer(x_data)
model = tf.keras.Model(inputs=x_data, outputs=outputs, name="gate_1")
</code></pre>
      </li>
      <li class="numbered">We will now declare our optimizing algorithm as the stochastic gradient descent as follows:
        <pre class="programlisting code"><code class="hljs-code">optimizer=tf.keras.optimizers.SGD(<span class="hljs-number">0.01</span>)
</code></pre>
      </li>
      <li class="numbered">We can now optimize our model output toward the desired value of 50. We will use the loss function as the L2 distance between the output and the desired target value of 50. We do this by continually feeding in the input value of 5 and backpropagating the loss to <a id="_idIndexMarker300"/>update the model variable toward the value of 10, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">print('Optimizing a Multiplication Gate Output to <span class="hljs-number">50.</span>')
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
    
    <span class="hljs-comment"># Open a GradientTape.</span>
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        
        <span class="hljs-comment"># Forward pass.</span>
        mult_output = model(x_val)
        
        <span class="hljs-comment"># Loss value as the difference between</span>
        <span class="hljs-comment"># the output and a target value, 50.</span>
        loss_value = tf.square(tf.subtract(mult_output, <span class="hljs-number">50.</span>))
        
    <span class="hljs-comment"># Get gradients of loss with reference to the variable "a" to adjust.</span>
    gradients = tape.gradient(loss_value, a)
    
    <span class="hljs-comment"># Update the variable "a" of the model.</span>
    optimizer.apply_gradients(zip([gradients], [a]))
    
    print("{} * {} = {}".format(a.numpy(), x_val, a.numpy() * x_val))
</code></pre>
      </li>
      <li class="numbered">The preceding step should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">Optimizing a Multiplication Gate Output to 50. 
7.0 * 5.0 = 35.0 
8.5 * 5.0 = 42.5 
9.25 * 5.0 = 46.25 
9.625 * 5.0 = 48.125 
9.8125 * 5.0 = 49.0625 
9.90625 * 5.0 = 49.5312 
9.95312 * 5.0 = 49.7656 
9.97656 * 5.0 = 49.8828 
9.98828 * 5.0 = 49.9414 
9.99414 * 5.0 = 49.9707 
</code></pre>
        <p class="bullet-para">Next, we will do the same with the two-nested operational gate, <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">a</em> · <em class="italic">x</em> + <em class="italic">b</em>.</p>
      </li>
      <li class="numbered">We will start in exactly <a id="_idIndexMarker301"/>the same way as the preceding example, but will initialize two model variables, <code class="Code-In-Text--PACKT-">a</code> and <code class="Code-In-Text--PACKT-">b</code>, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-comment"># Initialize variables and input data</span>
x_data = tf.keras.Input(dtype=tf.float32, shape=(<span class="hljs-number">1</span>,))
x_val = <span class="hljs-number">5.</span>
a = tf.Variable(<span class="hljs-number">1.</span>, dtype=tf.float32)
b = tf.Variable(<span class="hljs-number">1.</span>, dtype=tf.float32)
<span class="hljs-comment"># Add a layer which computes f(x) = a * x</span>
multiply_layer = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x:tf.multiply(a, x))
<span class="hljs-comment"># Add a layer which computes f(x) = b + x</span>
add_layer = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x:tf.add(b, x))
res = multiply_layer(x_data)
outputs = add_layer(res)
<span class="hljs-comment"># Build the model</span>
model = tf.keras.Model(inputs=x_data, outputs=outputs, name="gate_2")
<span class="hljs-comment"># Optimizer</span>
optimizer=tf.keras.optimizers.SGD(<span class="hljs-number">0.01</span>)
</code></pre>
      </li>
      <li class="numbered">We now optimize the model variables to train the output toward the target value of 50, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">print('Optimizing two Gate Output to 50.')
for i in range(10):
    
    # Open a GradientTape.
    with tf.GradientTape(persistent=True) as tape:
        
        # Forward pass.
        two_gate_output = model(x_val)
        
        # Loss value as the difference between
        # the output and a target value, 50.
        loss_value = tf.square(tf.subtract(two_gate_output, 50.))
        
    # Get gradients of loss with reference to 
    # the variables "a" and "b" to adjust.
    gradients_a = tape.gradient(loss_value, a)
    gradients_b = tape.gradient(loss_value , b)
    
    # Update the variables "a" and "b" of the model.
    optimizer.apply_gradients(zip([gradients_a, gradients_b], [a, b]))
    
    print("Step: {} ==&gt; {} * {} + {}= {}".format(i, a.numpy(),
                                                 x_val, b.numpy(),
                                                 a.numpy()*x_val+b.numpy()))
</code></pre>
      </li>
      <li class="numbered">The preceding <a id="_idIndexMarker302"/>step should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">Optimizing Two Gate Output to 50. 
5.4 * 5.0 + 1.88 = 28.88 
7.512 * 5.0 + 2.3024 = 39.8624 
8.52576 * 5.0 + 2.50515 = 45.134 
9.01236 * 5.0 + 2.60247 = 47.6643 
9.24593 * 5.0 + 2.64919 = 48.8789 
9.35805 * 5.0 + 2.67161 = 49.4619 
9.41186 * 5.0 + 2.68237 = 49.7417 
9.43769 * 5.0 + 2.68754 = 49.876 
9.45009 * 5.0 + 2.69002 = 49.9405 
9.45605 * 5.0 + 2.69121 = 49.9714 
</code></pre>
      </li>
    </ol>
    <div class="packt_tip">
      <p class="Tip--PACKT-">It is important to note here that the solution to the second example is not unique. This does not matter as much in neural networks, as all parameters are adjusted toward reducing the loss. The final solution here will depend on the initial values of a and b. If these were randomly initialized, instead of to the value of 1, we would see different ending values for the model variables for each iteration.</p>
    </div>
    <h2 id="_idParaDest-170" class="title">How it works...</h2>
    <p class="normal">We achieved the optimization of a computational gate via TensorFlow's implicit backpropagation. TensorFlow <a id="_idIndexMarker303"/>keeps track of our model's operations and variable values and makes adjustments in respect of our optimization algorithm specification and the output of the loss function.</p>
    <p class="normal">We can keep expanding the operational gates while keeping track of which inputs are variables and which inputs are data. This is important to keep track of, because TensorFlow will change all variables to minimize the loss but not the data.</p>
    <p class="normal">The implicit ability to keep track of the computational graph and update the model variables automatically with every training step is one of the great features of TensorFlow and what makes it so powerful.</p>
    <h1 id="_idParaDest-171" class="title">Working with gates and activation functions</h1>
    <p class="normal">Now that we can link <a id="_idIndexMarker304"/>together operational gates, we want to run <a id="_idIndexMarker305"/>the computational graph output through an activation function. In this section, we will introduce common activation functions.</p>
    <h2 id="_idParaDest-172" class="title">Getting ready</h2>
    <p class="normal">In this section, we will compare and contrast two different activation functions: <strong class="keyword">sigmoid</strong> and <strong class="keyword">rectified linear unit</strong> (<strong class="keyword">ReLU</strong>). Recall that the two functions are given by the following equations:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_03.png" alt=""/></figure>
    <figure class="mediaobject"><img src="../Images/B16254_06_04.png" alt=""/></figure>
    <p class="normal">In this example, we will create two one-layer neural networks with the same structure, except that one will feed through the sigmoid activation and one will feed through the ReLU activation. The loss function will be governed by the L2 distance from the value 0.75. We will randomly pull batch data and then optimize the output toward 0.75.</p>
    <h2 id="_idParaDest-173" class="title">How to do it...</h2>
    <p class="normal">We <a id="_idIndexMarker306"/>proceed with the recipe as follows:</p>
    <ol>
      <li class="numbered" value="1">We will start by loading the <a id="_idIndexMarker307"/>necessary libraries. This is also a good point at which we can bring up how to set a random seed with TensorFlow. Since we will be using a random number generator from NumPy and TensorFlow, we need to set a random seed for both. With the same random seeds set, we should be able to replicate the results. We do this with the following input:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf 
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np 
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt 
tf.random.set_seed(<span class="hljs-number">5</span>)
np.random.seed(<span class="hljs-number">42</span>) 
</code></pre>
      </li>
      <li class="numbered">Now we need to declare our batch size, model variables, and data model inputs. Our computational graph will consist of feeding in our normally distributed data into two similar neural networks that differ only by the activation function at the end, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">50</span> 
x_data = tf.keras.Input(shape=(<span class="hljs-number">1</span>,))
x_data = tf.keras.Input(shape=(<span class="hljs-number">1</span>,))
a1 = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], seed=<span class="hljs-number">5</span>))
b1 = tf.Variable(tf.random.uniform(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], seed=<span class="hljs-number">5</span>))
a2 = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], seed=<span class="hljs-number">5</span>))
b2 = tf.Variable(tf.random.uniform(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], seed=<span class="hljs-number">5</span>))  
</code></pre>
      </li>
      <li class="numbered">Next, we'll <a id="_idIndexMarker308"/>declare our two models, the sigmoid activation <a id="_idIndexMarker309"/>model and the ReLU activation model, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">MyCustomGateSigmoid</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
 
 <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, units, a1, b1</span><span class="hljs-function">):</span>
   super(MyCustomGateSigmoid, self).__init__()
   self.units = units
   self.a1 = a1
   self.b1 = b1
 <span class="hljs-comment"># Compute f(x) = sigmoid(a1 * x + b1)</span>
 <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, inputs</span><span class="hljs-function">):</span>
   <span class="hljs-keyword">return</span> tf.math.sigmoid(inputs * self.a1 + self.b1)
<span class="hljs-comment"># Add a layer which computes f(x) = sigmoid(a1 * x + b1)</span>
my_custom_gate_sigmoid = MyCustomGateSigmoid(units=<span class="hljs-number">1</span>, a1=a1, b1=b1)
output_sigmoid = my_custom_gate_sigmoid(x_data)
<span class="hljs-comment"># Build the model</span>
model_sigmoid = tf.keras.Model(inputs=x_data, outputs=output_sigmoid, name="gate_sigmoid")
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">MyCustomGateRelu</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
 
 <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, units, a2, b2</span><span class="hljs-function">):</span>
   super(MyCustomGateRelu, self).__init__()
   self.units = units
   self.a2 = a2
   self.b2 = b2
 <span class="hljs-comment"># Compute f(x) = relu(a2 * x + b2)</span>
 <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, inputs</span><span class="hljs-function">):</span>
   <span class="hljs-keyword">return</span> tf.nn.relu(inputs * self.a2 + self.b2)
<span class="hljs-comment"># Add a layer which computes f(x) = relu(a2 * x + b2)</span>
my_custom_gate_relu = MyCustomGateRelu(units=<span class="hljs-number">1</span>, a2=a2, b2=b2)
outputs_relu = my_custom_gate_relu(x_data)
<span class="hljs-comment"># Build the model</span>
model_relu = tf.keras.Model(inputs=x_data, outputs=outputs_relu, name="gate_relu")
</code></pre>
      </li>
      <li class="numbered">Now we need to declare our optimization algorithm and initialize our variables, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">optimizer=tf.keras.optimizers.SGD(<span class="hljs-number">0.01</span>) 
</code></pre>
      </li>
      <li class="numbered">Now we'll loop through our training for 750 iterations for both models, as shown in the <a id="_idIndexMarker310"/>following code block. The loss functions will be the average L2 norm between the <a id="_idIndexMarker311"/>model output and the value of 0.75. We will also save the loss output and the activation output values for plotting later on:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Run loop across gate</span>
print('\n Optimizing Sigmoid AND Relu Output to <span class="hljs-number">0.75</span>')
loss_vec_sigmoid = []
loss_vec_relu = []
activation_sigmoid = []
activation_relu = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">500</span>):
    
    rand_indices = np.random.choice(len(x), size=batch_size)
    x_vals = np.transpose([x[rand_indices]])
    <span class="hljs-comment"># Open a GradientTape.</span>
    <span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
   
        <span class="hljs-comment"># Forward pass.</span>
        output_sigmoid = model_sigmoid(x_vals)
        output_relu = model_relu(x_vals)
        
        <span class="hljs-comment"># Loss value as the difference as the difference between</span>
        <span class="hljs-comment"># the output and a target value, 0.75.</span>
        loss_sigmoid = tf.reduce_mean(tf.square(tf.subtract(output_sigmoid, <span class="hljs-number">0.75</span>)))
        loss_vec_sigmoid.append(loss_sigmoid)
        loss_relu = tf.reduce_mean(tf.square(tf.subtract(output_relu, <span class="hljs-number">0.75</span>)))
        loss_vec_relu.append(loss_relu)
        
        
    <span class="hljs-comment"># Get gradients of loss_sigmoid with reference to the variable "a1" and "b1" to adjust.</span>
    gradients_a1 = tape.gradient(loss_sigmoid, my_custom_gate_sigmoid.a1)
    gradients_b1 = tape.gradient(loss_sigmoid , my_custom_gate_sigmoid.b1)
    
    <span class="hljs-comment"># Get gradients of loss_relu with reference to the variable "a2" and "b2" to adjust.</span>
    gradients_a2 = tape.gradient(loss_relu, my_custom_gate_relu.a2)
    gradients_b2 = tape.gradient(loss_relu , my_custom_gate_relu.b2)
    
    <span class="hljs-comment"># Update the variable "a1" and "b1" of the model.</span>
    optimizer.apply_gradients(zip([gradients_a1, gradients_b1], [my_custom_gate_sigmoid.a1, my_custom_gate_sigmoid.b1]))
    
    <span class="hljs-comment"># Update the variable "a2" and "b2" of the model.</span>
    optimizer.apply_gradients(zip([gradients_a2, gradients_b2], [my_custom_gate_relu.a2, my_custom_gate_relu.b2]))
    
    output_sigmoid = model_sigmoid(x_vals)
    output_relu = model_relu(x_vals)
    
    activation_sigmoid.append(np.mean(output_sigmoid))
    activation_relu.append(np.mean(output_relu))
    
    <span class="hljs-keyword">if</span> i%<span class="hljs-number">50</span>==<span class="hljs-number">0</span>:
        print('sigmoid = ' + str(np.mean(output_sigmoid)) + ' relu = ' + str(np.mean(output_relu)))
</code></pre>
      </li>
      <li class="numbered">To plot the loss and the activation outputs, we need to input the following code:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(activation_sigmoid, 'k-', label='Sigmoid Activation') 
plt.plot(activation_relu, 'r--', label='Relu Activation') 
plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>]) 
plt.title('Activation Outputs') 
plt.xlabel('Generation') 
plt.ylabel('Outputs') 
plt.legend(loc='upper right') 
plt.show() 
plt.plot(loss_vec_sigmoid, 'k-', label='Sigmoid Loss') 
plt.plot(loss_vec_relu, 'r--', label='Relu Loss') 
plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>]) 
plt.title('Loss per Generation') 
plt.xlabel('Generation') 
plt.ylabel('Loss') 
plt.legend(loc='upper right') 
plt.show() 
</code></pre>
      </li>
    </ol>
    <p class="normal">The <a id="_idIndexMarker312"/>activation output needs to be plotted as shown in the following <a id="_idIndexMarker313"/>diagram:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_05.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/B8A59E6E.tmp"/></figure>
    <p class="packt_figref">Figure 6.1: Computational graph outputs from the network with the sigmoid activation and a network with the ReLU activation</p>
    <p class="normal">The two neural networks work with a similar architecture and target (0.75) but with two different activation functions, sigmoid and ReLU. It is important to notice how much more rapidly the ReLU activation network converges to the desired target of 0.75 than the sigmoid activation, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_06.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/F46E84AC.tmp"/></figure>
    <p class="packt_figref">Figure 6.2: This figure depicts the loss value of the sigmoid and the ReLU activation networks. Notice how extreme the ReLU loss is at the beginning of the iterations</p>
    <h2 id="_idParaDest-174" class="title">How it works...</h2>
    <p class="normal">Because of the form of the ReLU activation function, it returns the value of zero much more often than the sigmoid function. We consider this behavior as a type of sparsity. This sparsity results in a speeding up of convergence, but a loss of controlled gradients. On the other hand, the sigmoid function has very well-controlled gradients and does not risk the extreme values that the ReLU <a id="_idIndexMarker314"/>activation does, as illustrated in the following table:</p>
    <table id="table001-2" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Activation function</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Advantages</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Disadvantages</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Sigmoid</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Less extreme outputs</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Slower convergence</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">ReLU</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Quick convergence</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Extreme output values possible</p>
          </td>
        </tr>
      </tbody>
    </table>
    <h2 id="_idParaDest-175" class="title">There's more...</h2>
    <p class="normal">In this section, we compared the ReLU activation function and the sigmoid activation function for neural networks. There are many other activation functions that are commonly used for neural networks, but most fall into either one of two categories; the first category contains functions that are shaped like the sigmoid function, such as arctan, hypertangent, heaviside step, and so on; the second category contains functions that are shaped like the ReLU function, such as softplus, leaky ReLU, and so on. Most of what we discussed in this section about comparing the two functions will hold true for activations in either category. However, it is important to note that the choice of activation function has a big impact on the convergence and the output of neural networks.</p>
    <h1 id="_idParaDest-176" class="title">Implementing a one-layer neural network</h1>
    <p class="normal">We have all of the tools <a id="_idIndexMarker315"/>needed to implement a neural network that operates on real data, so in this section, we will create a neural network with one layer that operates on the <code class="Code-In-Text--PACKT-">Iris</code> dataset.</p>
    <h2 id="_idParaDest-177" class="title">Getting ready</h2>
    <p class="normal">In this section, we will implement a neural network with one hidden layer. It will be important to understand that a fully connected neural network is based mostly on matrix multiplication. As such, it is important that the dimensions of the data and matrix are lined up correctly.</p>
    <p class="normal">Since this is a regression <a id="_idIndexMarker316"/>problem, we will use <strong class="keyword">mean squared error</strong> (<strong class="keyword">MSE</strong>) as the loss function.</p>
    <h2 id="_idParaDest-178" class="title">How to do it...</h2>
    <p class="normal">We proceed with the recipe as follows: </p>
    <ol>
      <li class="numbered" value="1">To create the computational graph, we'll start by loading the following necessary libraries:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets 
</code></pre>
      </li>
      <li class="numbered">Now we'll load the <code class="Code-In-Text--PACKT-">Iris</code> data and store the length as the target value with the following code:
        <pre class="programlisting code"><code class="hljs-code">iris = datasets.load_iris() 
x_vals = np.array([x[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> iris.data]) 
y_vals = np.array([x[<span class="hljs-number">3</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> iris.data]) 
</code></pre>
      </li>
      <li class="numbered">Since the dataset is smaller, we will want to set a seed to make the results reproducible, as follows:
        <pre class="programlisting code"><code class="hljs-code">seed = <span class="hljs-number">3</span> 
tf.set_random_seed(seed) 
np.random.seed(seed)
</code></pre>
      </li>
      <li class="numbered">To prepare the data, we'll create a 80-20 train-test split and normalize the <code class="Code-In-Text--PACKT-">x</code> features to be between <code class="Code-In-Text--PACKT-">0</code> and <code class="Code-In-Text--PACKT-">1</code> via min-max scaling, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">train_indices = np.random.choice(len(x_vals), round(len(x_vals)*<span class="hljs-number">0.8</span>), replace=<span class="hljs-literal">False</span>) 
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices))) 
x_vals_train = x_vals[train_indices] 
x_vals_test = x_vals[test_indices] 
y_vals_train = y_vals[train_indices] 
y_vals_test = y_vals[test_indices]
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">normalize_cols</span><span class="hljs-function">(</span><span class="hljs-params">m</span><span class="hljs-function">):</span> 
    col_max = m.max(axis=<span class="hljs-number">0</span>) 
    col_min = m.min(axis=<span class="hljs-number">0</span>) 
    <span class="hljs-keyword">return</span> (m-col_min) / (col_max - col_min) 
x_vals_train = np.nan_to_num(normalize_cols(x_vals_train)) 
x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))
</code></pre>
      </li>
      <li class="numbered">Now we will declare <a id="_idIndexMarker317"/>the batch size and the data model input with the following code:
        <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">50</span> 
x_data = tf.keras.Input(dtype=tf.float32, shape=(<span class="hljs-number">3</span>,))
</code></pre>
      </li>
      <li class="numbered">The important part is to declare our model variables with the appropriate shape. We can declare the size of our hidden layer to be any size we wish; in the following code block, we have set it to have five hidden nodes:
        <pre class="programlisting code"><code class="hljs-code">hidden_layer_nodes = <span class="hljs-number">5</span>
a1 = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">3</span>,hidden_layer_nodes], seed=seed)) 
b1 = tf.Variable(tf.random.normal(shape=[hidden_layer_nodes], seed=seed))   
a2 = tf.Variable(tf.random.normal(shape=[hidden_layer_nodes,<span class="hljs-number">1</span>], seed=seed)) 
b2 = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>], seed=seed))   
</code></pre>
      </li>
      <li class="numbered">We'll now declare our model in two steps. The first step will be creating the hidden layer output and the second will be creating the <code class="Code-In-Text--PACKT-">final_output</code> of the model, as follows:<div class="note">
          <p class="Information-Box--PACKT-">As a note, our model goes from three input features to five hidden nodes, and finally to one output value.</p>
        </div>
        <pre class="programlisting code"><code class="hljs-code">hidden_output = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.nn.relu(tf.add(tf.matmul(x, a1), b1)))
final_output = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.nn.relu(tf.add(tf.matmul(x, a2), b2)))
model = tf.keras.Model(inputs=x_data, outputs=output, name="<span class="hljs-number">1l</span>ayer_neural_network")
</code></pre>
      </li>
      <li class="numbered">Now we'll declare our optimizing algorithm with the following code:
        <pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.SGD(<span class="hljs-number">0.005</span>) 
</code></pre>
      </li>
      <li class="numbered">Next, we loop <a id="_idIndexMarker318"/>through our training iterations. We'll also initialize two lists in which we can store our <code class="Code-In-Text--PACKT-">train</code> and <code class="Code-In-Text--PACKT-">test_loss</code> functions. In every loop, we also want to randomly select a batch from the training data for fitting to the model, shown as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># First we initialize the loss vectors for storage. </span>
loss_vec = []
test_loss = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">500</span>):
    rand_index = np.random.choice(len(x_vals_train), size=batch_size)
    rand_x = x_vals_train[rand_index]
    rand_y = np.transpose([y_vals_train[rand_index]])
    
    <span class="hljs-comment"># Open a GradientTape.</span>
    <span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
   
        <span class="hljs-comment"># Forward pass.</span>
        output = model(rand_x)
        
        <span class="hljs-comment"># Apply loss function (MSE)</span>
        loss = tf.reduce_mean(tf.square(rand_y - output))
        loss_vec.append(np.sqrt(loss))       
        
    <span class="hljs-comment"># Get gradients of loss with reference to the variables to adjust.</span>
    gradients_a1 = tape.gradient(loss, a1)
    gradients_b1 = tape.gradient(loss, b1)
    gradients_a2 = tape.gradient(loss, a2)
    gradients_b2 = tape.gradient(loss, b2)
    
    <span class="hljs-comment"># Update the variables of the model.</span>
    optimizer.apply_gradients(zip([gradients_a1, gradients_b1, gradients_a2, gradients_b2], [a1, b1, a2, b2]))
    
    <span class="hljs-comment"># Forward pass.</span>
    output_test = model(x_vals_test)
    <span class="hljs-comment"># Apply loss function (MSE) on test</span>
    loss_test = tf.reduce_mean(tf.square(np.transpose([y_vals_test]) - output_test))
    test_loss.append(np.sqrt(loss_test))
    
    <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>)%<span class="hljs-number">50</span>==<span class="hljs-number">0</span>:
        print('Generation: ' + str(i+<span class="hljs-number">1</span>) + '. Loss = ' + str(np.mean(loss)))
        print('Generation: ' + str(i+<span class="hljs-number">1</span>) + '. Loss = ' + str(temp_loss))
</code></pre>
      </li>
      <li class="numbered">We can plot <a id="_idIndexMarker319"/>the losses with <code class="Code-In-Text--PACKT-">matplotlib</code> and the following code:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(loss_vec, 'k-', label='Train Loss') 
plt.plot(test_loss, 'r--', label='Test Loss') 
plt.title('Loss (MSE) per Generation') 
plt.xlabel('Generation') 
plt.ylabel('Loss') 
plt.legend(loc='upper right') 
plt.show() 
</code></pre>
      </li>
    </ol>
    <p class="normal">We proceed with the recipe by plotting the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_07.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/EE3BD051.tmp"/> </figure>
    <p class="packt_figref">Figure 6.3: We plot the loss (MSE) of the train and test set </p>
    <p class="normal">Note that we can also see <a id="_idIndexMarker320"/>that the train set loss is not as smooth as that in the test set. This is because of two reasons: the first is that we are using a smaller batch size than the test set, although not by much; the second cause is the fact that we are training on the train set, and the test set does not impact the variables of the model.</p>
    <h2 id="_idParaDest-179" class="title">How it works...</h2>
    <p class="normal">Our model has now been visualized <a id="_idIndexMarker321"/>as a neural network diagram, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.4: A neural network diagram</p>
    <p class="normal">The preceding figure is a <a id="_idIndexMarker322"/>visualization of our neural network that has five nodes in the <a id="_idIndexMarker323"/>hidden layer. We are feeding in <a id="_idIndexMarker324"/>three values: the <strong class="keyword">sepal length</strong> (<strong class="keyword">S.L.</strong>), the <strong class="keyword">sepal width</strong> (<strong class="keyword">S.W.</strong>), and the <strong class="keyword">petal length</strong> (<strong class="keyword">P.L.</strong>). The <a id="_idIndexMarker325"/>target will be the petal width. In total, there will be 26 total variables in the model.</p>
    <h1 id="_idParaDest-180" class="title">Implementing different layers</h1>
    <p class="normal">It is important to <a id="_idIndexMarker326"/>know how to implement different layers. In the preceding recipe, we implemented fully connected layers. In this recipe, we will further expand our knowledge of various layers.</p>
    <h2 id="_idParaDest-181" class="title">Getting ready</h2>
    <p class="normal">We have explored how to connect data inputs and a fully connected hidden layer, but there are more types of layers available as built-in functions inside TensorFlow. The most popular layers that are used are convolutional layers and maxpool layers. We will show you how to create and use such layers with input data and with fully connected data. First, we will look at how to use these layers on one-dimensional data, and then on two-dimensional data.</p>
    <p class="normal">While neural networks can be layered in any fashion, one of the most common designs is to use convolutional layers and fully connected layers to first create features. If we then have too many features, it is common to use a maxpool layer. </p>
    <p class="normal">After these layers, non-linear layers are commonly introduced as activation functions. <strong class="keyword">Convolutional neural networks</strong> (<strong class="keyword">CNNs</strong>), which we will consider in <em class="chapterRef">Chapter 8</em>, <em class="italic">Convolutional Neural Networks, </em>usually have convolutional, maxpool, and activation layers.</p>
    <h2 id="_idParaDest-182" class="title">How to do it...</h2>
    <p class="normal">We will first look at one-dimensional data. We need to generate a random array of data for this task using the following steps:</p>
    <ol>
      <li class="numbered" value="1">We'll start by loading the libraries we need, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf 
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np  
</code></pre>
      </li>
      <li class="numbered">Now we'll initialize <a id="_idIndexMarker327"/>some parameters and we'll create the input data layer with the following code:
        <pre class="programlisting code"><code class="hljs-code">data_size = <span class="hljs-number">25</span>
conv_size = <span class="hljs-number">5</span>
maxpool_size = <span class="hljs-number">5</span>
stride_size = <span class="hljs-number">1</span>
num_outputs = <span class="hljs-number">5</span>
x_input_1d = tf.keras.Input(dtype=tf.float32, shape=(data_size,<span class="hljs-number">1</span>), name="input_layer")
</code></pre>
      </li>
      <li class="numbered">Next, we will define a convolutional layer, as follows:<div class="note">
          <p class="Information-Box--PACKT-">For our example data, we have a batch size of <code class="Code-In-Text--PACKT-">1</code>, a width of <code class="Code-In-Text--PACKT-">1</code>, a height of <code class="Code-In-Text--PACKT-">25</code>, and a channel size of <code class="Code-In-Text--PACKT-">1</code>. Also note that we can calculate the output dimensions of convolutional layers with the <code class="Code-In-Text--PACKT-">output_size=(W-F+2P)/S+1</code> formula, where <code class="Code-In-Text--PACKT-">W</code> is the input size, <code class="Code-In-Text--PACKT-">F</code> is the filter size, <code class="Code-In-Text--PACKT-">P</code> is the padding size, and <code class="Code-In-Text--PACKT-">S</code> is the stride size.</p>
        </div>
        <pre class="programlisting code"><code class="hljs-code">my_conv_output = tf.keras.layers.Conv1D(kernel_size=(conv_size),
                                        filters=data_size, 
                                        strides=stride_size, 
                                        padding="VALID",
                                                       name="convolution_layer")(x_input_1d)
</code></pre>
      </li>
      <li class="numbered">Next, we add a ReLU activation layer, as follows:
        <pre class="programlisting code"><code class="hljs-code">my_activation_output = tf.keras.layers.ReLU(name="activation_layer")(my_conv_output)
</code></pre>
      </li>
      <li class="numbered">Now we'll add a maxpool layer. This layer will create a <code class="Code-In-Text--PACKT-">maxpool</code> on a moving window across our one-dimensional vector. For this example, we will initialize it to have a width of 5, shown as follows:<div class="note">
          <p class="Information-Box--PACKT-">TensorFlow's <code class="Code-In-Text--PACKT-">maxpool</code> arguments are very similar to those of the convolutional layer. While a <code class="Code-In-Text--PACKT-">maxpool</code> argument does not have a filter, it does have size, stride, and padding options. Since we have a window of 5 with valid padding (no zero padding), then our output array will have 4 fewer entries.</p>
        </div>
        <pre class="programlisting code"><code class="hljs-code">my_maxpool_output = tf.keras.layers.MaxPool1D(strides=stride_size,
                                                                 pool_size=maxpool_size,
                                              padding='VALID',
                                              name="maxpool_layer")(my_activation_output)
</code></pre>
      </li>
      <li class="numbered">The final layer that <a id="_idIndexMarker328"/>we will connect is the fully connected layer. Here, we will use a dense layer, as shown in the following code block:
        <pre class="programlisting code"><code class="hljs-code">my_full_output = tf.keras.layers.Dense(units=num_outputs,
                                       name="fully_connected_layer")(my_maxpool_output)
</code></pre>
      </li>
      <li class="numbered">Now we'll create the model, and print the output of each of the layers, as follows:
        <pre class="programlisting code"><code class="hljs-code">print('&gt;&gt;&gt;&gt; <span class="hljs-number">1</span>D Data &lt;&lt;&lt;&lt;')
model_1D = tf.keras.Model(inputs=x_input_1d, outputs=my_full_output, name="model_1D")
model_1D.summary()
<span class="hljs-comment"># Input</span>
print('\n== input_layer ==')
print('Input = array of length %d' % (x_input_1d.shape.as_list()[<span class="hljs-number">1</span>]))
<span class="hljs-comment"># Convolution </span>
print('\n== convolution_layer ==')
print('Convolution w/ filter, length = %d, stride size = %d, results <span class="hljs-keyword">in</span> an array of length %d' % 
      (conv_size,stride_size,my_conv_output.shape.as_list()[<span class="hljs-number">1</span>]))
<span class="hljs-comment"># Activation </span>
print('\n== activation_layer ==')
print('Input = above array of length %d' % (my_conv_output.shape.as_list()[<span class="hljs-number">1</span>]))
print('ReLU element wise returns an array of length %d' % (my_activation_output.shape.as_list()[<span class="hljs-number">1</span>]))
<span class="hljs-comment"># Max Pool </span>
print('\n== maxpool_layer ==')
print('Input = above array of length %d' % (my_activation_output.shape.as_list()[<span class="hljs-number">1</span>]))
print('MaxPool, window length = %d, stride size = %d, results <span class="hljs-keyword">in</span> the array of length %d' %
     (maxpool_size,stride_size,my_maxpool_output.shape.as_list()[<span class="hljs-number">1</span>]))
<span class="hljs-comment"># Fully Connected </span>
print('\n== fully_connected_layer ==')
print('Input = above array of length %d' % (my_maxpool_output.shape.as_list()[<span class="hljs-number">1</span>]))
print('Fully connected layer on all <span class="hljs-number">4</span> rows <span class="hljs-keyword">with</span> %d outputs' % 
      (my_full_output.shape.as_list()[<span class="hljs-number">1</span>]))
</code></pre>
      </li>
      <li class="numbered">The preceding step should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">&gt;&gt;&gt;&gt; 1D Data &lt;&lt;&lt;&lt;
Model: "model_1D"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     [(None, 25, 1)]           0         
_________________________________________________________________
convolution_layer (Conv1D)   (None, 21, 25)            150       
_________________________________________________________________
activation_layer (ReLU)      (None, 21, 25)            0         
_________________________________________________________________
maxpool_layer (MaxPooling1D) (None, 17, 25)            0         
_________________________________________________________________
fully_connected_layer (Dense (None, 17, 5)             130       
=================================================================
Total params: 280
Trainable params: 280
Non-trainable params: 0
_________________________________________________________________
== input_layer ==
Input = array of length 25
== convolution_layer ==
Convolution w/ filter, length = 5, stride size = 1, results in an array of length 21
== activation_layer ==
Input = above array of length 21
ReLU element wise returns an array of length 21
== maxpool_layer ==
Input = above array of length 21
MaxPool, window length = 5, stride size = 1, results in the array of length 17
== fully_connected_layer ==
Input = above array of length 17
Fully connected layer on all 4 rows with 17 outputs
</code></pre>
      </li>
    </ol>
    <div class="note">
      <p class="Information-Box--PACKT-">One-dimensional data is very important to consider for neural networks. Time series, signal processing, and some text embeddings are considered to be one-dimensional and are frequently used in neural networks.</p>
    </div>
    <p class="normal">We will now consider the same types of layer in an equivalent order but for two-dimensional data:</p>
    <ol>
      <li class="numbered" value="1">We will start by initializing the variables, as follows:
        <pre class="programlisting code"><code class="hljs-code">row_size = <span class="hljs-number">10</span>
col_size = <span class="hljs-number">10</span>
conv_size = <span class="hljs-number">2</span>
conv_stride_size = <span class="hljs-number">2</span>
maxpool_size = <span class="hljs-number">2</span>
maxpool_stride_size = <span class="hljs-number">1</span>
num_outputs = <span class="hljs-number">5</span> 
</code></pre>
      </li>
      <li class="numbered">Then we will initialize <a id="_idIndexMarker329"/>our input data layer. Since our data has a height and width already, we just need to expand it in two dimensions (a batch size of 1, and a channel size of 1) as follows:
        <pre class="programlisting code"><code class="hljs-code">x_input_2d = tf.keras.Input(dtype=tf.float32, shape=(row_size,col_size, <span class="hljs-number">1</span>), name="input_layer_2d")
</code></pre>
      </li>
      <li class="numbered">Just as in the one-dimensional example, we now need to add a 2D convolutional layer. For the filter, we will use a random 2x2 filter, a stride of 2 in both directions, and valid padding (in other words, no zero padding). Because our input matrix is 10x10, our convolutional output will be 5x5, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">my_convolution_output_2d = tf.keras.layers.Conv2D(kernel_size=(conv_size),
                                                  filters=conv_size,
                                                  strides=conv_stride_size,
                                                  padding="VALID",
                                                  name="convolution_layer_2d")(x_input_2d)
</code></pre>
      </li>
      <li class="numbered">Next, we add a ReLU activation layer, as follows:
        <pre class="programlisting code"><code class="hljs-code">my_activation_output_2d = tf.keras.layers.ReLU(name="activation_layer_2d")(my_convolution_output_2d)
</code></pre>
      </li>
      <li class="numbered">Our maxpool layer is very similar to the one-dimensional case, except we have to declare a width and height for the maxpool window and the stride. In our case, we will use the same value for all spatial dimensions so we will set integer values, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">my_maxpool_output_2d = tf.keras.layers.MaxPool2D(strides=maxpool_stride_size,
                                              pool_size=maxpool_size,
                                              padding='VALID',
                                              name="maxpool_layer_2d")(my_activation_output_2d)
</code></pre>
      </li>
      <li class="numbered">Our fully connected layer is very similar to the one-dimensional output. We use a dense layer, as follows:
        <pre class="programlisting code"><code class="hljs-code">my_full_output_2d = tf.keras.layers.Dense(units=num_outputs,
                                         
name="fully_connected_layer_2d")(my_maxpool_output_2d)
</code></pre>
      </li>
      <li class="numbered">Now we'll create the <a id="_idIndexMarker330"/>model, and print the output of each of the layers, as follows:
        <pre class="programlisting code"><code class="hljs-code">print('&gt;&gt;&gt;&gt; <span class="hljs-number">2</span>D Data &lt;&lt;&lt;&lt;')
model_2D = tf.keras.Model(inputs=x_input_2d, outputs=my_full_output_2d, name="model_2D")
model_2D.summary()
<span class="hljs-comment"># Input </span>
print('\n== input_layer ==')
print('Input = %s array' % (x_input_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
<span class="hljs-comment"># Convolution</span>
print('\n== convolution_layer ==')
print('%s Convolution, stride size = [%d, %d] , results <span class="hljs-keyword">in</span> the %s array' % 
      ([conv_size,conv_size],conv_stride_size,conv_stride_size,my_convolution_output_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
<span class="hljs-comment"># Activation</span>
print('\n== activation_layer ==')
print('Input = the above %s array' % (my_convolution_output_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
print('ReLU element wise returns the %s array' % (my_activation_output_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
<span class="hljs-comment"># Max Pool</span>
print('\n== maxpool_layer ==')
print('Input = the above %s array' % (my_activation_output_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
print('MaxPool, stride size = [%d, %d], results <span class="hljs-keyword">in</span> %s array' % 
      (maxpool_stride_size,maxpool_stride_size,my_maxpool_output_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
<span class="hljs-comment"># Fully Connected</span>
print('\n== fully_connected_layer ==')
print('Input = the above %s array' % (my_maxpool_output_2d.shape.as_list()[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]))
print('Fully connected layer on all %d rows results <span class="hljs-keyword">in</span> %s outputs' % 
      (my_maxpool_output_2d.shape.as_list()[<span class="hljs-number">1</span>],my_full_output_2d.shape.as_list()[<span class="hljs-number">3</span>]))
feed_dict = {x_input_2d: data_2d} 
</code></pre>
      </li>
      <li class="numbered">The preceding step <a id="_idIndexMarker331"/>should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">&gt;&gt;&gt;&gt; 2D Data &lt;&lt;&lt;&lt;
Model: "model_2D"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer_2d (InputLayer)  [(None, 10, 10, 1)]       0         
_________________________________________________________________
convolution_layer_2d (Conv2D (None, 5, 5, 2)           10        
_________________________________________________________________
activation_layer_2d (ReLU)   (None, 5, 5, 2)           0         
_________________________________________________________________
maxpool_layer_2d (MaxPooling (None, 4, 4, 2)           0         
_________________________________________________________________
fully_connected_layer_2d (De (None, 4, 4, 5)           15        
=================================================================
Total params: 25
Trainable params: 25
Non-trainable params: 0
_________________________________________________________________
== input_layer ==
Input = [10, 10] array
== convolution_layer ==
[2, 2] Convolution, stride size = [2, 2] , results in the [5, 5] array
== activation_layer ==
Input = the above [5, 5] array
ReLU element wise returns the [5, 5] array
== maxpool_layer ==
Input = the above [5, 5] array
MaxPool, stride size = [1, 1], results in [4, 4] array
== fully_connected_layer ==
Input = the above [4, 4] array
Fully connected layer on all 4 rows results in 5 outputs
</code></pre>
      </li>
    </ol>
    <h2 id="_idParaDest-183" class="title">How it works...</h2>
    <p class="normal">We should now <a id="_idIndexMarker332"/>know how to use the convolutional <a id="_idIndexMarker333"/>and maxpool layers in TensorFlow with one-dimensional and two-dimensional data. Regardless of the shape of the input, we ended up with outputs of the same size. This is important for illustrating the flexibility of neural network layers. This section should also impress upon us again the importance of shapes and sizes in neural network operations.</p>
    <h1 id="_idParaDest-184" class="title">Using a multilayer neural network</h1>
    <p class="normal">We will now apply our <a id="_idIndexMarker334"/>knowledge of different layers to real data by using a multilayer neural network on the low birth weight dataset.</p>
    <h2 id="_idParaDest-185" class="title">Getting ready</h2>
    <p class="normal">Now that we know how to create neural networks and work with layers, we will apply this methodology with the aim of predicting birth weights in the low birth weight dataset. We'll create a neural network with three hidden layers. The low birth weight dataset includes the actual birth weights and an indicator variable for whether the given birth weight is above or below 2,500 grams. In this example, we'll make the target the actual birth weight (regression) and then see what the accuracy is on the classification at the end. At the end, our model should be able to identify whether the birth weight will be &lt;2,500 grams.</p>
    <h2 id="_idParaDest-186" class="title">How to do it...</h2>
    <p class="normal">We proceed with the recipe as follows:</p>
    <ol>
      <li class="numbered" value="1">We will start by loading the libraries as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> os
</code></pre>
      </li>
      <li class="numbered">We'll now load the data from the website using the <code class="Code-In-Text--PACKT-">requests</code> module. After this, we will split the data into features of interest and the target value, shown as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># name of data file</span>
birth_weight_file = 'birth_weight.csv'
<span class="hljs-comment"># download data and create data file if file does not exist in current directory</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(birth_weight_file):
    birthdata_url = 'https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/blob/master/ch6/06_Using_Multiple_Layers/birth_weight.csv'
    birth_file = requests.get(birthdata_url)
    birth_data = birth_file.text.split('\r\n')
    birth_header = birth_data[<span class="hljs-number">0</span>].split('\t')
    birth_data = [[float(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> y.split('\t') <span class="hljs-keyword">if</span> 
                                        len(x)&gt;=<span class="hljs-number">1</span>] 
<span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> birth_data[<span class="hljs-number">1</span>:] <span class="hljs-keyword">if</span> len(y)&gt;=<span class="hljs-number">1</span>]
    <span class="hljs-keyword">with</span> open(birth_weight_file, "w") <span class="hljs-keyword">as</span> f:
        writer = csv.writer(f)
        writer.writerows([birth_header])
        writer.writerows(birth_data)
        f.close()
<span class="hljs-comment"># read birth weight data into memory</span>
birth_data = []
<span class="hljs-keyword">with</span> open(birth_weight_file, newline='') <span class="hljs-keyword">as</span> csvfile:
    csv_reader = csv.reader(csvfile)
    birth_header = next(csv_reader)
    <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> csv_reader:
        birth_data.append(row)
birth_data = [[float(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> row] <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> birth_data]
<span class="hljs-comment"># Extract y-target (birth weight)</span>
y_vals = np.array([x[<span class="hljs-number">8</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> birth_data])
<span class="hljs-comment"># Filter for features of interest</span>
cols_of_interest = ['AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI']
x_vals = np.array([[x[ix] <span class="hljs-keyword">for</span> ix, feature <span class="hljs-keyword">in</span> enumerate(birth_header) <span class="hljs-keyword">if</span> feature <span class="hljs-keyword">in</span> cols_of_interest] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> birth_data])
</code></pre>
      </li>
      <li class="numbered">To help with repeatability, we now need to set the random seed for both NumPy and TensorFlow. Then we declare our batch size as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># make results reproducible</span>
seed = <span class="hljs-number">3</span>
np.random.seed(seed)
tf.random.set_seed(seed)
<span class="hljs-comment"># set batch size for training</span>
batch_size = <span class="hljs-number">150</span>
</code></pre>
      </li>
      <li class="numbered">Next, we split the data into an 80-20 train-test split. After this, we need to normalize our input <a id="_idIndexMarker335"/>features so that they are between 0 and 1 with min-max scaling, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">train_indices = np.random.choice(len(x_vals), round(len(x_vals)*<span class="hljs-number">0.8</span>), replace=<span class="hljs-literal">False</span>)
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))
x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]
<span class="hljs-comment"># Record training column max and min for scaling of non-training data</span>
train_max = np.max(x_vals_train, axis=<span class="hljs-number">0</span>)
train_min = np.min(x_vals_train, axis=<span class="hljs-number">0</span>)
<span class="hljs-comment"># Normalize by column (min-max norm to be between 0 and 1)</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">normalize_cols</span><span class="hljs-function">(</span><span class="hljs-params">mat, max_vals, min_vals</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> (mat - min_vals) / (max_vals - min_vals)
x_vals_train = np.nan_to_num(normalize_cols(x_vals_train, train_max, train_min))
x_vals_test = np.nan_to_num(normalize_cols(x_vals_test, train_max, train_min)) 
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">Normalizing input features is a common feature transformation and is especially useful for neural networks. It will help with convergence if our data is centered between 0 and 1 for the activation functions.</p>
        </div>
      </li>
      <li class="numbered">Since we have multiple layers that have similar initialized variables, we now need to create a function to initialize both the weights and the bias. We do that with the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define Variable Functions (weights and bias)</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">init_weight</span><span class="hljs-function">(</span><span class="hljs-params">shape, st_dev</span><span class="hljs-function">):</span>
    weight = tf.Variable(tf.random.normal(shape, stddev=st_dev))
    <span class="hljs-keyword">return</span>(weight)
    
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">init_bias</span><span class="hljs-function">(</span><span class="hljs-params">shape, st_dev</span><span class="hljs-function">):</span>
    bias = tf.Variable(tf.random.normal(shape, stddev=st_dev))
    <span class="hljs-keyword">return</span>(bias)
</code></pre>
      </li>
      <li class="numbered">We now need to initialize our input data layer. There will be seven input features. The output will be the birth weight in grams:
        <pre class="programlisting code"><code class="hljs-code">x_data = tf.keras.Input(dtype=tf.float32, shape=(<span class="hljs-number">7</span>,)) 
</code></pre>
      </li>
      <li class="numbered">The fully connected layer will be used three times for all three hidden layers. To prevent repeated code, we will create a layer function for use when we initialize our model, shown <a id="_idIndexMarker336"/>as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create a fully connected layer:</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fully_connected</span><span class="hljs-function">(</span><span class="hljs-params">input_layer, weights, biases</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.nn.relu(tf.add(tf.matmul(x, weights), biases)))(input_layer) 
</code></pre>
      </li>
      <li class="numbered">Now it's time to create our model. For each layer (and output layer), we will initialize a weight matrix, bias matrix, and the fully connected layer. For this example, we will use hidden layers of sizes 25, 10, and 3:<div class="note">
          <p class="Information-Box--PACKT-">The model that we are using will have 522 variables to fit. To arrive at this number, we can see that between the data and the first hidden layer we have 7*25+25=200 variables. If we continue in this way and add them up, we'll have 200+260+33+4=497 variables. This is significantly larger than the nine variables that we used in the logistic regression model on this data.</p>
        </div>
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#--------Create the first layer (25 hidden nodes)--------</span>
weight_1 = init_weight(shape=[<span class="hljs-number">7</span>,<span class="hljs-number">25</span>], st_dev=<span class="hljs-number">5.0</span>)
bias_1 = init_bias(shape=[<span class="hljs-number">25</span>], st_dev=<span class="hljs-number">10.0</span>)
layer_1 = fully_connected(x_data, weight_1, bias_1)
<span class="hljs-comment">#--------Create second layer (10 hidden nodes)--------</span>
weight_2 = init_weight(shape=[<span class="hljs-number">25</span>, <span class="hljs-number">10</span>], st_dev=<span class="hljs-number">5.0</span>)
bias_2 = init_bias(shape=[<span class="hljs-number">10</span>], st_dev=<span class="hljs-number">10.0</span>)
layer_2 = fully_connected(layer_1, weight_2, bias_2)
<span class="hljs-comment">#--------Create third layer (3 hidden nodes)--------</span>
weight_3 = init_weight(shape=[<span class="hljs-number">10</span>, <span class="hljs-number">3</span>], st_dev=<span class="hljs-number">5.0</span>)
bias_3 = init_bias(shape=[<span class="hljs-number">3</span>], st_dev=<span class="hljs-number">10.0</span>)
layer_3 = fully_connected(layer_2, weight_3, bias_3)
<span class="hljs-comment">#--------Create output layer (1 output value)--------</span>
weight_4 = init_weight(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">1</span>], st_dev=<span class="hljs-number">5.0</span>)
bias_4 = init_bias(shape=[<span class="hljs-number">1</span>], st_dev=<span class="hljs-number">10.0</span>)
final_output = fully_connected(layer_3, weight_4, bias_4)
model = tf.keras.Model(inputs=x_data, outputs=final_output, name="multiple_layers_neural_network")
</code></pre>
      </li>
      <li class="numbered">We'll now declare our <a id="_idIndexMarker337"/>optimizer (using Adam optimization) and loop through our training iterations. We will use the L1 loss function (the absolute value). We'll also initialize two lists in which we can store our <code class="Code-In-Text--PACKT-">train</code> and <code class="Code-In-Text--PACKT-">test_loss</code> functions. In every loop, we also want to randomly select a batch from the training data for fitting to the model and print the status every 25 generations, shown as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Declare Adam optimizer</span>
optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">0.025</span>)
<span class="hljs-comment"># Training loop</span>
loss_vec = []
test_loss = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">200</span>):
    rand_index = np.random.choice(len(x_vals_train), size=batch_size)
    rand_x = x_vals_train[rand_index]
    rand_y = np.transpose([y_vals_train[rand_index]])
    
    <span class="hljs-comment"># Open a GradientTape.</span>
    <span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
   
        <span class="hljs-comment"># Forward pass.</span>
        output = model(rand_x)
        
        <span class="hljs-comment"># Apply loss function (MSE)</span>
        loss = tf.reduce_mean(tf.abs(rand_y - output))
        loss_vec.append(loss)       
        
    <span class="hljs-comment"># Get gradients of loss with reference to the weights and bias variables to adjust.</span>
    gradients_w1 = tape.gradient(loss, weight_1)
    gradients_b1 = tape.gradient(loss, bias_1)
    gradients_w2 = tape.gradient(loss, weight_2)
    gradients_b2 = tape.gradient(loss, bias_2)
    gradients_w3 = tape.gradient(loss, weight_3)
    gradients_b3 = tape.gradient(loss, bias_3)
    gradients_w4 = tape.gradient(loss, weight_4)
    gradients_b4 = tape.gradient(loss, bias_4)
    
    <span class="hljs-comment"># Update the weights and bias variables of the model.</span>
    optimizer.apply_gradients(zip([gradients_w1, gradients_b1, gradients_w2, gradients_b2,
                                  gradients_w3, gradients_b3, gradients_w4, gradients_b4], 
                                  [weight_1, bias_1, weight_2, bias_2, weight_3, bias_3, weight_4, bias_4]))
    
    <span class="hljs-comment"># Forward pass.</span>
    output_test = model(x_vals_test)
    <span class="hljs-comment"># Apply loss function (MSE) on test</span>
    temp_loss = tf.reduce_mean(tf.abs(np.transpose([y_vals_test]) - output_test))
    test_loss.append(temp_loss)
    
    <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">25</span> == <span class="hljs-number">0</span>:
        print('Generation: ' + str(i+<span class="hljs-number">1</span>) + '. Loss = ' + str(loss.numpy()))
        
</code></pre>
      </li>
      <li class="numbered">The preceding <a id="_idIndexMarker338"/>step should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">Generation: 25. Loss = 1921.8002
Generation: 50. Loss = 1453.3898
Generation: 75. Loss = 987.57074
Generation: 100. Loss = 709.81696
Generation: 125. Loss = 508.625
Generation: 150. Loss = 541.36774
Generation: 175. Loss = 539.6093
Generation: 200. Loss = 441.64032 
</code></pre>
      </li>
      <li class="numbered">The following is a snippet of code that plots the train and test loss with <code class="Code-In-Text--PACKT-">matplotlib</code>:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(loss_vec, 'k-', label='Train Loss') 
plt.plot(test_loss, 'r--', label='Test Loss') 
plt.title('Loss per Generation') 
plt.xlabel('Generation') 
plt.ylabel('Loss') 
plt.legend(loc='upper right') 
plt.show() 
</code></pre>
        <p class="bullet-para">We proceed with the <a id="_idIndexMarker339"/>recipe by plotting the following diagram:</p>
        <figure class="mediaobject"> <img src="../Images/B16254_06_09.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/AE276A4A.tmp"/></figure>
        <p class="packt_figref">Figure 6.5: In the preceding figure, we plot the train and test losses for our neural network that we trained to predict birth weight in grams. Notice that we have arrived at a good model after approximately 30 generations</p>
      </li>
      <li class="numbered">Now, we need to output the train and test regression results and turn them into classification results by creating an indicator for if they are above or below 2,500 grams. To find out the model's accuracy, we need to use the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Model Accuracy</span>
actuals = np.array([x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> birth_data])
test_actuals = actuals[test_indices]
train_actuals = actuals[train_indices]
test_preds = model(x_vals_test)
train_preds = model(x_vals_train)
test_preds = np.array([<span class="hljs-number">1.0</span> <span class="hljs-keyword">if</span> x &lt; <span class="hljs-number">2500.0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> test_preds])
train_preds = np.array([<span class="hljs-number">1.0</span> <span class="hljs-keyword">if</span> x &lt; <span class="hljs-number">2500.0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_preds])
<span class="hljs-comment"># Print out accuracies</span>
test_acc = np.mean([x == y <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(test_preds, test_actuals)])
train_acc = np.mean([x == y <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(train_preds, train_actuals)])
print('On predicting the category of low birthweight <span class="hljs-keyword">from</span> regression output (&lt;<span class="hljs-number">2500</span>g):')
print('Test Accuracy: {}'.format(test_acc))
print('Train Accuracy: {}'.format(train_acc))
</code></pre>
      </li>
      <li class="numbered">The preceding step should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">Test Accuracy: 0.7631578947368421
Train Accuracy: 0.7880794701986755 
</code></pre>
      </li>
    </ol>
    <p class="normal">As you can see, both the <a id="_idIndexMarker340"/>train set accuracy and the test set accuracy are quite good and the models learn without under- or overfitting.</p>
    <h2 id="_idParaDest-187" class="title">How it works...</h2>
    <p class="normal">In this recipe, we <a id="_idIndexMarker341"/>created a regression neural network with three fully connected hidden layers to predict the birth weight of the low birth weight dataset. In the next recipe, we will try to improve our logistic regression by making it a multiple-layer, logistic-type neural network.</p>
    <h1 id="_idParaDest-188" class="title">Improving the predictions of linear models</h1>
    <p class="normal">In this recipe, we <a id="_idIndexMarker342"/>will attempt to improve our logistic <a id="_idIndexMarker343"/>model by increasing the accuracy of the low birth weight prediction. We will use a neural network.</p>
    <h2 id="_idParaDest-189" class="title">Getting ready</h2>
    <p class="normal">For this recipe, we will load the low birth weight data and use a neural network with two hidden fully connected layers with sigmoid activations to fit the probability of a low birth weight.</p>
    <h2 id="_idParaDest-190" class="title">How to do it...</h2>
    <p class="normal">We proceed with the recipe as follows:</p>
    <ol>
      <li class="numbered" value="1">We start by loading the libraries and initializing our computational graph as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt 
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np 
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf 
<span class="hljs-keyword">import</span> requests 
<span class="hljs-keyword">import</span> os.path
<span class="hljs-keyword">import</span> csv 
</code></pre>
      </li>
      <li class="numbered">Next, we load, extract, and normalize our data as in the preceding recipe, except that here we are going to be using the low birth weight indicator variable as our target instead of the actual birth weight, shown as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Name of data file</span>
birth_weight_file = 'birth_weight.csv'
birthdata_url = 'https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/blob/master/ch6/06_Using_Multiple_Layers/birth_weight.csv'
<span class="hljs-comment"># Download data and create data file if file does not exist in current directory</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(birth_weight_file):
    birth_file = requests.get(birthdata_url)
    birth_data = birth_file.text.split('\r\n')
    birth_header = birth_data[<span class="hljs-number">0</span>].split('\t')
    birth_data = [[float(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> y.split('\t') <span class="hljs-keyword">if</span> len(x) &gt;= <span class="hljs-number">1</span>]
                  <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> birth_data[<span class="hljs-number">1</span>:] <span class="hljs-keyword">if</span> len(y) &gt;= <span class="hljs-number">1</span>]
    <span class="hljs-keyword">with</span> open(birth_weight_file, "w") <span class="hljs-keyword">as</span> f:
        writer = csv.writer(f)
        writer.writerows([birth_header])
        writer.writerows(birth_data) 
<span class="hljs-comment"># read birth weight data into memory</span>
birth_data = []
<span class="hljs-keyword">with</span> open(birth_weight_file, newline='') <span class="hljs-keyword">as</span> csvfile:
    csv_reader = csv.reader(csvfile)
    birth_header = next(csv_reader)
    <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> csv_reader:
        birth_data.append(row)
birth_data = [[float(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> row] <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> birth_data]
<span class="hljs-comment"># Pull out target variable</span>
y_vals = np.array([x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> birth_data])
<span class="hljs-comment"># Pull out predictor variables (not id, not target, and not birthweight)</span>
x_vals = np.array([x[<span class="hljs-number">1</span>:<span class="hljs-number">8</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> birth_data])
 
train_indices = np.random.choice(len(x_vals), round(len(x_vals)*<span class="hljs-number">0.8</span>), replace=<span class="hljs-literal">False</span>) 
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices))) 
x_vals_train = x_vals[train_indices] 
x_vals_test = x_vals[test_indices] 
y_vals_train = y_vals[train_indices] 
y_vals_test = y_vals[test_indices] 
 
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">normalize_cols</span><span class="hljs-function">(</span><span class="hljs-params">m, col_min=np.array([None]), col_max=np.array([None])</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> col_min[<span class="hljs-number">0</span>]:
        col_min = m.min(axis=<span class="hljs-number">0</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> col_max[<span class="hljs-number">0</span>]:
        col_max = m.max(axis=<span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> (m - col_min) / (col_max - col_min), col_min, col_max
x_vals_train, train_min, train_max = np.nan_to_num(normalize_cols(x_vals_train))
x_vals_test, _, _ = np.nan_to_num(normalize_cols(x_vals_test, train_min, train_max))
</code></pre>
      </li>
      <li class="numbered">Next, we need to <a id="_idIndexMarker344"/>declare our batch size, our seed in <a id="_idIndexMarker345"/>order to have reproductible results, and our input data layer as follows:
        <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">90</span> 
seed = <span class="hljs-number">98</span>
np.random.seed(seed)
tf.random.set_seed(seed)
x_data = tf.keras.Input(dtype=tf.float64, shape=(<span class="hljs-number">7</span>,))
</code></pre>
      </li>
      <li class="numbered">As previously, we now need to declare functions that initialize a variable and a layer in our model. To create a better logistic function, we need to create a function that returns a logistic layer on an input layer. In other words, we will just use a fully connected layer and return a sigmoid element for each layer. It is important to remember that our loss function will have the final sigmoid included, so we want to specify on our last layer that we will not return the sigmoid of the output, shown as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create variable definition</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">init_variable</span><span class="hljs-function">(</span><span class="hljs-params">shape</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span>(tf.Variable(tf.random.normal(shape=shape, dtype="float64", seed=seed)))
<span class="hljs-comment"># Create a logistic layer definition</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">logistic</span><span class="hljs-function">(</span><span class="hljs-params">input_layer, multiplication_weight, bias_weight, activation = True</span><span class="hljs-function">):</span>
    
    <span class="hljs-comment"># We separate the activation at the end because the loss function will</span>
    <span class="hljs-comment"># implement the last sigmoid necessary</span>
    <span class="hljs-keyword">if</span> activation:
        <span class="hljs-keyword">return</span> tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.nn.sigmoid(tf.add(tf.matmul(x, multiplication_weight), bias_weight)))(input_layer)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.add(tf.matmul(x, multiplication_weight), bias_weight))(input_layer)
</code></pre>
      </li>
      <li class="numbered">Now we will declare three layers (two hidden layers and an output layer). We will start by initializing a <a id="_idIndexMarker346"/>weight and bias matrix for each layer and defining the layer operations as <a id="_idIndexMarker347"/>follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># First logistic layer (7 inputs to 14 hidden nodes)</span>
A1 = init_variable(shape=[<span class="hljs-number">7</span>,<span class="hljs-number">14</span>])
b1 = init_variable(shape=[<span class="hljs-number">14</span>])
logistic_layer1 = logistic(x_data, A1, b1)
<span class="hljs-comment"># Second logistic layer (14 hidden inputs to 5 hidden nodes)</span>
A2 = init_variable(shape=[<span class="hljs-number">14</span>,<span class="hljs-number">5</span>])
b2 = init_variable(shape=[<span class="hljs-number">5</span>])
logistic_layer2 = logistic(logistic_layer1, A2, b2)
<span class="hljs-comment"># Final output layer (5 hidden nodes to 1 output)</span>
A3 = init_variable(shape=[<span class="hljs-number">5</span>,<span class="hljs-number">1</span>])
b3 = init_variable(shape=[<span class="hljs-number">1</span>])
final_output = logistic(logistic_layer2, A3, b3, activation=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># Build the model</span>
model = tf.keras.Model(inputs=x_data, outputs=final_output, name="improving_linear_reg_neural_network")
</code></pre>
      </li>
      <li class="numbered">Next, we define a loss function (cross-entropy) and declare the optimization algorithm, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Loss function (Cross Entropy loss)</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">cross_entropy</span><span class="hljs-function">(</span><span class="hljs-params">final_output, y_target</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=final_output, labels=y_target)) 
<span class="hljs-comment"># Declare optimizer</span>
optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">0.002</span>)
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">Cross-entropy is a way of measuring distances between probabilities. Here, we want to measure the difference between certainty (0 or 1) and our model probability (0 &lt; x &lt; 1). TensorFlow implements cross-entropy with the built-in sigmoid function. This is also important as part of the hyperparameter tuning, as we are more likely to find the best loss function, learning rate, and optimization algorithm for the problem at hand. For brevity in this recipe, we do not include hyperparameter tuning.</p>
        </div>
      </li>
      <li class="numbered">In order to evaluate and compare our model to previous models, we need to create a prediction and accuracy operation on the graph. This will allow us to feed in the whole test set and determine the accuracy, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Accuracy</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">compute_accuracy</span><span class="hljs-function">(</span><span class="hljs-params">final_output, y_target</span><span class="hljs-function">):</span>
    prediction = tf.round(tf.nn.sigmoid(final_output))
    predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)
    <span class="hljs-keyword">return</span> tf.reduce_mean(predictions_correct)
</code></pre>
      </li>
      <li class="numbered">We are now ready to <a id="_idIndexMarker348"/>start our training loop. We will train for 1,500 generations and save the model <a id="_idIndexMarker349"/>loss and train and test accuracies for plotting later. Our training loop is started with the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Training loop</span>
loss_vec = []
train_acc = []
test_acc = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1500</span>):
    rand_index = np.random.choice(len(x_vals_train), size=batch_size)
    rand_x = x_vals_train[rand_index]
    rand_y = np.transpose([y_vals_train[rand_index]])
    
     <span class="hljs-comment"># Open a GradientTape.</span>
    <span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
   
        <span class="hljs-comment"># Forward pass.</span>
        output = model(rand_x)
        
        <span class="hljs-comment"># Apply loss function (Cross Entropy loss)</span>
        loss = cross_entropy(output, rand_y)
        loss_vec.append(loss)
    <span class="hljs-comment"># Get gradients of loss with reference to the weights and bias variables to adjust.</span>
    gradients_A1 = tape.gradient(loss, A1)
    gradients_b1 = tape.gradient(loss, b1)
    gradients_A2 = tape.gradient(loss, A2)
    gradients_b2 = tape.gradient(loss, b2)
    gradients_A3 = tape.gradient(loss, A3)
    gradients_b3 = tape.gradient(loss, b3)
    
    <span class="hljs-comment"># Update the weights and bias variables of the model.</span>
    optimizer.apply_gradients(zip([gradients_A1, gradients_b1,gradients_A2, gradients_b2, gradients_A3, gradients_b3], 
                                  [A1, b1, A2, b2, A3, b3]))
    
    temp_acc_train = compute_accuracy(model(x_vals_train), np.transpose([y_vals_train]))
    train_acc.append(temp_acc_train)
    
    temp_acc_test = compute_accuracy(model(x_vals_test), np.transpose([y_vals_test]))
    test_acc.append(temp_acc_test)
    
    <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>)%<span class="hljs-number">150</span>==<span class="hljs-number">0</span>:
        print('Loss = ' + str(loss.numpy()))
        
</code></pre>
      </li>
      <li class="numbered">The preceding <a id="_idIndexMarker350"/>step should result in the <a id="_idIndexMarker351"/>following output:
        <pre class="programlisting code"><code class="hljs-code">Loss = 0.5885411040188063
Loss = 0.581099555117532
Loss = 0.6071769535895101
Loss = 0.5043174136225906
Loss = 0.5023625777095964
Loss = 0.485112570717733
Loss = 0.5906992621835641
Loss = 0.4280814147901789
Loss = 0.5425164697605331
Loss = 0.35608561907724867
</code></pre>
      </li>
      <li class="numbered">The following code blocks illustrate how to plot the cross-entropy loss and train and test set accuracies with <code class="Code-In-Text--PACKT-">matplotlib</code>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Plot loss over time </span>
plt.plot(loss_vec, 'k-') 
plt.title('Cross Entropy Loss per Generation') 
plt.xlabel('Generation') 
plt.ylabel('Cross Entropy Loss') 
plt.show() 
<span class="hljs-comment"># Plot train and test accuracy </span>
plt.plot(train_acc, 'k-', label='Train Set Accuracy') 
plt.plot(test_acc, 'r--', label='Test Set Accuracy') 
plt.title('Train <span class="hljs-keyword">and</span> Test Accuracy') 
plt.xlabel('Generation') 
plt.ylabel('Accuracy') 
plt.legend(loc='lower right') 
plt.show()
</code></pre>
        <p class="bullet-para">We get <a id="_idIndexMarker352"/>the plot for cross-entropy loss per generation <a id="_idIndexMarker353"/>as follows:</p>
        <figure class="mediaobject"><img src="../Images/B16254_06_10.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/E3D23CF0.tmp"/></figure>
      </li>
    </ol>
    <p class="packt_figref">Figure 6.6: Training loss over 1,500 iterations</p>
    <p class="normal">Within <a id="_idIndexMarker354"/>approximately 150 generations, we have reached a good model. As we <a id="_idIndexMarker355"/>continue to train, we can see that very little is gained over the remaining iterations, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_11.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/D849E07E.tmp"/></figure>
    <p class="packt_figref">Figure 6.7: Accuracy for the train set and test set</p>
    <p class="normal">As you can see in the preceding diagram, we arrived at a good model very quickly.</p>
    <h2 id="_idParaDest-191" class="title">How it works...</h2>
    <p class="normal">When considering <a id="_idIndexMarker356"/>using neural networks to model data, you have to <a id="_idIndexMarker357"/>consider the advantages and disadvantages. While our model has converged faster than previous models, and perhaps with greater accuracy, this comes with a price; we are training many more model variables and there is a greater chance of overfitting. To check if overfitting is occurring, we look at the accuracy of the test and train sets. If the accuracy of the training set continues to increase while the accuracy on the test set stays the same or even decreases slightly, we can assume overfitting is occurring.</p>
    <p class="normal">To combat underfitting, we can increase our model depth or train the model for more iterations. To address overfitting, we can add more data or add regularization techniques to our model.</p>
    <p class="normal">It is also important to note that our model variables are not as interpretable as a linear model. Neural network models have coefficients that are harder to interpret than linear models, as they explain the significance of features within the model.</p>
    <h1 id="_idParaDest-192" class="title">Learning to play Tic-Tac-Toe</h1>
    <p class="normal">To show how adaptable <a id="_idIndexMarker358"/>neural networks can be, we will now attempt to use a neural network in order to learn the optimal moves for Tic-Tac-Toe. We will approach this knowing that Tic-Tac-Toe is a deterministic game and that the optimal moves are already known.</p>
    <h2 id="_idParaDest-193" class="title">Getting ready</h2>
    <p class="normal">To train our model, we will use a list of board positions followed by the optimal response for a number of different boards. We can reduce the amount of boards to train on by considering only board positions that are different with regard to symmetry. The non-identity transformations of a Tic-Tac-Toe board are a rotation (in either direction) of 90 degrees, 180 degrees, and 270 degrees, a horizontal reflection, and a vertical reflection. Given this idea, we will use a shortlist of boards with the optimal move, apply two random transformations, and then feed that into our neural network for learning.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Since Tic-Tac-Toe is a deterministic game, it is worth noting that whoever goes first should either win or draw. We will hope for a model that can respond to our moves optimally and ultimately result in a draw.</p>
    </div>
    <p class="normal">If we denote Xs using 1, Os using -1, and empty spaces using 0, then the following diagram illustrates how we can consider a board position and an optimal move as a row of data:</p>
    <figure class="mediaobject"><img src="../Images/B16254_06_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.8: Here, we illustrate how to consider a board and an optimal move as a row of data. Note that X = 1, O = -1, empty spaces are 0, and we start indexing at 0</p>
    <p class="normal">In addition to the <a id="_idIndexMarker359"/>model loss, to check how our model is performing we will do two things. The first check we will perform is to remove a position and an optimal move row from our training set. This will allow us to see if the neural network model can generalize a move it hasn't seen before. The second way to evaluate our model is to actually play a game against it at the end.</p>
    <p class="normal">The list of possible boards and optimal moves can be found in the GitHub directory for this recipe at <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Lea"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Learning_Tic_Tac_Toe</span></a> and in the Packt repository at <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a>.</p>
    <h2 id="_idParaDest-194" class="title">How to do it...</h2>
    <p class="normal">We proceed with the recipe as follows:</p>
    <ol>
      <li class="numbered" value="1">We need to start by loading the necessary libraries for this script, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> random 
</code></pre>
      </li>
      <li class="numbered">Next, we declare the <a id="_idIndexMarker360"/>following batch size for training our model:
        <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">50</span> 
</code></pre>
      </li>
      <li class="numbered">To make visualizing the boards a bit easier, we will create a function that outputs Tic-Tac-Toe boards with Xs and Os. This is done with the following code:
        <pre class="programlisting code"><code class="hljs-code"> <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">print_board</span><span class="hljs-function">(</span><span class="hljs-params">board</span><span class="hljs-function">):</span>
    symbols = ['O', ' ', 'X']
    board_plus1 = [int(x) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> board]
    board_line1 = ' {} | {} | 
                           {}'.format(symbols[board_plus1[<span class="hljs-number">0</span>]],
                                      symbols[board_plus1[<span class="hljs-number">1</span>]],
                                      symbols[board_plus1[<span class="hljs-number">2</span>]])
    board_line2 = ' {} | {} | 
                           {}'.format(symbols[board_plus1[<span class="hljs-number">3</span>]],
                                      symbols[board_plus1[<span class="hljs-number">4</span>]],
                                      symbols[board_plus1[<span class="hljs-number">5</span>]])
    board_line3 = ' {} | {} | 
                           {}'.format(symbols[board_plus1[<span class="hljs-number">6</span>]],
                                      symbols[board_plus1[<span class="hljs-number">7</span>]],
                                      symbols[board_plus1[<span class="hljs-number">8</span>]])
    print(board_line1)
    print('___________')
    print(board_line2)
    print('___________')
    print(board_line3)
</code></pre>
      </li>
      <li class="numbered">Now we have to create a function that will return a new board and an optimal response position under a <a id="_idIndexMarker361"/>transformation. This is done with the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_symmetry</span><span class="hljs-function">(</span><span class="hljs-params">board, response, transformation</span><span class="hljs-function">):</span> 
    <span class="hljs-string">''' </span>
<span class="hljs-string">    :param board: list of integers 9 long: </span>
<span class="hljs-string">     opposing mark = -1 </span>
<span class="hljs-string">     friendly mark = 1 </span>
<span class="hljs-string">     empty space = 0 </span>
<span class="hljs-string">    :param transformation: one of five transformations on a </span>
<span class="hljs-string">                                            board: </span>
<span class="hljs-string">     rotate180, rotate90, rotate270, flip_v, flip_h </span>
<span class="hljs-string">    :return: tuple: (new_board, new_response) </span>
<span class="hljs-string">    '''</span> 
 
    <span class="hljs-keyword">if</span> transformation == <span class="hljs-string">'rotate180'</span>: 
        new_response = <span class="hljs-number">8</span> - response 
        <span class="hljs-keyword">return</span> board[::<span class="hljs-number">-1</span>], new_response 
 
    <span class="hljs-keyword">elif</span> transformation == <span class="hljs-string">'rotate90'</span>: 
        new_response = [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">8</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>].index(response) 
        tuple_board = list(zip(*[board[<span class="hljs-number">6</span>:<span class="hljs-number">9</span>], board[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>], board[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>]])) 
        <span class="hljs-keyword">return</span> [value <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> tuple_board <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> item], new_response 
 
    <span class="hljs-keyword">elif</span> transformation == <span class="hljs-string">'rotate270'</span>: 
        new_response = [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>].index(response) 
        tuple_board = list(zip(*[board[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>], board[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>], board[<span class="hljs-number">6</span>:<span class="hljs-number">9</span>]]))[::<span class="hljs-number">-1</span>] 
        <span class="hljs-keyword">return</span> [value <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> tuple_board <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> item], new_response 
 
    <span class="hljs-keyword">elif</span> transformation == <span class="hljs-string">'flip_v'</span>: 
        new_response = [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>].index(response) 
        <span class="hljs-keyword">return</span> board[<span class="hljs-number">6</span>:<span class="hljs-number">9</span>] +  board[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] + board[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>], new_response 
 
    <span class="hljs-keyword">elif</span> transformation == <span class="hljs-string">'flip_h'</span>: 
    <span class="hljs-comment"># flip_h = rotate180, then flip_v </span>
        new_response = [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>].index(response) 
        new_board = board[::<span class="hljs-number">-1</span>] 
        <span class="hljs-keyword">return</span> new_board[<span class="hljs-number">6</span>:<span class="hljs-number">9</span>] +  new_board[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] + new_board[<span class="hljs-number">0</span>:<span class="hljs-number">3</span>], new_response 
 
    <span class="hljs-keyword">else</span>: 
        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Method not implmented.'</span>) 
</code></pre>
      </li>
      <li class="numbered">The list of boards and their optimal responses is in a <code class="Code-In-Text--PACKT-">.csv</code> file in the directory available in the GitHub repository at <a href="https://github.com/nfmcclure/tensorflow_cookbook"><span class="url">https://github.com/nfmcclure/tensorflow_cookbook</span></a> or the Packt repository at <a href="https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition"><span class="url">https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition</span></a>. We <a id="_idIndexMarker362"/>will create a function that will load the file with the boards and responses and will store it as a list of tuples, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_moves_from_csv</span><span class="hljs-function">(</span><span class="hljs-params">csv_file</span><span class="hljs-function">):</span> 
    <span class="hljs-string">''' </span>
<span class="hljs-string">    :param csv_file: csv file location containing the boards w/ responses </span>
<span class="hljs-string">    :return: moves: list of moves with index of best response </span>
<span class="hljs-string">    '''</span> 
    moves = [] 
    <span class="hljs-keyword">with</span> open(csv_file, <span class="hljs-string">'rt'</span>) <span class="hljs-keyword">as</span> csvfile: 
        reader = csv.reader(csvfile, delimiter=<span class="hljs-string">','</span>) 
        <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> reader: 
            moves.append(([int(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span>                              row[<span class="hljs-number">0</span>:<span class="hljs-number">9</span>]],int(row[<span class="hljs-number">9</span>]))) 
    <span class="hljs-keyword">return</span> moves 
</code></pre>
      </li>
      <li class="numbered">Now we need to tie everything together to create a function that will return a randomly transformed board and response. This is done with the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_rand_move</span><span class="hljs-function">(</span><span class="hljs-params">moves, rand_transforms=</span><span class="hljs-number">2</span><span class="hljs-function">):</span> 
    <span class="hljs-comment"># This function performs random transformations on a board. </span>
    (board, response) = random.choice(moves) 
    possible_transforms = ['rotate90', 'rotate180', 'rotate270', 'flip_v', 'flip_h'] 
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(rand_transforms): 
        random_transform = random.choice(possible_transforms) 
        (board, response) = get_symmetry(board, response, random_transform) 
    <span class="hljs-keyword">return</span> board, response 
</code></pre>
      </li>
      <li class="numbered">Next, we load our data and create a training set as follows:
        <pre class="programlisting code"><code class="hljs-code">moves = get_moves_from_csv('base_tic_tac_toe_moves.csv') 
<span class="hljs-comment"># Create a train set: </span>
train_length = <span class="hljs-number">500</span> 
train_set = [] 
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(train_length): 
    train_set.append(get_rand_move(moves)) 
</code></pre>
      </li>
      <li class="numbered">Remember that we <a id="_idIndexMarker363"/>want to remove one board and an optimal response from our training set to see if the model can generalize making the best move. The best move for the following board will be to play at index number 6:
        <pre class="programlisting code"><code class="hljs-code">test_board = [<span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>] 
train_set = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_set <span class="hljs-keyword">if</span> x[<span class="hljs-number">0</span>] != test_board] 
</code></pre>
      </li>
      <li class="numbered">We can now initialize the weights and bias and create our models:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">init_weights</span><span class="hljs-function">(</span><span class="hljs-params">shape</span><span class="hljs-function">):</span> 
    <span class="hljs-keyword">return</span> tf.Variable(tf.random_normal(shape)) 
A1 = init_weights([<span class="hljs-number">9</span>, <span class="hljs-number">81</span>])
bias1 = init_weights([<span class="hljs-number">81</span>])
A2 = init_weights([<span class="hljs-number">81</span>, <span class="hljs-number">9</span>])
bias2 = init_weights([<span class="hljs-number">9</span>])
</code></pre>
      </li>
      <li class="numbered">Now, we create our model. Note that we do not include the <code class="Code-In-Text--PACKT-">softmax()</code> activation function in the following model because it is included in the loss function:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Initialize input data</span>
X = tf.keras.Input(dtype=tf.float32, batch_input_shape=[<span class="hljs-literal">None</span>, <span class="hljs-number">9</span>])
hidden_output = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.nn.sigmoid(tf.add(tf.matmul(x, A1), bias1)))(X)
final_output = tf.keras.layers.Lambda(<span class="hljs-keyword">lambda</span> x: tf.add(tf.matmul(x, A2), bias2))(hidden_output)
model = tf.keras.Model(inputs=X, outputs=final_output, name="tic_tac_toe_neural_network")
</code></pre>
      </li>
      <li class="numbered">Next, we will declare our optimizer, as follows:
        <pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.SGD(<span class="hljs-number">0.025</span>)
 
</code></pre>
      </li>
      <li class="numbered">We can now loop through the training of our neural network with the following code. Note that our <code class="Code-In-Text--PACKT-">loss</code> function <a id="_idIndexMarker364"/>will be the average softmax of the final output logits (unstandardized output):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Initialize variables </span>
loss_vec = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10000</span>):
    rand_indices = np.random.choice(range(len(train_set)), batch_size, replace=<span class="hljs-literal">False</span>)
    batch_data = [train_set[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> rand_indices]
    x_input = [x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> batch_data]
    y_target = np.array([y[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> batch_data])
    <span class="hljs-comment"># Open a GradientTape.</span>
    <span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
        <span class="hljs-comment"># Forward pass.</span>
        output = model(np.array(x_input, dtype=float))
        <span class="hljs-comment"># Apply loss function (Cross Entropy loss)</span>
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y_target))
        loss_vec.append(loss)
    <span class="hljs-comment"># Get gradients of loss with reference to the weights and bias variables to adjust.</span>
    gradients_A1 = tape.gradient(loss, A1)
    gradients_b1 = tape.gradient(loss, bias1)
    gradients_A2 = tape.gradient(loss, A2)
    gradients_b2 = tape.gradient(loss, bias2)
    <span class="hljs-comment"># Update the weights and bias variables of the model.</span>
    optimizer.apply_gradients(zip([gradients_A1, gradients_b1, gradients_A2, gradients_b2],
                                  [A1, bias1, A2, bias2]))
    <span class="hljs-keyword">if</span> i % <span class="hljs-number">500</span> == <span class="hljs-number">0</span>:
        print('Iteration: {}, Loss: {}'.format(i, loss)) 
</code></pre>
      </li>
      <li class="numbered">The following is the code needed to plot the loss over the model training:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(loss_vec, 'k-', label='Loss') 
plt.title('Loss (MSE) per Generation') 
plt.xlabel('Generation') 
plt.ylabel('Loss') 
plt.show() 
</code></pre>
        <p class="bullet-para">We should get the <a id="_idIndexMarker365"/>following plot for the loss per generation:</p>
        <figure class="mediaobject"> <img src="../Images/B16254_06_13.png" alt="/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/2B562542.tmp"/></figure>
        <p class="packt_figref">Figure 6.9: A Tic-Tac-Toe train set loss over 10,000 iterations</p>
        <p class="bullet-para">In the preceding diagram, we have plotted the loss over the training steps.</p>
      </li>
      <li class="numbered">To test the model, we need to see how it performs on the test board that we removed from the training set. We are hoping that the model can generalize and predict the optimal index for moving, which will be index number 6. Most of the time the model will succeed, shown as follows:
        <pre class="programlisting code"><code class="hljs-code">test_boards = [test_board] 
logits = model.predict(test_boards)
predictions = tf.argmax(logits, <span class="hljs-number">1</span>)
print(predictions)
</code></pre>
      </li>
      <li class="numbered">The preceding step should result in the following output:
        <pre class="programlisting code"><code class="hljs-code">[6] 
</code></pre>
      </li>
      <li class="numbered">In order to evaluate our model, we need to play against our trained model. To do this, we have to create a <a id="_idIndexMarker366"/>function that will check for a win. This way, our program will know when to stop asking for more moves. This is done with the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">check</span><span class="hljs-function">(</span><span class="hljs-params">board</span><span class="hljs-function">):</span> 
    wins = [[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">6</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">7</span>], [<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>]] 
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(wins)): 
        <span class="hljs-keyword">if</span> board[wins[i][<span class="hljs-number">0</span>]]==board[wins[i][<span class="hljs-number">1</span>]]==board[wins[i][<span class="hljs-number">2</span>]]==<span class="hljs-number">1.</span>: 
            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> 
        <span class="hljs-keyword">elif</span> board[wins[i][<span class="hljs-number">0</span>]]==board[wins[i][<span class="hljs-number">1</span>]]==board[wins[i][<span class="hljs-number">2</span>]]==<span class="hljs-number">-1.</span>: 
            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> 
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> 
</code></pre>
      </li>
      <li class="numbered">Now we can loop through and play a game with our model. We start with a blank board (all zeros), we ask the user to input an index (0-8) of where to play, and we then feed that into the model for a prediction. For the model's move, we take the largest available prediction that is also an open space. From this game, we can see that our model is not perfect, as follows:
        <pre class="programlisting code"><code class="hljs-code">game_tracker = [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]
win_logical = <span class="hljs-literal">False</span>
num_moves = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> win_logical:
    player_index = input('Input index of your move (<span class="hljs-number">0-8</span>): ')
    num_moves += <span class="hljs-number">1</span>
    <span class="hljs-comment"># Add player move to game</span>
    game_tracker[int(player_index)] = <span class="hljs-number">1.</span>
    
    <span class="hljs-comment"># Get model's move by first getting all the logits for each index</span>
    [potential_moves] = model(np.array([game_tracker], dtype=float))
    <span class="hljs-comment"># Now find allowed moves (where game tracker values = 0.0)</span>
    allowed_moves = [ix <span class="hljs-keyword">for</span> ix, x <span class="hljs-keyword">in</span> enumerate(game_tracker) <span class="hljs-keyword">if</span> x == <span class="hljs-number">0.0</span>]
    <span class="hljs-comment"># Find best move by taking argmax of logits if they are in allowed moves</span>
    model_move = np.argmax([x <span class="hljs-keyword">if</span> ix <span class="hljs-keyword">in</span> allowed_moves <span class="hljs-keyword">else</span> <span class="hljs-number">-999.0</span> <span class="hljs-keyword">for</span> ix, x <span class="hljs-keyword">in</span> enumerate(potential_moves)])
    
    <span class="hljs-comment"># Add model move to game</span>
    game_tracker[int(model_move)] = <span class="hljs-number">-1.</span>
    print('Model has moved')
    print_board(game_tracker)
    <span class="hljs-comment"># Now check for win or too many moves</span>
    <span class="hljs-keyword">if</span> check(game_tracker) == <span class="hljs-number">-1</span> <span class="hljs-keyword">or</span> num_moves &gt;= <span class="hljs-number">5</span>:
        print('Game Over!')
        win_logical = <span class="hljs-literal">True</span>
    <span class="hljs-keyword">elif</span> check(game_tracker) == <span class="hljs-number">1</span>:
        print('Congratulations, You won!')
        win_logical = <span class="hljs-literal">True</span>
</code></pre>
      </li>
      <li class="numbered">The preceding step <a id="_idIndexMarker367"/>should result in the following interactive output:
        <pre class="programlisting code"><code class="hljs-code">Input index of your move (0-8):  4
Model has moved
   |   |  
___________
   | X |  
___________
   |   | O
Input index of your move (0-8):  6
Model has moved
 O |   |  
___________
   | X |  
___________
 X |   | O
Input index of your move (0-8):  2
Model has moved
 O |   | X
___________
   | X |  
___________
 X | O | O
Congratulations, You won! 
</code></pre>
      </li>
    </ol>
    <p class="normal">As you can see, a human player beats the machine very quickly and easily.</p>
    <h2 id="_idParaDest-195" class="title">How it works...</h2>
    <p class="normal">In this section, we trained a <a id="_idIndexMarker368"/>neural network to play Tic-Tac-Toe by feeding in board positions and a nine-dimensional vector, and predicted the optimal response. We only had to feed in a few possible Tic-Tac-Toe boards and apply random transformations to each board to increase the training set size.</p>
    <p class="normal">To test our algorithm, we removed all instances of one specific board and saw whether our model could generalize to predict the optimal response. Finally, we played a sample game against our model. This model isn't perfect yet. Using more data or applying a more complex neural network architecture could be done to improve it. But the better thing to do is to change the type of learning: instead of using supervised learning, we're better off using a reinforcement learning-based approach.</p>
  </div>
</body></html>