<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer036">
<h1 class="chapter-number" id="_idParaDest-45"><a id="_idTextAnchor045"/>2</h1>
<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>Introduction to TensorFlow</h1>
<p>Before the era of TensorFlow, the landscape of deep learning was markedly different. Data professionals had fewer comprehensive tools to aid in the development, training, and deployment of neural networks. This posed challenges in experimenting with various architectures and tuning model settings to solve complex tasks, as data experts often had to construct their models from scratch. The process was time-consuming, with some experts spending days or even weeks developing effective models. Another bottleneck was the difficulty in deploying trained models, which made the practical application of neural networks challenging during those <span class="No-Break">early days.</span></p>
<p>But today, everything has changed; with TensorFlow, you can do lots of amazing things. In this chapter, we will begin by examining the TensorFlow ecosystem and discussing, at a high level, the various components relevant to building state-of-the-art applications with TensorFlow. We will begin our journey by setting up our workspace to meet the requirements of the exam and our upcoming experiments. We will also learn what TensorFlow is all about, understand the concept of tensors, explore basic data representation and operations in TensorFlow, and build our first model using this powerful tool. We will conclude this chapter by looking at how to debug and solve error messages <span class="No-Break">in TensorFlow.</span></p>
<p>By the end of this chapter, you will understand the basics of TensorFlow, including what tensors are and how to perform basic data operations with them. You will be equipped to confidently build your first model with TensorFlow and debug and solve any error messages that might arise in <span class="No-Break">the process.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>What <span class="No-Break">is TensorFlow?</span></li>
<li>Setting up <span class="No-Break">our environment</span></li>
<li><span class="No-Break">Data representation</span></li>
<li>Hello World <span class="No-Break">in TensorFlow</span></li>
<li>Debugging and solving <span class="No-Break">error messages</span></li>
</ul>
<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/>Technical requirements</h1>
<p>We will be using <strong class="bold">Google Colaboratory</strong> (<strong class="bold">Google Colab</strong>) notebooks as our work environment as it is a free, cloud-based Jupyter Notebook service that is easy to use and provides us with <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) and <strong class="bold">tensor processing unit</strong> (<strong class="bold">TPU</strong>) backends. We will be using Google Colab to run the coding exercise, which requires <strong class="source-inline">python &gt;= 3.8.0</strong> along with the following packages, which can be installed using the <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> command:</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">tensorflow&gt;=2.7.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">tensorflow-datasets==4.4.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pillow==8.4.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pandas==1.3.4</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">numpy==1.21.4</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">scipy==1.7.3</strong></span></li>
</ul>
<p>The code bundle for this book is available at the following GitHub link: <a href="https://github.com/PacktPublishing/TensorFlow-Developer-Certificate">https://github.com/PacktPublishing/TensorFlow-Developer-Certificate</a>. Also, solutions to all exercises can be found in the GitHub repo itself. If you are new to Google Colab, here is a great resource to get you started <span class="No-Break">quickly: </span><a href="https://www.youtube.com/watch?v=inN8seMm7UI&amp;list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL"><span class="No-Break">https://www.youtube.com/watch?v=inN8seMm7UI&amp;list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>What is TensorFlow?</h1>
<p>In the last <a id="_idIndexMarker052"/>chapter, we examined the different types of applications we could build with our knowledge <a id="_idIndexMarker053"/>of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), from chatbots to facial recognition systems and from house price prediction to detecting fraud in the banking industry – these are some of the exciting applications we can build using deep learning frameworks such as TensorFlow. The question we would logically ask is what exactly is TensorFlow? And why should we learn it <span class="No-Break">at all?</span></p>
<p><strong class="bold">TensorFlow</strong> is an open source end-to-end framework for building deep learning applications. It was developed by a team of data professionals at Google in 2011 and made openly available in 2015. TensorFlow is a flexible, scalable solution that enables us to build models with ease using the Keras API. It allows us to access a large array of pretrained <a id="_idIndexMarker054"/>deep learning models, thus making it the framework of choice for many data professionals in the industry and academia. Currently, TensorFlow is used at powerhouses such as Google, DeepMind, Airbnb, Intel, and so many <span class="No-Break">more companies.</span></p>
<p>Today, with TensorFlow, you can easily train a deep learning model on a single PC, using a cloud service such as AWS, or using distributed training with a cluster of computers. Model building is just a part of what data professionals do; what about visualizing, deploying, and monitoring our models? TensorFlow has a wide range of tools to cater to these processes, such as TensorBoard, TensorFlow lite, TensorFlow.js, TensorFlow Hub, and <strong class="bold">TensorFlow Extended</strong> (<strong class="bold">TFX</strong>). These<a id="_idIndexMarker055"/> tools enable data professionals to build and deploy scalable, low-latency, ML-powered applications across various domains – on the web, on mobile, and on edge devices. To support TensorFlow developers, there is comprehensive documentation and a large community of developers who report bugs and contribute to the further development and improvement of <span class="No-Break">this framework.</span></p>
<p>Another central feature of the TensorFlow ecosystem is its access to a diverse array of datasets, cutting across different ML problem types such as image data, text data, and time-series data. These datasets are available via TensorFlow Datasets, and they are a great way to master the use of TensorFlow in solving real-world problems. In subsequent chapters, we will be exploring how to build models to solve computer vision, natural language processing, and time-series forecasting problems using a range of datasets available within the <span class="No-Break">TensorFlow ecosystem.</span></p>
<p>We have explored <a id="_idIndexMarker056"/>some indispensable tools in the TensorFlow ecosystem. It is always a good idea to take a tour of these features (and the new ones that will be added) on the official website: <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>. However, in the exam, you will not be quizzed on this. The idea here is to get familiar with what is available in the ecosystem. The exam focuses on modeling with TensorFlow so we will only use tools in the ecosystem such as TensorFlow Datasets, the Keras API, and TensorFlow Hub to meet <span class="No-Break">this objective.</span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/>Setting up our environment</h1>
<p>Before<a id="_idIndexMarker057"/> we examine data representations in TensorFlow, let’s set up <a id="_idIndexMarker058"/>our work environment. We will begin by importing TensorFlow and checking <span class="No-Break">the version:</span></p>
<pre class="source-code">
import tensorflow as tf
#To check the version of TensorFlow
print(tf.__version__)</pre>
<p>When we run this block of code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
2.8.0</pre>
<p>Hurray! We have successfully imported TensorFlow. Next, let us import NumPy and a couple of data types, as we will be using them shortly in <span class="No-Break">this chapter:</span></p>
<pre class="source-code">
import numpy as np
from numpy import *</pre>
<p>We have successfully completed all our import steps without errors. We will now look at data representations in TensorFlow as our working environment is fully <span class="No-Break">set up.</span></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor050"/>Data representation</h1>
<p>In our quest to<a id="_idIndexMarker059"/> solve complex tasks using ML, we come across <a id="_idIndexMarker060"/>diverse types of raw data. Our primary role involves transforming this raw data (which could be text, images, audio, or video) into numerical representations. These representations allow our ML models to easily digest and learn the underlying patterns in the data efficiently. To achieve this, this is where TensorFlow and its fundamental data structure, tensors, come into play. While numerical data is commonly used in training models, our models are also adept at efficiently handling binary and categorical data. For such data types, we apply techniques such as one-hot encoding to transform them into a <span class="No-Break">model-friendly format.</span></p>
<p><strong class="bold">Tensors</strong> are<a id="_idIndexMarker061"/> multi-dimensional arrays designed for numerical data representation; although they share some similarities with NumPy arrays, they possess certain unique features that give them an advantage in deep learning tasks. One of these key <a id="_idIndexMarker062"/>advantages is their ability to utilize hardware acceleration from GPUs and TPUs to significantly speed up computational operations, which is especially useful when working with input data such as images, text, and videos, as we will see in later chapters of <span class="No-Break">this book.</span></p>
<p>Let us take a<a id="_idIndexMarker063"/> quick look at a real-world example. Let’s say we are building an automobile recognition system, as illustrated in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>. We would begin with collecting images of cars of various sizes, shapes, and colors. To train our model to recognize these different automobiles, we would transform each image into input tensors that encapsulate the height, width, and color channels. When we train the model on these input tensors, it learns patterns based on the pixel value representations of the cars in our training set. Once the model completes the training, we can use the trained model to identify cars of different shapes, colors, and sizes. If we now feed the trained model with the image of a car, it returns an output tensor that can be decoded into a human-readable format to enable us to identify the type of car that <span class="No-Break">it is.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 2.1 – Data representation in TensorFlow" height="350" src="image/B18118_02_001.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Data representation in TensorFlow</p>
<p>Now that we get the intuition, let’s examine and drill down into more details about tensors. We will start by learning a few ways to generate <span class="No-Break">tensors next.</span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Creating tensors</h2>
<p>There <a id="_idIndexMarker064"/>are a couple of ways we can generate tensors in TensorFlow. However, we will focus on creating tensor objects using <strong class="source-inline">tf.constant</strong>, <strong class="source-inline">tf.Variable</strong>, and <strong class="source-inline">tf.range</strong>. Recall that we have already imported TensorFlow, NumPy, and data types in the section on setting up our working environment. Next, let us run the following code to generate our first tensor <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">tf.constant</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
#Creating a tensor object using tf.constant
a_constant = tf.constant([1, 2, 3, 4 ,5, 6])
a_constant</pre>
<p>When we run this code, we generate our first tensor. If all goes well, we will get an output that looks <span class="No-Break">like this:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(6,), dtype=int32,
    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</pre>
<p>Excellent! Don’t worry, we<a id="_idIndexMarker065"/> will discuss the output and form a clearer picture as we proceed. But for now, let us generate a similar tensor object using the <span class="No-Break"><strong class="source-inline">tf.Variable</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
#Creating a tensor object using tf.Variable
a_variable = tf.Variable([1, 2, 3, 4 ,5, 6])
a_variable</pre>
<p>The <strong class="source-inline">a_variable</strong> variable returns the <span class="No-Break">following output:</span></p>
<pre class="source-code">
&lt;tf.Variable 'Variable:0' shape=(6,) dtype=int32,
    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</pre>
<p>Although the input in both cases is the same, <strong class="source-inline">tf.constant</strong> and <strong class="source-inline">tf.Variable</strong> are different. Tensors generated using <strong class="source-inline">tf.constant</strong> cannot be changed, whereas <strong class="source-inline">tf.Variable</strong> tensors can be reassigned in the future. We will touch more on this shortly as we go further in our exploration of tensors. In the meantime, let us look at another way of generating tensors <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">tf.range</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
# Creating tensors using the range function
a_range = tf.range(start=1, limit=7)
a_range</pre>
<p><strong class="source-inline">a_range</strong> returns the <span class="No-Break">following output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(6,), dtype=int32,
    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)&gt;</pre>
<p>Great! From the output, if we visually compare all three methods used for generating tensors, we <a id="_idIndexMarker066"/>can easily conclude that the output of <strong class="source-inline">a_constant</strong> and <strong class="source-inline">a_range</strong> is the same and is slightly different from the output of <strong class="source-inline">a_variable</strong>. This difference becomes clearer when performing tensor operations. To see this in action, let’s begin exploring tensor operations, starting with <a id="_idTextAnchor052"/><span class="No-Break">tensor rank.</span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>Tensor rank</h2>
<p>If you are <a id="_idIndexMarker067"/>not from a mathematical background, relax. We will cover everything together and we won’t be discussing rocket science here – that’s a promise. The rank of a tensor identifies the number of dimensions of the tensor. A tensor with a rank of <strong class="source-inline">0</strong> is called a scalar, as it has no dimensions. A vector is a rank <strong class="source-inline">1</strong> tensor as it has only one dimension, while a matrix of a two-dimension tensor has a rank <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 2.2 – Tensor rank" height="278" src="image/B18118_02_002.jpg" width="865"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Tensor rank</p>
<p>We have practiced how to generate tensors using three different functions. For context, we can safely define a scalar as a quantity that has only magnitude but no direction. Examples of scalar quantities are time, mass, energy, and speed; these quantities have a single numeric value, for example, <strong class="source-inline">1</strong>, <strong class="source-inline">23.4</strong>, or <strong class="source-inline">50</strong>. Let us return to our notebook and generate a scalar using the <span class="No-Break"><strong class="source-inline">tf.constant</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
#scalar
a = tf.constant(1)
a</pre>
<p>We start by creating a scalar, which is a single value that returns the <span class="No-Break">following output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;</pre>
<p>From the returned output, we can see that the shape has no value since the output is a scalar quantity with a single numeric output. If we try out a value of <strong class="source-inline">4</strong>, the <strong class="source-inline">numpy</strong> outp<a id="_idTextAnchor054"/>ut will be <strong class="source-inline">4</strong>, while other output properties will remain the same since <strong class="source-inline">4</strong> is still a <span class="No-Break">scalar quantity.</span></p>
<p>Now that we<a id="_idIndexMarker068"/> have seen what a scalar (rank <strong class="source-inline">0</strong> tensor) is, let us go a step further by looking at a vector. For context, a vector quantity has both magnitude and direction. Examples of vectors are acceleration, velocity, and force. Let us jump back into our notebook and try to generate a vector of four numbers. For a change, this time, we will use floats since we can generate tensors with floats. Also, if you noticed, the default data type returned has been <strong class="source-inline">int32</strong> for integers, which we have previously used to <span class="No-Break">generate tensors:</span></p>
<pre class="source-code">
#vector
b= tf.constant([1.2,2.3,3.4,4.5])
b</pre>
<p>From our result, we see the data type returned is <strong class="source-inline">float32</strong> with a shape <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">4</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(4,), dtype=float32,
    numpy=array([1.2, 2.3, 3.4, 4.5], dtype=float32)&gt;</pre>
<p>Next, let us generate a matrix. A matrix is an array of numbers listed in rows and columns. Let us try out a matrix in <span class="No-Break">our notebook:</span></p>
<pre class="source-code">
#matrix
c =tf.constant([[1,2],[3,4]])
c
&lt;tf.Tensor: shape=(2, 2), dtype=int32,
    numpy= array([[1, 2], [3, 4]], dtype=int32)&gt;</pre>
<p>The preceding matrix is a 2 x 2 matrix, which we can infer by inspecting the <strong class="source-inline">shape</strong> output. We see that the data type is also <strong class="source-inline">int32</strong>. Let us generate a <span class="No-Break">higher-dimensional tensor:</span></p>
<pre class="source-code">
#3-dimensional tensor
d=tf.constant([[[1,2],[3,4],[5,6]],[[7,8],[9,10],[11,12]]])
d</pre>
<p>The output is a 2 x 3 x 2 tensor, with a data type <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">int32</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(2, 3, 2), dtype=int32,
    numpy= array([[[ 1,  2],[ 3,  4],[ 5,  6]],
       [[ 7,  8], [ 9, 10], [11, 12]]], dtype=int32)&gt;</pre>
<p>You <a id="_idIndexMarker069"/>should play around with some tensors. Try making some tensors with <strong class="source-inline">tf.Variable</strong> and see whether you can reproduce our results so far. Next, let us see how we can interpret the properties of <span class="No-Break">a tensor.</span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor055"/>Properties of tensors</h2>
<p>Now that we <a id="_idIndexMarker070"/>have established an understanding of scalars, vectors, and tensors, let us explore how to interpret tensor outputs in detail. Previously, we examined tensors with a piecemeal approach. Here, we will learn how to identify the key properties of a tensor – its rank, shape, and data type – from its printed representation. When we print a tensor, it displays the variable name, shape, and data type. Thus far, we utilized default arguments when creating tensors. Let us make some adjustments to see how this changes <span class="No-Break">the output.</span></p>
<p>We will use <strong class="source-inline">tf.</strong><strong class="source-inline">V</strong><strong class="source-inline">ariable</strong> to generate a scalar tensor, selecting <strong class="source-inline">float16</strong> as the data type and naming it <strong class="source-inline">TDC</strong>. (If you are wondering what <strong class="bold">TDC</strong> means, it is the <strong class="bold">TensorFlow Developer Certificate</strong>.) Next, we<a id="_idIndexMarker071"/> will run <span class="No-Break">the code:</span></p>
<pre class="source-code">
#scalar
a = tf.Variable(1.1, name="TDC", dtype=float16)
a
&lt;tf.Variable 'TDC:0' shape=() dtype=fl<a id="_idTextAnchor056"/>oat16, numpy=1.1&gt;</pre>
<p>When we examine the output, we can see the name of the tensor is now <strong class="source-inline">TDC: 0</strong>, and the shape of the tensor is <strong class="source-inline">0</strong> since the tensor is of rank <strong class="source-inline">0</strong>. The data type we selected was <strong class="source-inline">float16</strong>. And finally, the tensor has a <strong class="source-inline">numpy</strong> value of <strong class="source-inline">1.1</strong> also. This example shows how we can configure properties such as data type and name when constructing tensor<a id="_idTextAnchor057"/>s <span class="No-Break">in TensorFlow.</span></p>
<p>Next, let us look at a vector and see what information we can learn from <span class="No-Break">its properties:</span></p>
<pre class="source-code">
#vector
b= tf.Variable([1.2,2.3,3.4,4.5], name="Vector", dtype=float16)
b</pre>
<p>Here, again, we<a id="_idIndexMarker072"/> included arguments and the name of the tensor and we changed the default data type. From the output, we can see the result is similar to what we got with th<a id="_idTextAnchor058"/>e <span class="No-Break">scalar quantity:</span></p>
<pre class="source-code">
&lt;tf.Variable ''Vector:0' shape=(4,) dtype=float16,
    numpy=array([1.2, 2.3, 3.4, 4.5])&gt;</pre>
<p>Here, the tensor has the name <strong class="source-inline">'Vector:0</strong>, the shape has a value of <strong class="source-inline">4</strong> (which corresponds to the count of the number of entries), and the tensor has a data type of <strong class="source-inline">float16</strong>. To have some fun, you can experiment with different configurations and see the impact the changes you make have on the returned output; this is an excellent way to learn and understand how things work. When we print the result of a tensor output, we can see the different properties of the tensor, like when we examined the scalar and vector quantities. However, by leveraging TensorFlow functions, we can gain more information about a tensor. Let us start by using the <strong class="source-inline">tf.rank()</strong> function to inspect the rank of a scalar, vector, <span class="No-Break">and matrix:</span></p>
<pre class="source-code">
#scalar
a = tf.constant(1.1)
#vector
b= tf.constant([1.2,2.3,3.4,4.5])
#matrix
c =tf.constant([[1,2],[3,4]])
#Generating tensor rank
print("The rank of the scalar is: ",tf.rank(a))
print(" ")
print("The rank of the vector is: ",tf.rank(b))
print(" ")
print("The rank of the matrix is: ",tf.rank(c))</pre>
<p>We run the<a id="_idIndexMarker073"/> preceding code to generate a scalar, vector, and matrix. After this, we print the rank of each of them using the <strong class="source-inline">tf.rank</strong> function. Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
The rank of the scalar is: tf.Tensor(0, shape=(), dtype=int32)
The rank of the vector is: tf.Tensor(1, shape=(), dtype=int32)
The rank of the matrix is: tf.Tensor(2, shape=(), dtype=int32)</pre>
<p>The returned output is a tensor object that displays the rank of the tensors along with the shape and the data type of the tensor. To access the rank of the tensor as a numeric value, we have to use <strong class="source-inline">.numpy()</strong> on the returned tensor to retrieve the actual rank of <span class="No-Break">the tensor:</span></p>
<pre class="source-code">
print("The rank of the scalar is: ",tf.rank(a).numpy())
The rank of the scalar is:  0</pre>
<p>However, an easier way to directly obtain the rank of a tensor without the need for reevaluating is by using <strong class="source-inline">ndim</strong>. Let’s see <span class="No-Break">this next:</span></p>
<pre class="source-code">
#Generating details of the dimension
print("The dimension of the scalar is: ",a.ndim)
print(" ")
print("The dimension of the vector is: ",b.ndim)
print(" ")
print("The dimension of the matrix is: ",c.ndim)</pre>
<p>When we run the code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
The dimension of the scalar is: 0
The dimension of the vector is: 1
The dimension of the matrix is: 2</pre>
<p>Next, let us proceed <a id="_idIndexMarker074"/>by printing out the data type of all three quantities using the <strong class="source-inline">dtype</strong> argument to generate the data type for each of <span class="No-Break">our tensors:</span></p>
<pre class="source-code">
#printing the data type
print("The data type of the scalar is: ",a.dtype)
print(" ")
print("The data type of the vector is: ",b.dtype)
print(" ")
print("The data type of the matrix is: ",c.dtype)</pre>
<p>When we run the code, we get the <span class="No-Break">following output.</span></p>
<pre class="source-code">
The data type of the scalar is:  &lt;dtype: 'float32'&gt;
The data type of the vector is:  &lt;dtype: 'float32'&gt;
The data type of the matrix is:  &lt;dtype: 'int32'&gt;</pre>
<p>From the preceding output, we can see the data types. Next, let us look at the shape of <span class="No-Break">our tensors:</span></p>
<pre class="source-code">
#Generating details of the tensor shape
print("The Shape of the scalar is: ",a.shape)
print(" ")
print("The Shape of the vector is: ",b.shape)
print(" ")
print("The Shape of the matrix is: ",c.shape)</pre>
<p>When we run the code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
The Shape of the scalar is:  ()
The Shape of the vector is:  (4,)
The Shape of the matrix is:  (2, 2)</pre>
<p>From the results, we <a id="_idIndexMarker075"/>can see that the scalar has no shape value while the vector has a shape value of one unit, and our matrix has a shape value of two units. Next, let us compute the number of elements in each of <span class="No-Break">our tensors:</span></p>
<pre class="source-code">
#Generating number of elements in a tensor
print("The Size of the scalar is: ",tf.size(a))
print(" ")
print("The Size of the vector is: ",tf.size(b))
print(" ")
print("The Size of the matrix is: ",tf.size(c))</pre>
<p>When we run the code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
The Size of the scalar is:  tf.Tensor(1, shape=(), dtype=int32)
The Size of the vector is:  tf.Tensor(4, shape=(), dtype=int32)
The Size of the matrix is:  tf.Tensor(4, shape=(), dtype=int32)</pre>
<p>We can see that the scalar has only 1 count since it is a single unit; our vector and matrix both have <strong class="source-inline">4</strong> in them and hence they have 4 numeric values in each of them. Now, we can confidently use different ways to investigate the properties of tensors. Let us proceed to implement basic operations <span class="No-Break">with tensors.</span></p>
<h2 id="_idParaDest-54">Ba<a id="_idTextAnchor059"/>sic tensor operations</h2>
<p>We now know <a id="_idIndexMarker076"/>that TensorFlow is a powerful tool for deep learning. One big hurdle with learning TensorFlow is understanding what tensor operations are and<a id="_idIndexMarker077"/> why we need them. We have established that tensors are fundamental data structures in TensorFlow and they can be used to store, manipulate, and analyze data in ML models. On the other hand, tensor operations are mathematical operations that can be applied to tensors in order to manipulate, decode, or analyze data. These operations range from simple operations such as element-wise operations to more complex computations performed within the layers of a neural network. Let us look at some tensor <a id="_idIndexMarker078"/>operations. We<a id="_idIndexMarker079"/> will start with changing data types. Then, we will look at indexing and aggregating tensors. Finally, we will carry out element-wise operations on tensors, reshaping tensors, and <span class="No-Break">matrix multiplication.</span></p>
<h3>Changing data types</h3>
<p>Let’s say we <a id="_idIndexMarker080"/>have a  tensor and we want to change the data type from <strong class="source-inline">int32</strong> to <strong class="source-inline">float32</strong>, perhaps to accommodate some operation that would require the decimal numbers. Fortunately, in TensorFlow, there is a way around this problem. Remember that we identified that the default data type for integers is <strong class="source-inline">int32</strong> and for decimal numbers, it is <strong class="source-inline">float32</strong>. Let us return to Google Colab and see how we can get this done <span class="No-Break">in TensorFlow:</span></p>
<pre class="source-code">
a=tf.constant([1,2,3,4,5])
a</pre>
<p>We generated a vector of integers, which produces the <span class="No-Break">following output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5], dtype=int32)&gt;</pre>
<p>We can see that the data type is <strong class="source-inline">int32</strong>. Let us proceed with a data type operation, changing the data type to <strong class="source-inline">float32</strong>. We use the <strong class="source-inline">tf.cast()</strong> function and we set the data type argument to <strong class="source-inline">float32</strong>. Let us implement this in <span class="No-Break">our notebook:</span></p>
<pre class="source-code">
a =tf.cast(a,dtype=tf.float32)
a</pre>
<p>The operation returns a data type of <strong class="source-inline">float32</strong>. We can also see the <strong class="source-inline">numpy</strong> array is now an array of decimal numbers and not <span class="No-Break">integers anymore:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(5,), dtype=float32,
    numpy=array([1., 2., 3., 4., 5.], dtype=float32)&gt;</pre>
<p>You can try it out with <strong class="source-inline">int16</strong> or <strong class="source-inline">float64</strong> and see how it goes. When you are done, let us move on with indexing <span class="No-Break">in TensorFlow.</span></p>
<h3>Indexing</h3>
<p>Let’s start by <a id="_idIndexMarker081"/>creating a 2 x 2 matrix, which we will use to walk through our <span class="No-Break">indexing operation:</span></p>
<pre class="source-code">
# Create a 2 x 2 matrix
a = tf.constant([[1, 2],[3, 4]], dtype=float32)
a</pre>
<p>Here is the <span class="No-Break">returned output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(2, 2), dtype=float32, 
    numpy=array([[1., 2.], [3., 4.]], dtype=float32)&gt;</pre>
<p>What if we want to extract some information from the matrix? Let’s say we want to extract <strong class="source-inline">[1,2]</strong>. How do we go about this? Not to worry: we can apply indexing to get the desired information. Let us get it done in <span class="No-Break">our notebook:</span></p>
<pre class="source-code">
# Indexing
a[0]</pre>
<p>Here is the <span class="No-Break">returned output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(2,), dtype=float32,
    numpy=array([1., 2.], dtype=float32)&gt;</pre>
<p>What if we want to extract value <strong class="source-inline">2</strong> from the matrix? Let us see how we can get <span class="No-Break">it done:</span></p>
<pre class="source-code">
# Indexing
a[0][1]</pre>
<p>Here is the <span class="No-Break">returned output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;</pre>
<p>Now, we have successfully extracted the value we wanted using indexing. To extract all the values in the matrix shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em>, we can use indexing to extract the desired element in the 2 x <span class="No-Break">2 matrix.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 2.3 – Matrix indexing" height="178" src="image/B18118_02_003.jpg" width="485"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Matrix indexing</p>
<p>Next, let us look at <a id="_idIndexMarker082"/>another example of indexing – this time, using the <strong class="source-inline">tf.slice()</strong> function to extract information from <span class="No-Break">a tensor:</span></p>
<pre class="source-code">
c = tf.constant([0, 1, 2, 3, 4, 5])
print(tf.slice(c,begin=[2],size=[4]))</pre>
<p>We generate a tensor, <strong class="source-inline">c</strong>. Then, we use the <strong class="source-inline">tf.slice</strong> function to slice the vector, starting at index <strong class="source-inline">2</strong> with a size or count of <strong class="source-inline">4</strong>. When we run the code, we get the <span class="No-Break">following result:</span></p>
<pre class="source-code">
tf.Tensor([2 3 4 5], shape=(4,), dtype=int32)</pre>
<p>We can see that the result contains values from index <strong class="source-inline">2</strong>, and we take 4 elements in the vector to generate our slice. Next, let us look at how to expand the dimension of <span class="No-Break">a matrix.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Remember, in Python, we start counting from 0, <span class="No-Break">not 1.</span></p>
<h3>Expanding a matrix</h3>
<p>We already <a id="_idIndexMarker083"/>now know how to check the dimension of the matrix using <strong class="source-inline">ndim</strong>. So, let us see how we can expand the dimension of this matrix. We continue using our <strong class="source-inline">a</strong> matrix, which is a 2 x 2 matrix, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 2.4 – A 2x2 matrix" height="155" src="image/B18118_02_004.jpg" width="173"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – A 2x2 matrix</p>
<p>We can use the following code to expand <span class="No-Break">the dimension:</span></p>
<pre class="source-code">
tf.expand_dims(a,axis=0)</pre>
<p>We use the <strong class="source-inline">expand_dims()</strong> function, and the code expands the dimensions of the <strong class="source-inline">a</strong> tensor along the <strong class="source-inline">0</strong> axis. This is useful when you want to add a new dimension to the tensor – for example, when you want to convert a 2D tensor into a 3D tensor (a technique that will be applied in <a href="B18118_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Image Classification with Convolutional Neural Networks</em>, where we will work on an interesting classic <span class="No-Break">image dataset):</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(1, 2, 2), dtype=float32,
    numpy= array([[[1., 2.], [3., 4.]]], dtype=float32)&gt;</pre>
<p>If you take <a id="_idIndexMarker084"/>a look at the shape of our output tensor, we can now see it has an extra dimension of <strong class="source-inline">1</strong> at the <strong class="source-inline">0</strong> axis. Let us proceed by examining the shape of the tensor when we expand across different axes, so we can understand how this <span class="No-Break">works better:</span></p>
<pre class="source-code">
(tf.expand_dims(a,axis=0)).shape,
(tf.expand_dims(a,axis=1)).shape,
(tf.expand_dims(a,axis=-1)).shape</pre>
<p>When we run the code to see how the dimension has expanded across the <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, and <strong class="source-inline">-1</strong> axes, we get the <span class="No-Break">following results:</span></p>
<pre class="source-code">
(TensorShape([1, 2, 2]), TensorShape([2, 1, 2]),
    TensorShape([2, 2, 1]))</pre>
<p>In the first line of code, the dimensions of <strong class="source-inline">a</strong> are expanded by 1 on the <strong class="source-inline">0</strong> axis. This means that the dimensions of <strong class="source-inline">a</strong> will now be 1 x 2 x 2, adding an extra dimension at the beginning of the tensor. The second line of code is expanding the dimensions of <strong class="source-inline">a</strong> by 1 on the <strong class="source-inline">1</strong> axis. This means that the dimensions of <strong class="source-inline">a</strong> will now be 2 x 1 x 2; here, we are adding an extra dimension in the second position of the tensor. The third line of code is expanding the dimensions of <strong class="source-inline">a</strong> by 1 on the <strong class="source-inline">-1</strong> axis. This means that the dimensions of <strong class="source-inline">a</strong> will now be 2 x 2 x 1, thereby adding an extra dimension at the end of our tensor. We have now explained how to expand the dimension of a matrix. Next, let us look at <span class="No-Break">tensor aggregation.</span></p>
<h3>Tensor aggregation</h3>
<p>Let us <a id="_idIndexMarker085"/>continue our journey by understanding how to aggregate tensors. We start by generating some random numbers by importing the <strong class="source-inline">random</strong> library. Then, we generate a range from 1 to 100 in which we generate 50 random numbers. We will now use these random numbers to generate <span class="No-Break">a tensor:</span></p>
<pre class="source-code">
import random
random.seed(22)
a = random.sample(range(1, 100), 50)
a = tf.constant(a)</pre>
<p>When we print <strong class="source-inline">a</strong>, we get the <span class="No-Break">following numbers:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(50,), dtype=int32, numpy=array(
    [16, 83,  6, 74, 19, 80, 95, 68, 66, 86, 54, 12, 91,
    13, 23,  9, 82, 84, 30, 62, 89, 33, 78,  2, 97, 21,
    59, 34, 48, 38, 35, 18, 46, 60, 27, 26, 73, 76, 94,
    72, 15, 40, 96, 44, 61,  8, 79, 93, 11, 14],
    dtype=int32)&gt;</pre>
<p>Let’s say we want to find the smallest number in our tensor. It may be difficult to manually read through all the numbers and tell me in 5 seconds which is the smallest. What if our range of values was up to a thousand or a million? Manually checking would take up all our time. Thankfully, in TensorFlow, we can find not just the minimum in one strike but we can also find the maximum value, the sum of all values, the mean, and much more. Let us do this together in the <span class="No-Break">Colab notebook:</span></p>
<pre class="source-code">
print("The smallest number in our vector is : ",
    tf.reduce_min(a).numpy())
print(" ")
print("The largest number in our vector is: ",
    tf.reduce_max(a).numpy())
print(" ")
print("The sum of our vector is : ",
    tf.reduce_sum(a).numpy())
print(" ")
print("The mean of our vector is: ",
    tf.reduce_mean(a).numpy())</pre>
<p>We use these<a id="_idIndexMarker086"/> functions to extract the details we require in one click, which generates the <span class="No-Break">following result:</span></p>
<pre class="source-code">
The smallest number in our vector is :  1
The largest number in our vector is:  99
The sum of our vector is :  2273
The mean of our vector is:  45</pre>
<p>Now that we have used TensorFlow to extract some important details, we know that the smallest value in our vector is 1, the largest value is 99, the sum of our vector is 2,273, and the mean value is 45. Not bad, right? What if we want to find the position that holds the minimum and maximum value in a vector? How do we go <span class="No-Break">about this?</span></p>
<pre class="source-code">
print("The position that holds the lowest value is : ",
    tf.argmin(a).numpy())
print(" ")
print("The position that holds the highest value is: ",
    tf.argmax(a).numpy())</pre>
<p>We use the <strong class="source-inline">tf.argmin</strong> and <strong class="source-inline">tf.argmax</strong> functions to generate the index of the lowest value and the index of the highest value, respectively. The output is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
The position that holds the lowest value is :  14
The position that holds the highest value is:  44</pre>
<p>From the result of the <strong class="source-inline">print</strong> statement, we can tell that the lowest value is at index <strong class="source-inline">14</strong> and the highest value is at index <strong class="source-inline">44</strong>. If we manually inspect the array, we will see that this is true. Also, we<a id="_idIndexMarker087"/> can pass the index position into the array to get the lowest and <span class="No-Break">highest value:</span></p>
<pre class="source-code">
a[14].numpy(), a[44].numpy()</pre>
<p>If we run the code, we get the <span class="No-Break">following result:</span></p>
<pre class="source-code">
(1,99)</pre>
<p>There are a few other functions you can try out. The TensorFlow documentation gives us a lot to try out and have fun with. Next, let us explore how to transpose and <span class="No-Break">reshape tensors.</span></p>
<h3>Transposing and reshaping tensors</h3>
<p>Let us look at <a id="_idIndexMarker088"/>how<a id="_idIndexMarker089"/> to transpose and reshape a matrix. First, let’s generate a 3 x <span class="No-Break">4 matrix:</span></p>
<pre class="source-code">
# Create a 3 x 4 matrix
a = tf.constant([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
a</pre>
<p>When we run the code, we get <span class="No-Break">this result:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(3, 4), dtype=int32, 
    numpy=array([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]], dtype=int32)&gt;</pre>
<p>We can reshape the matrix by using <strong class="source-inline">tf.reshape</strong>. Since the matrix has 12 values in it, we can use 2 x 2 x 3. If we multiply the values, we get a total <span class="No-Break">of 12:</span></p>
<pre class="source-code">
tf.reshape(a, shape=(2, 2, 3))</pre>
<p>When we run the code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, 
    numpy=array([[[ 1,  2,  3], [ 4,  5,  6]],
        [[ 7,  8,  9], [10, 11, 12]]], dtype=int32)&gt;</pre>
<p>We can<a id="_idIndexMarker090"/> also <a id="_idIndexMarker091"/>reshape the matrix by changing the <strong class="source-inline">shape</strong> argument in the <strong class="source-inline">tf.reshape</strong> function to a 4 x 3 matrix or a 1 x 2 x 6 matrix. You can also try out a few other possibilities with regard to reshaping this matrix. Next, let us look at how to transpose this matrix <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">tf.transpose()</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
tf.transpose(a)</pre>
<p>When we run the code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(4, 3), dtype=int32, 
    numpy=array([[ 1,  5,  9],
        [ 2,  6, 10],
        [ 3,  7, 11],
        [ 4,  8, 12]], dtype=int32)&gt;</pre>
<p>From the output, we can see that transposing flips the axes. We now have a 4 x 3 matrix rather than our initial 3 x 4 matrices. Next, let us look at element-wise <span class="No-Break">matrix operations.</span></p>
<h3>Element-wise operations</h3>
<p>Let’s start by <a id="_idIndexMarker092"/>creating a simple vector <span class="No-Break">in Colab:</span></p>
<pre class="source-code">
a= tf.constant([1,2,3])
a</pre>
<p>Let us display our output so we can see what happens when we perform element-wise operations on <span class="No-Break">the vector:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3],
    dtype=int32)&gt;</pre>
<p>This is our initial output. Now, let us try out a few element-wise operations and see what <span class="No-Break">happens next:</span></p>
<pre class="source-code">
#Addition operation
print((a+4).numpy())
print(" ")
#Subtraction operation
print((a-4).numpy())
print(" ")
#Multiplication Operation
print((a*4).numpy())
print(" ")
#Division Operation
print((a/4).numpy())
print(" ")</pre>
<p>We can see the <a id="_idIndexMarker093"/>results for the addition, subtraction, multiplication, and division operations. These operations are carried out on each element in <span class="No-Break">our vector:</span></p>
<pre class="source-code">
[5 6 7]
[-3 -2 -1]
[ 4  8 12]
[0.25 0.5  0.75]</pre>
<p>Next, let us look at <span class="No-Break">matrix multiplication.</span></p>
<h3>Matrix multiplication</h3>
<p>Let us<a id="_idIndexMarker094"/> look at matrix multiplication and see how it works in TensorFlow. We return to our notebook in Colab and generate matrix <strong class="source-inline">a</strong>, which is a 3 x 2 matrix, and matrix <strong class="source-inline">b</strong>, which is a 2 x 3 matrix. We will use these for our <span class="No-Break">matrix operations:</span></p>
<pre class="source-code">
# 3 X 2 MATRIX
a = tf.constant([[1, 2], [3, 4], [5, 6]])
#2 X 3 MATRIX
b = tf.constant([[7,8,9], [10,11,12]])</pre>
<p>Now, let us multiply matrix <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> and see what our result will look like in TensorFlow by using <strong class="source-inline">tf.matmul</strong> in <span class="No-Break">our notebook:</span></p>
<pre class="source-code">
tf.matmul(a,b)</pre>
<p>We use<a id="_idIndexMarker095"/> the <strong class="source-inline">tf.matmul</strong> function for matrix multiplication in TensorFlow. Here, we see the output of <span class="No-Break">this operation:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(3, 3), dtype=int32,
numpy= array([[ 27,  30,  33],[ 61,  68,  75],
    [ 95, 106, 117]], dtype=int32)&gt;</pre>
<p>Great! Now, what if we want to multiply matrix <strong class="source-inline">a</strong> by itself? What will our result look like? If we tried this out, we will get an error because the shape of the matrix does not conform to the rule of matrix multiplication. The rule requires that matrix <strong class="source-inline">a</strong> should be made up of <em class="italic">i</em> rows x <em class="italic">m</em> columns, and matrix <strong class="source-inline">b</strong> should be made up of <em class="italic">m</em> rows x <em class="italic">n</em> columns, where the value of <em class="italic">m</em> must be the same in both matrices. The new matrix will have a shape of <em class="italic">i</em> x <em class="italic">n</em>, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<img alt="Figure 2.5 – Matrix multiplication" height="451" src="image/B18118_02_005.jpg" width="821"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Matrix multiplication</p>
<p>Now we can see why we cannot multiply matrix <strong class="source-inline">a</strong> by itself, because the number of rows in the first matrix must be equal to the number of columns in the second matrix. However, we can fulfill the requirement of the matrix multiplication rule by either transposing or reshaping matric <strong class="source-inline">a</strong> if we want to multiply <strong class="source-inline">a</strong> by itself. Let us try <span class="No-Break">this out:</span></p>
<pre class="source-code">
tf.matmul(a,tf.transpose(a, perm=[1,0]))</pre>
<p>When we transpose <a id="_idIndexMarker096"/>matrix <strong class="source-inline">a</strong>, we swap the rows and columns of the matrix based on the <strong class="source-inline">perm</strong> parameter, which we set to <strong class="source-inline">[1,0]</strong>. When we execute the <strong class="source-inline">matmul</strong> function using <strong class="source-inline">a</strong> and the transpose of <strong class="source-inline">a</strong>, we get a new matrix that complies with the rule of <span class="No-Break">matrix multiplication:</span></p>
<pre class="source-code">
&lt;tf.Tensor: shape=(3, 3), dtype=int32,
    numpy= array([[ 5, 11, 17], [11, 25, 39],
    [17, 39, 61]], dtype=int32)&gt;</pre>
<p>How about we try out matrix multiplication using <strong class="source-inline">reshape</strong>? Give this a shot and compare your result with our working Colab notebook results. We have looked at a whole lot of operations already. How about we build our first model? Let us do <span class="No-Break">that next.</span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor060"/>Hello World in TensorFlow</h1>
<p>We have covered a lot<a id="_idIndexMarker097"/> of basic operations in TensorFlow. Now, let’s build our first model in TensorFlow. For this example, let us say you are part of a research team studying the correlation between the number of hours a student studied in <a id="_idIndexMarker098"/>a term and their final grade. Of course, this is a theoretical scenario and there are a lot more factors that come into play when it comes to how well a student will perform. However, in this case, we will take only one attribute as the determinant of success – hours of study. After a term of study, we successfully collated the hours of study of students and their corresponding grades, as shown in <span class="No-Break"><em class="italic">Table 2.1</em></span><span class="No-Break">.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Hours <span class="No-Break">of Study</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">20</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">23</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">25</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">30</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">37</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">43</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">46</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Test Score</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">45</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">51</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">55</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">61</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">65</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">79</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">85</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">91</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">97</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1 – Students’ performance table</p>
<p>Now, we want to<a id="_idIndexMarker099"/> build a model to predict how well a student will perform in the future based on the hours of study they put in. Ready? Let’s do this <span class="No-Break">together now:</span></p>
<ol>
<li>Let’s build this together by opening the accompanying notebook called <strong class="source-inline">hello world</strong>. First, we import TensorFlow. Remember in <a href="B18118_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Machine Learning</em>, we talked about features and labels. Here, we just have one feature – hours of study – and our label or target variable is the test score. Using the powerful Keras API, in a few lines of code, we will build and train a model to get predictions. Let’s <span class="No-Break">get started:</span><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow import keras</pre><pre class="source-code">
from tensorflow.keras import Sequential</pre><pre class="source-code">
from tensorflow.keras.layers import Dense</pre><pre class="source-code">
print(tf.__version__)</pre></li>
</ol>
<p>We start by importing TensorFlow and the Keras API; don’t worry about all the terms, we will unpack everything in detail in <a href="B18118_03.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Linear Regression with TensorFlow</em>. The goal here is to show you how we build a basic model. Don’t worry so much about the technicalities; running the code and seeing how it works is the goal here. After importing the necessary libraries, we continue with our tradition of printing our TensorFlow version. The code <span class="No-Break">runs fine.</span></p>
<ol>
<li value="2">Next, we proceed to import <strong class="source-inline">numpy</strong> for carrying out mathematical operations and <strong class="source-inline">matplotlib</strong> for visualizing <span class="No-Break">our data:</span><pre class="source-code">
#import additional libraries</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li>
</ol>
<p>We run the code and we get no errors, so we are good <span class="No-Break">to proceed.</span></p>
<ol>
<li value="3">We set up a<a id="_idIndexMarker100"/> list of <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> values representing our hours of study and test <span class="No-Break">scores, respectively:</span><pre class="source-code">
# Hours of study</pre><pre class="source-code">
X = [20,23,25,28,30,37,40,43,46]</pre><pre class="source-code">
# Test Scores</pre><pre class="source-code">
y = [45, 51, 55, 61, 65, 79, 85, 91, 97]</pre></li>
<li>To get a good sense of data distribution, we use <strong class="source-inline">matplotlib</strong> to visualize <span class="No-Break">our data:</span><pre class="source-code">
plt.plot(X, y)</pre><pre class="source-code">
plt.title("Exam Performance graph")</pre><pre class="source-code">
plt.xlabel('Hours of Study')</pre><pre class="source-code">
plt.ylabel('Test Score')</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>The code block plots a graph of <strong class="source-inline">X</strong> (hours of study) against <strong class="source-inline">y</strong> (test score) and displays the title (<strong class="source-inline">Exam Performance graph</strong>) of our plot. We use the <strong class="source-inline">show()</strong> function to display the graph, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 2.6 – Students’ performance plot" height="449" src="image/B18118_02_006.jpg" width="555"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Students’ performance plot</p>
<p>From the plot, we <a id="_idIndexMarker101"/>can see the data shows a linear relationship. This assumption is not so bad considering we would logically expect a student who works harder to score <span class="No-Break">better marks.</span></p>
<ol>
<li value="5">Without getting into a debate about whether this theory holds, let us use the Keras API to build a <span class="No-Break">simple model:</span><pre class="source-code">
study_model = Sequential([Dense(units=1,</pre><pre class="source-code">
    input_shape=[1])])</pre><pre class="source-code">
study_model.compile(optimizer='adam',</pre><pre class="source-code">
    loss='mean_squared_error')</pre><pre class="source-code">
X= np.array(X, dtype=int)</pre><pre class="source-code">
y= np.array(y, dtype=int)</pre></li>
</ol>
<p>We build a one-layer model, which we call <strong class="source-inline">study_model</strong>, and we convert our list of <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> values into a <span class="No-Break">NumPy array.</span></p>
<ol>
<li value="6">Next, we fit our <a id="_idIndexMarker102"/>model and run it for <span class="No-Break">2,500 epochs:</span><pre class="source-code">
#fitting the model</pre><pre class="source-code">
history= study_model.fit(X, y, epochs=2500)</pre></li>
</ol>
<p>When we run the model, it should take less than 5 minutes. We can see that the loss drops rapidly initially and gradually flattens out at around 2,000, epochs as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 2.7 – Model loss plot" height="276" src="image/B18118_02_007.jpg" width="390"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Model loss plot</p>
<p>And just like that, we have trained a model that can be used to determine how a student will perform at the end of a term. This is a very basic task and feels like using a hammer on a fly. However, let us try our <span class="No-Break">model out:</span></p>
<pre class="source-code">
#Let us predict how well a student will perform based on their study time
n=38 #Hours of study
result =study_model.predict([n])[0][0] #Result
rounded_number = round(81.0729751586914, 2)</pre>
<p>If we run this code, we<a id="_idIndexMarker103"/> generate the result for a student who studied for 38 hours. Remember our model was not trained on this value. So, let us see what our model thinks this student <span class="No-Break">will score:</span></p>
<pre class="source-code">
print(f"If I study for {n} hours,
     I will get { rounded_number} marks as my grade.")
If I study for 38 hours, I will get 81.07 marks as my grade.</pre>
<p>Our model predicted that this student would score 81.07 marks. Good result, but how do we know whether our model was right or wrong? If you look at <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.6</em>, you may guess that our predicted result should be around this score, but you may also have figured out that we used <em class="italic">2x + 5 = y</em> to generate our <strong class="source-inline">y</strong> values. If we input <strong class="source-inline">X=38</strong>, we get <em class="italic">2(38) + 5= 81</em>. Our model did an excellent job of getting the correct score with a minute error of .07; however, we had to train it for a very long time to achieve this result for a very simple task. In the coming chapters, we will learn how to train a much better model using techniques such as normalization and, of course, with a larger dataset, where we will work with a training set and a validation set and make predictions on a test set. The goal here was to get a feel of what is to come, so try out a few numbers to see how the model will perform. Do not go above 47, as you will get a score <span class="No-Break">above 100.</span></p>
<p>Now that we have built our first model, let us look at how to debug and solve error messages. This is something you will encounter many times if you decide to pursue a career in <span class="No-Break">this space.</span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor061"/>Debugging and solving error messages</h1>
<p>As you go through<a id="_idIndexMarker104"/> the exercises or walk through the code in this book, in any other resource, or in your own personal projects, you will quickly realize how often code breaks, and mastering how to resolve these errors will help you to move quickly through your learning process or when building projects. First, when you get an error, it is<a id="_idIndexMarker105"/> important to check what the error message is. Next is to understand the meaning of the error message. Let us look at some errors that a few students stumbled upon when implementing basic operations in TensorFlow. Let’s run the following code to generate a <span class="No-Break">new vector:</span></p>
<pre class="source-code">
tf.variable([1,2,3,4])</pre>
<p>Running this code will throw the error shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 2.8 – Example of an error" height="185" src="image/B18118_02_008.jpg" width="736"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Example of an error</p>
<p>From the error message, we can see that there is no attribute called <strong class="source-inline">variable</strong> in TensorFlow. This draws our attention to where the error is coming from and we immediately notice that we wrote <strong class="source-inline">variable</strong> instead of <strong class="source-inline">Variable</strong> with a capital <em class="italic">V</em>, as stipulated in the documentation. However, if we are not able to debug this ourselves, we can click on the <strong class="bold">SEARCH STACK OVERFLOW</strong> button, as this is a good place to find solutions to everyday coding problems we might encounter. The odds are someone else has faced the same problem and a solution can be found on <span class="No-Break">Stack Overflow.</span></p>
<p>Let us click on the link and see what we can find on <span class="No-Break">Stack Overflow:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 2.9 – Stack Overflow solution for AttributeError" height="335" src="image/B18118_02_009.jpg" width="786"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Stack Overflow solution for AttributeError</p>
<p>Hurray! On Stack Overflow, we<a id="_idIndexMarker106"/> see <a id="_idIndexMarker107"/>the solution to the problem and a link to the documentation for more details. Remember, it is best to first look at the error message and see whether you can resolve it yourself before heading to Stack Overflow. If you put this into practice, as well as dedicate time to reading the documentation, you will get better and better at debugging issues and make fewer mistakes, but you will still need Stack Overflow or the documentation. It comes with the terrain. Before we draw the curtains on this chapter, let us summarize quickly what <span class="No-Break">we learned.</span></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor062"/>Summary</h1>
<p>In this chapter, we covered the TensorFlow ecosystem at a high level. We looked at some of the key components that make TensorFlow the platform of choice for building deep learning applications and solutions for many ML engineers, researchers, and enthusiasts. Next, we discussed what tensors are and how they are useful in our models. After this, we looked at a few ways of creating tensors. We explored various tensor properties and we saw how to implement some basic tensor operations with TensorFlow. We built a simple model and used it to make predictions. Finally, we looked at how to debug and solve error messages in TensorFlow and ML <span class="No-Break">at large.</span></p>
<p>In the next chapter, we will look at regression modeling in a hands-on manner. We will learn how to extend our simple model to solve a regression problem for a company’s HR department. Also, what you have learned about debugging could prove useful in the next chapter – see <span class="No-Break">you there.</span></p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor063"/>Questions</h1>
<p>Let’s test what we have learned in <span class="No-Break">this chapter:</span></p>
<ol>
<li>What <span class="No-Break">is TensorFlow?</span></li>
<li>What <span class="No-Break">are tensors?</span></li>
<li>Generate a matrix using <strong class="source-inline">tf.</strong><strong class="source-inline">V</strong><strong class="source-inline">ariable</strong> with the <strong class="source-inline">tf.float64</strong> data type and name <span class="No-Break">the variable.</span></li>
<li>Generate 15 random numbers between 1 and 20 and extract the lowest number, the highest number, the mean, and the index with the lowest and <span class="No-Break">highest numbers.</span></li>
<li>Generate a 4 x 3 matrix, and multiply the matrix by <span class="No-Break">its transpose.</span></li>
</ol>
<h1 id="_idParaDest-59"><a id="_idTextAnchor064"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Amr, T., 2020. <em class="italic">Hands-On Machine Learning with scikit-learn and Scientific Python Toolkits</em>. [S.l.]: <span class="No-Break">Packt Publishing.</span></li>
<li><em class="italic">TensorFlow </em><span class="No-Break"><em class="italic">guide</em></span><span class="No-Break">: </span><a href="https://www.TensorFlow.org/guide"><span class="No-Break">https://www.TensorFlow.org/guide</span></a></li>
</ul>
</div>
</div></body></html>