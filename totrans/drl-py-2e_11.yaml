- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actor-Critic Methods – A2C and A3C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered two types of methods for learning the optimal policy.
    One is the value-based method, and the other is the policy-based method. In the
    value-based method, we use the Q function to extract the optimal policy. In the
    policy-based method, we compute the optimal policy without using the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about another interesting method called the actor-critic
    method for finding the optimal policy. The actor-critic method makes use of both
    the value-based and policy-based methods. We will begin the chapter by understanding
    what the actor-critic method is and how it makes use of value-based and policy-based
    methods. We will acquire a basic understanding of actor-critic methods, and then
    we will learn about them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we will also learn how actor-critic differs from the policy gradient
    with baseline method, and we will learn the algorithm of the actor-critic method
    in detail. Next, we will understand what **Advantage Actor-Critic** (**A2C**)
    is, and how it makes use of the advantage function.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we will learn about one of the most popularly used
    actor-critic algorithms, called **Asynchronous Advantage Actor-Critic** (**A3C**).
    We will understand what A3C is and the details of how it works along with its
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the actor-critic method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the actor-critic method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actor-critic method algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantage actor-critic (A2C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous advantage actor-critic (A3C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of asynchronous advantage actor-critic (A3C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mountain car climbing using A3C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the chapter by getting a basic understanding of the actor-critic
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the actor-critic method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The actor-critic method is one of the most popular algorithms in deep reinforcement
    learning. Several modern deep reinforcement learning algorithms are designed based
    on actor-critic methods. The actor-critic method lies at the intersection of value-based
    and policy-based methods. That is, it takes advantage of both value-based and policy-based
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, without going into further detail, first, let's acquire a basic
    understanding of how the actor-critic method works and then, in the next section,
    we will get into more detail and understand the math behind the actor-critic method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actor-critic, as the name suggests, consists of two types of network—the actor
    network and the critic network. The role of the actor network is to find an optimal
    policy, while the role of the critic network is to evaluate the policy produced
    by the actor network. So, we can think of the critic network as a feedback network
    that evaluates and guides the actor network in finding the optimal policy, as
    *Figure 11.1* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The actor-critic network'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so what actually are the actor and critic networks? How do they work together
    and improve the policy? The actor network is basically the policy network, and
    it finds the optimal policy using a policy gradient method. The critic network
    is basically the value network, and it estimates the state value.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using its state value, the critic network evaluates the action produced
    by the actor network and sends its feedback to the actor. Based on the critic's
    feedback, the actor network then updates its parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in the actor-critic method, we use two networks—the actor network (policy
    network), which computes the policy, and the critic network (value network), which
    evaluates the policy produced by the actor network by computing the value function
    (state values). Isn't this similar to something we just learned in the previous
    chapter?
  prefs: []
  type: TYPE_NORMAL
- en: Yes! If you recall, it is similar to the policy gradient method with the baseline
    (REINFORCE with baseline) we learned in the previous chapter. Similar to REINFORCE
    with baseline, here also, we have an actor (policy network) and a critic (value
    network) network. However, actor-critic is NOT the same as REINFORCE with baseline.
    In the REINFORCE with baseline method, we learned that we use a value network
    as the baseline and it helps to reduce the variance in the gradient updates. In
    the actor-critic method as well, we use the critic to reduce variance in the gradient
    updates of the actor, but it also helps to improve the policy iteratively in an
    online fashion. The distinction between these two will be made clear in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of the actor-critic method, in the next
    section, we will learn how the actor-critic method works in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the actor-critic method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the REINFORCE with baseline method, we learned that we have two networks—policy
    and value networks. The policy network finds the optimal policy, while the value
    network acts as a baseline and corrects the variance in the gradient updates.
    Similar to REINFORCE with baseline, the actor-critic method also consists of a
    policy network, known as the actor network, and the value network, known as the
    critic network.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental difference between the REINFORCE with baseline method and the
    actor-critic method is that in the REINFORCE with baseline method, we update the
    parameter of the network at the end of an episode. But in the actor-critic method,
    we update the parameter of the network at every step of the episode. But why do
    we have to do this? What is the use of updating the network parameter at every
    step of the episode? Let's explore this in further detail.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of the REINFORCE with baseline method being similar to the **Monte
    Carlo** (**MC**) method, which we covered in *Chapter 4*, *Monte Carlo Methods*,
    and the actor-critic method being similar to the TD learning method, which we
    covered in *Chapter 5*, *Understanding Temporal Difference Learning*. So, first,
    let's recap these two methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the MC method, to compute the value of a state, we generate some *N* trajectories
    and compute the value of a state as an average return of a state across the *N*
    trajectories. We learned that when the trajectory is too long, then the MC method
    will take us a lot of time to compute the value of the state and is also unsuitable
    for non-episodic tasks. So, we resorted to the TD learning method.
  prefs: []
  type: TYPE_NORMAL
- en: In the TD learning method, we learned that instead of waiting until the end
    of the episode to compute the value of the state, we can make use of bootstrapping
    and estimate the value of the state as the sum of the immediate reward and the
    discounted value of the next state.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how the MC and TD methods relate to the REINFORCE with baseline
    and actor-critic methods, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s recall what we learned in the REINFORCE with baseline method.
    In the REINFORCE with baseline method, we generate *N* number of trajectories
    using the policy ![](img/B15558_10_111.png) and compute the gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_163.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can observe, in order to compute the gradients, we need a complete trajectory.
    That is, as the following equation shows, in order to compute the gradient, we
    need to compute the return of the trajectory. We know that the return is the sum
    of rewards of the trajectory, so in order to compute the return (reward-to-go),
    first, we need a complete trajectory generated using the policy ![](img/B15558_10_120.png).
    So, we generate several trajectories using the policy ![](img/B15558_10_111.png)
    and then we compute the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the gradients, we update the parameter as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of generating the complete trajectory and then computing the return,
    can we make use of bootstrapping, as we learned in TD learning? Yes! In the actor-critic
    method, we approximate the return by just taking the immediate reward and the
    discounted value of the next state as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *r* is the immediate reward and ![](img/B15558_05_006.png) is the discounted
    value of the next state.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can rewrite the policy gradient by replacing the return *R* by the bootstrap
    estimate, ![](img/B15558_05_008.png), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_009.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we don't have to wait till the end of the episode to compute the return.
    Instead, we bootstrap, compute the gradient, and update the network parameter
    at every step of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between how we compute the gradient and update the parameter
    of the policy network in REINFORCE with baseline and the actor-critic method is
    shown in *Figure 11.2*. As we can observe in REINFORCE with baseline, first we
    generate complete episodes (trajectories), and then we update the parameter of
    the network. Whereas, in the actor-critic method, we update the parameter of the
    network at every step of the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: The difference between the REINFORCE with baseline and actor-critic
    methods'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, what about the critic network (value network)? How can we update the
    parameter of the critic network? Similar to the actor network, we update the parameter
    of the critic network at every step of the episode. The loss of the critic network
    is the TD error, which is the difference between the target value of the state
    and the value of the state predicted by the network. The target value of the state
    can be computed as the sum of reward and the discounted value of the next state
    value. Thus, the loss of the critic network is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_11_011.png) is the target value of the state and ![](img/B15558_11_012.png)
    is the predicted value of the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the loss of the critic network, we compute gradients ![](img/B15558_11_014.png)
    and update the parameter ![](img/B15558_11_043.png) of the critic network at every
    step of the episode using gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have learned how the actor (policy network) and critic (value network)
    work in the actor-critic method; let's look at the algorithm of the actor-critic
    method in the next section for more clarity.
  prefs: []
  type: TYPE_NORMAL
- en: The actor-critic algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for the actor-critic algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the actor network parameter ![](img/B15558_09_008.png) and the critic
    network parameter ![](img/B15558_11_016.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 3*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,. . . ., *T*-1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action using the policy, ![](img/B15558_11_017.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the action *a*[t] in the state *s*[t], observe the reward *r*, and move
    to the next state ![](img/B15558_11_018.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the policy gradients:![](img/B15558_11_009.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter ![](img/B15558_09_054.png) using gradient
    ascent:![](img/B15558_11_005.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network:![](img/B15558_11_010.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients ![](img/B15558_11_014.png) and update the critic network parameter
    ![](img/B15558_11_023.png) using gradient descent:![](img/B15558_11_013.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can observe from the preceding algorithm, the actor network (policy network)
    parameter is being updated at every step of the episode. So, in each step of the
    episode, we select an action based on the updated policy while the critic network
    (value network) parameter is also getting updated at every step, and thus the
    critic also improves at evaluating the actor network at every step of the episode.
    While with the REINFORCE with baseline method, we only update the parameter of
    the network after generating the complete episodes.
  prefs: []
  type: TYPE_NORMAL
- en: One more important difference we should note down between the REINFORCE with
    baseline and the actor-critic method is that, in the REINFORCE with baseline we
    use the full return of the trajectory whereas in the actor-critic method we use
    the bootstrapped return.
  prefs: []
  type: TYPE_NORMAL
- en: The actor-critic algorithm we just learned is often referred to as the **Advantage
    Actor-Critic** (**A2C**). In the next section, we will look into the advantage
    function and learn why our algorithm is called the advantage actor-critic in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Advantage actor-critic (A2C)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before moving on, first, let''s recall the advantage function. The advantage
    function is defined as the difference between the Q function and the value function.
    We can express the advantage function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_025.png)'
  prefs: []
  type: TYPE_IMG
- en: The advantage function tells us, in state *s*, how good action *a* is compared
    to the average actions.
  prefs: []
  type: TYPE_NORMAL
- en: In A2C, we compute the policy gradient with the advantage function. So, first,
    let's see how to compute the advantage function. We know that the advantage function
    is the difference between the Q function and the value function, that is, *Q*(*s*,
    *a*) – *V*(*s*), so we can use two function approximators (neural networks), one
    for estimating the Q function and the other for estimating the value function.
    Then, we can subtract the values of these two networks to get the advantage value.
    But this will definitely not be an optimal method and, computationally, it will
    be expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can approximate the Q value as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_026.png)'
  prefs: []
  type: TYPE_IMG
- en: But how can we approximate the Q value like this? Do you recall in *Chapter
    3*, *The Bellman Equation and Dynamic Programming*, in *The relationship between
    the value and Q functions* section, we learned we could derive the Q function
    from the value function? Using that identify, we can approximate the Q function
    as the sum of the immediate reward and the discounted value of the next state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Substituting the preceding Q value in the advantage function, equation (1),
    we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, now we have the advantage function. We learned that in A2C, we compute
    the policy gradient with the advantage function. So, we can write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expanding the advantage function, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can observe, our policy gradient is now computed using the advantage
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, check the preceding equation with how we computed the gradient in the previous
    section. We can observe that both are essentially the same. Thus, the A2C method
    is the same as what we learned in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous advantage actor-critic (A3C)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous advantage actor-critic, hereinafter referred to as A3C, is one
    of the popular actor-critic algorithms. The main idea behind the asynchronous
    advantage actor-critic method is that it uses several agents for learning in parallel
    and aggregates their overall experience.
  prefs: []
  type: TYPE_NORMAL
- en: In A3C, we will have two types of networks, one is a global network (global
    agent), and the other is the worker network (worker agent). We will have many
    worker agents, each worker agent uses a different exploration policy, and they
    learn in their own copy of the environment and collect experience. Then, the experience
    obtained from these worker agents is aggregated and sent to the global agent.
    The global agent aggregates the learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a very basic idea of how A3C works, let's go into more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The three As
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving in, let's first learn what the three A's in A3C signify.
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous**: Asynchronous implies the way A3C works. That is, instead
    of having a single agent that tries to learn the optimal policy, here, we have
    multiple agents that interact with the environment. Since we have multiple agents
    interacting with the environment at the same time, we provide copies of the environment
    to every agent so that each agent can then interact with their own copy of the
    environment. So, all these multiple agents are called worker agents and we have
    a separate agent called the global agent. All the worker agents report to the
    global agent asynchronously and the global agent aggregates the learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantage**:We have already learned what an advantage function is in the
    previous section. An advantage function can be defined as the difference between
    the Q function and the value function.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor-critic**: Each of the worker networks (worker agents) and the global
    network (global agent) basically follow an actor-critic architecture. That is,
    each of the agents consists of an actor network for estimating the policy and
    the critic network for evaluating the policy produced by the actor network.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us move on to the architecture of A3C and understand how A3C works
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of A3C
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s understand the architecture of A3C. The architecture of A3C is
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: The architecture of A3C'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from the preceding figure, we have multiple worker agents
    and each worker agent interacts with their own copies of the environment and collects
    experience. We can also observe that each worker agent follows an actor-critic
    architecture. So, the worker agents compute the actor network loss (policy loss)
    and critic network loss (value loss).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we learned that our actor network is updated by computing
    the policy gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the actor loss is basically the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can observe, actor loss is the product of log probability of the action
    and the TD error. Now, we add a new term to our actor loss called the entropy
    (measure of randomness) of the policy and redefine the actor loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_031.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_11_032.png) denotes the entropy of the policy. Adding the
    entropy of the policy promotes sufficient exploration, and the parameter ![](img/B15558_06_030.png)
    is used to control the significance of the entropy.
  prefs: []
  type: TYPE_NORMAL
- en: The critic loss is just the mean squared TD error.
  prefs: []
  type: TYPE_NORMAL
- en: After computing the losses of the actor and critic networks, worker agents compute
    the gradients of the loss and then they send those gradients to the global agent.
    That is, the worker agents compute the gradients and their gradients are asynchronously
    accumulated to the global agent. The global agent updates their parameters using
    the asynchronously received gradients from the worker agents. Then, the global
    agent sends the updated parameter periodically to the worker agents, so now the
    worker agents will get updated.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, each worker agent computes loss, calculates gradients, and sends
    those gradients to the global agent asynchronously. Thus, the global agent parameter
    is updated by gradients received from the worker agents. Then, the global agent
    sends the updated parameter to the worker agents periodically.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have many worker agents interacting with their own copies of the environment
    and aggregating the information to the global network, there will be low to no
    correlation between the experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in A3C are:'
  prefs: []
  type: TYPE_NORMAL
- en: The worker agent interacts with their own copies of the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each worker follows a different policy and collects the experience.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the worker agents compute the losses of the actor and critic networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the loss, they calculate gradients of the loss, and send those gradients
    to the global agent asynchronously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The global agent updates their parameters with the gradients received from the
    worker agents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the updated parameter from the global agent will be sent to the worker agents
    periodically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We repeat the preceding steps for several iterations to find the optimal policy.
    To get a clear understanding of how A3C works, in the next section, we will learn
    how to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Mountain car climbing using A3C
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s implement the A3C algorithm for the mountain car climbing task. In the
    mountain car climbing environment, a car is placed between two mountains and the goal
    of the agent is to drive up the mountain on the right. But the problem is, the agent
    can''t drive up the mountain in one pass. So, the agent has to drive back and forth
    to build momentum to drive up the mountain on the right. A high reward will be
    assigned if our agent spends less energy while driving up. *Figure 11.4* shows
    the mountain car environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: The mountain car environment'
  prefs: []
  type: TYPE_NORMAL
- en: The code used in this section is adapted from the open source implementation
    of A3C ([https://github.com/stefanbo92/A3C-Continuous](https://github.com/stefanbo92/A3C-Continuous))
    provided by Stefan Boschenriedter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Creating the mountain car environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s create a mountain car environment using Gym. Note that our mountain
    car environment is a continuous environment, meaning that our action space is
    continuous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the state shape of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the action shape of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we created the continuous mountain car environment, and thus our
    action space consists of continuous values. So, we get the bounds of our action
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Defining the variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's define some of the important variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the number of workers as the number of CPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the number of episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the number of time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the global network (global agent) scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the time step at which we want to update the global network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the discount factor, ![](img/B15558_03_190.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the beta value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the directory where we want to store the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Defining the actor-critic class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that in A3C, both the global and worker agents follow the actor-critic
    architecture. So, let's define the class called `ActorCritic`, where we will implement
    the actor-critic algorithm. For a clear understanding, let's look into the code
    line by line. You can also access the complete code from the book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Defining the init method:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, let''s define the init method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the actor network optimizer as RMS prop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the critic network optimizer as RMS prop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If the scope is the global network (global agent):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the global network (global agent) and get the actor and critic parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If the network is not the global network, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We learned that our environment is the continuous environment, so our actor
    network (policy network) returns the mean and variance of the action and then
    we build the action distribution out of this mean and variance and select the
    action based on this action distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the placeholder to obtain the action distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the worker network (worker agent) and get the mean and variance of the
    action, the value of the state, and the actor and critic network parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the TD error, which is the difference between the target value of the
    state and its predicted value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the critic network loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a normal distribution based on the mean and variance of the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the actor network loss. We learned that the loss of the
    actor network is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_041.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the log probability of the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the entropy of the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the actor network loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action based on the normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the gradients of the actor and critic network losses of the worker
    agent (local agent):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform the sync operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After computing the gradients of the losses of the actor and critic networks,
    the worker agents send (push) those gradients to the global agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The global agent updates their parameters with the gradients received from
    the worker agents (local agents). Then, the worker agents pull the updated parameters
    from the global agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Building the network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s define the function for building the actor-critic network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the actor network, which returns the mean and variance of the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the critic network, which returns the value of the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the parameters of the actor and critic networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the mean and variance of the action produced by the actor network, the
    value of the state computed by the critic network, and the parameters of the actor
    and critic networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Updating the global network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `update_global` to update the parameters of
    the global network with the gradients of loss computed by the worker networks,
    that is, the push operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Updating the worker network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We also define a function called `pull_from_global` to update the parameters
    of the worker networks by pulling from the global network, that is, the pull operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Selecting the action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Define a function called `select_action` to select the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Defining the worker class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s define the class called `Worker`, where we will implement the worker
    agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Defining the init method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, let''s define the `init` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that each worker agent works with their own copies of the environment.
    So, let''s create a mountain car environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the name of the worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an object for our `ActorCritic` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function called `work` for the worker to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a list to store the states, actions, and rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'When the global episodes are less than the number of episodes and the coordinator
    is active:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment of only the worker 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `done` to `True` if we have reached the final step of the episode `else`
    set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the state, action, and reward in the lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s update the global network. If `done` is `True`, then set the value
    of the next state to `0` `else` compute the value of the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the target value, which is ![](img/B15558_11_042.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Reverse the target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Stack the state, action, and target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the feed dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the global network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Empty the lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the worker network by pulling the parameters from the global network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the state to the next state and increment the total step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the global rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s start training the network. Initialize the global rewards list
    and also initialize the global episodes counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a global agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Create *n* number of worker agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the TensorFlow coordinator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the TensorFlow variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the TensorFlow computational graph in the log directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the worker threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: For a better understanding of the A3C architecture, let's take a look at the
    computational graph of A3C in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the computational graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we can observe, we have four worker agents (worker networks) and one global
    agent (global network):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: A computation graph of A3C'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the architecture of the worker agent. As we can observe,
    our worker agents follow the actor-critic architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: A computation graph of A3C with the W_0 node expanded'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s examine the sync node. As *Figure 11.7* shows, we have two operations
    in the sync node, called *push* and *pull*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: A computation graph of A3C showing the push and pull operations
    of the sync node'
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the gradients of the losses of the actor and critic networks,
    the worker agent pushes those gradients to the global agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: A computation graph of A3C—worker agents push their gradients
    to the global agent'
  prefs: []
  type: TYPE_NORMAL
- en: 'The global agent updates their parameters with the gradients received from
    the worker agents. Then, the worker agents pull the updated parameters from the
    global agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: A computation graph of A3C – worker agents pull updated parameters
    from the global agent'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how A3C works, in the next section, let's revisit the
    A2C method.
  prefs: []
  type: TYPE_NORMAL
- en: A2C revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can design our A2C algorithm with many worker agents, just like the A3C algorithm.
    However, unlike A3C, A2C is a synchronous algorithm, meaning that in A2C, we can
    have multiple worker agents, each interacting with their own copies of the environment,
    and all the worker agents perform synchronous updates, unlike A3C, where the worker
    agents perform asynchronous updates.
  prefs: []
  type: TYPE_NORMAL
- en: That is, in A2C, each worker agent interacts with the environment, computes
    losses, and calculates gradients. However, it won't send those gradients to the
    global network independently. Instead, it waits for all other worker agents to
    finish their work and then updates the weights to the global network in a synchronous
    fashion. Performing synchronous weight updates reduces the inconsistency introduced
    by A3C.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding what the actor-critic method is. We
    learned that in the actor-critic method, the actor computes the optimal policy,
    and the critic evaluates the policy computed by the actor network by estimating
    the value function. Next, we learned how the actor-critic method differs from
    the policy gradient method with the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in the policy gradient method with the baseline, first, we generate
    complete episodes (trajectories), and then we update the parameter of the network.
    Whereas, in the actor-critic method, we update the parameter of the network at
    every step of the episode. Moving forward, we learned what the advantage actor-critic
    algorithm is and how it uses the advantage function in the gradient update.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about another interesting actor-critic
    algorithm, called asynchronous advantage actor-critic method. We learned that
    A3C consists of several worker agents and one global agent. All the worker agents
    send their gradients to the global agent asynchronously and then the global agent
    updates their parameters with gradients received from the worker agents. After
    updating the parameters, the global agent sends the updated parameters to the
    worker agents periodically.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, in this chapter, we learned about two interesting actor-critic algorithms
    – A2C and A3C. In the next chapter, we will understand several state-of-the-art
    actor-critic algorithms, including DDPG, TD3, and SAC.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assess our understanding of the actor-critic method by answering the
    following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the actor-critic method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of the actor and critic networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the actor-critic method differ from the policy gradient with the baseline
    method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the gradient update equation of the actor network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does A2C work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does *asynchronous* mean in A3C?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does A2C differ from A3C?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, refer to the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Asynchronous Methods for Deep Reinforcement Learning*, by Volodymyr Mnih et
    al.: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
