<html><head></head><body>
  <div id="_idContainer124">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-87" class="chapterTitle">Generating Text with RNNs and GPT-2</h1>
    <p class="normal">When your mobile phone completes a word as you type a message or when Gmail suggests a short reply or completes a sentence as you reply to an email, a text generation model is working in the background. The Transformer architecture forms the basis of state-of-the-art text generation models. BERT, as explained in the previous chapter, uses only the encoder part of the Transformer architecture.</p>
    <p class="normal">However, BERT, being bi-directional, is not suitable for the generation of text. A left-to-right (or right-to-left, depending on the language) language model built on the decoder part of the Transformer architecture is the foundation of text generation models today.</p>
    <p class="normal">Text can be generated a character at a time or with words and sentences together. Both of these approaches are shown in this chapter. Specifically, we will cover the following topics:</p>
    <ul>
      <li class="bullet">Generating text with:<ul>
          <li class="bullet-l2">Character-based RNNs for generating news headlines and completing text messages</li>
          <li class="bullet-l2">GPT-2 to generate full sentences</li>
        </ul>
      </li>
      <li class="bullet">Improving the quality of text generation using techniques such as:<ul>
          <li class="bullet-l2">Greedy search</li>
          <li class="bullet-l2">Beam search</li>
          <li class="bullet-l2">Top-K sampling</li>
        </ul>
      </li>
      <li class="bullet">Using advanced techniques such as learning rate annealing and checkpointing to enable long training times:</li>
      <li class="bullet">Details of the Transformer decoder architecture</li>
      <li class="bullet">Details of the GPT and GPT-2 models</li>
    </ul>
    <p class="normal">A character-based approach for generating text is shown first. Such models can be quite useful for generating completions of a partially typed word in a sentence on a messaging platform, for example.</p>
    <h1 id="_idParaDest-88" class="title">Generating text – one character at a time</h1>
    <p class="normal">Text generation yields a <a id="_idIndexMarker302"/>window into whether deep learning models are learning about the underlying structure of language. Text will be generated using two different approaches in this chapter. The first approach is an RNN-based model that generates a character at a time.</p>
    <p class="normal">In the previous chapters, we have seen different tokenization methods based on words and sub-words. Text is tokenized into characters, which include capital and small letters, punctuation symbols, and digits. There are 96 tokens in total. This tokenization is an extreme example to test how much a model can learn about the language structure. The model will be trained to predict the next character based on a given set of input characters. If there is indeed an underlying structure in the language, the model should pick it up and generate reasonable-looking sentences.</p>
    <p class="normal">Generating coherent sentences one character at a time is a very challenging task. The model does not have a dictionary or vocabulary, and it has no sense of capitalization of nouns or any grammar rules. Yet, we are expecting it to generate reasonable-looking sentences. The structure of words and their order in a sentence is not random but driven by grammar rules in a language. Words have some structure, based on parts of speech and word roots. A character-based model has the smallest possible vocabulary, but we hope that the model learns a lot about the use of the letters. This may seem like a tall order but be prepared to be surprised. Let's get started with the data loading and pre-processing steps.</p>
    <h2 id="_idParaDest-89" class="title">Data loading and pre-processing</h2>
    <p class="normal">For this particular example, we are<a id="_idIndexMarker303"/> going to use data from a constrained domain – a set of news headlines. The hypothesis is that news headlines are usually short<a id="_idIndexMarker304"/> and follow a particular structure. These headlines are usually a summary of an article and contain a large number of proper nouns like names of companies and celebrities. For this particular task, data from two different datasets are joined together and used. The first dataset is called the News Aggregator<a id="_idIndexMarker305"/> dataset generated by the Artificial Intelligence Lab, part of the Faculty of Engineering at Roma Tre University in Italy. The University of California, Irvine, has made the dataset available for download from <a href="https://archive.ics.uci.edu/ml/datasets/News+Aggregator"><span class="url">https://archive.ics.uci.edu/ml/datasets/News+Aggregator</span></a>. This dataset has over 420,000 news article titles, URLs, and other information. The second dataset is a set of over 200,000 news articles from The Huffington Post, called the News Category dataset, collected by Rishabh Mishra and posted on Kaggle at <a href="https://www.kaggle.com/rmisra/news-category-dataset"><span class="url">https://www.kaggle.com/rmisra/news-category-dataset</span></a>. </p>
    <p class="normal">News article headlines <a id="_idIndexMarker306"/>from both datasets are extracted and <a id="_idIndexMarker307"/>compiled into one file. This step is already done to save time. The compressed output file is called <code class="Code-In-Text--PACKT-">news-headlines.tsv.zip</code> and is located in the <code class="Code-In-Text--PACKT-">chapter5-nlg-with-transformer-gpt/char-rnn</code> GitHub folder corresponding to this chapter. The folder is located inside the GitHub repository for this book. The format of this file is pretty simple. It has two columns separated by a tab. The first column is the original headline, and the second column is an uncased version of the same headline. This example uses the first column of the file only.</p>
    <p class="normal">However, you can try the uncased version to see how the results differ. Training such models usually takes a lot of time, often several hours. Training in an IPython notebook can be difficult as a number of issues, such as the loss of the connection to the kernel or the kernel process dying, can result in the loss of the trained model. What we are attempting to do in this example is akin to training BERT from scratch. Don't worry; we train the model for a much shorter time than it took to train BERT. Running long training loops runs the risk of training loops crashing in the middle. In such a case, we don't want to restart training from scratch. The model is checkpointed frequently during training so that the model state can be restored from the last checkpoint if a failure occurs. Then, training can be restarted from the last checkpoint. Python files executed from the command line give the most control when running long training loops.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The command-line instructions shown in this example were tested on an Ubuntu 18.04 LTS machine. These commands should work as is on a macOS command line but may need some adjustments. Windows users may need to translate these commands for their operating system. Windows 10 power users should be able to use <a id="_idIndexMarker308"/>the <strong class="keyword">Windows Subsystem for Linux</strong> (<strong class="keyword">WSL</strong>) capabilities to execute the same commands.</p>
    </div>
    <p class="normal">Going back to the data format, all that needs to be done for loading the data is to unzip the prepared headline file. Navigate to the folder where the ZIP file has been pulled down from GitHub. The compressed file of headlines can be unzipped and inspected:</p>
    <pre class="programlisting con"><code class="hljs-con">$ unzip news-headlines.tsv.zip
Archive:  news-headlines.tsv.zip
  inflating: news-headlines.tsv
</code></pre>
    <p class="normal">Let's inspect the contents of the file to get a sense of the data:</p>
    <pre class="programlisting con"><code class="hljs-con">$ head -3 news-headlines.tsv
There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV there were 2 mass shootings in texas last week, but only 1 on tv
Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song will smith joins diplo and nicky jam for the 2018 world cup's official song
Hugh Grant Marries For The First Time At Age 57 hugh grant marries for the first time at age 57
</code></pre>
    <p class="normal">The model is<a id="_idIndexMarker309"/> trained on the headlines shown above. We are ready to<a id="_idIndexMarker310"/> move on to the next step and load the file to perform normalization and tokenization.</p>
    <h2 id="_idParaDest-90" class="title">Data normalization and tokenization</h2>
    <p class="normal">As discussed above, this model <a id="_idIndexMarker311"/>uses a token per character. So, each letter, including <a id="_idIndexMarker312"/>punctuation, numbers, and space, becomes a token. Three additional tokens are added. These are:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">&lt;EOS&gt;</code>: Denotes end of sentences. The model can use this token to indicate that the generation of text is complete. All headlines end with this token.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">&lt;UNK&gt;</code>: While this is a character-level model, it is possible to have different characters from other languages or character sets in the dataset. When a character is detected that is not present in our set of 96 characters, this token is used. This approach is consistent with word-based vocabulary approaches where it is common to replace out-of-vocabulary words with a special token.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">&lt;PAD&gt;</code>: This is a unique padding token used to pad all headlines to the same length. Padding is done by hand in this example as opposed to using TensorFlow methods, which we have seen previously.</li>
    </ul>
    <p class="normal">All the code in this section will refer to the <code class="Code-In-Text--PACKT-">rnn-train.py</code> file from the <code class="Code-In-Text--PACKT-">chapter5-nlg-with-transformer-gpt</code> folder of the GitHub repo of the book. The first part of this file has the imports and optional instructions for setting up a GPU. Ignore this section if your setup does not use a GPU.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">A GPU is an excellent investment for deep learning engineers and researchers. A GPU could speed up your training times by orders of magnitude or more! It would be worthwhile to outfit your deep learning setup with a GPU like the Nvidia GeForce RTX 2070.</p>
    </div>
    <p class="normal">The code for data normalization <a id="_idIndexMarker313"/>and tokenization is between lines 32<a id="_idIndexMarker314"/> and 90 of this file. To start, the tokenization function needs to be set up:</p>
    <pre class="programlisting code"><code class="hljs-code">chars = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(<span class="hljs-string">"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:'''/\|_@#$%ˆ&amp;*˜'+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span>))
chars = <span class="hljs-built_in">list</span>(chars)
EOS = <span class="hljs-string">'&lt;EOS&gt;'</span>
UNK = <span class="hljs-string">"&lt;UNK&gt;"</span>
PAD = <span class="hljs-string">"&lt;PAD&gt;"</span>      <span class="hljs-comment"># need to move mask to '0'index for Embedding layer</span>
chars.append(UNK)
chars.append(EOS)  <span class="hljs-comment"># end of sentence</span>
chars.insert(<span class="hljs-number">0</span>, PAD)  <span class="hljs-comment"># now padding should get index of 0</span>
</code></pre>
    <p class="normal">Once the token list is ready, methods need to be defined for converting characters to tokens and vice versa. Creating mapping is relatively straightforward:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Creating a mapping from unique characters to indices</span>
char2idx = {u:i <span class="hljs-keyword">for</span> i, u <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(chars)}
idx2char = np.array(chars)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">char_idx</span><span class="hljs-functio">(</span><span class="hljs-params">c</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># takes a character and returns an index</span>
    <span class="hljs-comment"># if character is not in list, returns the unknown token</span>
    <span class="hljs-keyword">if</span> c <span class="hljs-keyword">in</span> chars:
        <span class="hljs-keyword">return</span> char2idx[c]
    
    <span class="hljs-keyword">return</span> char2idx[UNK]
</code></pre>
    <p class="normal">Now, the data needs can be read in from the TSV file. A maximum length of 75 characters is used for the headlines. If the headlines are shorter than this length, they are padded. Any headlines longer than 75 characters are snipped. The <code class="Code-In-Text--PACKT-">&lt;EOS&gt;</code> token is appended to the end of every headline. Let's set this up:</p>
    <pre class="programlisting code"><code class="hljs-code">data = []     <span class="hljs-comment"># load into this list of lists </span>
MAX_LEN = <span class="hljs-number">75</span>  <span class="hljs-comment"># maximum length of a headline </span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">"news-headlines.tsv"</span>, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> file:
    lines = csv.reader(file, delimiter=<span class="hljs-string">'\t'</span>)
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines:
        hdln = line[<span class="hljs-number">0</span>]
        cnvrtd = [char_idx(c) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> hdln[:<span class="hljs-number">-1</span>]]  
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(cnvrtd) &gt;= MAX_LEN:
            cnvrtd = cnvrtd[<span class="hljs-number">0</span>:MAX_LEN<span class="hljs-number">-1</span>]
            cnvrtd.append(char2idx[EOS])
        <span class="hljs-keyword">else</span>:
            cnvrtd.append(char2idx[EOS])
            <span class="hljs-comment"># add padding tokens</span>
            remain = MAX_LEN - <span class="hljs-built_in">len</span>(cnvrtd)
            <span class="hljs-keyword">if</span> remain &gt; <span class="hljs-number">0</span>:
                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(remain):
                    cnvrtd.append(char2idx[PAD])
        data.append(cnvrtd)
print(<span class="hljs-string">"**** Data file loaded ****"</span>)
</code></pre>
    <p class="normal">All the data is loaded into <a id="_idIndexMarker315"/>a list with the code above. You may be wondering about the ground truth here for training as we only have a line of text. Since <a id="_idIndexMarker316"/>we want this model to generate text, the objective can be reduced to predicting the next character given a set of characters. Hence, a trick will be used to construct the ground truth – we will just shift the input sequence by one character and set it as the expected output. This transformation is quite easy do with <code class="Code-In-Text--PACKT-">numpy</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># now convert to numpy array</span>
np_data = np.array(data)
<span class="hljs-comment"># for training, we use one character shifted data</span>
np_data_in = np_data[:, :<span class="hljs-number">-1</span>]
np_data_out = np_data[:, <span class="hljs-number">1</span>:]
</code></pre>
    <p class="normal">With this nifty trick, we have both inputs and expected outputs ready for training. The final step is to convert it into <code class="Code-In-Text--PACKT-">tf.Data.DataSet</code> for ease of batching and shuffling:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create TF dataset</span>
x = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out))
</code></pre>
    <p class="normal">Now everything is ready to start training.</p>
    <h2 id="_idParaDest-91" class="title">Training the model</h2>
    <p class="normal">The code for model training <a id="_idIndexMarker317"/>starts at line 90 in the <code class="Code-In-Text--PACKT-">rnn-train.py</code> file. The model is quite simple. It has an embedding layer, followed by a GRU layer and a dense layer. The size of the vocabulary, the number of RNN units, and the size of the embeddings are set up:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Length of the vocabulary in chars</span>
vocab_size = <span class="hljs-built_in">len</span>(chars)
<span class="hljs-comment"># The embedding dimension</span>
embedding_dim = <span class="hljs-number">256</span>
<span class="hljs-comment"># Number of RNN units</span>
rnn_units = <span class="hljs-number">1024</span>
<span class="hljs-comment"># batch size</span>
BATCH_SIZE=<span class="hljs-number">256</span>
</code></pre>
    <p class="normal">With the batch size being defined, training data can be batched and ready for use by the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># create tf.DataSet</span>
x_train = x.shuffle(<span class="hljs-number">100000</span>, reshuffle_each_iteration=<span class="hljs-literal">True</span>).batch(BATCH_SIZE, drop_remainder=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Similar to code in previous chapters, a convenience method to build models is defined like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># define the model</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">build_model</span><span class="hljs-functio">(</span><span class="hljs-params">vocab_size, embedding_dim, rnn_units, batch_size</span><span class="hljs-functio">):</span>
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                              mask_zero=<span class="hljs-literal">True</span>,
                              batch_input_shape=[batch_size, <span class="hljs-literal">None</span>]),
    tf.keras.layers.GRU(rnn_units,
                        return_sequences=<span class="hljs-literal">True</span>,
                        stateful=<span class="hljs-literal">True</span>,
                        recurrent_initializer=<span class="hljs-string">'glorot_uniform'</span>),
    tf.keras.layers.Dropout(<span class="hljs-number">0.1</span>),
    tf.keras.layers.Dense(vocab_size)
  ])
  <span class="hljs-keyword">return</span> model 
</code></pre>
    <p class="normal">A model can be <a id="_idIndexMarker318"/>instantiated with this method:</p>
    <pre class="programlisting code"><code class="hljs-code">model = build_model(
                  vocab_size = vocab_size,
                  embedding_dim=embedding_dim,
                  rnn_units=rnn_units,
                  batch_size=BATCH_SIZE)
print(<span class="hljs-string">"**** Model Instantiated ****"</span>)
print(model.summary())
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">**** Model Instantiated ****
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        (256, None, 256)          24576
_________________________________________________________________
gru (GRU)                    (256, None, 1024)         3938304
_________________________________________________________________
dropout (Dropout)            (256, None, 1024)         0
_________________________________________________________________
dense (Dense)                (256, None, 96)           98400
=================================================================
Total params: 4,061,280
Trainable params: 4,061,280
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">There are just over 4 million trainable parameters in this model. The Adam optimizer, with a sparse categorical loss function, is used for training this model:</p>
    <pre class="programlisting code"><code class="hljs-code">loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>)
model.<span class="hljs-built_in">compile</span>(optimizer = <span class="hljs-string">'adam'</span>, loss = loss)
</code></pre>
    <p class="normal">Since training is potentially going to take a long time, we need to set up checkpoints along with the training. If there is any problem in training and training stops, these checkpoints can be used to restart the training from the last saved checkpoint. A directory is created using the current timestamp for saving these checkpoints:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setup checkpoints </span>
<span class="hljs-comment"># dynamically build folder names</span>
dt = datetime.datetime.today().strftime(<span class="hljs-string">"%Y-%b-%d-%H-%M-%S"</span>)
<span class="hljs-comment"># Directory where the checkpoints will be saved</span>
checkpoint_dir = <span class="hljs-string">'./training_checkpoints/'</span>+dt
<span class="hljs-comment"># Name of the checkpoint files</span>
checkpoint_prefix = os.path.join(checkpoint_dir, <span class="hljs-string">"ckpt_{epoch}"</span>)
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">A custom callback that saves <a id="_idIndexMarker319"/>checkpoints during training is defined in the last line of code above. This is passed to the <code class="Code-In-Text--PACKT-">model.fit()</code> function to be called at the end of every epoch. Starting the training loop is straightforward:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"**** Start Training ****"</span>)
EPOCHS=<span class="hljs-number">25</span>
start = time.time()
history = model.fit(x_train, epochs=EPOCHS, 
                    callbacks=[checkpoint_callback])
print(<span class="hljs-string">"**** End Training ****"</span>)
print(<span class="hljs-string">"Training time: "</span>, time.time()- start)
</code></pre>
    <p class="normal">The model will be trained for 25 epochs. The time taken in training will be logged as well in the code above. The final piece of code uses the history to plot the loss and save it as a PNG file in the same directory:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Plot accuracies</span>
lossplot = <span class="hljs-string">"loss-"</span> + dt + <span class="hljs-string">".png"</span>
plt.plot(history.history[<span class="hljs-string">'loss'</span>])
plt.title(<span class="hljs-string">'model loss'</span>)
plt.xlabel(<span class="hljs-string">'epoch'</span>)
plt.ylabel(<span class="hljs-string">'loss'</span>)
plt.savefig(lossplot)
print(<span class="hljs-string">"Saved loss to: "</span>, lossplot) 
</code></pre>
    <p class="normal">The best way to start training is to start the Python process so that it can run in the background without needing a Terminal or command-line. On Unix systems, this can be done with the <code class="Code-In-Text--PACKT-">nohup</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">$ nohup python rnn-train.py &gt; training.log &amp;
</code></pre>
    <p class="normal">This command line starts the <a id="_idIndexMarker320"/>process in a way that disconnecting the Terminal would not interrupt the training process. On my machine, this training took approximately 1 hour and 43 minutes. Let's check out the loss curve:</p>
    <figure class="mediaobject"><img src="image/B16252_05_01.png" alt="A close up of a mans face  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.1: Loss curve</p>
    <p class="normal">As we can see, the loss decreases to a<a id="_idIndexMarker321"/> point and then shoots up. The standard expectation is that loss would monotonically decrease as the model was trained for more epochs. In the case shown above, the loss suddenly shoots up. In other cases, you may observe a NaN, or Not-A-Number, error. NaNs result from the exploding gradient problem during backpropagation through RNNs. The gradient direction causes weights to grow very large quickly and overflow, resulting in NaNs. Given how prevalent this is, there are quite a few jokes about NLP engineers and Indian food to go with the nans (referring to a type of Indian bread).</p>
    <p class="normal">The primary reason behind these occurrences is gradient descent overshooting the minima and starting to climb the slope before reducing again. This happens when the steps gradient descent is<a id="_idIndexMarker322"/> taking are too large. Another way to prevent the NaN issue is gradient clipping where gradients are clipped to an absolute maximum, preventing loss from exploding. In the RNN model above, a scheme needs to be used that reduces the learning rate over time. Reducing the learning rate over epochs reduces the chances for gradient descent to overshoot the minima. This technique of reducing the learning rate<a id="_idIndexMarker323"/> over time is called <strong class="keyword">learning rate annealing</strong> or <strong class="keyword">learning rate decay</strong>. The next section walks through implementing learning rate decay while <a id="_idIndexMarker324"/>training the model.</p>
    <h2 id="_idParaDest-92" class="title">Implementing learning rate decay as custom callback</h2>
    <p class="normal">There are two ways <a id="_idIndexMarker325"/>to implement learning rate decay in TensorFlow. The first way is to use one of the prebuilt schedulers that are part of the <code class="Code-In-Text--PACKT-">tf.keras.optimizers.schedulers</code> package and use a configured instance with the optimizer. An example of a prebuilt scheduler is <code class="Code-In-Text--PACKT-">InverseTimeDecay</code>, and it can be set up as shown below:</p>
    <pre class="programlisting code"><code class="hljs-code">lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  <span class="hljs-number">0.001</span>,
  decay_steps=STEPS_PER_EPOCH*(EPOCHS/<span class="hljs-number">10</span>),
  decay_rate=<span class="hljs-number">2</span>,
  staircase=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">The first parameter, 0.001 in the example above, is the initial learning rate. The number of steps per epoch can be calculated by dividing the number of training examples by batch size. The number of decay steps determines how the learning rate is reduced. The equation used to compute the learning rate is:</p>
    <figure class="mediaobject"><img src="image/B16252_05_001.png" alt="" style="max-height:50px;"/></figure>
    <p class="normal">After being set up, all this function needs is the step number for computing the new learning rate. Once the schedule is set up, it can be passed to the optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.Adam(lr_schedule)
</code></pre>
    <p class="normal">That's it! The rest of the training loop code is unchanged. However, this learning rate scheduler starts reducing the learning rate from the first epoch itself. A lower learning rate increases the amount of training time. Ideally, we would keep the learning rate unchanged for the first few epochs and then reduce it.</p>
    <p class="normal">Looking at <em class="italic">Figure 5.1</em> above, the learning rate is probably effective until about the tenth epoch. BERT also uses <strong class="keyword">learning rate warmup</strong> before<a id="_idIndexMarker326"/> learning rate decay. Learning rate warmup generally refers to increasing the learning rate for a few epochs. BERT was trained for 1,000,000 steps, which roughly translates to 40 epochs. For the first 10,000 steps, the learning rate was <a id="_idIndexMarker327"/>increased, and then it was linearly decayed. Implementing such a learning rate schedule is better accomplished by a custom callback.</p>
    <p class="normal">Custom callbacks in TensorFlow enable the execution of custom logic at various points during training and inference. We saw an example of a prebuilt callback that saves checkpoints during training. A custom callback provides hooks that enable desired logic that can be executed at various points during training. This main step is to define a subclass of <code class="Code-In-Text--PACKT-">tf.keras.callbacks.Callback</code>. Then, one or more of the following functions can be implemented to hook onto the events exposed by TensorFlow:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">on_[train,test,predict]_begin</code> / <code class="Code-In-Text--PACKT-">on_[train,test,predict]_end</code>: This callback happens at the start of training or the end of the training. There are methods for training, testing, and prediction loops. Names for these methods can be constructed using the appropriate stage name from the possibilities shown in brackets. The method naming convention is a common pattern across other methods in the rest of the list.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">on_[train,test,predict]_batch_begin</code> / <code class="Code-In-Text--PACKT-">on_[train,test,predict] _batch_end</code>: These callbacks happen when training for a specific batch starts or ends.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">on_epoch_begin</code> / <code class="Code-In-Text--PACKT-">on_epoch_end</code>: This is a training-specific function called at the start or end of an epoch.</li>
    </ul>
    <p class="normal">We will implement a callback for the start of the epoch that adjusts that epoch's learning rate. Our implementation will keep the learning rate constant for a configurable number of initial epochs and then reduce the learning rate in a fashion similar to the inverse time decay function described above. This learning rate would look like the following <em class="italic">Figure 5.2</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_05_02.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.2: Custom learning rate decay function</p>
    <p class="normal">First, a subclass is created with the <a id="_idIndexMarker328"/>function defined in it. The best place to put this in <code class="Code-In-Text--PACKT-">rnn_train.py</code> is just around the checkpoint callback, before the start of training. This class definition is shown below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">LearningRateScheduler</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.callbacks.Callback</span><span class="hljs-class">):</span>
  <span class="hljs-string">"""Learning rate scheduler which decays the learning rate"""</span>
  <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, init_lr, decay, steps, start_epoch</span><span class="hljs-functio">):</span>
    <span class="hljs-built_in">super</span>().__init__()
    self.init_lr = init_lr          <span class="hljs-comment"># initial learning rate</span>
    self.decay = decay              <span class="hljs-comment"># how sharply to decay</span>
    self.steps = steps              <span class="hljs-comment"># total number of steps of decay</span>
    self.start_epoch = start_epoch  <span class="hljs-comment"># which epoch to start decaying</span>
  <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">on_epoch_begin</span><span class="hljs-functio">(</span><span class="hljs-params">self, epoch, logs=</span><span class="hljs-literal">None</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(self.model.optimizer, <span class="hljs-string">'lr'</span>):
      <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Optimizer must have a "lr" attribute.'</span>)
    <span class="hljs-comment"># Get the current learning rate</span>
    lr = <span class="hljs-built_in">float</span>(tf.keras.backend.get_value(self.model.optimizer.lr))
    <span class="hljs-keyword">if</span>(epoch &gt;= self.start_epoch):
        <span class="hljs-comment"># Get the scheduled learning rate.</span>
        scheduled_lr = self.init_lr / (<span class="hljs-number">1</span> + self.decay * (epoch / self.steps))
        <span class="hljs-comment"># Set the new learning rate</span>
        tf.keras.backend.set_value(self.model.optimizer.lr, 
                                     scheduled_lr)
    print(<span class="hljs-string">'\nEpoch %05d: Learning rate is %6.4f.'</span> % (epoch, scheduled_lr))
</code></pre>
    <p class="normal">Using this callback in the training loop<a id="_idIndexMarker329"/> requires the instantiation of the callback. The following parameters are set while instantiating the callback:</p>
    <ul>
      <li class="bullet">The initial learning rate is set to 0.001.</li>
      <li class="bullet">The decay rate is set to 4. Please feel free to play around with different settings.</li>
      <li class="bullet">The number of steps is set to the number of epochs. The model is trained for 150 epochs. </li>
      <li class="bullet">Learning rate decay should start after epoch 10, so the start epoch is set to 10.</li>
    </ul>
    <p class="normal">The training loop is updated to include the callback like so:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"**** Start Training ****"</span>)
EPOCHS=<span class="hljs-number">150</span>
lr_decay = LearningRateScheduler(<span class="hljs-number">0.001</span>, <span class="hljs-number">4.</span>, EPOCHS, <span class="hljs-number">10</span>)
start = time.time()
history = model.fit(x_train, epochs=EPOCHS,
                    callbacks=[checkpoint_callback, lr_decay])
print(<span class="hljs-string">"**** End Training ****"</span>)
print(<span class="hljs-string">"Training time: "</span>, time.time()- start)
print(<span class="hljs-string">"Checkpoint directory: "</span>, checkpoint_dir)
</code></pre>
    <p class="normal">Changes are highlighted above. Now, the model is ready to be trained using the command shown above. Training 150 epochs took over 10 hours on the GPU-capable machine. The loss surface is shown in <em class="italic">Figure 5.3</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_05_03.png" alt="A close up of a piece of paper  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.3: Model loss after learning rate decay</p>
    <p class="normal">In the figure above, the loss drops very<a id="_idIndexMarker330"/> fast for the first few epochs before plateauing near epoch 10. Learning rate decay kicks in at that point, and the loss starts to fall again. This can be verified from a snippet of the log file:</p>
    <pre class="programlisting code"><code class="hljs-code">...
Epoch 8/150
2434/2434 [==================] - 249s 102ms/step - loss: 0.9055
Epoch 9/150
2434/2434 [==================] - 249s 102ms/step - loss: 0.9052
Epoch 10/150
2434/2434 [==================] - 249s 102ms/step - loss: <code style="font-weight: bold;" class="codeHighlighted">0.9064</code>
Epoch 00010: Learning rate is 0.00078947.
Epoch 11/150
2434/2434 [==================] - 249s 102ms/step - loss: <code style="font-weight: bold;" class="codeHighlighted">0.8949</code>
Epoch 00011: Learning rate is 0.00077320.
Epoch 12/150
2434/2434 [==================] - 249s 102ms/step - loss: 0.8888
...
Epoch 00149: Learning rate is 0.00020107.
Epoch 150/150
2434/2434 [==================] - 249s 102ms/step - loss: <code style="font-weight: bold;" class="codeHighlighted">0.7667</code>
**** End Training ****
Training time:  37361.16723680496
Checkpoint directory:  ./training_checkpoints/2021-Jan-01-09-55-03
Saved loss to:  loss-2021-Jan-01-09-55-03.png
</code></pre>
    <p class="normal">Note the highlighted loss above. The loss<a id="_idIndexMarker331"/> slightly increased around epoch 10 as learning rate decay kicked in, and the loss started falling again. The small bumps in the loss that can be seen in <em class="italic">Figure 5.3</em> correlate with places where the learning rate was higher than needed, and learning rate decay kicked it down to make the loss go lower. The learning rate started at 0.001 and ended at a fifth of that at 0.0002.</p>
    <p class="normal">Training this model took much time and advanced tricks like learning rate decay to train. But how does this model do in terms of generating text? That is the focus of the next section.</p>
    <h2 id="_idParaDest-93" class="title">Generating text with greedy search</h2>
    <p class="normal">Checkpoints were taken <a id="_idIndexMarker332"/>during the training process at the end of every epoch. These checkpoints are used to load a trained model for generating text. This part of the code is implemented in an IPython notebook. The code for this section is found in the <code class="Code-In-Text--PACKT-">charRNN-text-generation.ipynb</code> file in this chapter's folder<a id="_idIndexMarker333"/> in GitHub. The generation of text is dependent on the same normalization and tokenization logic used during training. The <em class="italic">Setup Tokenization</em> section of the notebook has this code replicated.</p>
    <p class="normal">There are two main steps in generating text. The first step is restoring a trained model from the checkpoint. The second step is generating a character at a time from a trained model until a specific end condition is met.</p>
    <p class="normal">The <em class="italic">Load the Model</em> section of the notebook has the code to define the model. Since the checkpoints only stored the weights for the layers, defining the model structure is important. The main difference from the training network is the batch size. We want to generate a sentence at a time, so we set the batch size as 1:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Length of the vocabulary in chars</span>
vocab_size = <span class="hljs-built_in">len</span>(chars)
<span class="hljs-comment"># The embedding dimension</span>
embedding_dim = <span class="hljs-number">256</span>
<span class="hljs-comment"># Number of RNN units</span>
rnn_units = <span class="hljs-number">1024</span>
<span class="hljs-comment"># Batch size</span>
BATCH_SIZE=<span class="hljs-number">1</span>
</code></pre>
    <p class="normal">A convenience function for <a id="_idIndexMarker334"/>setting up the model structure is<a id="_idIndexMarker335"/> defined like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># this one is without padding masking or dropout layer</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">build_gen_model</span><span class="hljs-functio">(</span><span class="hljs-params">vocab_size, embedding_dim, rnn_units, batch_size</span><span class="hljs-functio">):</span>
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                              batch_input_shape=[batch_size, <span class="hljs-literal">None</span>]),
    tf.keras.layers.GRU(rnn_units,
                        return_sequences=<span class="hljs-literal">True</span>,
                        stateful=<span class="hljs-literal">True</span>,
                        recurrent_initializer=<span class="hljs-string">'glorot_uniform'</span>),
    tf.keras.layers.Dense(vocab_size)
  ])
  <span class="hljs-keyword">return</span> model
gen_model = build_gen_model(vocab_size, embedding_dim, rnn_units, 
                            BATCH_SIZE)
</code></pre>
    <p class="normal">Note that the embedding layer does not use masking because, in text generation, we are not passing an entire sequence but only part of a sequence that needs to be completed. Now that the model is defined, the weights for the layers can be loaded in from the checkpoint. Please remember to replace the checkpoint directory with your local directory containing the checkpoints from training:</p>
    <pre class="programlisting code"><code class="hljs-code">checkpoint_dir = <span class="hljs-string">'./training_checkpoints/</span><span class="code-highlight"><strong class="hljs-string-slc">&lt;YOUR-CHECKPOINT-DIR&gt;'</strong></span> 
gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
gen_model.build(tf.TensorShape([<span class="hljs-number">1</span>, <span class="hljs-literal">None</span>]))
</code></pre>
    <p class="normal">The second main step is to <a id="_idIndexMarker336"/>generate text a character at a time. Generating<a id="_idIndexMarker337"/> text needs a seed or a starting few letters, which are completed by the model into a sentence. The process of generation is encapsulated in the function below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">generate_text</span><span class="hljs-functio">(</span><span class="hljs-params">model, start_string, temperature=</span><span class="hljs-number">0.7</span><span class="hljs-params">, num_generate=</span><span class="hljs-number">75</span><span class="hljs-functio">):</span>
  <span class="hljs-comment"># Low temperatures results in more predictable text.</span>
  <span class="hljs-comment"># Higher temperatures results in more surprising text.</span>
  <span class="hljs-comment"># Experiment to find the best setting.</span>
  <span class="hljs-comment"># Converting our start string to numbers (vectorizing)</span>
  input_eval = [char2idx[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> start_string]
  input_eval = tf.expand_dims(input_eval, <span class="hljs-number">0</span>)
  <span class="hljs-comment"># Empty string to store our results</span>
  text_generated = []
  <span class="hljs-comment"># Here batch size == 1</span>
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_generate):
      predictions = model(input_eval)
      <span class="hljs-comment"># remove the batch dimension</span>
      predictions = tf.squeeze(predictions, <span class="hljs-number">0</span>)
      <span class="hljs-comment"># using a categorical distribution to predict the </span>
<span class="hljs-comment">      # word returned by the model</span>
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, 
                               num_samples=<span class="hljs-number">1</span>)[<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>].numpy()
      <span class="hljs-comment"># We pass the predicted word as the next input to the model</span>
      <span class="hljs-comment"># along with the previous hidden state</span>
      input_eval = tf.expand_dims([predicted_id], <span class="hljs-number">0</span>)
        
      text_generated.append(idx2char[predicted_id])
      <span class="hljs-comment"># lets break is &lt;EOS&gt; token is generated</span>
      <span class="hljs-comment"># if idx2char[predicted_id] == EOS:</span>
      <span class="hljs-comment"># break #end of a sentence reached, let's stop</span>
  <span class="hljs-keyword">return</span> (start_string + <span class="hljs-string">''</span>.join(text_generated))
</code></pre>
    <p class="normal">The generation method takes in a seed string that is used as the starting point for the generation. This seed string is vectorized. The actual generation happens in a loop, where one character is <a id="_idIndexMarker338"/>generated at a time and appended to the sequence generated. At every point, the character with the highest likelihood is<a id="_idIndexMarker339"/> chosen. Choosing the next letter with the highest probability is called <strong class="keyword">greedy search</strong>. However, there is a configuration parameter called <strong class="keyword">temperature</strong>, which can be used to adjust the predictability of the generated text.</p>
    <p class="normal">Once probabilities for all<a id="_idIndexMarker340"/> characters are predicted, dividing the probabilities by the temperature changes the distribution of the generated characters. Smaller values of the temperature generate text that is closer to the original text. Larger values of the temperature generate more creative text. Here, a value of 0.7 is chosen to bias more on the surprising side.</p>
    <p class="normal">To generate the text, all that is needed is one line of code:</p>
    <pre class="programlisting code"><code class="hljs-code">print(generate_text(gen_model, start_string=<span class="hljs-string">u"Google"</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Google plans to release the Xbox One vs. Samsung Galaxy Gea&lt;EOS&gt;&lt;PAD&gt;ote on Mother's Day 
</code></pre>
    <p class="normal">Each execution of the command may generate slightly different results. The line generated above, while obviously nonsensical, is pretty well structured. The model has learned capitalization rules and headline structure. Normally, we would not generate text beyond the <code class="Code-In-Text--PACKT-">&lt;EOS&gt;</code> token, but all 75 characters are generated here for the sake of understanding the model output.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that the output shown for text generation is indicative. You may see a different output for the same prompt. There is some inherent randomness that is built into this process, which we can try and control by setting random seeds. When a model is retrained, it may end up on a slightly different point on the loss surface, where even though the loss numbers look similar, there may be slight differences in the model weights. Please take the outputs presented in the entire chapter as indicative versus actual.</p>
    </div>
    <p class="normal">Here are some other examples of seed strings and model outputs, snipped after the end-of-sentence tag:</p>
    <table id="table001-3">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Seed</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Generated Sentence</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">S&amp;P</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">S&amp;P 500 closes above 190&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">S&amp;P: Russell Slive to again find any business manufacture&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">S&amp;P closes above 2000 for first tim&lt;EOS&gt;</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Beyonce</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Beyonce and Solange pose together for 'American Idol' contes&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">Beyonce's sister Solange rules' Dawn of the Planet of the Apes' report&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">Beyonce &amp; Jay Z Get Married&lt;EOS&gt;</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Note the model's use of quotes <a id="_idIndexMarker341"/>in the first two sentences for <em class="italic">Beyonce</em> as the seed word. The following table shows the impact of different temperature<a id="_idIndexMarker342"/> settings for similar seed words:</p>
    <table id="table002-2">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Seed</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Temperature</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Generated Sentence</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">S&amp;P</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">0.1</p>
            <p class="Table-Column-Content--PACKT-">0.3</p>
            <p class="Table-Column-Content--PACKT-">0.5</p>
            <p class="Table-Column-Content--PACKT-">0.9</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">S&amp;P 500 Closes Above 1900 For First Tim&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">S&amp;P Close to $5.7 Billion Deal to Buy Beats Electronic&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">S&amp;P 500 index slips to 7.2%, signaling a strong retail sale&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">S&amp;P, Ack Factors at Risk of what you see This Ma&lt;EOS&gt;</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Kim</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">0.1</p>
            <p class="Table-Column-Content--PACKT-">0.3</p>
            <p class="Table-Column-Content--PACKT-">0.5 </p>
            <p class="Table-Column-Content--PACKT-">0.9</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Kim Kardashian and Kanye West wedding photos release&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">Kim Kardashian Shares Her Best And Worst Of His First Look At The Met Gala&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">Kim Kardashian Wedding Dress Dress In The Works From Fia&lt;EOS&gt;</p>
            <p class="Table-Column-Content--PACKT-">Kim Kardashian's en&lt;EOS&gt;</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Generally, the quality of the text goes down at higher values of temperature. All these examples were generated by passing in the different temperature values to the generation function.</p>
    <p class="normal">A practical application of such a character-based model is to complete words in a text messaging or email app. By default, the <code class="Code-In-Text--PACKT-">generate_text()</code> method is generating 75 characters to complete the headline. It is easy to pass in much shorter lengths to see what the model proposes as the next few letters or words. </p>
    <p class="normal">The table below shows some experiments of trying to complete the next 10 characters of text fragments. These completions were generated using:</p>
    <pre class="programlisting code"><code class="hljs-code">print(generate_text(gen_model, start_string=<span class="hljs-string">u"Lets meet tom"</span>, 
                    temperature=<span class="hljs-number">0.7</span>, num_generate=<span class="hljs-number">10</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Lets meet tomorrow to t
</code></pre>
    <table id="table003-1">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Prompt</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Completion</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">I need some money from ba</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">I need some money from bank chairma</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Swimming in the p</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Swimming in the profitabili</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Can you give me a </p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Can you give me a Letter to </p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">are you fr</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">are you from around</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">The meeting is</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">The meeting is back in ex</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Lets have coffee at S</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Lets have coffee at Samsung hea</p>
            <p class="Table-Column-Content--PACKT-">Lets have coffee at Staples stor</p>
            <p class="Table-Column-Content--PACKT-">Lets have coffee at San Diego Z</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Given that the dataset <a id="_idIndexMarker343"/>used was only from news headlines, it is biased toward certain types of activities. For example, the second sentence could be completed with <em class="italic">pool</em> instead of the model trying to fill it in with profitability. If a <a id="_idIndexMarker344"/>more general text dataset was used, then this model could do quite well at generating completions for partially typed words at the end of the sentence. However, there is one limitation that this text generation method has – the use of the greedy search algorithm.</p>
    <p class="normal">The greedy search process is a crucial part of the text generation above. It is one of several ways to generate text. Let's take an example to understand this process. For this example, bigram frequencies were analyzed by Peter Norvig and published on <a href="http://norvig.com/mayzner.html"><span class="url">http://norvig.com/mayzner.html</span></a>. Over 743 billion English words were analyzed in this work. With 26 characters in an uncased model, there are theoretically 26 x 26 = 676 bigram combinations. However, the article reports that the following bigrams were never seen in roughly 2.8 trillion bigram instances: JQ, QG, QK, QY, QZ, WQ, and WZ.</p>
    <p class="normal">The <em class="italic">Greedy Search with Bigrams</em> section of the notebook has code to download and process the full dataset and show the process of greedy search. After downloading the set of all n-grams, bigrams are extracted. A set of dictionaries is constructed to help look up the highest-probability next letter given a starting letter. Then, using some recursive code, a tree is constructed, picking the top three choices for the next letter. In the generation code above, only the top letter is chosen. However, the top three letters are chosen to show how greedy search works and its shortcomings. </p>
    <p class="normal">Using the nifty <code class="Code-In-Text--PACKT-">anytree</code> Python package, a <a id="_idIndexMarker345"/>nicely formatted tree can<a id="_idIndexMarker346"/> be visualized. This tree is shown in the following figure:</p>
    <figure class="mediaobject"><img src="image/B16252_05_04.png" alt="A close up of text on a white background  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.4: Greedy search tree starting with WI</p>
    <p class="normal">The algorithm was given the task of completing <strong class="keyword">WI</strong> in a total of five characters. The preceding tree shows cumulative probabilities for a given path. More than one path is shown so that the branches not taken by greedy search can also be seen. If a three-character word was being built, the highest probability choice is <strong class="keyword">WIN</strong> with a probability of 0.243, followed by <strong class="keyword">WIS</strong> at 0.01128. If four-letter words are considered, then the greedy search<a id="_idIndexMarker347"/> would consider only those words that start with <strong class="keyword">WIN</strong> as that was the path with the highest probability considering the<a id="_idIndexMarker348"/> first three letters. <strong class="keyword">WIND</strong> has the highest probability of 0.000329 in this path. However, a quick scan across all four-letter words shows that the highest probability word should be <strong class="keyword">WITH</strong> having a probability of 0.000399.</p>
    <p class="normal">This, in essence, is the challenge of the greedy search algorithm for text generation. Higher-probability options considering joint probabilities are hidden due to optimization at each character instead of cumulative probability. Whether the text is generated a character or a word at a time, greedy search suffers from the same issue.</p>
    <p class="normal">An alternative algorithm, called <strong class="keyword">beam search</strong>, allows tracking multiple options, and pruning out the lower-probability options as<a id="_idIndexMarker349"/> generation proceeds. The tree shown in <em class="italic">Figure 5.4</em> can also be seen as an illustration of tracking beams of probabilities. To see the power of this technique, a more sophisticated model for generating text would be better. The <strong class="keyword">GPT-2</strong>, or <strong class="keyword">Generative Pre-Training</strong>, based model published by OpenAI set many<a id="_idIndexMarker350"/> benchmarks including in open-ended text generation. This is the subject of the next half of this chapter, where the GPT-2 model is explained first. The next topic is fine-tuning a GPT-2 model for completing email messages. Beam search and other options to improve the quality of the generated text are also shown.</p>
    <h1 id="_idParaDest-94" class="title">Generative Pre-Training (GPT-2) model</h1>
    <p class="normal">OpenAI released the first<a id="_idIndexMarker351"/> version of the GPT model in June 2018. They followed up with GPT-2 in February 2019. This paper attracted much attention as full details of the large GPT-2 model were not released with the paper due to concerns of nefarious uses. The large GPT-2 model was released subsequently in November 2019. The GPT-3 model is the most recent, released in May 2020. </p>
    <p class="normal"><em class="italic">Figure 5.5</em> shows the number of parameters in the largest of each of these models:</p>
    <figure class="mediaobject"><img src="image/B16252_05_05.png"/></figure>
    <p class="packt_figref">Figure 5.5: Parameters in different GPT models</p>
    <p class="normal">The first model used the <a id="_idIndexMarker352"/>standard Transformer decoder architecture with twelve layers, each with twelve attention heads and 768-dimensional embeddings, for a total of approximately 110 million parameters, which is very similar to the BERT model. The largest GPT-2 has over 1.5 billion parameters, and the most recently released GPT-3 model's largest variant has over 175 billion parameters!</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">Cost of training language models</strong></p>
      <p class="Information-Box--PACKT-">As the number of parameters and <a id="_idIndexMarker353"/>dataset sizes increase, the time taken for training also increases. As per a Lambda Labs article, If the GPT-3 model were to be trained on a single Nvidia V100 GPU, it would take 342 years. Using stock Microsoft Azure pricing, this would cost over $3 million. GPT-2 model training is estimated to run to $256 per hour. Assuming a similar running time as BERT, which is about four days, that would cost about $25,000. If the cost of training multiple models during research is factored in, the overall cost can easily increase ten-fold.</p>
      <p class="Information-Box--PACKT-">At such costs, training these models from scratch is out of reach for individuals and even most companies. Transfer learning and the availability of pre-trained models from companies like Hugging Face make it possible for the general public to use these models.</p>
    </div>
    <p class="normal">The base architecture of GPT models uses the decoder part of the Transformer architecture. The decoder is a <em class="italic">left-to-right</em> language model. The BERT model, in contrast, is a bidirectional model. A left-to-right model is autoregressive, that is, it uses tokens generated thus far to generate the next token. Since it cannot see future tokens like a bi-directional model, this<a id="_idIndexMarker354"/> language model is ideal for text generation.</p>
    <p class="normal"><em class="italic">Figure 5.6</em> shows the full Transformer architecture with the encoder blocks on the left and decoder blocks on the right:</p>
    <figure class="mediaobject"><img src="image/B16252_05_06.png"/></figure>
    <p class="packt_figref">Figure 5.6: Full Transformer architecture with encoder and decoder blocks</p>
    <p class="normal">The left side of <em class="italic">Figure 5.6</em> should be familiar – it is essentially <em class="italic">Figure 4.6</em> from the <em class="italic">Transformer model</em> section of the previous chapter. The encoder blocks shown are the same as the BERT model. The decoder blocks are very similar to the encoder blocks with a couple of notable differences.</p>
    <p class="normal">In the encoder block, there is only one source of input – the input sequence and all of the input tokens are available for the multi-head attention to operate on. This enables the encoder to understand the context of the token from both the left and right sides.</p>
    <p class="normal">In the decoder block, there are <a id="_idIndexMarker355"/>two inputs to each block. The outputs generated by the encoder blocks are available to all the decoder blocks and fed to the middle of the decoder block through multi-head attention and layer norms.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">What is layer normalization?</strong></p>
      <p class="Information-Box--PACKT-">Large deep neural networks are trained <a id="_idIndexMarker356"/>using the <strong class="keyword">Stochastic Gradient Descent</strong> (<strong class="keyword">SGD</strong>) optimizer or a variant like Adam. Training large models on big datasets can take a significant amount of<a id="_idIndexMarker357"/> time for the model to converge. Techniques such as weight normalization, batch normalization, and layer normalization are aimed at reducing training time by helping models to converge faster while also acting as a regularizer. The idea behind layer normalization is to scale the inputs of a given hidden layer with the mean and standard deviation of the inputs. First, the mean and standard deviation are computed:</p>
      <figure class="mediaobject"><img src="image/B16252_05_002.png" alt="" style="max-height:70px;"/></figure>
      <p class="Information-Box--PACKT-"><em class="italic">H</em> denotes the number of hidden units in layer <em class="italic">l</em>. Inputs to the layer are normalized using the above-calculated values:</p>
      <figure class="mediaobject"><img src="image/B16252_05_003.png" alt="" style="max-height:60px;"/></figure>
      <p class="Information-Box--PACKT-">where <em class="italic">g</em> is a gain parameter. Note that the formulation of the mean and standard deviation is not dependent on the size of the mini-batches or dataset size. Hence, this type of normalization can be used for RNNs and other sequence modeling problems.</p>
    </div>
    <p class="normal">However, the tokens generated by the <a id="_idIndexMarker358"/>decoder thus far are fed back through a masked multi-head self-attention and added to the output from the encoder blocks. Masked here refers to the fact that tokens to the right of the token being generated are masked, and the decoder cannot see them. Similar to the encoder, there are several such blocks stacked on top of each other. However, GPT architecture is only one half of the Transformer. This requires some modifications to the architecture.</p>
    <p class="normal">The modified architecture for GPT is shown in <em class="italic">Figure 5.7</em>. Since there is no encoder block to feed the representation of the input sequence, the multi-head layer is no longer required. The outputs generated by the model are recursively fed back to generate the next token.</p>
    <p class="normal">The smallest GPT-2 model has twelve layers and 768 dimensions for each token. The largest GPT-2 model has 48 layers and 1,600 dimensions per token. To pre-train models of this size, the authors of GPT-2 needed to create a new dataset. Web pages provide a great source of text, but the text comes with quality issues. To solve this challenge, they scraped all outbound links from Reddit, which had received at least three karma points. The assumption made by the authors is that karma points are an indicator of the quality of the web page being linked. This assumption allows scraping a huge set of text data. The resulting dataset was approximately 45 million links.</p>
    <p class="normal">To extract text from the HTML on the web pages, two Python libraries were used: Dragnet and Newspaper. After some quality checks and deduplication, the final dataset was about 8 million documents with 40 GB of text. One exciting thing that the authors did was to remove any Wikipedia documents as they felt many of the test datasets used Wikipedia, and adding these pages would cause an overlap between test and training data sets. The pre-training objective is a standard LM training objective of predicting the next word given a set of previous words:</p>
    <figure class="mediaobject"><img src="image/B16252_05_07.png" alt="A close up of a sign  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.7: GPT architecture (Source: Improving Language Understanding by Generative Pre-Training by Radford et al.)</p>
    <p class="normal">During pre-training, the GPT-2 model is trained with a maximum sequence length of 1,024 tokens. A <strong class="keyword">Byte Pair Encoding</strong> (<strong class="keyword">BPE</strong>) algorithm is used for tokenization, with a vocabulary size of about 50,000 tokens. GPT-2 uses byte sequences rather than Unicode code points for the byte pair merges. If GPT-2 only used bytes for encoding, then the vocabulary would only be 256 tokens. On the other hand, using Unicode code points would yield a vocabulary of over 130,000 tokens. By cleverly using bytes in BPE, GPT-2 is able to keep the vocabulary size to a<a id="_idIndexMarker359"/> manageable 50,257 tokens.</p>
    <p class="normal">Another peculiarity of the tokenizer in GPT-2 is that it converts all text to lowercase and uses spaCy and <code class="Code-In-Text--PACKT-">ftfy</code> tokenizers prior to using BPE. The <code class="Code-In-Text--PACKT-">ftfy</code> library is quite useful for fixing Unicode issues. If these two are not available, then the basic BERT tokenizer is used.</p>
    <p class="normal">There are several ways to encode the inputs to solve various problems, even though the left-to-right model may seem limiting. These are shown in <em class="italic">Figure 5.8</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_05_08.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.8: Input transformations in GPT-2 for different problems (Source: Improving Language Understanding by Generative Pre-Training by Radford et al.)</p>
    <p class="normal">The figure above shows how a pre-trained GPT-2 model can be used for a variety of tasks other than text generation. In each instance, start and end tokens are added before and after the input <a id="_idIndexMarker360"/>sequence. In all cases, a linear layer is added to the end that is trained during model fine-tuning. The major advantage being claimed is that many different types of tasks can be accomplished using the same architecture. The topmost architecture in <em class="italic">Figure 5.8</em> shows how it can be used for classification. GPT-2 could be used for IMDb sentiment analysis using this approach, for example.</p>
    <p class="normal">The second example is of textual entailment. Textual entailment is an NLP task where the relationship between two fragments of text needs to be established. The first text fragment is called a premise, and the second fragment is called the hypothesis. Different relationships can exist between the premise and hypothesis. The premise can validate or contradict the hypothesis, or they may be unrelated.</p>
    <p class="normal">Let's say the premise is <em class="italic">Exercising every day is an important part of a healthy lifestyle and longevity</em>. If the hypothesis is <em class="italic">exercise increases lifespan</em>, then the premise <em class="italic">entails</em> or <em class="italic">validates</em> the hypothesis. Alternatively, if the hypothesis is <em class="italic">Running has no benefits</em>, then the premise <em class="italic">contradicts</em> the hypothesis. Lastly, if the hypothesis is that <em class="italic">lifting weights can build a six-pack</em>, then the premise neither entails nor contradicts the hypothesis. To perform entailment with GPT-2, the premise and hypothesis are concatenated with a delimiter, usually <code class="Code-In-Text--PACKT-">$</code>, in between them.</p>
    <p class="normal">For text similarity, two input <a id="_idIndexMarker361"/>sequences are constructed, one with the first text sequence first and the second with the second text sequence first. The output from the GPT model is added together and fed to the linear layer. A similar approach is used for multiple-choice questions. However, our focus in this chapter is text generation.</p>
    <h2 id="_idParaDest-95" class="title">Generating text with GPT-2</h2>
    <p class="normal">Hugging Face's transformers library simplifies the process of generating text with GPT-2. Similar to the pre-trained BERT model, as shown in the previous chapter, Hugging Face provides pre-trained GPT and<a id="_idIndexMarker362"/> GPT-2 models. These pre-trained models are used in the rest of the chapter. Code for this and the<a id="_idIndexMarker363"/> rest of the sections of this chapter can be found in the IPython notebook named <code class="Code-In-Text--PACKT-">text-generation-with-GPT-2.ipynb</code>. After running the setup, scoot over to the <em class="italic">Generating Text with GPT-2</em> section. A section showing the generation of text with GPT is also provided for reference. The first step in generating text is to download the pre-trained model, and its corresponding tokenizer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFGPT2LMHeadModel, GPT2Tokenizer
gpt2tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">"gpt2"</span>)
<span class="hljs-comment"># add the EOS token as PAD token to avoid warnings</span>
gpt2 = TFGPT2LMHeadModel.from_pretrained(<span class="hljs-string">"gpt2"</span>, 
                          pad_token_id=gpt2tokenizer.eos_token_id)
</code></pre>
    <p class="normal">This may take a few minutes as the models need to be downloaded. You may see a warning if spaCy and <code class="Code-In-Text--PACKT-">ftfy</code> are not available in your environment. These two libraries are not mandatory for text generation. The following code can be used to generate text using a greedy search algorithm:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># encode context the generation is conditioned on</span>
input_ids = gpt2tokenizer.encode(<span class="hljs-string">'Robotics is the domain of '</span>, return_tensors=<span class="hljs-string">'tf'</span>)
<span class="hljs-comment"># generate text until the output length </span>
<span class="hljs-comment"># (which includes the context length) reaches 50</span>
greedy_output = gpt2.generate(input_ids, max_length=<span class="hljs-number">50</span>)
print(<span class="hljs-string">"Output:\n"</span> + <span class="hljs-number">50</span> * <span class="hljs-string">'-'</span>)
print(gpt2tokenizer.decode(greedy_output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Output:
-----------------------------------------------------------
Robotics is the domain of the United States Government.
The United States Government is the primary source of information on the use of drones in the United States.
The United States Government is the primary source of information on the use of drones
</code></pre>
    <p class="normal">A prompt was supplied <a id="_idIndexMarker364"/>for the model to complete. The model started in a promising manner but soon resorted to repeating the<a id="_idIndexMarker365"/> same output.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that the output shown for text generation is indicative. You may see different outputs for the same prompt. There are a few different reasons for this. There is some inherent randomness that is built into this process, which we can try and control by setting random seeds. The models themselves may be retrained periodically by the Hugging Face team and may evolve with newer versions.</p>
    </div>
    <p class="normal">Issues with the greedy search were noted in the previous section. Beam search can be considered as an alternative. At each step of generating a token, a set of top probability tokens are kept as part of the beam instead of just the highest-probability token. The sequence with the highest overall probability is returned at the end of the generation. <em class="italic">Figure 5.4</em>, in the previous section with a greedy search, can be considered as the output of a beam search algorithm with a beam size of 3. </p>
    <p class="normal">Generating text using beam search is trivial:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># BEAM SEARCH</span>
<span class="hljs-comment"># activate beam search and early_stopping</span>
beam_output = gpt2.generate(
    input_ids, 
    max_length=<span class="hljs-number">50</span>, 
    num_beams=<span class="hljs-number">5</span>, 
    early_stopping=<span class="hljs-literal">True</span>
)
print(<span class="hljs-string">"Output:\n"</span> + <span class="hljs-number">50</span> * <span class="hljs-string">'-'</span>)
print(gpt2tokenizer.decode(beam_output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Output:
--------------------------------------------------
Robotics is the domain of science and technology. It is the domain of science and technology. It is the domain of science and technology. It is the domain of science and technology. It is the domain of science and technology. It is the domain
</code></pre>
    <p class="normal">Qualitatively, the first sentence <a id="_idIndexMarker366"/>makes a lot more sense than the one generated by the greedy search. The <code class="Code-In-Text--PACKT-">early_stopping</code> parameter signals generation<a id="_idIndexMarker367"/> to stop when all beams reach the EOS token. However, there is still much repetition going on. One parameter that can be used to control the repetition is by setting a limit on n-grams being repeated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># set no_repeat_ngram_size to 2</span>
beam_output = gpt2.generate(
    input_ids, 
    max_length=<span class="hljs-number">50</span>, 
    num_beams=<span class="hljs-number">5</span>, 
    no_repeat_ngram_size=<span class="hljs-number">3</span>, 
    early_stopping=<span class="hljs-literal">True</span>
)
print(<span class="hljs-string">"Output:\n"</span> + <span class="hljs-number">50</span> * <span class="hljs-string">'-'</span>)
print(gpt2tokenizer.decode(beam_output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Output:
--------------------------------------------------
Robotics is the domain of science and technology.
In this article, we will look at some of the most important aspects of robotics and how they can be used to improve the lives of people around the world. We will also take a look
</code></pre>
    <p class="normal">This has made a considerable difference in the quality of the generated text. The <code class="Code-In-Text--PACKT-">no_repeat_ngram_size</code> parameter prevents the model from generating any 3-grams or triplets of tokens more than once. While this improves the quality of the text, using the n-gram constraint can have a significant impact on the quality of the generated text. If the generated text is<a id="_idIndexMarker368"/> about <em class="italic">The White House</em>, then these three words can only be used once in the entire generated text. In such a<a id="_idIndexMarker369"/> case, using the n-gram constraint will be counter-productive.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-"><strong class="scree Text">To beam or not to beam</strong></p>
      <p class="Tip--PACKT-">Beam search works well in cases<a id="_idIndexMarker370"/> where the generated sequence is of a restricted length. As the length of the sequence increases, the number of beams to be maintained and computed increases significantly. Consequently, beam search works well in tasks like summarization and translation but performs poorly in open-ended text generation. Further, beam search, by trying to maximize the cumulative probability, generates more predictable text. The text feels less natural. The following piece of code can be used to get a feel for the various beams being generated. Just make sure that the number of beams is greater than or equal to the number of sequences to be returned:</p>
      <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Returning multiple beams</span>
beam_outputs = gpt2.generate(
    input_ids, 
    max_length=<span class="hljs-number">50</span>, 
    num_beams=<span class="hljs-number">7</span>, 
    no_repeat_ngram_size=<span class="hljs-number">3</span>, 
    num_return_sequences=<span class="hljs-number">3</span>,  
    early_stopping=<span class="hljs-literal">True</span>,
    temperature=<span class="hljs-number">0.7</span>
)
print(<span class="hljs-string">"Output:\n"</span> + <span class="hljs-number">50</span> * <span class="hljs-string">'-'</span>)
<span class="hljs-keyword">for</span> i, beam_output <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(beam_outputs):
  print(<span class="hljs-string">"\n{}: {}"</span>.<span class="hljs-built_in">format</span>(i, 
                   gpt2tokenizer.decode(beam_output,
                          skip_special_tokens=<span class="hljs-literal">True</span>)))
</code></pre>
      <pre class="programlisting con"><code class="hljs-con">Output:
--------------------------------------------------
0: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, Canada, Australia, New Zealand, and the European Union.
1: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, France, Germany, Italy, Japan, and the European Union.
2: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, Canada, Australia, New Zealand, the European Union, and the United
The text generated is very similar but differs near the end. Also, note that temperature is available to control the creativity of the generated text. 
</code></pre>
    </div>
    <p class="normal">There is another method for improving the coherence and creativity of the text being generated called Top-K sampling. This is the<a id="_idIndexMarker371"/> preferred <a id="_idIndexMarker372"/>method in GPT-2 and plays an essential role in the success of GPT-2 in story generation. Before explaining <a id="_idIndexMarker373"/>how this works, let's try it out and see the output:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Top-K sampling</span>
tf.random.set_seed(<span class="hljs-number">42</span>)  <span class="hljs-comment"># for reproducible results</span>
beam_output = gpt2.generate(
    input_ids, 
    max_length=<span class="hljs-number">50</span>, 
    do_sample=<span class="hljs-literal">True</span>, 
    top_k=<span class="hljs-number">25</span>,
    temperature=<span class="hljs-number">2</span>
)
print(<span class="hljs-string">"Output:\n"</span> + <span class="hljs-number">50</span> * <span class="hljs-string">'-'</span>)
print(gpt2tokenizer.decode(beam_output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Output:
--------------------------------------------------
Robotics is the domain of people with multiple careers working with robotics systems. The purpose of Robotics &amp; Machine Learning in Science and engineering research is not necessarily different for any given research type because the results would be much more diverse.
Our team uses
</code></pre>
    <p class="normal">The above sample was generated by selecting a high temperature value. A random seed was set to ensure repeatable results. The Top-K sampling method was<a id="_idIndexMarker374"/> published in a paper titled <em class="italic">Hierarchical Neural Story Generation</em> by Fan Lewis and Dauphin in 2018. The algorithm is relatively simple – at every step, it picks a token from the top <em class="italic">K</em> highest probability tokens. If <em class="italic">K</em> is set to 1, then this algorithm is identical to the greedy search.</p>
    <p class="normal">In the code example above, the model looks at the 25 top tokens out of the 50,000+ tokens while generating text. Then, it picks a random word from these and continues the generation. Choosing <a id="_idIndexMarker375"/>larger values will result in more surprising or creative text. Choosing lower values of <em class="italic">K</em> will result in more predictable<a id="_idIndexMarker376"/> text. If you are a little underwhelmed by the results thus far, that is because the prompt selected is a really tough one. Consider this output generated with Top-K of 50 for the prompt <em class="italic">In the dark of the night, there was a</em>:</p>
    <p class="normal"><em class="italic">In the dark of the night, there was a sudden appearance of light.</em></p>
    <p class="normal"><em class="italic">Sighing, Xiao Chen slowly stood up and looked at Tian Cheng standing over. He took a step to look closely at Tian Cheng's left wrist and frowned.</em></p>
    <p class="normal"><em class="italic">Lin Feng was startled, and quickly took out a long sword!</em></p>
    <p class="normal"><em class="italic">Lin Feng didn't understand what sort of sword that Long Fei had wielded in the Black and Crystal Palace!</em></p>
    <p class="normal"><em class="italic">The Black and Crystal Palace was completely different than his original Black Stone City. Long Fei carried a sword as a souvenir, which had been placed on the back of his father's arm by Tian Cheng.</em></p>
    <p class="normal"><em class="italic">He drew the sword from his dad's arm again!</em></p>
    <p class="normal"><em class="italic">The black blade was one of the most valuable weapons within the Black and Crystal Palace. The sword was just as sharp as the sharpest of all weapons, which had been placed on Long Fei's father's arm by the Black Stone City's Black Ice, for him to</em></p>
    <p class="normal">The above longer form text was generated by the smallest GPT-2 model, which has roughly 124 million parameters. Several different settings and model sizes are available for you to now play with. Remember, with great power comes great responsibility.</p>
    <p class="normal">Between the last <a id="_idIndexMarker377"/>chapter and this<a id="_idIndexMarker378"/> one, we have covered both the encoder and decoder parts of the Transformer architecture conceptually. Now, we are ready to put both parts together in the next chapter. Let's quickly review what we covered in this chapter.</p>
    <h1 id="_idParaDest-96" class="title">Summary</h1>
    <p class="normal">Generating text is a complicated task. There are practical uses that can make typing text messages or composing emails easier. On the other hand, there are creative uses, like generating stories. In this chapter, we covered a character-based RNN model to generate headlines one character at a time and noted that it picked up the structure, capitalization, and other things quite well. Even though the model was trained on a particular dataset, it showed promise in completing short sentences and partially typed words based on the context. The next section covered the state-of-the-art GPT-2 model, which is based on the Transformer decoder architecture. The previous chapter had covered the Transformer encoder architecture, which is used by BERT.</p>
    <p class="normal">Generating text has many knobs to tune like temperature to resample distributions, greedy search, beam search, and Top-K sampling to balance <a id="_idIndexMarker379"/>the creativity and predictability of the generated text. We saw the impact of these settings on text generation and used a pre-trained GPT-2 model provided by Hugging Face to generate text.</p>
    <p class="normal">Now that both the encoder and decoder parts of the Transformer architecture have been covered, the next chapter will use the full Transformer to build a text summarization model. Text summarization is at the cutting edge of NLP today. We will build a model that will read news articles and summarize them in a few sentences. Onward!</p>
  </div>
</body></html>