- en: Language Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is the first of several in which we'll discuss different neural
    network algorithms in the context of **natural language processing** (**NLP**).
    NLP teaches computers to process and analyze natural language data in order to
    perform tasks such as machine translation, sentiment analysis, natural language
    generation, and so on. But to successfully solve such complex problems, we have
    to represent the natural language in a way that the computer can understand, and
    this is not a trivial task.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why, let's go back to image recognition. The neural network input
    is fairly intuitive—a 2D tensor with preprocessed pixel intensities, which preserves
    the spatial features of the image. Let's take a 28 x 28 MNIST image, which contains
    784 pixels. All the information about the digit in the image is contained within
    these pixels only and we don't need any external information to classify the image.
    We can also safely assume that each pixel (perhaps excluding the ones near the
    image borders) carries the same information weight. Therefore, we feed them all
    to the network to do its magic and we let the results speak for themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's focus on text data. Unlike an image, we have 1D (as opposed to 2D)
    data—a single long sequence of words. A general rule of thumb is that a single-spaced
    A4 page contains 500 words. To feed a network (or any ML algorithm) the informational
    equivalent of a single MNIST image, we need 1.5 pages of text. The text structure
    has several hierarchical levels; starting from characters, then words, sentences,
    and paragraphs, all of which can fit within 1.5 pages of text. All the pixels
    of the image relate to one digit; however, we don't know whether all the words
    relate to the same subject. To avoid this complexity, NLP algorithms usually work
    with shorter sequences. Even though some algorithms use **recurrent neural networks**
    (**RNNs**), which take into account all previous inputs, in practice, they are
    still limited to a relatively short window of the immediately preceding words.
    Therefore, an NLP algorithm has to do more (perform well) with less (a smaller
    amount of input information).
  prefs: []
  type: TYPE_NORMAL
- en: To help us with this, we'll use a special type of vector word representation
    (language model). The language models we'll discuss use the context of a word
    (its surrounding words) to create a unique embedding vector associated with that
    word. These vectors carry more information about the word, compared to, say, one-hot
    encoding. They serve as a base for various NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding *n*-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introducing neural language models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural probabilistic language model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec and fastText
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Global Vectors for Word Representation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A word-based language model defines a probability distribution over sequences
    of words. Given a sequence of words of length *m* (for example, a sentence), it
    assigns a probability *P*(*w1, ... , w[m]*) to the full sequence of words. We
    can use these probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the likelihood of different phrases in NLP applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a generative model to create new text. A word-based language model can compute
    the likelihood of a given word following a sequence of words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The inference of the probability of a long sequence, say *w[1], ..., w[m]*,
    is typically infeasible. We can calculate the joint probability of *P*(*w[1],
    ... , w[m]*) with the chain rule of joint probability ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d57f7d17-249c-4cbf-9346-c37d15d16f32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability of the later words given the earlier words would be especially
    difficult to estimate from the data. That''s why this joint probability is typically
    approximated by an independence assumption that the *i-*th word is only dependent
    on the *n-1* previous words. We''ll only model the joint probabilities of combinations
    of *n* sequential words, called *n*-grams. For example, in the phrase *the quick
    brown fox*, we have the following *n*-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1-gram**: *The*, *quick*, *brown*, and *fox* (also known as a unigram).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2-grams**: *The quick*, *quick brown*, and *brown fox* (also known as a bigram).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3-grams**: *The quick brown* and *quick brown fox* (also known as a trigram).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4-grams**: *The quick brown fox*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inference of the joint distribution is approximated with the help of *n*-gram
    models that split the joint distribution into multiple independent parts.
  prefs: []
  type: TYPE_NORMAL
- en: The term *n*-grams can refer to other types of sequences of length *n*, such
    as *n* characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a large corpus of text, we can find all the *n*-grams up until a
    certain *n* (typically 2 to 4) and count the occurrence of each *n*-gram in that
    corpus. From these counts, we can estimate the probabilities of the last word
    of each *n*-gram, given the previous *n-1* words:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1-gram**:[![](img/fb25070e-7589-4481-89ba-4ae57914aea4.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2-gram**: [![](img/25b8576c-7d29-43b9-b271-9f93061e8f1a.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N-gram**: [![](img/25f22551-a6df-486d-92b9-7f1cfdd37d1f.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The independent assumption that the *i-*th word is only dependent on the previous *n-1*
    words can now be used to approximate the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for a unigram, we can approximate the joint distribution by using
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a638abc-d9c6-43ab-98f0-d0800f5b8579.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For a trigram, we can approximate the joint distribution by using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6de5cc70-b831-454a-b92a-d7d6b822a585.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that, based on the vocabulary size, the number of *n*-grams grows
    exponentially with *n*. For example, if a small vocabulary contains 100 words,
    then the number of possible 5-grams would be *100⁵ = 10,000,000,000* different
    5-grams. In comparison, the entire works of Shakespeare contain around 30,000
    different words, illustrating the infeasibility of using *n*-grams with a large *n*.
    Not only is there the issue of storing all the probabilities, but we would also
    need a very large text corpus to create decent *n*-gram probability estimations
    for larger values of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is known as the curse of dimensionality. When the number of possible
    input variables (words) increases, the number of different combinations of these
    input values increases exponentially. The curse of dimensionality arises when
    the learning algorithm needs at least one example per relevant combination of
    values, which is the case in *n*-gram modeling. The larger our *n*, the better
    we can approximate the original distribution and the more data we would need to
    make good estimations of the *n*-gram probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are familiar with the *n*-gram model and the curse of dimensionality,
    let's discuss how to solve it with the help of neural language models.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing neural language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One way to overcome the curse of dimensionality is by learning a lower-dimensional,
    distributed representation of the words (*A Neural Probabilistic Language Model*, [http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)).
    This distributed representation is created by learning an embedding function that
    transforms the space of words into a lower-dimensional space of word embeddings
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/651755db-3fcd-4cb6-b39d-91f34cb191a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Words -> one-hot encoding -> word embedding vectors
  prefs: []
  type: TYPE_NORMAL
- en: Words from the vocabulary with size *V* are transformed into one-hot encoding
    vectors of size *V* (each word is encoded uniquely). Then, the embedding function
    transforms this *V*-dimensional space into a distributed representation of size *D* (here,
    *D*=4).
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the embedding function learns semantic information about the
    words. It associates each word in the vocabulary with a continuous-valued vector
    representation, that is, the word embedding. Each word corresponds to a point
    in this embedding space, and different dimensions correspond to the grammatical
    or semantic properties of these words.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to ensure that the words close to each other in the embedding space
    have similar meanings. In this way, the information that some words are semantically
    similar can be exploited by the language model. For example, it might learn that
    *fox* and *cat* are semantically related and that both *the quick brown fox* and
    *the quick brown cat* are valid phrases. A sequence of words can then be replaced
    with a sequence of embedding vectors that capture the characteristics of these
    words. We can use this sequence as a base for various NLP tasks. For example,
    a classifier trying to classify the sentiment of an article might be trained on
    using previously learned word embeddings, instead of one-hot encoding vectors.
    In this way, the semantic information of the words becomes readily available for
    the sentiment classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are one of the central paradigms when solving NLP tasks. We
    can use them to improve the performance of other tasks where there might not be
    a lot of labeled data available. Next, we'll discuss the first neural language
    model that was introduced in 2001 (which serves as an example that many of the
    concepts in deep learning are not new).
  prefs: []
  type: TYPE_NORMAL
- en: We usually denote vectors with bold non-italic lowercase letters, such as **w**.
    But the convention in neural language models is to use italic lowercase, such
    as *w.* In this chapter, we'll use this convention.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at the **neural probabilistic language
    model** (**NPLM**).
  prefs: []
  type: TYPE_NORMAL
- en: Neural probabilistic language model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is possible to learn the language model and, implicitly, the embedding function
    via a feedforward fully connected network. Given a sequence of *n-1* words (*w[t-n+1] ,
    ..., w[t-1]*), it tries to output the probability distribution of the next word, *w[t]* (the
    following diagram is based on [http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ddf599f-8890-4f57-86bf-872e19c666f9.png)'
  prefs: []
  type: TYPE_IMG
- en: A neural network language model that outputs the probability distribution of
    the word w[t], given the words *w[t-n+1]* ... *w[t-1]*. *C* is the embedding matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The network layers play different roles, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer takes the one-hot representation of the word *w[i]* and
    transforms it into the word's embedding vector by multiplying it with the embedding
    matrix, **C**. This computation can be efficiently implemented with table lookup.
    The embedding matrix, **C**, is shared between the words, so all words use the
    same embedding function. **C** is a *V * D* matrix, where *V* is the size of the
    vocabulary and *D* is the size of the embedding. In other words, the matrix, **C**,
    represents the network weights of the hidden *tanh* layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting embeddings are concatenated and serve as an input to the hidden
    layer, which uses *tanh* activation. The output of the hidden layer is thus represented
    by the [![](img/75975090-e212-41c9-b67a-286040eb447a.png) ]function, where **H**
    represents the embedding to hidden layer weights and *d* represents the hidden
    biases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we have the output layer with weights, **U**, bias, *b*, and softmax
    activation, which map the hidden layer to the word space probability distribution:
    [![](img/1b6b697c-5fc7-4038-9f98-f897be806696.png)]*.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This model simultaneously learns an embedding of all the words in the vocabulary
    (embedding layer) and a model of the probability function for sequences of words
    (network output). It is able to generalize this probability function to sequences
    of words that were not seen during training. A specific combination of words in
    the test set might not be seen in the training set, but a sequence with similar
    embedding features is much more likely to be seen during training. Since we can
    construct the training data and labels based on the positions of the words (which
    already exist in the text), training this model is an unsupervised learning task.
    Next, we'll discuss the word2vec language model, which was introduced in 2013
    and sparked an interest in the field of NLP in the context of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of research has gone into creating better word embedding models, in particular
    by omitting learning the probability function over sequences of words. One of
    the most popular ways to do this is with word2vec ([http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781),
    [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)). Similar to
    NPLM, word2vec creates embedding vectors based on the context (surrounding words)
    of the word in focus. It comes in two flavors: **continuous bag of words** (**CBOW**)
    and **Skip-gram**. We'll start with CBOW and then we'll discuss Skip-gram.
  prefs: []
  type: TYPE_NORMAL
- en: CBOW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CBOW predicts the most likely word given its context (surrounding words). For
    example, given the sequence *The quick* _____ *fox jumps*, the model will predict
    *brown*. The context is the *n* preceding and the *n* following words of the word
    in focus (unlike NPLM, where only the preceding words participate). The following
    screenshot shows the context window as it slides across the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf31bab8-c35a-4cfd-817f-49f3bd0aa8d6.png)'
  prefs: []
  type: TYPE_IMG
- en: A word2vec sliding context window with *n = 2*. The same type of context window
    applies to both CBOW and Skip-gram
  prefs: []
  type: TYPE_NORMAL
- en: 'CBOW takes all words within the context with equal weights and doesn''t consider
    their order (hence the *bag* in the name). It is somewhat similar to NPLM, but
    because it learns only the embedding vectors, we''ll train the model with the
    help of the following simple neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bc7f5b4-39f6-4097-9b72-a0f4a0c64f8a.png)'
  prefs: []
  type: TYPE_IMG
- en: A CBOW model network
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: The network has input, hidden, and output layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input is the one-hot encoded word representations. The one-hot encoded vector
    size of each word is equal to the size of the vocabulary, *V*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding vectors are represented by the input-to-hidden weights, **W***[V×D]*,
    of the network. They are *V × D-*shaped matrix, where *D* is the length of the
    embedding vector (which is the same as the number of units in the hidden layer).
    As in NPLM, we can think of the weights as a lookup table, where each row represents
    one word embedding vector. Because each input word is one-hot encoded, it will
    always activate a single row of the weights. That is, for each input sample (word),
    only the word's own embedding vector will participate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding vectors of all context words are averaged to produce the output
    of the hidden network layer (there is no activation function).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden activations serve as input to the output softmax layer of size *V*
    (with the weight vector **W^'***[D×V]*), which predicts the most likely word to
    be found in the context (proximity) of the input words. The index with the highest
    activation represents the one-hot encoded related word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll train the network with gradient descent and backpropagation. The training
    set consists of (context and label) one-hot encoded pairs of words, appearing
    in close proximity to each other in the text. For example, if part of the text
    is the sequence `[the, quick, brown, fox, jumps]` and *n = 2*, the training tuples
    will include `([quick, brown], the)`, `([the, brown, fox], quick)`, `([the, quick,
    fox jumps], brown)`, and so on. Since we are only interested in the embeddings, **W***[V×D]*,
    we'll discard the rest of the network weights, **W^'***[V×D]*, when the training
    is finished.
  prefs: []
  type: TYPE_NORMAL
- en: CBOW will tell us which word is most likely to appear in a given context. This
    could be a problem for rare words. For example, given the context *The weather
    today is really* ____, the model will predict the word *beautiful* rather than
    *fabulous* (hey, it's just an example). CBOW is several times faster to train
    than the Skip-gram and achieves slightly better accuracy for frequent words.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given an input word, the Skip-gram model can predict its context (the opposite
    of CBOW). For example, the word *brown* will predict the words *The quick fox
    jumps*. Unlike CBOW, the input is a single one-hot word. But how do we represent
    the context words in the output? Instead of trying to predict the whole context
    (all surrounding words) simultaneously, Skip-gram transforms the context into
    multiple training pairs such as `(fox, the)`, `(fox, quick)`, `(fox, brown)`,
    and `(fox, jumps)`. Once again, we can train the model with a simple one-layer
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/463ba7aa-d28f-4a91-bfb9-a43718ae33d5.png)'
  prefs: []
  type: TYPE_IMG
- en: A Skip-gram model network
  prefs: []
  type: TYPE_NORMAL
- en: As with CBOW, the output is a softmax, which represents the one-hot encoded
    most probable context word. The input-to-hidden weights, **W***[V×D]*, represent
    the word embeddings lookup table and the hidden-to-output weights, **W^'***[D×V]*,
    are only relevant during training. The hidden layer doesn't have an activation
    function (that is, it uses linear activation).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll train the model with backpropagation (no surprises here). Given a sequence
    of words, *w[1], ..., w[M]*, the objective of the Skip-gram model is to maximize
    the average log probability where *n* is the window size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57a3d779-efa4-4e17-b47f-9ea3e2ac7cba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model defines the probability, [![](img/cfd1f18b-16bd-4da0-b9ca-6f037633c6fb.png),] as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1bf9184-f2d2-4ed3-87f9-def2a0b5f267.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, *w[I]* and *w[O]* are the input and output words and **v***[w]* and **v***^'[w]*
    are the corresponding word vectors in the input and output weights **W***[V×D]*
    and **W^'***[D×V]*, respectively (we keep the original notation of the paper).
    Since the net doesn't have a hidden activation function, its output value for
    one input/output word pair is simply the multiplication of the input word vector, [![](img/198b51f8-3363-4fe6-b6be-69cea1dd7fdc.png),] and
    the output word vector, [![](img/408cc438-39b1-44e4-8904-02094aa20d34.png)] (hence
    the transpose operation).
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the word2vec paper note that word representations cannot represent
    idiomatic phrases that are not compositions of the individual words. For example,
    *New York Times* is a newspaper, and not just a natural combination of the meanings
    of *New*, *York, *and *Times*. To overcome this, the model can be extended to
    include whole phrases. However, this significantly increases the vocabulary size.
    And, as we can see from the preceding formula, the softmax denominator needs to
    compute the output vectors for all words of the vocabulary. Additionally, every
    weight of the **W^'***[D×V]* matrix is updated on every training step, which slows
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this, we can replace the softmax with the so-called **negative sampling**
    (**NEG**). For each training sample, we'll take the positive training pair (for
    example, `(fox, brown)`), as well as *k* additional negative pairs (for example,
    `(fox, puzzle)`), where *k* is usually in the range of [5,20]. Instead of predicting
    the word that best matches the input word (softmax), we'll simply predict whether
    the current pair of words is true or not. In effect, we convert the multinomial
    classification problem (classify as one of many classes) to a binary logistic
    regression (or binary classification) problem. By learning the distinction between
    positive and negative pairs, the classifier will eventually learn the word vectors
    in the same way, as with multinomial classification. In word2vec, the words for
    the negative pairs are drawn from a special distribution, which draws less frequent
    words more often, compared to more frequent ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most frequent words to occur carry less information value compared
    to the rare words. Examples of such words are the definite and indefinite articles
    *a*, *an*, and *the*. The model will benefit more from observing the pairs *London* and
    *city* compared to *the* and *city* because almost all words co-occur frequently
    with *the*. The opposite is also true—the vector representations of frequent words
    do not change significantly after training on a large number of examples. To counter
    the imbalance between the rare and frequent words, the authors of the paper propose
    a subsampling approach, where each word, *w[i]*, of the training set is discarded
    with some probability, computed by the heuristic formula where *f(w[i])* is the
    frequency of word *w[i]* and *t* is a threshold (usually around 10^(-5)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95e5cdee-8daa-4410-8b6b-e56d7b5653ce.png)'
  prefs: []
  type: TYPE_IMG
- en: It aggressively subsamples words with a frequency of greater than *t*, but also
    preserves the ranking of the frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, we can say that, in general, Skip-gram performs better on rare
    words compared to CBOW, but it takes longer to train.
  prefs: []
  type: TYPE_NORMAL
- en: fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'fastText ([https://fasttext.cc/](https://fasttext.cc/)) is a library for learning word
    embeddings and text classification created by the **Facebook AI Research** (**FAIR**)
    group. Word2Vec treats each word in the corpus as an atomic entity and generates
    a vector for each word, but this approach ignores the internal structure of the
    words. In contrast, fastText decomposes each word, *w*, to a bag of character
    *n*-grams. For example, if *n = 3*, we can decompose the word *there* to the character
    3-grams and the special sequence *<there>* for the whole word:'
  prefs: []
  type: TYPE_NORMAL
- en: '*<th*, *the*, *her*, *ere*, *re>*'
  prefs: []
  type: TYPE_NORMAL
- en: Note the use of the special characters *<* and *>* to indicate the start and
    the end of the word. This is necessary to avoid mismatching between *n*-grams
    from different words. For example, the word *her* will be represented as *<her>*
    and it will not be mistaken for the *n*-gram *her* from the word *there*. The
    authors of fastText suggest ***3 ≤ n ******≤ 6***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the softmax formula we introduced in the *Skip-gram* section. Let''s
    generalize it by replacing the vector multiplication operation of the word2vec
    network with a generic scoring function, ***s***,where *w[t]* is the input word
    and *w[c]* is the context word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e4e75aa-b63f-41af-bd4c-9fb77323debb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of fastText, we''ll represent a word with the sum of the vector
    representations of its *n*-grams. Let''s denote the set of *n*-grams that appear
    in word *w* with *G[w] = {1 ... G}*, the vector representation of an *n*-gram, *g*,
    with **v*[g]***, and the potential vector of the context word, *c*, with **v***''**[c]***.
    Then, the scoring function defined by fastText becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6470be2a-59c7-461e-869d-1e98f2cd83d3.png)'
  prefs: []
  type: TYPE_IMG
- en: In effect, we train the fastText model with Skip-gram type word pairs, but the
    input word is represented as a bag of *n*-grams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using character *n*-grams has several advantages over the traditional word2vec
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: It can classify unknown or misspelled words if they share *n*-grams with other
    words familiar to the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can generate better word embeddings for rare words. Even if a word is rare,
    its character *n*-grams are still shared with other words, so the embeddings can
    still be good.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we are familiar with word2vec, we'll introduce the Global Vectors for
    Word Representation language model, which improves some word2vec deficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: Global Vectors for Word Representation model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One disadvantage of word2vec is that it only uses the local context of words
    and doesn't consider their global co-occurrences. In this way, the model loses
    a readily available, valuable source of information. As the name suggests, the
    **Global Vectors for Word Representation** (**GloVe**) model tries to solve this
    ([https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm starts with the global word-word co-occurrence matrix, **X**.
    A cell, *X[ij]*, indicates how often the word *j* appears in the context of word
    *i*. The following table shows the co-occurrence matrix for a window with size
    *n *= 2 of the sequence *I like DL. I like NLP. I enjoy cycling*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec13a661-4de4-403e-93dc-99355e343fcd.png)'
  prefs: []
  type: TYPE_IMG
- en: A co-occurrence matrix of the sequence I like DL. I like NLP. I enjoy cycling
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s denote the number of times any word appears in the context of word *i*
    with ![](img/89690123-8c01-410a-a982-61a58a6ea93b.png) and the probability that
    word *j* appears in the context of word *i* with ![](img/760aa31f-aec2-40d3-9a1a-a0597141079e.png).
    To better understand how this can help us, we''ll use an example that shows the
    co-occurrence probabilities for the target words *ice* and *steam* with selected
    context words from a 6 billion token corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e01e3c90-35c6-49cf-9cb9-408a1cc6ff0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Co-occurrence probabilities for the target words ice and steam with selected
    context words from a 6 billion token corpus: source: https://nlp.stanford.edu/pubs/glove.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: The bottom row shows the ratio of the probabilities. The word **solid** (the
    first column) is related to **ice**, but less related to **steam**, so the ratio
    between their probabilities is large. Conversely, **gas** is more related to **steam**
    than to **ice** and the ratio between their probabilities is very small. The words
    **water** and **fashion** are equally related to both target words, hence the
    ratio of the probabilities is close to one. The ratio is better in distinguishing
    relevant words *(***solid** and **gas***)* from irrelevant words *(***water**
    and **fashion***),* compared to the raw probabilities*.* Additionally, it is better
    at discriminating between the two relevant words.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the previous argument, the authors of GloVe propose starting the word
    vector learning with the ratios of co-occurrence probabilities, rather than the
    probabilities themselves. With that starting point, and keeping in mind that the
    ratio [![](img/a59c08e9-51d4-4beb-8523-4ef9f0ca0ca9.png)] depends on three words—*i*,
    *j*, and *k—*we can define the most general form of the GloVe model as follows, where [![](img/a4e0ad61-266b-42b5-97eb-67e1b19d4fa8.png) ]are
    the word vectors and [![](img/f2464866-daa8-4cb7-99e4-60f99a9a8536.png)] is a
    special context vector, which we''ll discuss later ([![](img/b5075698-73c1-4bbd-99eb-4ff0d272b288.png)] is
    the *D*-dimensional vector space of real numbers):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d2c29e4-9762-486b-a188-e70f17501842.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, *F* is such a function that, when computed with these three
    specific vectors (we assume that we already know them), will output the ratio
    of probabilities. Furthermore, *F* should encode the information of the probabilities
    ratio because we''ve already identified its importance. Since vector spaces are
    inherently linear, one way to encode this information is with the vector difference
    of the two target words. Therefore, the function becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a938fcb-379e-4210-a6a3-3a3ade67f6d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s note that the function arguments are vectors, but the ratio of
    probabilities is scalar. To solve this issue, we can take the dot product of the
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7494734-0fd3-4297-a393-de1ec3c20c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, let''s observe that the distinction between a word and its context word
    is arbitrary and we can freely exchange the two roles. Therefore, we should have
    [![](img/f11ecfab-81ef-4534-ac16-afd696d1af6d.png)], but the preceding equation
    doesn''t satisfy this condition. Long story short (there is a more detailed explanation
    in the paper), to satisfy this condition, we need to introduce another restriction
    in the form of the following equation where, [![](img/8edde8d5-a0e9-40e3-ba81-3e61184d7bbd.png)]
    and [![](img/e189ae1c-eb9b-4faf-8ceb-6f7faaddb01b.png)] are bias scalar values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/109aeb0d-0d5a-421b-a793-02edb89b4361.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One issue with this formula is that *log(0)* is undefined, but the majority
    of the *X[ik]* entries will be *0*. Additionally, it takes all co-occurrences
    with the same weight, but rare co-occurrences are noisy and carry less information
    than the more frequent ones. To solve all these issues, the authors propose a
    least squares regression model with a weighting function, *f(X[ij])*, for each
    co-occurrence. The model has the following cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75a06b7d-7aaf-486b-8759-a77da7a76184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the weighting function, *f*, should satisfy several properties. First, *f(0)
    = 0*. Then, *f(x)* should be non-decreasing so that rare co-occurrences are not
    overweighted. And finally, *f(x)* should be relatively small for large values
    of *x*, so that frequent co-occurrences are not overweighted. Based on these properties
    and their experiments, the authors propose the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7a7a719-5075-4504-b703-0ac6c60ff2e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows *f(x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49d5223a-7ba8-42a4-b3ea-5bc5c794964c.png)'
  prefs: []
  type: TYPE_IMG
- en: Weighting function *f(X[ij])* with a cut-off value of *x[max]*= 100 and *α =
    3/4.* The authors' experiments show that these parameters work best; source: https://nlp.stanford.edu/pubs/glove.pdf
  prefs: []
  type: TYPE_NORMAL
- en: 'The model generates two sets of word vectors: *W* and ![](img/e95f0f4d-b4cb-40e3-bb19-e5df3bbcfd59.png).
    When *X* is symmetric, *W* and ![](img/f0df870d-e98c-411b-b409-ce351e2fbb86.png) are
    equivalent and differ only as a result of their random initializations. But the
    authors note that training an ensemble of networks and averaging their results
    usually helps to prevent overfitting. To mimic this behavior, they choose to use the
    sum [![](img/3b86fd4d-d51e-4d81-87d3-073d63bd0b21.png)] as the final word vectors,
    observing a small increase in the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion about neural language models. In the next section,
    we'll see how to train and visualize a word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement a short pipeline for preprocessing text sequences
    and training a word2vec model with the processed data. We'll also implement another
    example to visualize embedding vectors and check some of their interesting properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in this section requires the following Python packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gensim** (version 3.80, [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/))
    is an open source Python library for unsupervised topic modeling and NLP. It supports
    all three models that we have discussed so far (word2vec, GloVe, and fastText).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Natural Language Toolkit** (**NLTK**, [https://www.nltk.org/](https://www.nltk.org/), ver
    3.4.4) is a Python suite of libraries and programs for symbolic and statistical
    NLP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn (ver 0.19.1, [https://scikit-learn.org/](https://scikit-learn.org/))
    is an open source Python ML library with various classification, regression, and
    clustering algorithms. More specifically, we'll use **t-Distributed Stochastic
    Neighbor Embedding** (**t-SNE**, [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))
    to visualize high-dimensional embedding vectors (more on that later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this introduction, let's continue with training the language model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the embedding model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first example, we''ll train a word2vec model on the classic novel *War
    and Peace* by Leo Tolstoy. The novel is stored as a regular text file in the code
    repository. Let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the tradition goes, we''ll do the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll set the logging level to `INFO` so we can track the training progress:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement the text tokenization pipeline. Tokenization refers
    to the breaking up of a text sequence into pieces (or **tokens**) such as words,
    keywords, phrases, symbols, and other elements. Tokens can be individual words,
    phrases, or even whole sentences. We''ll implement two-level tokenization; first,
    we''ll split the text into sentences and then we''ll split each sentence into
    individual words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TokenizedSentences` iterator takes as an argument the text filename, where
    the novel is located. Here''s how the rest of it works:'
  prefs: []
  type: TYPE_NORMAL
- en: The iteration starts by reading the full contents of the file in the `corpus`
    variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The raw text is split into a list of sentences (the `raw_sentences` variable)
    with the help of NLTK's `nltk.tokenize.sent_tokenize(corpus)` function. For example,
    it will return a `['I like DL.', 'I like NLP.'`, `'I enjoy cycling.']` for input
    list `'I like DL. I like NLP. I enjoy cycling.'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, each `sentence` is preprocessed with the `gensim.utils.simple_preprocess(sentence,
    min_len=2, max_len=15)` function. It converts a document into a list of lowercase
    tokens, ignoring tokens that are too short or too long. For example, the `'I like
    DL'` sentence will be tokenized to the `['like', 'dl']` list. The punctuation
    characters are also removed. The tokenized sentence is yielded as the final result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we''ll instantiate `TokenizedSentences`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll instantiate Gensim''s word2vec training model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The model takes `sentences` as a training dataset. `Word2Vec` supports all parameters
    and variants of the model that we've discussed in this chapter. For example, you
    can switch between CBOW or Skip-gram with the `sg` parameter. You can also set
    the context window size, negative sampling count, number of epochs, and other
    things. You can explore all parameters in the code itself.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the fastText model by replacing `gensim.models.word2vec.Word2Vec`
    with `gensim.models.fasttext.FastText` (it works with the same input parameters).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Word2Vec` constructor also initiates the training. After a short time
    (you don''t need the GPU, as the training dataset is small), the generated embedding
    vectors are stored in the `model.wv` object. On one hand, it acts like a dictionary
    and you can access the vector for each word with `model.wv[''WORD_GOES_HERE''],`
    however, it also supports some other interesting functionality. You can measure
    the similarity between different words based on the difference of their word vectors
    with the `model.wv.most_similar` method. First, it converts each word vector to
    a unit vector (a vector with a length of one). Then, it computes the dot product
    between the unit vector of the target word and the unit vectors of all other words.
    The higher the dot product between two vectors, the more similar they are. For
    example, `pprint.pprint(model.wv.most_similar(positive=''mother'', topn=5))` will
    output the five most similar words to the word `''mother''` and their dot products:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The result serves as a kind of proof that the word vectors correctly encode
    the meaning of the words. The word `'mother'` is indeed related by meaning to
    `'sister'`, `'daughter'`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also find the most similar words to a combination of target words. For
    example, `model.wv.most_similar(positive=[''woman'', ''king''], topn=5)` will
    take the mean of the word vectors of `''woman''` and `''king''` and then it will
    find the words with the most similar vectors to the new mean value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can see that some of the words are relevant (`'heiress'`), but most aren't
    (`'creature'`, `'admirable'`). Perhaps our training dataset is too small to capture
    more complex relations like these.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing embedding vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To obtain better word vectors, compared to the ones in the *Training embedding
    model* section, we''ll train another word2vec model. However, this time, we will
    use a larger corpus—the `text8` dataset, which consists of the first 100,000,000
    bytes of plain text from Wikipedia. The dataset is included in Gensim and it''s tokenized
    as a single long list of words. With that, let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, the imports are first. We''ll also set the logging to `INFO` for
    good measure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll train the `Word2vec` model. This time, we''ll use CBOW for faster
    training. We''ll load the dataset with `gensim_downloader.load(''text8'')`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To see if this model is better, we can try to find the words most similar to
    `''woman''` and `''king''`, but most dissimilar to `''man''`. Ideally, one of
    the words would be `''queen''`. We can do this with the expression `pprint.pprint(model.wv.most_similar(positive=[''woman'',
    ''king''], negative=[''man'']))`. The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, the most similar word is `'queen'`, but the rest of the words are relevant
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll display the words in a 2D plot with the help of a t-SNE visualization
    model on the collected word vectors. The t-SNE models each high-dimensional embedding
    vector on a two- or three-dimensional point in a way where similar objects are
    modeled on nearby points and dissimilar objects are modeled on distant points
    with a high probability. We''ll start with several `target_words` and then we''ll
    collect clusters of the *n* most similar words (and their vectors) to each target
    word. The following is the code that does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll train a t-SNE visualization model on the collected clusters with the
    following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`perplexity` is loosely related to the number of nearest neighbors considered
    when matching the original and the reduced vectors for each point. In other words,
    it determines whether the algorithm will focus on the local or the global properties
    of the data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_components=2` specifies the number of output vector dimensions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_iter=5000` is the number of training iterations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init=''pca''` to use **principal component analysis** (**PCA**)-based initialization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model takes the `embedding_groups` clusters as input and outputs the `embeddings_2d` array
    with 2D embedding vectors. The following is the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll display the new 2D embeddings. To do this, we''ll initialize the
    plot and some of its properties for better visibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll iterate over each `similar_words` cluster and we''ll display its
    words on a scatter plot as points. We''ll use a unique marker for each cluster.
    The points will be annotated with their corresponding words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll display the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how each cluster of related words is grouped in a close region of
    the 2D plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65557fb2-c974-4a23-a244-897b53e74984.png)'
  prefs: []
  type: TYPE_IMG
- en: t-SNE visualization of the target words and their clusters of the most similar
    words
  prefs: []
  type: TYPE_NORMAL
- en: The graph, once again, proves that the obtained word vectors contain relevant
    information for the words. With the end of this example, we conclude the chapter
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was the first chapter devoted to NLP. Appropriately, we started with the
    basic building blocks of most NLP algorithms today—the words and their context-based
    vector representations. We started with *n*-grams and the need to represent words
    as vectors. Then, we discussed the word2vec, fastText, and GloVe models. Finally,
    we implemented a simple pipeline to train an embedding model and we visualized
    word vectors with t-SNE.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss RNNs—a neural network architecture that naturally
    lends itself to NLP tasks.
  prefs: []
  type: TYPE_NORMAL
