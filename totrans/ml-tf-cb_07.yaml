- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting with Tabular Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the available data that can be easily found is not composed of images
    or text documents, but it is instead made of relational tables, each one possibly
    containing numbers, dates, and short text, which can be all joined together. This
    is because of the widespread adoption of database applications based on the relational
    paradigm (data tables that can be combined together by the values of certain columns
    that act as joining keys). These tables are the main source of tabular data nowadays
    and because of that, there are certain challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the challenges commonly faced by **Deep Neural Networks** (**DNNs**)
    when applied to tabular data:'
  prefs: []
  type: TYPE_NORMAL
- en: Mixed features data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data in a sparse format (there are more zeros than non-zero data), which is
    not the best for a DNN converging to an optimum solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No state-of-the-art architecture has emerged yet, there are just some various
    best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less data is available for a single problem than in a usual image recognition
    problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's suspicion from non-technical people because DNNs are less interpretable
    than simpler machine learning algorithms for tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, DNNs are not the best-in-class solution for tabular data, because gradient
    boosting solutions (such as LightGBM, XGBoost, and CatBoost) might perform better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if these challenges seem quite difficult, simply do not get discouraged.
    The challenges when applying DNNs to tabular data are certainly serious, but on
    the other hand, so are the opportunities. Andrew Ng, Adjunct Professor at Stanford
    University and deep learning expert ([https://www.coursera.org/instructor/andrewng](https://www.coursera.org/instructor/andrewng)),
    recently stated: *"Deep learning has seen tremendous adoption in consumer Internet
    companies with a huge number of users and thus big data, but for it to break into
    other industries where datasets sizes are smaller, we now need better techniques
    for small data."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we introduce you to some of the best recipes for handling
    small, tabular data with TensorFlow. In doing so, we will be using TensorFlow,
    Keras, and two specialized machine learning packages: pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    and scikit-learn ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)).
    In the previous chapters, we often used TensorFlow Datasets ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets))
    and specialized layers for feature columns ([https://www.tensorflow.org/api_docs/python/tf/feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column)).
    We could have reused them for this chapter, but then we would have missed some
    interesting transformations that only scikit-learn can provide, and doing cross-validation
    would have proved difficult.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider moreover that using scikit-learn makes sense if you are comparing the
    performance of different algorithms on a problem, and you need to standardize
    a data preparation pipeline not only for the TensorFlow model but also for other
    more classical machine learning and statistical models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install pandas and scikit-learn (if you are using Anaconda, they
    should already be on your system), please follow these guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For pandas: [https://pandas.pydata.org/docs/getting_started/install.html](https://pandas.pydata.org/docs/getting_started/install.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For scikit-learn: [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will deal with a series of recipes focused on learning from
    tabular data, which is data arranged in the form of a table, where rows represent
    observations and columns are the observed values for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular data is the common input data for most machine learning algorithms,
    but not a usual one for DNNs, since DNNs excel with other kinds of data, such
    as images and text.
  prefs: []
  type: TYPE_NORMAL
- en: Recipes for deep learning for tabular data require solving problems, such as
    data heterogeneity, which are not mainstream, and they require using many common
    machine learning strategies, such as cross-validation, which are not currently
    implemented in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should have knowledge of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing dates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing ordinal data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing high-cardinality categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping up all the processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a data generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating custom activations for tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a test run on a difficult problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start immediately with how to deal with numerical data. You will be amazed
    by how these recipes can be effective with many tabular data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Processing numerical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by preparing numerical data. You have numerical data when:'
  prefs: []
  type: TYPE_NORMAL
- en: Your data is expressed by a floating number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your data is an integer and it has a certain number of unique values (otherwise
    if there are only few values in sequence, you are dealing with an ordinal variable,
    such as a ranking)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your integer data is not representing a class or label (otherwise you are dealing
    with a categorical variable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When working with numerical data, a few situations may affect the performance
    of a DNN when processing such data:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing data (NULL or NaN values, or even INF values) that will prevent your
    DNN from working at all
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constant values that will make computations slower and interfere with the bias
    each neuron in the network is already providing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skewed distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-standardized data, especially data with extreme values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before feeding numerical data to your neural network, you have to be sure that
    all these issues have been properly dealt with or you may encounter errors or
    a learning process that will not work.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to address all the potential issues, we will mostly be using specialized
    functions from scikit-learn. Before starting our recipe, we will import them into
    our environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to test our recipe we will use a simple 3x4 table, with some columns
    containing NaN values, and some constant columns that contain no NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our recipe will build a scikit-learn pipeline, based on our indications relative
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum acceptable variance for a feature to be kept, or you may just be
    introducing unwanted constants into your network that may hinder the learning
    process (the `variance_threshold` parameter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to use as a baseline strategy for imputing missing values (the `imputer`
    parameter, by default set to replace missing values with the mean of the feature)
    so that your input matrix will be completed and matrix multiplication will be
    possible (the basic computation in a neural network)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether we should use a more sophisticated imputation strategy based on the
    missing values of all the numeric data (the `multivariate_imputer` parameter),
    because sometimes points are not missing at random and other variables may supply the
    information you need for a proper estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to add a binary feature denoting for each feature where the missing
    values were, which is a good strategy because you often find information also
    on missing patterns (the `add_indicator` parameter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to transform the distribution of variables in order to force them to
    resemble a symmetric distribution (`quantile_transformer` parameter, set to `normal`
    by default) because your network will learn better from symmetrical data distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether we should rescale our output based on the statistical normalization,
    that is, dividing by the standard deviation after having removed the mean (the
    `scaler` parameter, set to `True` by default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, bearing all that in mind, let''s build our pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our numerical pipeline by specifying our transformation preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can immediately try our new function on the example by applying first the
    `fit` and then the `transform` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting output NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see all the original data has been completely transformed, with all
    the missing values replaced.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we previously mentioned, we are using scikit-learn for comparability with
    other machine learning solutions and because there are a few unique scikit-learn
    functions involved in the building of this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VarianceThreshold` ([https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IterativeImputer` ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SimpleImputer` ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QuantileTransformer` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline` ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each function, you will find a link pointing to the scikit-learn documentation
    with detailed information on how the function works. It is paramount to explain
    why the scikit-learn approach is so important for this recipe (and for the others
    you will find in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: When processing images or text, you usually don't need to define specific processes
    for respectively training and testing data. That's because you apply deterministic
    transformations to both. For instance, in images, you just divide the pixels'
    values by 255 in order to normalize them.
  prefs: []
  type: TYPE_NORMAL
- en: However, with tabular data you need transformations that are more complex and
    not deterministic at all because they involve learning and memorizing specific
    parameters. For instance, when imputing a missing value for a feature by using
    the mean, you have first to compute the mean from your training data. Then you
    have to reuse that exact value for any other new data you will apply the same
    imputation on (it won't work to compute again the mean on any new data because
    it could be from a slightly different distribution and may not match what your
    DNN has learned).
  prefs: []
  type: TYPE_NORMAL
- en: All of this involves keeping track of many parameters learned from your training
    data. scikit-learn may help you in that because when you use the `fit` method,
    it learns and stores away all the parameters it derives from training data. Using
    the `transform` method, you will apply the transformations with the learned-by-fit
    parameters on any new data (or on the very same training data).
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'scikit-learn functions usually return a NumPy array. It is not a problem to
    label the resulting array using the input columns, if no further feature creation
    has occurred. Unfortunately, this is not the case because of the transformation
    pipeline we created:'
  prefs: []
  type: TYPE_NORMAL
- en: The variance threshold will remove features that are not useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing value imputation will create missing binary indicators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can actually explore this by inspecting the fitted pipeline and finding
    out which columns have been removed and what has been added from the original
    data. A function can be created to do just that for us automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we try it on our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain a pandas index containing the remaining columns and the binary indicators
    (denoted by the name of the original feature and the `_missing` suffix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Keeping track of your columns as you transform them can help you when you need
    to debug your transformed data and if you need to explain how your DNN works using
    tools such as shap ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    or lime ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)).
  prefs: []
  type: TYPE_NORMAL
- en: This recipe should suffice for all your needs with regard to numerical data.
    Now let's proceed to examine dates and times.
  prefs: []
  type: TYPE_NORMAL
- en: Processing dates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dates are common in databases and, especially when processing the forecasting
    of future estimates (such as in sales forecasting), they can prove indispensable.
    Neural networks cannot process dates as they are, since they are often expressed
    as strings. Hence, you have to transform them by separating their numerical elements,
    and once you have split a date into its components, you have just numbers that
    can easily be dealt with by any neural network. Certain time elements, however,
    are cyclical (days, months, hours, days of the week) and lower and higher numbers
    are actually contiguous. Consequently, you need to use sine and cosine functions,
    which will render such cyclical numbers in a format that can be both understood
    and correctly interpreted by a DNN.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we need to code a class operating using the fit/transform operations
    that are typical of scikit-learn, we import the `BaseEstimator` and `TransformerMixin`
    classes from scikit-learn to inherit from. This inheritance will help us to make
    our recipe perfectly compatible with all other functions from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For testing purposes, we also prepare an example dataset of dates in string
    form, using the day/month/year format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The provided example is quite short and simplistic, but it should illustrate
    all the relevant points as we work through it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This time we will design a class of our own, `DateProcessor`. After being initialized,
    instances of this class can pick a pandas DataFrame and filter and process each
    date into a new DataFrame that can be processed by a DNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process focuses on one date at a time, extracting days, days of the week,
    months, and years (additionally also hours and minutes), and transforming all
    cyclical time measures using sine and cosine transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have scripted down the recipe in the form of a `DateProcessor` class,
    let's explore more of its inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to the entire class is the transformation operated by the pandas `to_datetime`
    function, which turns any string representing a date into the `datetime64[ns]`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: '`to_datetime` works because you provide it a template (the `format` parameter)
    for turning strings into dates. For a complete guide on how to define such a template,
    please visit [https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you need to fit and transform your data, the class will automatically
    process all the dates into the right format and furthermore, perform transformations
    using sine and cosine functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Some resulting transformations will be obvious, but some others related to cyclical
    time may appear puzzling. Let's spend a bit of time exploring how they work and
    why.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The class doesn''t return the raw extraction of time elements such as the hour,
    the minute, or the day, but it transforms them using first a sine, then a cosine
    transformation. Let''s plot how it transforms the 24 hours in order to get an
    better understanding of this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot that you will obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Plotting of hourly time after sine and cosine transformations'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plot, we can figure out how the start and end of the day coincide,
    thus closing the time cycle. Each transformation also returns the same value for
    a couple of different hours. That''s the reason why we should pick both sine and
    cosine together; if you use both, each point in time has a different tuple of
    sine and cosine values, and so you can detect exactly where you are in continuous
    time. This can also be explained visually by plotting the sine and cosine values
    in a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Combining the sine and cosine transformations of hourly time into
    a scatter plot'
  prefs: []
  type: TYPE_NORMAL
- en: As in a clock, the hours are plotted in a circle, each one separate and distinct,
    yet in full cyclical continuity.
  prefs: []
  type: TYPE_NORMAL
- en: Processing categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Strings usually represent categorical data in tabular data. Each unique value
    in a categorical feature represents a quality that refers to the example we are
    examining (hence, we consider this information to be **qualitative** whereas numerical
    information is **quantitative**). In statistical terms, each unique value is called
    a **level** and the categorical feature is called a **factor**. Sometimes you
    can find numeric codes used as categorical (identifiers), when the qualitative
    information has been previously encoded into numbers, but the way to deal with
    them doesn''t change: the information is in numeric values but it should be treated
    as categorical.'
  prefs: []
  type: TYPE_NORMAL
- en: Since you don't know how each unique value in a categorical feature is related
    to every other value present in the feature (if you jump ahead and group values
    together or order them you are basically expressing a hypothesis you have about
    the data), you can treat each of them as a value in itself. Hence, you can derive
    the idea of creating a binary feature from each unique categorical value. This
    process is called one-hot encoding and it is the most common data processing approach
    that can make categorical data usable by DNNs and other machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you have a categorical variable containing the unique values
    of red, blue, and green, you can turn it into three distinct binary variables,
    each one representing uniquely a single value, as represented in the following
    schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach presents a problem for DNNs, though. When your categorical variable
    has too many levels (conventionally more than 255), the resulting binary derived
    features are not only too numerous, making your dataset huge, but also carry little
    information since most of the numerical values will be just zeros (we call this
    situation **sparse data**). Sparse data is somewhat problematic for a DNN because
    backpropagation doesn't work optimally when there are too many zeros in the data
    since the lack of information can stop the signal from making a meaningful difference
    as it's sent back through the network.
  prefs: []
  type: TYPE_NORMAL
- en: We therefore distinguish between low-cardinality and high-cardinality categorical
    variables, on the basis of their number of unique values and process (by one-hot
    encoding) only those categorical variables that we consider to have low cardinality
    (conventionally if there are less than 255 unique values, but you can choose a
    lower threshold, such as 64, 32, or even 24).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the scikit-learn function for one-hot encoding and we prepare a simple
    example dataset containing categorical data both in string and numerical form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now we can proceed to the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We prepare a class that can turn numbers to strings, so, after using it, every
    numerical categorical feature will be processed in the same way as the strings.
    We then prepare our recipe, which is a scikit-learn pipeline that combines our
    string converter and one-hot encoding together (we won't forget to automatically
    deal with any missing values by converting them into unique values).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Though the code snippet is short, it indeed achieves quite a lot. Let's understand
    how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the other methods we''ve seen, we just fit and transform our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Since the returned array will be sparse (a special format for datasets where
    zero values prevail), we can convert it back to our usual NumPy array format using
    the `.todense` method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encoding, by converting every categorical unique value into a variable
    of its own, produces many new features. In order to label them we have to inspect
    the scikit-learn one-hot encoding instance we used and extract the labels from
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, in our example, now we can figure out what each new feature represents
    by calling the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The results provide us indication about both the original feature and the unique
    value represented by the binary variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the results provide an indication of both the original feature
    and the unique value represented by the binary variable.
  prefs: []
  type: TYPE_NORMAL
- en: Processing ordinal data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ordinal data (for instance, rankings or star values in a review) is certainly
    more similar to numerical data than it is to categorical data, yet we have to
    first consider certain differences before dealing with it plainly as a number.
    The problem with categorical data is that you can process it as numerical data,
    but probably the distance between one point and the following one in the scale
    is different than the distance between the following one and the next (technically
    the steps could be different). This is because ordinal data doesn't represent
    quantities, but just ordering. On the other hand, we also treat it as categorical
    data, because categories are independent and we will lose the information implied
    in the ordering. The solution for ordinal data is simply to treat it as both a
    numerical and a categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to import the `OrdinalEncoder` function from scikit-learn, which
    will help us in numerically recoding ordinal values, even when they are textual
    (such as the ordinal scale bad, neutral, and good):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then prepare our example using two features containing ordinal information
    recorded as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Again, the example is just a toy dataset, but it should allow us to test the
    functionalities demonstrated by this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we can prepare two pipelines. The first pipeline will be working
    on the ordinal data by turning it into ordered numeric (this transformation will
    preserve the ordering of the original feature). The second transformation one-hot
    encodes the ordinal data (a transformation that will preserve the step information
    between ordinal grades, but not their ordering). As with the date transformation
    in the recipe *Processing dates*, earlier in this chapter, just two pieces of
    information derived from your original data will be enough for you to process
    ordinal data in a DNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As this recipe is mainly composed of a scikit-learn pipeline, it should be quite
    familiar to you. Let's delve into it to understand more of its workings.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All you have to do is to operate the transformations separately and then stack
    the resulting vectors together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result from our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Columns can be easily derived using the `derive_ohe_columns` function that
    we have seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the list containing the transformed column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: By combining the variables covering the numerical part and the unique values
    of an ordinal variable, we should now be able to utilize all the real information
    from our data.
  prefs: []
  type: TYPE_NORMAL
- en: Processing high-cardinality categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When processing high-cardinality categorical features, we can use the previously
    mentioned one-hot encoding strategy. However, we may encounter problems because
    the resulting matrix is too sparse (many zero values), thus preventing our DNN
    from converging to a good solution, or making the dataset unfeasible to handle
    (because sparse matrices made dense can occupy a large amount of memory).
  prefs: []
  type: TYPE_NORMAL
- en: The best solution instead is to pass them to our DNN as numerically labeled
    features and let a Keras embedding layer take care of them ([https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)).
    An embedding layers is just a matrix of weights that can convert the high-cardinality
    categorical input into a lower-dimensionality numerical output. It is basically
    a weighted linear combination whose weights are optimized to convert categories
    into numbers that can best help the prediction process.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, the embedding layer converts your categorical data into one-hot-encoded
    vectors that become the input of a small neural network. The purpose of this small
    neural network is just to mix and combine the inputs together into a smaller output
    layer. The one-hot encoding performed by the layer works only on numerically labeled
    categories (no strings), so it is paramount to transform our high-cardinality
    categorical data in the correct way.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn package provides the `LabelEncoder` function as a possible
    solution, but this method presents some problems, because it cannot handle previously
    unseen categories, nor can it properly work in a fit/transform regime. Our recipe
    has to wrap it up and make it suitable for producing the correct input and information
    for a Keras embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will need to redefine the `LabelEncoder` function from scikit-learn
    and make it suitable for a fit/transform process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we need to simulate a high-cardinality categorical variable, we will
    use random unique values (made of letters and digits) created by a simple script.
    That will allow us to test a larger number of examples, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output of our random example generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: The first column contains a two-letter code, the second uses three letters,
    and the last one four letters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will prepare another scikit-learn class. It extends the
    existing `LabelEncoder` function because it automatically handles missing values.
    It keeps records of the mapping between the original categorical values and their
    resulting numeric equivalents and at transformation time, it can handle previously
    unseen categories, labeling them as unknown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Like the other classes we've seen so far, `LEncoder` has a fitting method that
    stores information for future uses and a transform method that applies transformations
    based on the information previously stored after fitting it to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After instancing our label encoder, we simply fit and transform our example,
    turning each categorical feature into a sequence of numeric labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: After all the coding to complete the recipe, the execution of this class is
    indeed simple and straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order for the Keras embeddings layers to work properly, we need to specify
    the input size of our high-cardinality categorical variable. By accessing the
    `le.dictionary_size` in our examples, we had `412`, `497`, and `502` distinct
    values in our example variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In our examples, we had `412`, `497`, and `502` distinct values, respectively,
    in our example variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This number includes the **missing** and **unknown** labels, even if there were
    no missing or unknown elements in the examples we fitted.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up all the processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have completed the recipes relating to processing different kinds
    of tabular data, in this recipe we will be wrapping everything together in a class
    that can easily handle all the fit/transform operations with a pandas DataFrame
    as input and explicit specifications of what columns to process and how.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we will combine multiple transformations, we will take advantage of the
    `FeatureUnion` function from scikit-learn, a function that can concatenate them
    together easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As a testing dataset, we will then simply combine all our previously used test
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As for as our toy dataset, we just combine all the datasets we have used up
    to now.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The wrapper class of this recipe has been split into parts, in order to help
    you to inspect and study the code better. The first part comprises the initialization,
    which effectively incorporates all the recipes we have seen so far in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: After having recorded all the key parameters of the wrappers, we proceed to
    examine all the individual parts of it. Please don't forget that all these code
    snippets are part of the same `__init__` method and that we are simply re-using
    the recipes we have seen previously, therefore for any details of these code snippets,
    just refer to the previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we record the numeric pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we record the pipeline processing time-related features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it is the turn of ordinal variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We close with the categorical pipelines, both the low-and high-categorical
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part regards the fitting. Depending on the different variable types
    available, the appropriate fit process will be applied and the newly processed
    or generated columns will be recorded in the `.columns` index list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `transform` method provides all the transformations and matrix joining
    in order to return a list of arrays containing, as their first element, the numerical
    parts of the processed data, followed by the numerical label vectors representing
    the high-cardinality categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set the `fit_transform` method, which sequentially executes the
    fit and transform operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have finished wrapping everything together, we can take a look at
    how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our test, we assign the list of column names to variables depending on their
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'After having instantiated the `TabularTransformer`, and mapped the variables
    we need to be processed to their type, we proceed to fit and transform our example
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a list of NumPy arrays. We can iterate through them and print
    their shape in order to check how the output is composed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The printed result reports a larger array as its first element (the combined
    result of all processes except the high-cardinality categorical one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Our DNN can now expect a list as input, where the first element is a numerical
    matrix and the following elements are vectors to be sent to categorical embeddings
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to be able to retrace each column and vector name, the `TabularTransformer`
    has a `columns` method, `tt.columns`, that can be invoked. The `TabularTransformer`
    can also call `tt.vocabulary` for information about the dimensionality of the
    categorical variables, which is necessary in order to correctly set the input
    shape of the embeddings layers in the network. The returned result is a dictionary
    in which the column name is the key and the dictionary size is the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have these two methods for tracking down variable names (`tt.columns`)
    and defining the vocabulary of high-cardinality variables (`tt.vocabulary`), we
    are just a step away from a complete deep leaning framework for deep learning
    processing of tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a data generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are just missing one key ingredient before we try our framework out on a
    difficult test task. The previous recipe presented a `TabularTransformer` that
    can effectively turn a pandas DataFrame into numerical arrays that a DNN can process.
    Yet, the recipe can only deal with all the data at once. The next step is to provide
    a way to create batches of the data of different sizes. This could be accomplished
    using `tf.data` or a Keras generator and, since previously in the book we have
    already explored quite a few examples with `tf.data`, this time we will prepare
    the code for a Keras generator that's capable of generating random batches on
    the fly when our DNN is learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our generator will inherit from the `Sequence` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `Sequence` class is the base object for fitting a sequence of data and it
    requires you to implement custom `__getitem__` (which will return a complete batch)
    and `__len__` (which will report how many batches are necessary to complete an
    epoch) methods.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now script a new class called `DataGenerator` that inherits from the Keras
    `Sequence` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The generator is now set up. Let's proceed to the next section and explore how
    it works in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from the `__init__` method, which instantiates the internal variables
    of the class, the `DataGenerator` class consists of these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_build_index`: This creates an index of the provided data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`on_epoch_end`: At the end of each epoch, this method will randomly shuffle
    the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__len__`: This reports how many batches are required to complete an epoch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__iter__`: This renders the class an iterable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__next__`: This calls the next batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__call__`: This returns the `__iter__` method call'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__data_generation`: Where the `TabularTransformer` operates on data batches,
    returning the transformed output (returning it as a list of arrays or as a dictionary
    of arrays)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__`: This splits the data into batches and calls the `__data_generation`
    method for the transformations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes the final piece of the puzzle. Using the last two recipes you
    can fully transform and deliver to a TensorFlow model any mixed variable tabular
    dataset to a TensorFlow model, just by filling in a few parameters. In the next
    two recipes we will provide you with some specific tricks to make our DNN work
    better with tabular data, and we'll look at a fully fledged example from a famous
    Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom activations for tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With images and text, it is more difficult to backpropagate errors in DNNs working
    on tabular data because the data is sparse. While the ReLU activation function
    is used widely, new activation functions have been found to work better in such
    cases and can improve the network performances. These activations functions are
    SeLU, GeLU, and Mish. Since SeLU is already present in Keras and TensorFlow (see
    [https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu)
    and [https://www.tensorflow.org/api_docs/python/tf/nn/selu](https://www.tensorflow.org/api_docs/python/tf/nn/selu)),
    in this recipe we'll use the GeLU and Mish activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You need the usual imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We've added `matplotlib`, so we can plot how these new activation functions
    work and get an idea of the reason for their efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GeLU and Mish are defined by their mathematics, which you can find in their
    original papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gaussian Error Linear Units (GELUs)*: [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mish, A Self Regularized* *Non-Monotonic Neural Activation Function*: [https://arxiv.org/abs/1908.08681](https://arxiv.org/abs/1908.08681)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the formulas translated into code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The interesting part of the recipe is that `get_custom_objects` is a function
    that allows you to record your new functions in custom TensorFlow objects and
    then easily recall them as strings in layer parameters. You can find more information
    about how custom objects work in Keras by having a look at the TensorFlow documentation:
    [https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can get an idea of how these two activation functions work by plotting positive
    and negative inputs against their outputs. A few commands from matplotlib will
    help us with the visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, you should get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: GeLU and Mish activation functions mapped from inputs to outputs'
  prefs: []
  type: TYPE_NORMAL
- en: As with the ReLU activation function, inputs from zero onward are just identically
    mapped as output (preserving linearity in the positive activations). The interesting
    thing happens when the input is below zero, actually, because it is not suppressed
    as happens with ReLU. In both the GeLU and Mish activation functions, the output
    is a dampened transformation of the negative input that recedes to zero when the
    input is very negative. This prevents both the case of dying neurons, because
    negative inputs can still pass information, and the case of saturated neurons,
    because overly negative values are turned off.
  prefs: []
  type: TYPE_NORMAL
- en: With different strategies, negative input is therefore processed and propagated
    both by the GeLU and Mish activations functions. This allows a defined gradient
    from negative inputs, which doesn't cause harm to the network.
  prefs: []
  type: TYPE_NORMAL
- en: Running a test on a difficult problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the chapter, we have provided recipes to handle tabular data in a
    successful way. Each recipe is not actually a solution in itself, but a piece
    of a puzzle. When the pieces are combined you can get excellent results and in
    this last recipe, we will demonstrate how to assemble all the recipes together
    to successfully complete a difficult Kaggle challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The Kaggle competition, *Amazon.com – Employee Access Challenge* ([https://www.kaggle.com/c/amazon-employee-access-challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)),
    is a competition that's notable for the high-cardinality variables involved and
    is a solid benchmark that's used to compare gradient boosting algorithms. The
    aim of the competition is to develop a model that can predict whether an Amazon
    employee should be given access to a specific resource based on their role and
    activities. The answer should be given as likelihood. As predictors, you have
    different ID codes corresponding to the type of resource you are evaluating access
    to, the role of the employee in the organization, and the referring manager.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we start by importing TensorFlow and Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Using sequential-based data generators may trigger some errors in TensorFlow
    2.2\. This is due to eager execution and, as a precaution, we have to disable
    it for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get hold of the Amazon dataset, the best and fastest way is to
    install **CatBoost**, a gradient boosting algorithm that uses the dataset as a
    benchmark. If it is not already present in your installed environment, you easily
    install it using the `pip install catboost` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Since the test data (uploaded into the `Xt` variable) has an unlabeled target
    variable, we will be using just the training data in the `X` variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a first step, we will define the DNN architecture for this problem. Since
    the problem involves only categorical variables with high cardinality, we start
    setting an input and an embedding layer for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define an input for each feature, where the data flows into the network,
    and then each input is directed into its respective embedding layer. The size
    of the input is based on the number of unique values of the feature, and the size
    of the output is based on the logarithm of the input size. The output of each
    embedding is then passed to a spatial dropout (since the embedding layer will
    return a matrix, the spatial dropout will blank out entire columns of the matrix)
    and then flattened. Finally, all the flattened results are concatenated into a
    single layer. From there on, the data has to pass through two dense layers with
    dropout before reaching the output response node, a sigmoid activated node that
    will return a probability as an answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The architecture works only with categorical data. It takes each categorical
    input (expecting a single integer code) and fits it into an embedding layer, whose
    output is a reduced dimensionality vector (whose dimensions are computed using
    the heuristic `int(np.log1p(category_counts)+1)`). It applies a `SpatialDropout1D`
    and finally it flattens the output. `SpatialDropout1D` removes all the connections
    in a row of the output matrix from all channels, thus effectively dropping some
    information from the embedding. All the outputs of all the categorical variables
    are then concatenated and passed on to a series of dense layers with GeLU activations
    and dropout. It all ends with a single sigmoid node (so you can get the emission
    of a probability in the range [0,1]).
  prefs: []
  type: TYPE_NORMAL
- en: After defining the architecture, we define the score functions, taking them
    from scikit-learn and converting them for use in Keras using the `tf.py_function`
    from TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/py_function](https://www.tensorflow.org/api_docs/python/tf/py_function)),
    a wrapper that can turn any function into a once-differentiable TensorFlow operation
    that can be executed eagerly.
  prefs: []
  type: TYPE_NORMAL
- en: As score functions, we use the average precision and the ROC AUC. Both of these
    can help us figure out how we are performing on a binary classification by telling
    us how closely the predicted probabilities resemble the true values. More on ROC
    AUC and average precision can be found in the scikit-learn documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)
    and [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also instantiate a simple plotting function that can plot selected error
    and score measures as recorded during the training both on the training and validation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you need to set up the training phase. Given the limited number
    of examples and your need to test your solution, using cross-validation is the
    best choice. The `StratifiedKFold` function from scikit-learn will provide you
    with the right tool for the job.
  prefs: []
  type: TYPE_NORMAL
- en: In `StratifiedKFold`, your data is randomly (you can provide a seed value for
    reproducibility) split into *k* parts, each one with the same proportion of the
    target variable as is found in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: These *k* splits are used to generate *k* training tests that can help you infer
    the performance of the DNN architecture you have set up. In fact, *k* times over,
    *all but one* of the splits are used to train your model and the one kept apart
    is left out for testing each time. This ensures that you have *k* tests made on
    splits that have not been used for training.
  prefs: []
  type: TYPE_NORMAL
- en: This approach, especially when dealing with only a few training examples, is
    preferable to picking up a single test set to verify your models on, because by
    sampling a test set you could find a sample that is differently distributed from
    your train set. Moreover, by using a single test set, you also risk overfitting
    your test set. If you repeatedly test different solutions, eventually you may
    find a solution that fits the test set very well but is not a generalizable solution
    in itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put it into practice here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The script runs a training and validation test for each fold and stores the
    results that will help you correctly evaluate the performances of your DNN for
    tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each fold will print a plot detailing how the DNN performed, both on log-loss
    and ROC AUC, for the training and the validation sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: DNN performance on the training set and the validation set'
  prefs: []
  type: TYPE_NORMAL
- en: All the folds have a similar trajectory, with a significant decoupling of the
    train and validation curves after 5 epochs and a widening gap after 15 epochs,
    implying a certain overfitting during the training phase. By modifying your DNN
    architecture, and changing parameters such as the learning rate or the optimization
    algorithm, you can safely experiment to try to achieve better results because
    the cross-validation procedure ensures that you are making the right decisions.
  prefs: []
  type: TYPE_NORMAL
