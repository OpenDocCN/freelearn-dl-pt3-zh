- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer models changed the playing field for most machine learning problems
    that involve sequential data. They have advanced the state of the art by a significant
    margin compared to the previous leaders, RNN-based models. One of the primary
    reasons that the Transformer model is so performant is that it has access to the
    whole sequence of items (e.g. sequence of tokens), as opposed to RNN-based models,
    which look at one item at a time. The term Transformer has come up several times
    in our conversations as a method that has outperformed other sequential models
    such as LSTMs and GRUs. Now, we will learn more about Transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first learn about the Transformer model in detail.
    Then we will discuss the details of a specific model from the Transformer family
    known as **Bidirectional Encoder Representations from Transformers** (**BERT**).
    We will see how we can use this model to complete a question-answering task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Using BERT to answer questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Transformer is a type of Seq2Seq model (discussed in the previous chapter).
    Transformer models can work with both image and text data. The Transformer model
    takes in a sequence of inputs and maps that to a sequence of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Transformer model was initially proposed in the paper *Attention is all
    you need* by Vaswani et al. ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    Just like a Seq2Seq model, the Transformer consists of an encoder and a decoder
    (*Figure 10.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intuition behind NMT](img/B14070_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: The encoder-decoder architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand how the Transformer model works using the previously studied
    Machine Translation task. The encoder takes in a sequence of source language tokens
    and produces a sequence of interim outputs. Then the decoder takes in a sequence
    of target language tokens and predicts the next token for each time step (the
    teacher forcing technique). Both the encoder and the decoder use attention mechanisms
    to improve performance. For example, the decoder uses attention to inspect all
    the past encoder states and previous decoder inputs. The attention mechanism is
    conceptually similar to Bahdanau attention, which we discussed in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder and the decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s discuss in detail what the encoder and the decoder consist of. They
    have more or less the same architecture with a few differences. Both the encoder
    and the decoder are designed to consume a sequence of input items at a time. But
    their goals during the task differ; the encoder produces a latent representation
    with the inputs, whereas the decoder produces a target output with the inputs
    and the encoder’s outputs. To perform these computations, these inputs are propagated
    through several stacked layers. Each layer within these models takes in a sequence
    of elements and outputs another sequence of elements. Each layer is also made
    from several sub-layers that encapsulate different computations performed on a
    sequence of input tokens to produce a sequence of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A layer found in the Transformer mainly comprises the following two sub-layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A self-attention layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The self-attention layer produces its output using matrix multiplications and
    activation functions (this is similar to a fully connected layer, which we will
    discuss in a minute). The self-attention layer takes in a sequence of inputs and
    produces a sequence of outputs. However, a special characteristic of the self-attention
    layer is that, when producing an output at each time step, it has access to all
    the other inputs in that sequence [(](Chapter_10.xhtml)**Figure 10.2*). This makes
    learning and remembering long sequences of inputs trivial for this layer. For
    comparison, RNNs struggle to remember long sequences of inputs as they need to
    go through each input sequentially. Additionally, by design, the self-attention
    layer can select and combine different inputs at each time step based on the task
    it’s solving. This makes Transformers very powerful in sequential learning tasks.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s discuss why it’s important to selectively combine different input elements
    this way. In an NLP context, the self-attention layer enables the model to peek
    at other words while processing a certain word. This means that while the encoder
    is processing the word *it* in the sentence *I kicked the ball and it disappeared*,
    the model can attend to the word *ball*. By doing this, the Transformer can learn
    dependencies and disambiguate words, which leads to better language understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even understand how self-attention helps us to solve a task conveniently
    through a real-world example. Assume you are playing a game with two other people:
    person A and person B. Person A holds a question written on a board, and you need
    to answer that question. Say person A reveals one word of the question at a time,
    and after the last word of the question is revealed, you answer it. For long and
    complex questions, this would be challenging as you cannot physically see the
    complete question and would have to heavily rely on memory. This is what it feels
    like to perform computations without self-attention for a Transformer. On the
    other hand, say person B reveals the full question on the board in one go instead
    of word by word. Now it is much easier to answer the question as you can see the
    complete question at once. If the question is a complex question requiring a complex
    answer, you can look at different parts of the question as you are providing various
    sections of the full answer. This is what the self-attention layer enables.'
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention layer is followed by a fully connected layer. A fully connected
    layer has all the input nodes connected to all the output nodes, optionally followed
    by a non-linear activation function. It takes the output elements produced by
    the self-attention sub-layer and produces a hidden representation for each output
    element. Unlike the self-attention layer, the fully connected layer treats individual
    sequence items independently, performing computations on them in an element-wise
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'They introduce non-linear transformations while making the model deeper, thus
    allowing the model to perform better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: The difference between the self-attention sub-layer and the fully
    connected sub-layer. The self-attention sub-layer looks at all the inputs in the
    sequence, whereas the fully-connected sub-layer only looks at the input that is
    processed.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basic building blocks of a Transformer layer, let’s
    look at the encoder and the decoder separately. Before diving in, let’s establish
    some basics. The encoder takes in an input sequence and the decoder takes in an
    input sequence as well (a different sequence to the encoder input). Then the decoder
    produces an output sequence. Let’s call a single item in these sequences a *token*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder consists of a stack of layers, where each layer consists of two
    sub-layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A self-attention layer – Generates a latent representation for each encoder
    input token in the sequence. For each input token, this layer looks at the whole
    sequence and selects other tokens in the sequence that enrich the semantics of
    the generated hidden output for that token (that is, ‘attended’ representation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully-connected layer – Generates an element-wise deeper hidden representation
    of the attended representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The decoder layer consists of three sub-layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A masked self-attention layer – For each decoder input, a token looks at all
    the tokens to the left of it. The decoder needs to mask words to the right to
    prevent the model from seeing words in the future. Having access to successive
    words during prediction can make the prediction task trivial for the decoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An attention layer – For each input token in the decoder, it looks at both the
    encoder’s outputs and the decoder’s masked attended output to generate a semantically
    rich hidden output. Since this layer is not only focused on decoder inputs, we’ll
    call this an attention layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully-connected layer – Generates an element-wise hidden representation of
    the attended representation of the decoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is shown in *Figure 10.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: How a Transformer model is used to translate an English sentence
    to French. The diagram shows various layers in the encoder and the decoder, and
    various connections formed within the encoder, within the decoder, and between
    the encoder and the decoder. The squares represent the inputs and outputs of the
    models. The rectangular shaded boxes represent interim outputs of the sub-layers.
    The <sos> token represents the beginning of the decoder’s input.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s learn about the computational mechanics of the self-attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the output of the self-attention layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is no doubt that the self-attention layer is at the center of the Transformer.
    The computations that govern the self-attention mechanism can be difficult to
    understand. Therefore, this section is dedicated to understanding the self-attention
    technique in detail. There are three key concepts to understand: query, key, and
    value. The query and the key are used to generate an affinity matrix. For the
    decoder’s attention layer, the affinity matrix’s position *i,j* represents how
    similar the encoder state (key) *i* is to the decoder input *j* (query). Then,
    we create a weighted average of encoder states (value) for each position, where
    the weights are given by the affinity matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reinforce our understanding, let’s imagine a scenario where the decoder
    is generating a self-attention output. Say we have an English to French machine
    translation task. Take the example sentence *Dogs are great*, which becomes *Les
    chiens sont super* in French. Say we are at time step 2, trying to produce the
    word *chiens*. Let’s represent each word with a single floating point number (such
    as a simplified embedding representation of words):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dogs -> 0.8*'
  prefs: []
  type: TYPE_NORMAL
- en: '*are -> 0.3*'
  prefs: []
  type: TYPE_NORMAL
- en: '*great -> -0.2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*chiens -> 0.5*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s compute the affinity matrix (to be specific, the affinity vector
    since we are only considering a single decoder input). The query would be 0.5
    and the key (i.e. the encoder state sequence) would be `[0.8, 0.3, -0.2]`. If
    we take the dot product, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0.4, 0.15, -0.1]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand what this affinity matrix is saying. With respect to the word
    *chiens*, the word *Dogs* has the highest similarity, and the word *are* also
    has a positive similarity (since *chiens* is plural, carrying a reference to the
    word *are* in English). However, the word *great* has a negative similarity to
    the word *chiens*. Then, we can compute the final attended output for that time
    step as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0.4 * 0.8, 0.15 * 0.3, -0.1 * -0.2] = [0.32 + 0.45 + 0.02] = 0.385`'
  prefs: []
  type: TYPE_NORMAL
- en: We have ended up with a final output somewhere in the middle of matching words
    from the English language, where the word *great* has the highest distance. This
    example was presented to show how the query, key, and value come into play to
    compute the final attended output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the actual computation that transpires in the layer. To compute
    the query, key, and value, we use a linear projection of the actual inputs provided
    using weight matrices. The three weight matrices are:'
  prefs: []
  type: TYPE_NORMAL
- en: Query weights matrix (![](img/B14070_10_001.png))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key weights matrix (![](img/B14070_10_002.png))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value weights matrix (![](img/B14070_10_003.png))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these weight matrices produces three outputs for a given token (at
    position ![](img/B14070_10_004.png)) in a given input sequence by multiplying
    with the weight matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_005.png), ![](img/B14070_10_006.png), and ![](img/B14070_10_007.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Q*, *K*, and *V* are *[B, T, d]* sized tensors, where *B* is the batch size,
    *T* is the number of time steps, and *d* is a hyperparameter that defines the
    dimensionality of the latent representation. These are then used to compute the
    affinity matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: The computations in the self-attention layer. The self-attention
    layer starts with an input sequence and computes sequences of query, key, and
    value vectors. Then the queries and keys are converted to a probability matrix,
    which is used to compute a weighted sum of values'
  prefs: []
  type: TYPE_NORMAL
- en: 'The affinity matrix *P* is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the final attended output of the self-attention layer is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_009.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Q* represents the queries tensor, *K* represents the keys tensor, and
    *V* represents the values tensor. This is what makes Transformer models so powerful;
    unlike LSTM models, Transformer models aggregate all tokens in a sequence to a
    single matrix multiplication, making these models highly parallelizable. *Figure
    10.4* also depicts the computations that take place within the self-attention
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layers in the Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word embeddings provide a semantic-preserving representation of words based
    on the context in which words are used. In other words, if two words are used
    in the same context, they will have similar word vectors. For example, the words
    *cat* and *dog* will have similar representations, whereas *cat* and *volcano*
    will have vastly different representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word vectors were initially introduced in the paper titled *Efficient Estimation
    of Word Representations in Vector Space* by Mikolov et al. ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)).
    It came in two variants: skip-gram and continuous bag-of-words. Embeddings work
    by first defining a large matrix of size *V* x *E*, where *V* is the size of the
    vocabulary and *E* is the size of the embeddings. *E* is a user-defined hyperparameter;
    a larger *E* typically leads to more powerful word embeddings. In practice, you
    do not need to increase the size of embeddings beyond 300.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by the original word vector algorithms, modern deep learning models
    use embedding layers to represent words/tokens. The following general approach
    (along with pre-training later to fine-tune these embeddings) is taken to incorporate
    word embeddings into a machine learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a randomly initialized word embedding matrix (or pre-trained embeddings,
    available to download for free)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the model (randomly initialized) that uses word embeddings as the inputs
    and produces an output (for example, sentiment, or a language translation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the whole model (embeddings and the model) end-to-end on the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same technique is used in Transformer models. However, in Transformer models,
    there are two different embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: Token embeddings (provide a unique representation for each token seen by the
    model in an input sequence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional embeddings (provide a unique representation for each position in
    the input sequence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The token embeddings have a unique embedding vector for each token (such as
    character, word, and sub-word), depending on the model’s tokenizing mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The positional embeddings are used to signal the model where a token is appearing.
    The primary purpose of the positional embeddings server is to inform the Transformer
    model where a word is appearing. This is because, unlike LSTMs/GRUs, Transformer
    models don’t have a notion of sequence, as it processes the whole text in one
    go. Furthermore, a change to the position of a word can alter the meaning of a
    sentence/or a word. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ralph loves his tennis ball.* **It** *likes to chase the ball*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ralph loves his tennis ball. Ralph likes to chase* **it**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the sentences above, the word *it* refers to different things and the position
    of the word *it* can be used as a cue to identify this difference. The original
    Transformer paper uses the following equations to generate positional embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_010.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_10_011.png)'
  prefs: []
  type: TYPE_IMG
- en: where *pos* denotes the position in the sequence and ![](img/B14070_10_012.png)
    denotes the ![](img/B14070_10_013.png) feature dimension (![](img/B14070_10_014.png)).
    Even-numbered features use a sine function and odd numbered features use a cosine
    function. *Figure 10.5* presents how positional embeddings change as the time
    step and the feature position change. It can be seen that feature positions with
    higher indices have lower-frequency sinusoidal waves. It is not entirely clear
    how the authors came up with the exact equation.
  prefs: []
  type: TYPE_NORMAL
- en: However, they do mention that they did not see a significant performance difference
    between the above equation and letting the model learn positional embeddings jointly
    during the training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: How positional embeddings change with the time step and the feature
    position. Even-numbered feature positions use the sine function and odd-numbered
    positions use the cosine function. Additionally, the frequency of the signals
    decreases as the feature position increases'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that both token and positional embeddings will have
    the same dimensionality ![](img/B14070_10_015.png), making it possible to perform
    element-wise addition. Finally, as the input to the model, the token embeddings
    and the positional embeddings are summed to form a single hybrid embedding vector
    (*Figure 10.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: The embeddings generated in a Transformer model and how the final
    embeddings are computed'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now discuss two optimization techniques used in each layer of the Transformer:
    residual connections and layer normalizations.'
  prefs: []
  type: TYPE_NORMAL
- en: Residuals and normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important characteristic of the Transformer models is the existence
    of the residual connections and the normalization layers in between the individual
    layers of the Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Residual connections are formed by adding a given layer’s output to the output
    of one or more layers ahead. This in turn forms shortcut connections through the
    model and provides a stronger gradient flow by reducing the changes of the phenomenon
    known as vanishing gradients (*Figure 10.7*). The vanishing gradients problem
    causes the gradients in the layers closest to the inputs to be very small so that
    the training in those layers is hindered. The residual connections for deep learning
    models were popularized by the paper “*Deep Residual Learning for Image Recognition*”
    by Kaiming He et.al. ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: How residual connections work'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Transformer models, in each layer, residual connections are created the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Input to the self-attention sub-layer is added to the output of the self-attention
    sub-layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input to the fully-connected sub-layer is added to the output of the fully-connected
    sub-layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the output reinforced by residual connections goes through a layer normalization
    layer. Layer normalization, similar to batch normalization, is a way to reduce
    the covariate shift in neural networks, allowing them to be trained faster and
    achieve better performance. Covariate shift refers to changes in the distribution
    of neural network activations (caused by changes in the data distribution), which
    transpires as the model goes through model training. These changes in the distribution
    damage consistency during model training and negatively impact the model. It was
    introduced in the paper *Layer Normalization* by Ba et al. ([https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization computes the mean and variance of activations as an average
    over the samples in the batch, causing its performance to rely on mini-batches
    used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: However, layer normalization computes the mean and variance (that is, the normalization
    terms) of the activations in such a way that the normalization terms are the same
    for every hidden unit. In other words, layer normalization has a single mean and
    a variance value for all the hidden units in a layer. This is in contrast to batch
    normalization, which maintains individual mean and variance values for each hidden
    unit in a layer. Moreover, unlike batch normalization, layer normalization does
    not average over the samples in the batch; instead, it leaves the averaging out
    and has different normalization terms for different inputs. By having a mean and
    variance per-sample, layer normalization gets rid of the dependency on the mini-batch
    size. For more details about this method, please refer to the original paper by
    Ba et al.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow provides a convenient implementation of the layer normalization algorithm
    at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization).
    You can simply use this layer with any model you define using TensorFlow Keras
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.8* depicts how residual connections and layer normalization are
    used in Transformer models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: How residual connections and layer normalization layers are used
    in the transformer model'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we end our discussion on the components of the Transformer model.
    We have discussed all the bells and whistles of the Transformer model. The Transformer
    model is an encoder-decoder based model. Both the encoder and the decoder have
    the same structure, apart from a few small differences. The Transformer uses self-attention,
    a powerful parallelizable attention mechanism to attend to other inputs at every
    time step. The Transformer also uses several embedding layers, such as token embeddings
    and positional embeddings, to inject information about tokens and their positioning.
    The Transformer also uses residual connections and layer normalization to improve
    the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss a specific Transformer model known as BERT, which we’ll
    be using to solve a question-answering problem.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BERT** (**Bidirectional Encoder Representation from Transformers**) is a
    Transformer model among a plethora of Transformer models that have come to light
    over the past few years.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT was introduced in the paper *BERT: Pre-training of Deep Bidirectional
    Transformers for Language Understanding* by Delvin et al. ([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)).
    The Transformer models are divided into two main factions:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-based models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder-based (autoregressive) models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, either the encoder or the decoder part of the Transformer provides
    the foundation for these models, compared to using both the encoder and the decoder.
    The main difference between the two is how attention is used. Encoder-based models
    use bidirectional attention, whereas decoder-based models use autoregressive (that
    is, left to right) attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT is an encoder-based Transformer model. It takes an input sequence (a collection
    of tokens) and produces an encoded output sequence. *Figure 10.9* depicts the
    high-level architecture of BERT :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: The high-level architecture of BERT. It takes a set of input tokens
    and produces a sequence of hidden representations generated using several hidden
    layers'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s discuss a few details pertinent to BERT, such as inputs consumed by
    BERT and the tasks it is designed to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Input processing for BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When BERT takes an input, it inserts some special tokens into the input. First,
    at the beginning, it inserts a `[CLS]` (an abbreviated form of the term classification)
    token that is used to generate the final hidden representation for certain types
    of tasks (such as sequence classification). It represents the output after attending
    to all the tokens in the sequence. Next, it also inserts a `[SEP]` (meaning ‘separation’)
    token depending on the type of input. The `[SEP]` token marks the end and beginning
    of different sequences in the input. For example, in question-answering, the model
    takes a question and a context (such as a paragraph) that may have the answer
    as an input, and `[SEP]` is used in between the question and the context. Additionally,
    we have the `[PAD]` token, which can be used to pad short sequences to a required
    length.
  prefs: []
  type: TYPE_NORMAL
- en: The `[CLS]` token is appended to any input sequence fed to BERT. This denotes
    the beginning of the input. It also forms the basis for the input fed into the
    classification head used on top of BERT to solve your NLP task. As you know, BERT
    produces a hidden representation for each input token in the sequence. As a convention,
    the hidden representation corresponding to the `[CLS]` token is used as the input
    to the classification model that sits on top of BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the final embedding of the tokens is generated using three different embedding
    spaces. The token embedding has a unique vector for each token in the vocabulary.
    The positional embeddings encode the position of each token, as discussed earlier.
    Finally, the segment embedding provides a distinct representation for each sub-component
    in the input, when the input consists of multiple components. For example, in
    question-answering, the question will have a unique vector as its segment embedding
    vector and the context will have a different embedding vector. This is done by
    having ![](img/B14070_10_016.png) embedding vectors for the ![](img/B14070_10_017.png)
    different components in the input sequence. Depending on the component index specified
    for each token in the input, the corresponding segment embedding vector is retrieved.
    ![](img/B14070_10_018.png) needs to be specified in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks solved by BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The task-specific NLP tasks solved by BERT can be classified into four different
    categories. These are motivated by the tasks found in the **General Language Understanding
    Evaluation** (**GLUE**) benchmark task suite ([https://gluebenchmark.com](https://gluebenchmark.com)):'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence classification – Here, a single input sequence is given and the model
    is asked to predict a label for the whole sequence (for example, sentiment analysis
    or spam identification).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token classification – Here, a single input sequence is given and the model
    is asked to predict a label for each token in the sequence (for example, named
    entity recognition or part-of-speech tagging).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question-answering – Here, the input consists of two sequences: a question
    and a context. The question and the context are separated by a `[SEP]` token.
    The model is trained to predict the starting and ending indices of the span of
    tokens belonging to the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple choice – Here, the input consists of multiple sequences; a question
    followed by multiple candidates that may or may not be the answer to the question.
    These multiple sequences are separated by the token `[SEP]` and provided as a
    single input sequence to the model. The model is trained to predict the correct
    answer (that is, the class label) for that question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 10.10* depicts how BERT is used to solve these different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: How BERT is used for different NLP tasks'
  prefs: []
  type: TYPE_NORMAL
- en: BERT is designed in such a way that it can be used to complete these tasks without
    any modifications to the base model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In tasks that involve multiple sequences (such as multiple-choice questions),
    you need the model to tell different inputs belonging to different segments apart
    (that is, which tokens are the question and which tokens are the context in a
    question-answering task). In order to make that distinction, the `[SEP]` token
    is used. A `[SEP]` token is inserted between the different sequences. For example,
    if you are solving a question-answering problem, you might have the following
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Question: What color is the ball?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Paragraph: Tippy is a dog. She loves to play with her red ball.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the input to BERT might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[CLS]` *What color is the ball* `[SEP]` *Tippy is a dog She loves to play
    with her red ball* `[SEP]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed all the elements of BERT so we can use it successfully
    to solve a downstream NLP task, let’s reiterate the key points about BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT is an encoder-based Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT outputs a hidden representation for every token in the input sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BERT has three embedding spaces: token embedding, positional embedding, and
    segment embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT uses a special token `[CLS]` to denote the beginning of an input and is
    used as the input to a downstream classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BERT is designed to solve four types of NLP tasks: sequence classification,
    token classification, free-text question-answering, and multiple-choice question-answering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT uses the special token `[SEP]` to separate between sequence A and sequence
    B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The power within BERT doesn’t just lie within its structure. BERT is pre-trained
    on a large corpus of text using a few different pre-training techniques. In other
    words, BERT already comes with a solid understanding of the language, making downstream
    NLP tasks easier to solve. Next, let’s discuss how BERT is pre-trained.
  prefs: []
  type: TYPE_NORMAL
- en: How BERT is pre-trained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The real value of BERT comes from the fact that it has been pre-trained on
    a large corpus of data in a self-supervised fashion. In the pre-training stage,
    BERT is trained on two different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked language modeling** (sometimes abbreviated as **MLM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next sentence prediction** (sometimes abbreviated as **NSP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now discuss the details of the above two tasks and how they provide language
    understanding for BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Masked Language Modeling (MLM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The MLM task is inspired by the Cloze task, or the Cloze test, where a student
    is given a sentence with one or more blanks and is asked to fill the blanks. Similarly,
    given a text corpus, words are masked from sentences and then the model is asked
    to predict the masked tokens. For example, the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I went to the bakery to buy bread*'
  prefs: []
  type: TYPE_NORMAL
- en: 'might become:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I went to the* `[MASK]` *to buy bread*'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT uses a special token, `[MASK]`, to represent masked words. Then the target
    for the model will be the word *bakery*. But this introduces a practical issue
    to the model. The special `[MASK]` token does not appear in the actual text. This
    means that the text the model will see during the finetuning phase (that is, when
    training on a classification problem) will be different to what it will see during
    pre-training. This is sometimes referred to as the **pre-training-finetuning discrepancy**.
    Therefore, the authors of BERT suggest the following approach to cope with the
    issue. When masking a word, do one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `[MASK]` token as it is (with 80% probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a random word (with 10% probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the true word (with 10% probability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, instead of always seeing `[MASK]`, the model will see actual
    words on certain occasions, alleviating the discrepancy.
  prefs: []
  type: TYPE_NORMAL
- en: Next Sentence Prediction (NSP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the NSP task, the model is given a pair of sentences, A and B (in that order),
    and is asked to predict whether the B is the next sentence after A. This can be
    done by fitting a binary classifier onto BERT and training the whole model from
    end to end on selected pairs of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating pairs of sentences as inputs for the model is not hard and can be
    done in an unsupervised manner:'
  prefs: []
  type: TYPE_NORMAL
- en: A sample with the label TRUE is generated by picking two sentences that are
    adjacent to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample with the label FALSE is generated by picking two sentences randomly
    that are not adjacent to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following this approach, a labeled dataset is generated for the next sentence
    prediction task. Then BERT, along with the binary classifier, is trained from
    end to end using the labeled dataset to solve a downstream task. To see this in
    action, we’ll be using Hugging Face’s `transformers` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use case: Using BERT to answer questions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s learn how to implement BERT, train it on a question-answer dataset,
    and ask the model to answer a given question.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Hugging Face transformers library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `transformers` library built by Hugging Face. The `transformers`
    library is a high-level API that is built on top of TensorFlow, PyTorch, and JAX.
    It provides easy access to pre-trained Transformer models that can be downloaded
    and fine-tuned with ease. You can find models in the Hugging Face’s model registry
    at [https://huggingface.co/models](https://huggingface.co/models). You can filter
    models by task, examine the underlying deep learning frameworks, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `transformers` library was designed with the aim of providing a very low
    barrier for entry to using complex Transformer models. For this reason, there’s
    only a handful of concepts that you need to learn in order to hit the ground running
    with the library. Three important classes are required to load and use a model
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: Model class (such as `TFBertModel`) – Contains the trained weights of the model
    in the form of `tf.keras.models.Model` or the PyTorch equivalent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration (such as `BertConfig`) – Stores various parameters and hyperparameters
    needed to load the model. If you’re using the pre-trained model as is, you don’t
    need to explicitly define its configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizer (such as `BertTokenizerFast`) – Contains the vocabulary and token-to-ID
    mapping needed to tokenize the words for the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these classes can be used with two straightforward functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from_pretrained()` – Provides a way to instantiate a model/configuration/tokenizer
    available from the model repository or locally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_pretrained()` – Provides a way to save the model/configuration/tokenizer
    so that it can be reloaded later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow hosts a variety of Transformer models (released by both TensorFlow
    and third parties) in TensorFlow Hub (at [https://tfhub.dev/](https://tfhub.dev/)).
    If you would like to know how to use TensorFlow Hub and the raw TensorFlow API
    to implement a model such as BERT, please visit [https://www.tensorflow.org/text/tutorials/classify_text_with_bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).
  prefs: []
  type: TYPE_NORMAL
- en: We will soon see how these classes and functions are used in an actual use case.
    It is also important to note the side-effects of having such an easy-to-grasp
    interface for using models. Due to serving the very specific purpose of providing
    a way to use Transformer models built with TensorFlow, PyTorch, or Jax, you don’t
    have the modularity or flexibility found in TensorFlow, for example. In other
    words, you cannot use the `transformers` library in the same way you would use
    TensorFlow to build a `tf.keras.models.Model` using `tf.keras.layers.Layer` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset we are going to use for this task is a popular question-answering
    dataset called SQUAD. Each datapoint consists of four items:'
  prefs: []
  type: TYPE_NORMAL
- en: A question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A context that may contain the answer to the question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The start index of the answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can download the dataset using Hugging Face’s `datasets` library and call
    the `load_dataset()` function with the `"squad"` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s print some examples using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'which will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `answer_start` indicates the character index at which this answer starts
    in the context provided. With a good understanding of what’s available in the
    dataset, we’ll perform a simple processing step. When training the model, we will
    be asking the model to predict the start and end indices of the answer. In its
    original form, only the `answer_start` is present. We will need to manually add
    `answer_end` to our dataset. The following function does this. Furthermore, it
    does a few sanitary checks on the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will download a pre-trained BERT model from the Hugging Face repository
    and learn about the model in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use a pre-trained Transformer model from the Hugging Face repository, we
    need three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tokenizer` – Responsible for splitting a long bit of text (such as a sentence)
    into smaller tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` – Contains the configuration of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Model` – Takes in the tokens, looks up the embeddings, and produces the final
    output(s) using the provided inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can ignore the `config` as we are using the pre-trained model as is. However,
    to paint a full picture, we will use the configuration nevertheless.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and using the Tokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will look at how to download the Tokenizer. You can download the
    Tokenizer using the `transformers` library. Simply call the `from_pretrained()`
    function provided by the `PreTrainedTokenizerFast` base class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using a Tokenizer called `bert-base-uncased`. It is the Tokenizer
    developed for the BERT base model and is uncased (that is, there’s no distinction
    between uppercase and lowercase characters). Next, let’s see the Tokenizer in
    action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s understand the arguments we’ve provided to the tokenizer’s call:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text` – A single or batch of text sequences to be encoded by the tokenizer.
    Each text sequence is a string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` – An optional single or batch of text sequences to be encoded by
    the tokenizer. It’s useful in situations where the model takes a multi-part input
    (such as a question and a context in question-answering).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` – Indicates the padding strategy. If set to `True`, it will be padded
    to the maximum sequence length in the dataset. If set to `max_length`, it will
    be padded to the length specified by the `max_length` argument. If set to `False`,
    no padding will be done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` – An argument that defines the type of tensors returned. It
    could be either `pt` (PyTorch) or `tf` (TensorFlow). Since we want TensorFlow
    tensors, we define it as `''tf''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs a `transformers.tokenization_utils_base.BatchEncoding` object,
    which is essentially a dictionary. It has three keys and tensors as values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` – Provides the IDs of the tokens for the tokens found in the text
    sequences. Additionally, it introduces the `[CLS]` token ID at the beginning of
    the sequence and two instances of the `[SEP]` token ID, one between the question
    and context, and the other one at the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` – This is the segment ID we use for the segment embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` – The attention mask represents the words that are allowed
    to be attended to during the forward pass. Since BERT is an encoder model, any
    token can pay attention to any other token. The only exception is the padded tokens
    that will be ignored during the attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We could also convert these token IDs to actual tokens to know what they represent.
    To do that, we use the `convert_ids_to_tokens()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see how the tokenizer inserts special tokens like `[CLS]` and `[SEP]`
    into the text sequence. With the functionality of the tokenizer understood, let’s
    use it to encode the train and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the size of the train encodings by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'which will give:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum sequence length in our dataset is 512\. Therefore, we see that
    the maximum length of the sequences is 512\. Once we tokenize our data, we need
    to perform one more data processing step. Our `answer_start` and `answer_end`
    indices are character-based. However, since we are working with tokens, we need
    to convert our character-based indices to token-based indices. We will define
    a function for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes in a set of `BatchEncodings` called `encodings` generated
    by the tokenizer and a set of answers (a list of dictionaries). Then it updates
    the provided encodings with two new keys: `start_positions` and `end_positions`.
    These keys respectively hold the token-based indices denoting the start and end
    of the answer. If the answer is not found, we set the start and end indices to
    the last token. To convert our existing character-based indices to token-based
    indices, we use a function called `char_to_token()` provided by the `BatchEncodings`
    class. It takes a character index as the input and provides the corresponding
    token index as the output. With the function defined, let’s call it on our training
    and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With the clean data, we will now define a TensorFlow dataset. Note that this
    function modifies the encodings in place.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a TensorFlow dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, let’s implement a TensorFlow dataset to generate the data for the model.
    Our data will consist of two tuples: one containing inputs and the other containing
    the targets. The input tuple contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Input token IDs – A batch of padded token IDs of size `[batch size, sequence
    length]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention mask – A batch of attention masks of size `[batch size, sequence length]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output tuple contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Start index of the answer – A batch of start indices of the answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End index of the answer – A batch of end indices of the answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will first define a generator that generates the data in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Since we have already processed the data, it’s a matter of reorganizing the
    already existing data to return using the code above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will define a partial function that we can simply call without passing
    any arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is then passed to the `tf.data.Dataset.from_generator()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We then shuffle the data in our training dataset. When shuffling a TensorFlow
    dataset we need to provide a buffer size. The buffer size defines how many samples
    are chosen to shuffle. Here we set that to 1,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we split our dataset into two: a training set and a validation dataset.
    We will use the first 10,000 samples as the validation set. The rest of the data
    is used as the training set. Both datasets will be batched using a batch size
    of 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we follow the same procedure to create the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s see how BERT’s architecture can be used to answer questions.
  prefs: []
  type: TYPE_NORMAL
- en: BERT for answering questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a few modifications introduced on top of the pre-trained BERT model
    to leverage it for question-answering. First, the model takes in a question, followed
    by a context. As we discussed before, the context may or may not contain the answer
    to the question. The input has the format `[CLS] <question tokens> [SEP] <context
    tokens> [SEP]`. Then, for each token position of the context, we have two classification
    heads predicting a probability. One head predicts the probability of each context
    token being the start of the answer, whereas the other one predicts the probability
    of each context token being the end of the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Once we figure out the start and end indices of the answer, we can simply extract
    the answer from the context using those indices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Using BERT for question-answering. The model takes in a question
    followed by a context. The model has two heads: one to predict the probability
    of each token in the context being the start of the answer and another to predict
    the end of the answer for each context token.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the config and the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Hugging Face, you have several variants of each Transformer model. These
    variants are based on different tasks solved by these models. For example, for
    BERT we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TFBertForPretraining` – The pre-trained model without a task-specific head'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TFBertForSequenceClassification` – Used for classifying a sequence of text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TFBertForTokenClassification` – Used for classifying each token in the sequence
    of text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TFBertForMultipleChoice` – Used for answering multiple-choice questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TFBertForQuestionAnswering` – Used for extracting answers to a question from
    a given context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TFBertForMaskedLM` – Used for pre-training BERT on the masked language modeling
    task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TFBertForNextSentencePrediction` – Used for pre-training BERT to predict the
    next sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we are interested in `TFBertForQuestionAnswering`. Let’s import this
    class along with the `BertConfig` class, which we will extract important hyperparameters
    from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the pre-trained `config`, we call the `from_pretrained()` function of
    `BertConfig` with the model we’re interested in. Here, we’ll use the `bert-base-uncased`
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can print the `config` and see what’s in there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we get the model by calling the same function `from_pretrained()`
    from the `TFBertForQuestionAnswering` class and pass the `config` we just obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this, you will get a warning saying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is expected and totally fine. It’s saying that there are some layers that
    have not been initialized from the pre-trained model; the output heads of the
    model need to be introduced as new layers, thus they are not pre-initialized.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we will define a function that will wrap the returned model as
    a `tf.keras.models.Model` object. We need to perform this step because if we try
    to use the model as it is, TensorFlow returns the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we will define two input layers: one takes in the input token IDs
    and the other takes the attention mask and passes it to the model. Finally, we
    get the output of the model. We then define a `tf.keras.models.Model` using these
    inputs and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As we learned when studying the structure of the model, the question-answering
    BERT has two heads: one to predict the starting index of the answer and the other
    to predict the end. Therefore, we have to optimize two losses coming from the
    two heads. This means we need to add the two losses to get the final loss. When
    we have a multi-output model such as this, we can pass multiple loss functions
    aimed at each output head. Here, we define a single loss function. This means
    the same loss will be used across both heads and will be summed to generate the
    final loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We will now see how we can train and evaluate our model on the question-answering
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We already have the data prepared and the model defined. Training the model
    is quite easy, and is just a one-liner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the accuracy on the validation set reaching an accuracy between
    ~73 and 75%. This is quite high, given we only trained the model for two epochs.
    This performance can be attributed to the high level of language understanding
    the pre-trained model already had when we downloaded it. Let’s evaluate the model
    on our test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that it performs comparably well on the test dataset as well. Finally,
    we can save the model. We will save the `TFBertForQuestionAnswering` component
    of the model. We’ll also save the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We have trained our model and evaluated it to ensure the model performs well.
    Once we confirmed that the model is performing well, we finally saved it for future
    use. Next, let’s discuss how we can use this model to generate answers for a given
    question.
  prefs: []
  type: TYPE_NORMAL
- en: Answering questions with Bert
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now write a simple script to generate answers to questions from the trained
    model. First, let’s define a sample question to generate an answer for. We’ll
    also store the inputs and the ground truth answer to compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll define the inputs to the model. The input to the model needs to
    have a batch dimension. Therefore we use the `[i:i+1]` syntax to make sure the
    batch dimension is not flattened:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now define a simple function called `ask_bert` to find an answer from
    the context for a given question. This function takes in an input, a tokenizer,
    and a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then it generates the token IDs from the tokenizer, passes them to the model,
    outputs the start and end indices for the answer, and finally extracts the corresponding
    answer from the text of the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s execute the following lines to print the answer given by our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'which will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We can see that BERT has answered the question correctly. We have learned a
    lot about Transformers in general as well as BERT-specific architecture. We then
    used this knowledge to adapt BERT to solve a question-answering problem. Here
    we end our discussion about Transformers and BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about Transformer models. First, we looked at the
    Transformer at a microscopic level to understand the inner workings of the model.
    We saw that Transformers use self-attention, a powerful technique to attend to
    other inputs in the text sequences while processing one input. We also saw that
    Transformers use positional embeddings to inform the model about the relative
    position of tokens in addition to token embeddings. We also discussed that Transformers
    leverage residual connections (that is, shortcut connections) and layer normalization
    in order to improve model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then discussed BERT, an encoder-based Transformer model. We looked at the
    format of the data accepted by BERT and the special tokens it uses in the input.
    Next, we discussed four different types of task BERT can solve: sequence classification,
    token classification, multiple-choice, and question-answering.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how BERT is pre-trained on a large corpus of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we started on a use case: answering questions with BERT. To implement
    the solution, we used the `transformers` library by Hugging Face. It’s an extremely
    useful high-level library built on top of deep learning frameworks such as TensorFlow,
    PyTorch, and Jax. The `transformers` library is specifically designed for quickly
    loading and using pre-trained Transformer models. In this use case, we first processed
    the data and created a `tf.data.Dataset` to stream the data in batches. Then we
    trained the model on that data and evaluated it on a test set. Finally, we used
    the model to infer answers to a sample question given to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will learn a bit more about Transformers and how they
    can be used in a more complicated task that involves both images and text: image
    caption generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)*'
  prefs: []
  type: TYPE_NORMAL
