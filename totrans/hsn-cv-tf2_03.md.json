["```\nimport tensorflow as tf\n\nnum_classes = 10\nimg_rows, img_cols = 28, 28\nnum_channels = 1\ninput_shape = (img_rows, img_cols, num_channels)\n\n(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n```", "```\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n```", "```\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type) Output Shape Param # \n=================================================================\nflatten_1 (Flatten) (None, 784) 0 \n_________________________________________________________________\ndense_1 (Dense) (None, 128) 100480 \n_________________________________________________________________\ndense_2 (Dense) (None, 10) 1290 \n=================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\n```", "```\nmodel.compile(optimizer='sgd',\n loss='sparse_categorical_crossentropy',\n metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5, verbose=1, validation_data=(x_test, y_test))\n```", "```\nimport tensorflow as tf\n\na = tf.constant([1, 2, 3])\nb = tf.constant([0, 0, 1])\nc = tf.add(a, b)\n\nprint(c)\n```", "```\nTensor(\"Add:0\", shape=(3,), dtype=int32)\n```", "```\ntf.Tensor([1 2 4], shape=(3,), dtype=int32)\n```", "```\ndef compute(a, b, c):\n    d = a * b + c\n    e = a * b * c\n    return d, e\n```", "```\n@tf.function\ndef compute(a, b, c):\n    d = a * b + c\n    e = a * b * c\n    return d, e\n```", "```\nA, B = tf.constant(3.0), tf.constant(6.0)\nX = tf.Variable(20.0) # In practice, we would start with a random value\nloss = tf.math.abs(A * X - B)\n```", "```\n<tf.Tensor: id=18525, shape=(), dtype=float32, numpy=54.0>\n```", "```\ndef train_step():\n    with tf.GradientTape() as tape:\n        loss = tf.math.abs(A * X - B)\n    dX = tape.gradient(loss, X)\n\n    print('X = {:.2f}, dX = {:2f}'.format(X.numpy(), dX))\n    X.assign(X - dX)\n\nfor i in range(7):\n    train_step()\n```", "```\n X = 20.00, dX = 3.000000\n X = 17.00, dX = 3.000000\n X = 14.00, dX = 3.000000\n X = 11.00, dX = 3.000000\n X = 8.00, dX = 3.000000\n X = 5.00, dX = 3.000000\n X = 2.00, dX = 0.000000\n```", "```\nmodel_input = tf.keras.layers.Input(shape=input_shape)\noutput = tf.keras.layers.Flatten()(model_input)\noutput = tf.keras.layers.Dense(128, activation='relu')(output)\noutput = tf.keras.layers.Dense(num_classes, activation='softmax')(output)\nmodel = tf.keras.Model(model_input, output)\n```", "```\n@tf.function\ndef identity(x):\n  print('Creating graph !')\n  return x\n```", "```\nx1 = tf.random.uniform((10, 10))\nx2 = tf.random.uniform((10, 10))\n\nresult1 = identity(x1) # Prints 'Creating graph !'\nresult2 = identity(x2) # Nothing is printed\n```", "```\nx3 = tf.random.uniform((10, 10), dtype=tf.float16)\nresult3 = identity(x3) # Prints 'Creating graph !'\n```", "```\n@tf.function\ndef identity(x):\n  tf.print(\"Running identity\")\n  return x\n```", "```\nprint([variable.name for variable in model.variables])\n# Prints ['sequential/dense/kernel:0', 'sequential/dense/bias:0', 'sequential/dense_1/kernel:0', 'sequential/dense_1/bias:0']\n```", "```\na = tf.Variable(3, name='my_var')\nprint(a) # Prints <tf.Variable 'my_var:0' shape=() dtype=int32, numpy=3>\n```", "```\na.assign(a + 1)\nprint(a.numpy()) # Prints 4\n```", "```\nb = a + 1\nprint(b) # Prints <tf.Tensor: id=21231, shape=(), dtype=int32, numpy=4>\n```", "```\nmirrored_strategy = tf.distribute.MirroredStrategy()\nwith mirrored_strategy.scope():\n  model = make_model() # create your model here\n  model.compile([...])\n```", "```\nestimator = tf.keras.estimator.model_to_estimator(model, model_dir='./estimator_dir')\n```", "```\nBATCH_SIZE = 32\ndef train_input_fn():\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    train_dataset = train_dataset.batch(BATCH_SIZE).repeat()\n    return train_dataset\n```", "```\nestimator.train(train_input_fn, steps=len(x_train)//BATCH_SIZE)\n```", "```\ncallbacks = [tf.keras.callbacks.TensorBoard('./logs_keras')]\nmodel.fit(x_train, y_train, epochs=5, verbose=1, validation_data=(x_test, y_test), callbacks=callbacks)\n```", "```\n$ tensorboard --logdir ./logs_keras\n```", "```\nwriter = tf.summary.create_file_writer('./model_logs')\nwith writer.as_default():\n  tf.summary.scalar('custom_log', 10, step=3)\n```", "```\naccuracy = tf.keras.metrics.Accuracy()\nground_truth, predictions = [1, 0, 1], [1, 0, 0] # in practice this would come from the model\naccuracy.update_state(ground_truth, predictions)\ntf.summary.scalar('accuracy', accuracy.result(), step=4)\n```"]