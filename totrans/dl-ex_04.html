<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Get Up and Running with TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we are going to give an overview of one of the most widely used deep learning frameworks. TensorFlow has big community support that is growing day by day, which makes it a good option for building your complex deep learning applications. From the TensorFlow website:</p>
<div class="packtquote"><q class="calibre52">"TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well."</q></div>
<p class="calibre2">The following topics are going to  be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">TensorFlow installation</li>
<li class="calibre8">The TensorFlow environment</li>
<li class="calibre8">Computational graphs</li>
<li class="calibre8">TensorFlow data types, variables, and placeholders</li>
<li class="calibre8">Getting output from TensorFlow</li>
<li class="calibre8">TensorBoard—visualizing learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow installation</h1>
                </header>
            
            <article>
                
<p class="calibre2">TensorFlow installation comes with two modes: CPU and GPU. We will start off the installation tutorial by installing TensorFlow in GPU mode.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow GPU installation for Ubuntu 16.04</h1>
                </header>
            
            <article>
                
<p class="calibre2">The GPU mode installation of TensorFlow requires an up-to-date installation of the NVIDIA drivers because the GPU version of TensorFlow only supports CUDA at the moment. The following section will take you through a step-by-step process of installing <span class="calibre10">NVIDIA</span> drivers and CUDA 8.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing NVIDIA drivers and CUDA 8</h1>
                </header>
            
            <article>
                
<p class="calibre2">First off, you need to install the correct <span class="calibre10">NVIDIA</span> driver based on your GPU. I have a GeForce GTX 960M GPU, so I will go ahead and install <kbd class="calibre12">nvidia-375</kbd> (if you have a different GPU, you can use the NVIDIA search tool <a href="http://www.nvidia.com/Download/index.aspx" class="calibre11">http://www.nvidia.com/Download/index.aspx</a> to help you find your correct driver version). If you want to know your machine's GPU, you can issue the following command in the terminal:</p>
<pre class="calibre21">lspci | grep -i nvidia</pre>
<p class="innercell">You should get the following output in the terminal:</p>
<div class="CDPAlignCenter"><img src="assets/b73594fd-a0b9-48d2-a972-db1fc651aa25.png" class="calibre53"/></div>
<p class="calibre2">Next, we need to add a proprietary repository of NVIDIA drivers to be able to install the drivers using <kbd class="calibre12">apt-get</kbd>:</p>
<div class="title-page-name">
<pre class="calibre21">sudo add-apt-repository ppa:graphics-drivers/ppa<br class="title-page-name"/>sudo apt-get update<br class="title-page-name"/>sudo apt-get install nvidia-375</pre></div>
<div class="title-page-name">
<p class="calibre2">After successfully installing the <span class="calibre10">NVIDIA</span> drivers, restart the machine. To verify whether the drivers installed correctly, issue the following command in the Terminal:</p>
<pre class="calibre21">cat /proc/driver/nvidia/version</pre></div>
<p class="calibre2">You should get the following output in the Terminal:</p>
<div class="CDPAlignCenter"><img src="assets/d82ab7c0-53ee-4fb3-a5e7-e6a98d33fcc6.png" class="calibre54"/></div>
<p class="calibre2">Next, we need to install CUDA 8. Open the following CUDA download link: <a href="https://developer.nvidia.com/cuda-downloads" class="calibre11">https://developer.nvidia.com/cuda-downloads</a>. Select your operating system, architecture, distribution, version, and finally, installer type as per the following screenshot:</p>
<div class="CDPAlignCenter"><img src="assets/8e23b031-65b4-4c1c-9df7-a036dd2af7ac.png" class="calibre55"/></div>
<p class="calibre2">The installer file is about 2 GB. You need to issue the following installation instructions:</p>
<div class="title-page-name">
<pre class="calibre21">sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb<br class="title-page-name"/>sudo apt-get update<br class="title-page-name"/>sudo apt-get install cuda</pre></div>
<div class="title-page-name">
<p class="calibre2">Next, we need to add the libraries to the <kbd class="calibre12">.bashrc</kbd> file by issuing the following commands:</p>
</div>
<pre class="calibre21">echo 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc</pre>
<pre class="calibre21">echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc</pre>
<pre class="calibre21">source ~/.bashrc</pre>
<p class="calibre2">Next, you need to verify the installation of CUDA 8 by issuing the following command:</p>
<pre class="calibre21">nvcc -V</pre>
<p class="calibre2">You should get the following output in the terminal:</p>
<div class="CDPAlignCenter"><img src="assets/430d007d-a24c-4921-b941-b0e96bc1945d.png" class="calibre56"/></div>
<p class="calibre2">Finally, in this section, we need to install cuDNN 6.0. The <strong class="calibre13">NVIDIA CUDA Deep Neural Network library</strong> (<strong class="calibre13">cuDNN</strong>) is a GPU-accelerated library of primitives for deep neural networks. You can download it from NVIDIA's web page. Issue the following commands to extract and install cuDNN:</p>
<div class="title-page-name">
<pre class="calibre21">cd ~/Downloads/</pre>
<pre class="calibre21">tar xvf cudnn*.tgz</pre>
<pre class="calibre21">cd cuda</pre>
<pre class="calibre21">sudo cp */*.h /usr/local/cuda/include/</pre>
<pre class="calibre21">sudo cp */libcudnn* /usr/local/cuda/lib64/</pre>
<pre class="calibre21">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</pre></div>
<p class="calibre2">To ensure that your installation has been successful, you can use the <kbd class="calibre12">nvidia-smi</kbd> tool in the terminal. If you had a successful installation, this tool will provide you with a monitoring information such as RAM and the running process for your GPU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">After preparing the GPU environment for TensorFlow, we are now ready to install TensorFlow in GPU mode. But for going through the TensorFlow installation process, you can first install a few helpful Python packages that will help you in the next chapters and make your development environment easier.</p>
<p class="calibre2">We can start by installing some data manipulation, analysis, and visualization libraries by issuing the following commands:</p>
<pre class="calibre21">sudo apt-get update &amp;&amp; apt-get install -y python-numpy python-scipy python-nose python-h5py python-skimage python-matplotlib python-pandas python-sklearn python-sympy</pre>
<pre class="calibre21">sudo apt-get clean &amp;&amp; sudo apt-get autoremove</pre>
<pre class="calibre21">sudo rm -rf /var/lib/apt/lists/*</pre>
<p class="calibre2">Next, you can install more useful libraries, such as the virtual environment, Jupyter Notebook, and so on:</p>
<pre class="calibre21">sudo apt-get update</pre>
<pre class="calibre21">sudo apt-get install git python-dev python3-dev python-numpy python3-numpy build-essential  python-pip python3-pip python-virtualenv swig python-wheel libcurl3-dev</pre>
<pre class="calibre21">sudo apt-get install -y libfreetype6-dev libpng12-dev</pre>
<pre class="calibre21">pip3 install -U matplotlib ipython[all] jupyter pandas scikit-image</pre>
<p class="calibre2">Finally, we can start to install TensorFlow in GPU mode by issuing the following command:</p>
<pre class="calibre21">pip3 install --upgrade tensorflow-gpu</pre>
<p class="calibre2">You can verify the successful installation of TensorFlow using Python:</p>
<pre class="calibre21">python3<br class="title-page-name"/>&gt;&gt;&gt; import tensorflow as tf<br class="title-page-name"/>&gt;&gt;&gt; a = tf.constant(5)<br class="title-page-name"/>&gt;&gt;&gt; b = tf.constant(6)<br class="title-page-name"/>&gt;&gt;&gt; sess = tf.Session()<br class="title-page-name"/>&gt;&gt;&gt; sess.run(a+b)<br class="title-page-name"/>// this should print bunch of messages showing device status etc. // If everything goes well, you should see gpu listed in device<br class="title-page-name"/>&gt;&gt;&gt; sess.close()</pre>
<p class="calibre2">You should get the following output in the terminal:</p>
<div class="CDPAlignCenter"><img src="assets/0ec98d26-4d2a-4c15-840d-1064375bad66.png" class="calibre31"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow CPU installation for Ubuntu 16.04</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to install the CPU version, which doesn't require any drivers prior to installation. So, let's start off by installing some useful packages for data manipulation and visualization:</p>
<pre class="calibre21">sudo apt-get update &amp;&amp; apt-get install -y python-numpy python-scipy python-nose python-h5py python-skimage python-matplotlib python-pandas python-sklearn python-sympy</pre>
<pre class="calibre21">sudo apt-get clean &amp;&amp; sudo apt-get autoremove</pre>
<pre class="calibre21">sudo rm -rf /var/lib/apt/lists/*</pre>
<p class="calibre2">Next, you can install more useful libraries, such as the virtual environment, Jupyter Notebook, and so on:</p>
<pre class="calibre21">sudo apt-get update</pre>
<pre class="calibre21">sudo apt-get install git python-dev python3-dev python-numpy python3-numpy build-essential  python-pip python3-pip python-virtualenv swig python-wheel libcurl3-dev</pre>
<pre class="calibre21">sudo apt-get install -y libfreetype6-dev libpng12-dev</pre>
<pre class="calibre21">pip3 install -U matplotlib ipython[all] jupyter pandas scikit-image</pre>
<p class="calibre2">Finally, you can install the latest TensorFlow in CPU mode by issuing the following command:</p>
<pre class="calibre21">pip3 install --upgrade tensorflow</pre>
<p class="calibre2">You can check whether TensorFlow was installed successfully be running the following TensorFlow statements:</p>
<pre class="calibre21">python3<br class="title-page-name"/>&gt;&gt;&gt; import tensorflow as tf<br class="title-page-name"/>&gt;&gt;&gt; a = tf.constant(5)<br class="title-page-name"/>&gt;&gt;&gt; b = tf.constant(6)<br class="title-page-name"/>&gt;&gt;&gt; sess = tf.Session()<br class="title-page-name"/>&gt;&gt;&gt; sess.run(a+b)<br class="title-page-name"/>&gt;&gt; sess.close()</pre>
<p class="calibre2">You should get the following output in the terminal:</p>
<div class="CDPAlignCenter"><img src="assets/f1741db2-73e4-4a95-8ffe-94aba75c3c4d.png" class="calibre31"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow CPU installation for macOS X</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to install TensorFlow for macOS X using <kbd class="calibre12">virtualenv</kbd>. So, let's start off by installing the <kbd class="calibre12">pip</kbd> tool by issuing the following command:</p>
<pre class="calibre21">sudo easy_install pip</pre>
<p class="calibre2">Next, we need to install the virtual environment library:</p>
<pre class="calibre21">sudo pip install --upgrade virtualenv</pre>
<p class="calibre2">After installing the virtual environment library, we need to create a container or virtual environment which will host the installation of TensorFlow and any packages that you might want to install without affecting the underlying host system:</p>
<pre class="calibre21">virtualenv --system-site-packages targetDirectory # for Python 2.7</pre>
<pre class="calibre21">virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.n</pre>
<p class="calibre2">This assumes that the <kbd class="calibre12">targetDirectory</kbd> is <kbd class="calibre12">~/tensorflow</kbd>.</p>
<p class="calibre2">Now that you have created the virtual environment, you can access it by issuing the following command:</p>
<pre class="calibre21">source ~/tensorflow/bin/activate </pre>
<p class="calibre2">Once you issue this command, you'll get access to the virtual machine that you have just created and you can install any packages that will be only installed in this environment and won't affect the underlying or host system that you're using.</p>
<p class="calibre2">In order to exit from the environment, you can issue the following command:</p>
<pre class="calibre21">deactivate</pre>
<p class="calibre2">Note that, for now, we do want to be inside the virtual environment, so turn it back on for now. Once you're done playing with TensorFlow, you should deactivate it:</p>
<pre class="calibre21">source bin/activate</pre>
<p class="calibre2">In order to install the CPU version of TensorFlow, you can issue the following commands, which will also install any dependent libraries that TensorFlow requires:</p>
<pre class="calibre21">(tensorflow)$ pip install --upgrade tensorflow      # for Python 2.7</pre>
<pre class="calibre21">(tensorflow)$ pip3 install --upgrade tensorflow     # for Python 3.n</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow GPU/CPU installation for Windows</h1>
                </header>
            
            <article>
                
<p class="calibre2">We will assume that you have Python 3 already installed on your system. To install TensorFlow, start a terminal as an administrator as follows. Open up the <span class="calibre10">Start</span> menu, search for <span class="calibre10">cmd,</span> and then right-click on it and click <span class="calibre10">Run as an administrator</span>:</p>
<div class="CDPAlignCenter"><img src="assets/da53c90a-8a64-464d-a807-0fa362e3cfd2.png" class="calibre57"/></div>
<p class="calibre2">Once you have a command window opened, you can issue the following command to install TensorFlow in GPU mode:</p>
<div class="packtinfobox">You need to have <kbd class="calibre12">pip</kbd> or <kbd class="calibre12">pip3</kbd> (depending on your Python version) installed before issuing the next command.</div>
<pre class="calibre21">C:\&gt; pip3 install --upgrade tensorflow-gpu</pre>
<p class="calibre2">Issue the following command to install TensorFlow in CPU mode:</p>
<pre class="calibre21">C:\&gt; pip3 install --upgrade tensorflow</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The TensorFlow environment</h1>
                </header>
            
            <article>
                
<p class="calibre2">TensorFlow is another deep learning framework from Google and, as the name <strong class="calibre13">TensorFlow</strong> implies, it's derived from the operations which neural networks perform on multidimensional data arrays or tensors! It's literally a flow of tensors.</p>
<p class="calibre2">But first off, why are we going to use a deep learning framework in this book?</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">It scales machine learning code</strong>: Most of the research into deep learning and machine learning can be applied/attributed because of these deep learning frameworks. They have allowed data scientists to iterate extremely quickly and have made deep learning and other ML algorithms much more accessible to practitioners. Big companies such as Google, Facebook, and so on are using such deep learning frameworks to scale to billions of users.</li>
<li class="calibre8"><strong class="calibre1">It computes gradients</strong>: <span>Deep learning frameworks can also compute gradients automatically. If you go through gradient calculation step by step, you will find out that gradient calculation is not trivial and it could be tricky to implement a bug-free version of it yourself.</span></li>
<li class="calibre8"><strong class="calibre1">It standardizes machine learning applications for sharing</strong>: <span>Also, pretrained models are available online, which can be used across different deep learning frameworks, and these pretrained models help people who have limited resources in terms of GPU so that they don't have to start from scratch every time. We can stand on the shoulders of giants and it take it from there.</span></li>
<li class="calibre8"><strong class="calibre1">There are lots of deep learning frameworks available</strong> with different advantages, paradigms, levels of abstraction, programming languages, and so on.</li>
<li class="calibre8"><strong class="calibre1">Interface with GPUs for parallel processing</strong>: <span>Using GPUs for computations is a fascinating feature, because GPUs speed up your code a lot faster than the CPU because of number of cores and parallelization.</span></li>
</ul>
<p class="calibre2">That's why Tensorflow is almost necessary in order to make progress in deep learning, because it can facilitate your projects.</p>
<p class="calibre2">So, briefly, what is TensorFlow?</p>
<ul class="calibre7">
<li class="calibre8">TensorFlow is a deep learning framework from Google which is open source for numerical computations using data flow graphs</li>
<li class="calibre8">It was originally developed by the Google Brain Team to facilitate their machine learning research</li>
<li class="calibre8">TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms</li>
</ul>
<p class="calibre2">How does TensorFlow work and what is the underlying paradigm?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computational graphs</h1>
                </header>
            
            <article>
                
<p class="calibre2">The biggest idea of all of the big ideas about TensorFlow is that the numeric computations are expressed as a computation graph, as shown in the following figure. So, the backbone of any TensorFlow program is going to be a computational graph, where the following is true:</p>
<ul class="calibre7">
<li class="calibre8">Graph nodes are operations which have any number of inputs and outputs</li>
<li class="calibre8">Graph edges between our nodes are going to be tensors that flow between these operations, and the best way of thinking about what tensors are in practice is as <em class="calibre25">n</em>-dimensional arrays</li>
</ul>
<p class="calibre2">The advantage of using such flow graphs as the backbone of your deep learning framework is that it allows you to build complex models in terms of small and simple operations. Also, this is going to make the gradient calculations extremely simple when we address that in a later section:</p>
<div class="CDPAlignCenter"><img src="assets/e996731b-b5d1-4fd3-9409-937bffdea297.png" class="calibre58"/></div>
<div class="title-page-name">
<p class="calibre2">Another way of thinking about a TensorFlow graph is that each operation is a function that can be evaluated at that point.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow data types, variables, and placeholders</h1>
                </header>
            
            <article>
                
<p class="calibre2">The understanding of computational graphs will help us to think of complex models in terms of small subgraphs and operations.</p>
<p class="calibre2">Let's look at an example of a neural network with only one hidden layer and what its computation graph might look like in TensorFlow:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation7" src="assets/e7423ec6-bcff-4926-aa39-1dfaeef0ea5f.png"/></div>
<p class="calibre2">So, we have some hidden layer that we are trying to compute, as the ReLU activation of some parameter matrix <em class="calibre19">W</em> time some input <em class="calibre19">x</em> plus a bias term <em class="calibre19">b</em>. The ReLU function takes the max of your output and zero.</p>
<p class="calibre2">The following diagram shows what the graph might look like in TensorFlow:</p>
<div class="CDPAlignCenter"><img src="assets/efac83f1-6997-4383-a90b-64d40eb1aabf.png" class="calibre59"/></div>
<div class="title-page-name">
<p class="calibre2">In this graph, we have variables for our <em class="calibre19">b</em> and <em class="calibre19">W</em> and we have something called a placeholder for <em class="calibre19">x</em>; we also have nodes for each of the operations in our graph. So, let's get into more detail about those node types.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variables</h1>
                </header>
            
            <article>
                
<p class="calibre2">Variables are going to be stateful nodes which output their current value. In this example, it's just <em class="calibre19">b</em> and <em class="calibre19">W</em>. What we mean by saying that variables are stateful is that they retain their current value over multiple executions and it's easy to restore saved values to variables:</p>
<div class="CDPAlignCenter"><img src="assets/40b4497a-5fd5-48c1-8603-d839b777bdfb.png" class="calibre60"/></div>
<div class="title-page-name">
<p class="calibre2">Also, variables have other useful features; for example, they can be saved to your disk during and after training, which facilities the use that we mentioned earlier that it allows people from different companies and groups to save, store, and send over their model parameters to other people. Also, the variables are the things that you want to tune to minimize the loss and we will see how to do that soon.</p>
<p class="calibre2">It's important to know that variables in the graph, such as <em class="calibre19">b</em> and <em class="calibre19">W</em>, are still operations because, by definition, all of your nodes in the graph are operations. So, when you evaluate these operations that are holding the values of <em class="calibre19">b</em> and <em class="calibre19">W</em> during runtime, you will get the value of those variables.</p>
<p class="calibre2">We can use the <kbd class="calibre12">Variable()</kbd> function of TensorFlow to define a variable and give it some initial value:</p>
</div>
<pre class="calibre21"><span>var</span> <span>=</span> <span>tf</span><span>.</span><span>Variable</span><span>(</span><span>tf</span><span>.</span><span>random_normal</span><span>((</span><span>0</span><span>,</span><span>1</span><span>)),</span><span>name</span><span>=</span><span>'random_values'</span><span>)</span></pre>
<p class="calibre2">This line of code will define a variable of 2 by 2 and initialize it from the standard normal distribution. You can also give a name to the variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders</h1>
                </header>
            
            <article>
                
<p class="calibre2">The next type of nodes are placeholders. Placeholders are nodes whose values are fed at execution time:</p>
<div class="CDPAlignCenter"><img src="assets/4580007b-fbef-402c-b9ab-0c549ca2e9be.png" class="calibre61"/></div>
<div class="title-page-name">
<p class="calibre2">If you have inputs into your computational graph that depend on some external data, these are placeholders for values that we are going to add into our computation during training. So, for placeholders, we don't provide any initial values. We just assign a data type and shape of a tensor so the graph still knows what to compute even though it doesn't have any stored values yet.</p>
</div>
<p class="calibre2">We can use the placeholder function of TensorFlow to create a placeholder:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">ph_var1 <span>=</span> <span>tf</span><span>.</span><span>placeholder</span><span>(</span><span>tf</span><span>.</span><span>float32</span><span>,</span><span>shape</span><span>=</span><span>(</span><span>2</span><span>,</span><span>3</span><span>))</span>
<span>ph_var2</span> <span>=</span> <span>tf</span><span>.</span><span>placeholder</span><span>(</span><span>tf</span><span>.</span><span>float32</span><span>,</span><span>shape</span><span>=</span><span>(</span><span>3</span><span>,</span><span>2</span><span>))</span>
<span>result</span> <span>=</span> <span>tf</span><span>.</span><span>matmul</span><span>(</span><span>ph_var1,</span><span>ph_var2)</span></pre></div>
</div>
</div>
<p class="calibre2">These lines of code define two placeholder variables of a certain shape and then define an operation (see the next section) that multiplies these two values together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical operations</h1>
                </header>
            
            <article>
                
<p class="calibre2">The third type of nodes are mathematical operations, and these are going to be our matrix multiplication (MatMul), addition (Add), and ReLU. All of these are nodes in your TensorFlow graph, and it's very similar to NumPy operations:</p>
<div class="CDPAlignCenter"><img src="assets/80bde29f-4adb-45e7-bc6c-1ffc47fbebf0.png" class="calibre62"/></div>
<p class="calibre2">Let's see what this graph will look like in code.</p>
<p class="calibre2">We perform the following steps to produce the preceding graph:</p>
<ol class="calibre16">
<li class="calibre8">Create weights <em class="calibre25">W</em> and <em class="calibre25">b</em>, including initialization. We can initialize the weight matrix <em class="calibre25">W</em> by sampling from uniform distribution <em class="calibre25">W ~ Uniform(-1,1)</em> and initialize <em class="calibre25">b</em> to be 0.</li>
<li class="calibre8">Create input placeholder <em class="calibre25">x</em>, which is going to have a shape of <em class="calibre25">m * 784</em> input matrix.</li>
<li class="calibre8">Build a flow graph.</li>
</ol>
<p class="calibre2">Let's go ahead and follow those steps to build the flow graph:</p>
<pre class="calibre21"># import TensorFlow package<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/># build a TensorFlow variable b taking in initial zeros of size 100<br class="title-page-name"/># ( a vector of 100 values)<br class="title-page-name"/>b  = tf.Variable(tf.zeros((100,)))<br class="title-page-name"/># TensorFlow variable uniformly distributed values between -1 and 1<br class="title-page-name"/># of shape 784 by 100<br class="title-page-name"/>W = tf.Variable(tf.random_uniform((784, 100),-1,1))<br class="title-page-name"/># TensorFlow placeholder for our input data that doesn't take in<br class="title-page-name"/># any initial values, it just takes a data type 32 bit floats as<br class="title-page-name"/># well as its shape<br class="title-page-name"/>x = tf.placeholder(tf.float32, (100, 784))<br class="title-page-name"/># express h as Tensorflow ReLU of the TensorFlow matrix<br class="title-page-name"/>#Multiplication of x and W and we add b<br class="title-page-name"/>h = tf.nn.relu(tf.matmul(x,W) + b )</pre>
<p class="calibre2">As you can see from the preceding code, we are not actually manipulating any data with this code snippet. We are only building symbols inside our graph and you can not print off <em class="calibre19">h</em> and see its value until we run this graph. So, this code snippet is just for building a backbone for our model. If you try to print the value of <em class="calibre19">W</em> or <em class="calibre19">b</em> in the preceding code, you should get the following output in Python:</p>
<div class="CDPAlignCenter"><img src="assets/1a80ee6c-f094-4944-b153-0d6cdcb98b7d.png" class="calibre63"/></div>
<p class="calibre2">So far, we have defined our graph and now, we need to actually run it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting output from TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous section, we knew how to build a computational graph, but we need to actually run it and get its value.</p>
<p class="calibre2">We can deploy/run the graph with something called a session, which is just a binding to a particular execution context such as a CPU or a GPU. So, we are going to take the graph that we build and deploy it to a CPU or a GPU context.</p>
<p class="calibre2">To run the graph, we need to define a session object called <kbd class="calibre12">sess</kbd>, and we are going to call the function <kbd class="calibre12">run</kbd> which takes two arguments:</p>
<pre class="calibre21">sess.run(fetches, feeds)</pre>
<p class="calibre2">Here:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">fetches</kbd> are the list of the graph nodes that return the output of the nodes. These are the nodes we are interested in computing the value of.</li>
<li class="calibre8"><kbd class="calibre12">feeds</kbd> are going to be a dictionary mapping from graph nodes to actual values that we want to run in our model. So, this is where we actually fill in the placeholders that we talked about earlier.</li>
</ul>
<p class="calibre2">So, let's go ahead and run our graph:</p>
<pre class="calibre21"># importing the numpy package for generating random variables for<br class="title-page-name"/># our placeholder x<br class="title-page-name"/>import numpy as np<br class="title-page-name"/># build a TensorFlow session object which takes a default execution<br class="title-page-name"/># environment which will be most likely a CPU<br class="title-page-name"/>sess = tf.Session()<br class="title-page-name"/># calling the run function of the sess object to initialize all the<br class="title-page-name"/># variables.<br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/># calling the run function on the node that we are interested in,<br class="title-page-name"/># the h, and we feed in our second argument which is a dictionary<br class="title-page-name"/># for our placeholder x with the values that we are interested in.<br class="title-page-name"/>sess.run(h, {x: np.random.random((100,784))})   </pre>
<p class="calibre2">After running our graph through the <kbd class="calibre12">sess</kbd> object, we should get an output similar to the following:</p>
<div class="CDPAlignCenter"><img src="assets/7faee772-f55c-41db-b38f-9768eb147d71.png" class="calibre64"/></div>
<p class="calibre2">As you can see, in the second line of the above code snippet, we initialized our variables, and this is a concept in TensorFlow which is called <strong class="calibre13">lazy evaluation</strong>. It means that the evaluation of your graph only ever happens at runtime, and runtime in TensorFlow means the session. So, calling this function, <kbd class="calibre12">global_variables_initializer()</kbd>, will actually initialize anything called variable in your graph, such as <em class="calibre19">W</em> and <em class="calibre19">b</em> in our case.</p>
<p class="calibre2">We can also use the session variable in a with block to ensure that it will be closed after executing the graph:</p>
<pre class="calibre21">ph_var1 = tf.placeholder(tf.float32,shape=(2,3))<br class="title-page-name"/>ph_var2 = tf.placeholder(tf.float32,shape=(3,2))<br class="title-page-name"/>result = tf.matmul(ph_var1,ph_var2)
<span>with</span> <span>tf</span><span>.</span><span>Session</span><span>()</span> <span>as</span> <span>sess</span><span>:</span>
    <span>print(sess.run([result],feed_dict={ph_var1:[[1.,3.,4.],[1.,3.,4.]],ph_var2:[[1., 3.],[3.,1.],[.1,4.]]}))<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>[array([[10.4, 22. ],<br class="title-page-name"/>       [10.4, 22. ]], dtype=float32)]<br class="title-page-name"/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorBoard – visualizing learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">The computations you'll use TensorFlow for—such as training a massive deep neural network—can be complex and confusing, and its corresponding computational graph will be complex as well. To make it easier to understand, debug, and optimize TensorFlow programs, the TensorFlow team have included a suite of visualization tools called TensorBoard, which is a suite of web applications that can run through your browser. TensorBoard can be used to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data such as images that pass through it. When TensorBoard is fully configured, it looks like this:</p>
<div class="CDPAlignCenter"><img src="assets/f35cf3a4-a002-4007-94bd-c4be2f595744.png" class="calibre65"/></div>
<p class="calibre2">To understand how TensorBoard works, we are going to build a computational graph which will act as a classifier for the MNIST dataset, which is a dataset of handwritten images.</p>
<p class="calibre2">You don't have to understand all the bits and pieces of this model, but it will show you the general pipeline of a machine learning model implemented in TensorFlow.</p>
<p class="calibre2">So, let's start off by importing TensorFlow and loading the the required dataset using TensorFlow helper functions; these helper functions will check whether you're already downloaded the dataset, otherwise it will download it for you:</p>
<pre class="calibre21">import tensorflow as tf<br class="title-page-name"/><br class="title-page-name"/># Using TensorFlow helper function to get the MNIST dataset<br class="title-page-name"/>from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_dataset = input_data.read_data_sets("/tmp/data/", one_hot=True)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>Extracting /tmp/data/train-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting /tmp/data/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Extracting /tmp/data/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting /tmp/data/t10k-labels-idx1-ubyte.gz</pre>
<p class="calibre2">Next, we need to define the hyperparameters (parameters that could be used to fine-tune the performance of your model) and inputs of our model:</p>
<pre class="calibre21"># hyperparameters of the the model (you don't have to understand the functionality of each parameter)<br class="title-page-name"/>learning_rate = 0.01<br class="title-page-name"/>num_training_epochs = 25<br class="title-page-name"/>train_batch_size = 100<br class="title-page-name"/>display_epoch = 1<br class="title-page-name"/>logs_path = '/tmp/tensorflow_tensorboard/'<br class="title-page-name"/><br class="title-page-name"/># Define the computational graph input which will be a vector of the image pixels<br class="title-page-name"/># Images of MNIST has dimensions of 28 by 28 which will multiply to 784<br class="title-page-name"/>input_values = tf.placeholder(tf.float32, [None, 784], name='input_values')<br class="title-page-name"/><br class="title-page-name"/># Define the target of the model which will be a classification problem of 10 classes from 0 to 9<br class="title-page-name"/>target_values = tf.placeholder(tf.float32, [None, 10], name='target_values')<br class="title-page-name"/><br class="title-page-name"/># Define some variables for the weights and biases of the model<br class="title-page-name"/>weights = tf.Variable(tf.zeros([784, 10]), name='weights')<br class="title-page-name"/>biases = tf.Variable(tf.zeros([10]), name='biases')</pre>
<p class="calibre2">Now we need to build the model and define a cost function that we are going to optimize:</p>
<pre class="calibre21"># Create the computational graph and encapsulating different operations to different scopes<br class="title-page-name"/># which will make it easier for us to understand the visualizations of TensorBoard<br class="title-page-name"/>with tf.name_scope('Model'):<br class="title-page-name"/> # Defining the model<br class="title-page-name"/> predicted_values = tf.nn.softmax(tf.matmul(input_values, weights) + biases)<br class="title-page-name"/> <br class="title-page-name"/>with tf.name_scope('Loss'):<br class="title-page-name"/> # Minimizing the model error using cross entropy criteria<br class="title-page-name"/> model_cost = tf.reduce_mean(-tf.reduce_sum(target_values*tf.log(predicted_values), reduction_indices=1))<br class="title-page-name"/><br class="title-page-name"/>with tf.name_scope('SGD'):<br class="title-page-name"/> # using Gradient Descent as an optimization method for the model cost above<br class="title-page-name"/> model_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_cost)<br class="title-page-name"/> <br class="title-page-name"/>with tf.name_scope('Accuracy'):<br class="title-page-name"/> #Calculating the accuracy<br class="title-page-name"/> model_accuracy = tf.equal(tf.argmax(predicted_values, 1), tf.argmax(target_values, 1))<br class="title-page-name"/> model_accuracy = tf.reduce_mean(tf.cast(model_accuracy, tf.float32))<br class="title-page-name"/><br class="title-page-name"/># TensorFlow use the lazy evaluation strategy while defining the variables<br class="title-page-name"/># So actually till now none of the above variable got created or initialized<br class="title-page-name"/>init = tf.global_variables_initializer()</pre>
<p class="calibre2">We will define the summary variable that will be used to monitor the changes that will happen on specific variables such as the loss and how it's getting better through out the training process:</p>
<pre class="calibre21"># Create a summary to monitor the model cost tensor<br class="title-page-name"/>tf.summary.scalar("model loss", model_cost)<br class="title-page-name"/><br class="title-page-name"/># Create another summary to monitor the model accuracy tensor<br class="title-page-name"/>tf.summary.scalar("model accuracy", model_accuracy)<br class="title-page-name"/><br class="title-page-name"/># Merging the summaries to single operation<br class="title-page-name"/>merged_summary_operation = tf.summary.merge_all()</pre>
<p class="calibre2">Finally, we'll run the model by defining a session variable which will be used to execute the computation graph that we have built:</p>
<pre class="calibre21"># kick off the training process<br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/><br class="title-page-name"/> # Intialize the variables <br class="title-page-name"/> sess.run(init)<br class="title-page-name"/><br class="title-page-name"/> # operation to feed logs to TensorBoard<br class="title-page-name"/> summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())<br class="title-page-name"/><br class="title-page-name"/> # Starting the training cycle by feeding the model by batch at a time<br class="title-page-name"/> for train_epoch in range(num_training_epochs):<br class="title-page-name"/> <br class="title-page-name"/> average_cost = 0.<br class="title-page-name"/> total_num_batch = int(mnist_dataset.train.num_examples/train_batch_size)<br class="title-page-name"/> <br class="title-page-name"/> # iterate through all training batches<br class="title-page-name"/> for i in range(total_num_batch):<br class="title-page-name"/> batch_xs, batch_ys = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/> <br class="title-page-name"/> # Run the optimizer with gradient descent and cost to get the loss<br class="title-page-name"/> # and the merged summary operations for the TensorBoard<br class="title-page-name"/> _, c, summary = sess.run([model_optimizer, model_cost, merged_summary_operation],<br class="title-page-name"/> feed_dict={input_values: batch_xs, target_values: batch_ys})<br class="title-page-name"/> <br class="title-page-name"/> # write statistics to the log et every iteration<br class="title-page-name"/> summary_writer.add_summary(summary, train_epoch * total_num_batch + i)<br class="title-page-name"/> <br class="title-page-name"/> # computing average loss<br class="title-page-name"/> average_cost += c / total_num_batch<br class="title-page-name"/> <br class="title-page-name"/> # Display logs per epoch step<br class="title-page-name"/> if (train_epoch+1) % display_epoch == 0:<br class="title-page-name"/> print("Epoch:", '%03d' % (train_epoch+1), "cost=", "{:.9f}".format(average_cost))<br class="title-page-name"/><br class="title-page-name"/> print("Optimization Finished!")<br class="title-page-name"/><br class="title-page-name"/> # Testing the trained model on the test set and getting the accuracy compared to the actual labels of the test set<br class="title-page-name"/> print("Accuracy:", model_accuracy.eval({input_values: mnist_dataset.test.images, target_values: mnist_dataset.test.labels}))<br class="title-page-name"/><br class="title-page-name"/> print("To view summaries in the Tensorboard, run the command line:\n" \<br class="title-page-name"/> "--&gt; tensorboard --logdir=/tmp/tensorflow_tensorboard " \<br class="title-page-name"/>"\nThen open http://0.0.0.0:6006/ into your web browser")</pre>
<p class="calibre2">The output of the training process should be similar to this:</p>
<pre class="calibre21">Epoch: 001 cost= 1.183109128<br class="title-page-name"/>Epoch: 002 cost= 0.665210275<br class="title-page-name"/>Epoch: 003 cost= 0.552693334<br class="title-page-name"/>Epoch: 004 cost= 0.498636444<br class="title-page-name"/>Epoch: 005 cost= 0.465516675<br class="title-page-name"/>Epoch: 006 cost= 0.442618381<br class="title-page-name"/>Epoch: 007 cost= 0.425522513<br class="title-page-name"/>Epoch: 008 cost= 0.412194222<br class="title-page-name"/>Epoch: 009 cost= 0.401408134<br class="title-page-name"/>Epoch: 010 cost= 0.392437336<br class="title-page-name"/>Epoch: 011 cost= 0.384816745<br class="title-page-name"/>Epoch: 012 cost= 0.378183398<br class="title-page-name"/>Epoch: 013 cost= 0.372455584<br class="title-page-name"/>Epoch: 014 cost= 0.367275238<br class="title-page-name"/>Epoch: 015 cost= 0.362772711<br class="title-page-name"/>Epoch: 016 cost= 0.358591895<br class="title-page-name"/>Epoch: 017 cost= 0.354892231<br class="title-page-name"/>Epoch: 018 cost= 0.351451424<br class="title-page-name"/>Epoch: 019 cost= 0.348337946<br class="title-page-name"/>Epoch: 020 cost= 0.345453095<br class="title-page-name"/>Epoch: 021 cost= 0.342769080<br class="title-page-name"/>Epoch: 022 cost= 0.340236065<br class="title-page-name"/>Epoch: 023 cost= 0.337953151<br class="title-page-name"/>Epoch: 024 cost= 0.335739001<br class="title-page-name"/>Epoch: 025 cost= 0.333702818<br class="title-page-name"/>Optimization Finished!<br class="title-page-name"/>Accuracy: 0.9146<br class="title-page-name"/>To view summaries in the Tensorboard, run the command line:<br class="title-page-name"/>--&gt; tensorboard --logdir=/tmp/tensorflow_tensorboard <br class="title-page-name"/>Then open http://0.0.0.0:6006/ into your web browser</pre>
<p class="calibre2">To see the summarized statistics in TensorBoard, we are going to follow the message at the end of our output by issuing the following command in the terminal:</p>
<pre class="calibre21">tensorboard --logdir=/tmp/tensorflow_tensorboard</pre>
<p class="calibre2">Then, open <kbd class="calibre12">http://0.0.0.0:6006/</kbd> into your web browser.</p>
<p class="calibre2">When you open TensorBoard, you should get something similar to the following screenshot:</p>
<p class="calibre2"/>
<div class="CDPAlignCenter"><img src="assets/1f3b4ade-178f-41ca-aae9-76566e5db405.png" class="calibre31"/></div>
<p class="calibre2">This shows the variables that we were monitoring, such as the model accuracy and how it's getting higher, and the model loss and how it's getting lower throughout the training process. So, you observe that we had a normal learning process here. But sometimes you will find out that the accuracy and model loss are changing randomly or you want to keep track of some variables and how they are changing throughout the session, and TensorBoard will be very useful to help you spot any randomness or mistakes.</p>
<p class="calibre2">Also, if switched to the <span class="calibre10">GRAPHS</span> tab in TensorBoard, you will get to see the computational graph that we built in the preceding code:</p>
<div class="CDPAlignCenter"><img src="assets/e74b2f16-147f-44e7-a169-1bc1f731860b.png" class="calibre31"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">In this chapter, we covered the installation process for Ubuntu and Mac, gave an overview of the TensorFlow programming model, and explained the different types of simple nodes that could be used for building complex operations and how to get output from TensorFlow using a session object. Also, we covered TensorBoard and why it will helpful for debugging and analyzing complex deep learning applications.</p>
<p class="calibre2">Next, we will go through a basic explanation of neural networks and the intuition behind having multilayer neural networks. We will also cover some basic examples of TensorFlow and demonstrate how it could be used for regression and classification problems.</p>
</div>


            </article>

            
        </section>
    </body></html>