<html><head></head><body>
  <div id="_idContainer045" class="Basic-Text-Frame">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-32" class="chapterTitle">Understanding TensorFlow 2</h1>
    <p class="normal">In this chapter, you will get an in-depth understanding of TensorFlow. This is an open source distributed numerical computation framework, and it will be the main platform on which we will be implementing all our exercises. This chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">What is TensorFlow?</li>
      <li class="bulletList">The building blocks of TensorFlow (for example, variables and operations)</li>
      <li class="bulletList">Using Keras for building models</li>
      <li class="bulletList">Implementing our first neural network</li>
    </ul>
    <p class="normal">We will get started with TensorFlow by defining a simple calculation and trying to compute it using TensorFlow. After we complete this, we will investigate how TensorFlow executes this computation. This will help us to understand how the framework creates a computational graph to compute the outputs and execute this graph to obtain the desired outputs. Then we will dive into the details of how TensorFlow architecture operates by looking at how TensorFlow executes things, with the help of an analogy of how a fancy café works. We will then see how TensorFlow 1 used to work so that we can better appreciate the amazing features TensorFlow 2 offers. Note that when we use the word “TensorFlow” by itself, we are referring to TensorFlow 2. We will specifically mention TensorFlow 1 if we are referring to TensorFlow 1.</p>
    <p class="normal">Having gained a good conceptual and technical understanding of how TensorFlow operates, we will look at some of the important computations the framework offers. First, we will look at defining various data structures in TensorFlow, such as variables and tensors, and we’ll also see how to read inputs through data pipelines. Then we will work through some neural network-related operations (for example, convolution operation, defining losses, and optimization). </p>
    <p class="normal">Finally, we will apply this knowledge in an exciting exercise, where we will implement a neural network that can recognize images of handwritten digits. You will also see that you can implement or prototype neural networks very quickly and easily by using a high-level submodule such as Keras.</p>
    <h1 id="_idParaDest-33" class="heading-1">What is TensorFlow?</h1>
    <p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Natural Language Processing</em>, we briefly discussed what TensorFlow is. Now let’s take<a id="_idIndexMarker103"/> a closer look at it. TensorFlow is an open source, distributed numerical computation framework released by Google that is mainly intended to alleviate the painful details of implementing a neural network (for example, computing derivatives of the weights of the neural network). TensorFlow takes this a step further by providing efficient<a id="_idIndexMarker104"/> implementations of such numerical computations using <strong class="keyWord">Compute Unified Device Architecture </strong>(<strong class="keyWord">CUDA</strong>), which is a parallel computational platform introduced<a id="_idIndexMarker105"/> by NVIDIA (for more information on CUDA, visit <a href="https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/"><span class="url">https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/</span></a>). The <strong class="keyWord">Application Programming Interface </strong>(<strong class="keyWord">API</strong>) of TensorFlow at <a href="https://www.tensorflow.org/api_docs/python/tf/all_symbols"><span class="url">https://www.tensorflow.org/api_docs/python/tf/all_symbols</span></a> shows that TensorFlow<a id="_idIndexMarker106"/> provides thousands of operations<a id="_idIndexMarker107"/> that make our lives easier.</p>
    <p class="normal">TensorFlow was not developed overnight. This is a result of the persistence of talented, good-hearted developers and scientists who wanted to make a difference by bringing deep learning to a wider audience. If you are interested, you can take a look at the TensorFlow code at <a href="https://github.com/tensorflow/tensorflow"><span class="url">https://github.com/tensorflow/tensorflow</span></a>. Currently, TensorFlow has around 3,000 contributors, and it sits on top of more than 115,000 commits, evolving to be better and better every day.</p>
    <h2 id="_idParaDest-34" class="heading-2">Getting started with TensorFlow 2</h2>
    <p class="normal">Now let’s learn about<a id="_idIndexMarker108"/> a few essential components in the TensorFlow framework by working through a code example. Let’s write an example to perform the following computation, which is very common for neural networks:</p>
    <p class="center"><img src="../Images/B14070_02_002.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">This computation encompasses what happens in a single layer of a fully connected neural network. Here <code class="inlineCode">W</code> and <code class="inlineCode">x</code> are matrices<a id="_idIndexMarker109"/> and <code class="inlineCode">b</code> is a vector. Then, “<code class="inlineCode">.</code>" denotes the dot product. sigmoid is a non-linear transformation given by the following equation:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_001.png" alt="" style="height: 2.30em !important;"/></figure>
    <p class="normal">We will discuss how to do this computation through TensorFlow step by step.</p>
    <p class="normal">First, we will need to import TensorFlow and NumPy. NumPy<a id="_idIndexMarker110"/> is another scientific computation framework that provides various mathematical and other operations to manipulate data. Importing them is essential before you run any type of TensorFlow or NumPy-related operation in Python:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf 
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">First, we will write a function that can take the inputs <code class="inlineCode">x</code>, <code class="inlineCode">W</code>, and <code class="inlineCode">b</code> and perform this computation for us:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">layer</span>(<span class="hljs-params">x, W, b</span>):    
    <span class="hljs-comment"># Building the graph</span>
    h = tf.nn.sigmoid(tf.matmul(x,W) + b) <span class="hljs-comment"># Operation to perform</span>
    <span class="hljs-keyword">return</span> h
</code></pre>
    <p class="normal">Next, we add a Python decorator called <code class="inlineCode">tf.function</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">layer</span>(<span class="hljs-params">x, W, b</span>):    
    <span class="hljs-comment"># Building the graph</span>
    h = tf.nn.sigmoid(tf.matmul(x,W) + b) <span class="hljs-comment"># Operation to perform</span>
    <span class="hljs-keyword">return</span> h
</code></pre>
    <p class="normal">Put simply, a Python decorator is just another function. A Python decorator provides a clean way to call another function whenever you call the decorated function. In other words, every time the <code class="inlineCode">layer()</code> function is called, <code class="inlineCode">tf.function()</code> is called. This can be used for various purposes, such as:</p>
    <ul>
      <li class="bulletList">Logging the content and operations in a function</li>
      <li class="bulletList">Validating the inputs and outputs of another function</li>
    </ul>
    <p class="normal">When the <code class="inlineCode">layer()</code> function is passing through <code class="inlineCode">tf.function()</code>, TensorFlow will trace the content (in other words, the operations and data) in the function and build a computational<a id="_idIndexMarker111"/> graph automatically.</p>
    <p class="normal">The computational graph (also known as the dataflow graph) builds<a id="_idIndexMarker112"/> a DAG (a directed acyclic graph) that shows<a id="_idIndexMarker113"/> what kind of inputs are required, and what sort of computations need to be done in the program. </p>
    <p class="normal">In our example, the <code class="inlineCode">layer()</code> function produces <code class="inlineCode">h</code> by using inputs <code class="inlineCode">x</code>, <code class="inlineCode">W</code>, and <code class="inlineCode">b</code>, and some transformations or operations such as <code class="inlineCode">+</code> and <code class="inlineCode">tf.matmul()</code>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_01.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg"/></figure>
    <p class="packt_figref">Figure 2.1: A computational graph of the client</p>
    <p class="normal">If we look at an analogy for a DAG, if you think of the output as a <em class="italic">cake</em>, then the <em class="italic">graph </em>would be the recipe to make that cake using <em class="italic">ingredients </em>(that is, inputs). </p>
    <p class="normal">The feature that builds<a id="_idIndexMarker114"/> this computational graph automatically in TensorFlow is known as <strong class="keyWord">AutoGraph</strong>. AutoGraph is not just looking at the operations in the passed function; it also scrutinizes the flow of operations. This means that you can have <code class="inlineCode">if</code> statements, or <code class="inlineCode">for</code>/<code class="inlineCode">while</code> loops in your function, and AutoGraph will take care of those when building the graph. You will see more on AutoGraph in the next section.</p>
    <div class="note">
      <p class="normal">In TensorFlow 1.x, the user needed to implement<a id="_idIndexMarker115"/> the computational graph explicitly. This meant the user could not write typical Python code using <code class="inlineCode">if-else</code> statements or <code class="inlineCode">for</code> loops, but had to explicitly control the flow of operations using special bespoke TensorFlow operations such as <code class="inlineCode">tf.cond()</code> and <code class="inlineCode">tf.control_dependencies()</code>. This is because, unlike TensorFlow 2.x, TensorFlow 1.x did not immediately execute operations when you called them. Rather, after they were defined, they needed to be executed explicitly using the context of a TensorFlow <code class="inlineCode">Session</code>. For example, when you run the following in TensorFlow 1,</p>
      <p class="center"><code class="inlineCode"> h = tf.nn.sigmoid(tf.matmul(x,W) + b)</code></p>
      <p class="normal"><code class="inlineCode">h</code> will not have any value until <code class="inlineCode">h</code> is executed in the context of a <code class="inlineCode">Session</code>. Therefore, <code class="inlineCode">h</code> could not be treated like any other Python variable. Don’t worry if you don’t understand how the <code class="inlineCode">Session</code> works. It will be discussed in the coming sections.</p>
    </div>
    <p class="normal">Next, you can use this function<a id="_idIndexMarker116"/> right away, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">x = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]],dtype=np.float32)
</code></pre>
    <p class="normal">Here, <code class="inlineCode">x</code> is a simple NumPy array:</p>
    <pre class="programlisting code"><code class="hljs-code">init_w = tf.initializers.RandomUniform(minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>)(shape=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>])
W = tf.Variable(init_w, dtype=tf.float32, name=<span class="hljs-string">'W'</span>) 
init_b = tf.initializers.RandomUniform()(shape=[<span class="hljs-number">5</span>])
b = tf.Variable(init_b, dtype=tf.float32, name=<span class="hljs-string">'b'</span>)
</code></pre>
    <p class="normal"><code class="inlineCode">W</code> and <code class="inlineCode">b</code> are TensorFlow variables defined using the <code class="inlineCode">tf.Variable</code> object. <code class="inlineCode">W</code> and <code class="inlineCode">b</code> hold tensors. A tensor is essentially an <em class="italic">n</em>-dimensional array. For example, a one-dimensional vector<a id="_idIndexMarker117"/> or a two-dimensional matrix are called <strong class="keyWord">tensors</strong>. A <code class="inlineCode">tf.Variable</code> is a mutable structure, which means the values in the tensor stored in that variable can change over time. For example, variables are used to store neural network weights, which change during the model optimization.</p>
    <p class="normal">Also, note that for <code class="inlineCode">W</code> and <code class="inlineCode">b</code>, we provide some important arguments, such as the following:</p>
    <pre class="programlisting code"><code class="hljs-code">init_w = tf.initializers.RandomUniform(minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>)(shape=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>])
init_b = tf.initializers.RandomUniform()(shape=[<span class="hljs-number">5</span>])
</code></pre>
    <p class="normal">These are called variable initializers<a id="_idIndexMarker118"/> and are the tensors<a id="_idIndexMarker119"/> that will be assigned to the <code class="inlineCode">W</code> and <code class="inlineCode">b</code> variables initially. A variable must have an initial value provided. Here, <code class="inlineCode">tf.initializers.RandomUniform</code> means that we uniformly sample values between <code class="inlineCode">minval</code> <code class="inlineCode">(-0.1)</code> and <code class="inlineCode">maxval</code> <code class="inlineCode">(0.1)</code> to assign values to the tensors. There are many<a id="_idIndexMarker120"/> different initializers provided in TensorFlow (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/initializers</span></a>). It is also very important to define the <em class="italic">shape </em>of your initializer when you are defining the initializer itself. The <code class="inlineCode">shape</code> property defines the size of each dimension of the output tensor. For example, if <code class="inlineCode">shape</code> is <code class="inlineCode">[10, 5]</code>, this means that it will be a two-dimensional structure and will have <code class="inlineCode">10</code> elements on axis 0 (rows) and <code class="inlineCode">5</code> elements on axis 1 (columns):</p>
    <pre class="programlisting code"><code class="hljs-code">h = layer(x,W,b)
</code></pre>
    <p class="normal">Finally, <code class="inlineCode">h</code> is called a TensorFlow tensor in general. A TensorFlow tensor is an immutable structure. Once a value is assigned to a TensorFlow tensor, it cannot be changed. </p>
    <p class="normal">As you can see, the term “tensor” is used in two ways: </p>
    <ul>
      <li class="bulletList">To refer to an <em class="italic">n</em>-dimensional array</li>
      <li class="bulletList">To refer to an immutable data structure in TensorFlow</li>
    </ul>
    <p class="normal">For both, the underlying concept is the same as they hold an <em class="italic">n</em>-dimensional data structure, only differing in the context they are used. The term will be used interchangeably to refer to these structures in our discussion.</p>
    <p class="normal">Finally, you can immediately see the value of <code class="inlineCode">h</code> using,</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">f"h =</span> <span class="hljs-subst">{h.numpy()}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">which will give,</p>
    <pre class="programlisting con"><code class="hljs-con">h = [[0.7027744 0.687556  0.635395  0.6193934 0.6113584]]
</code></pre>
    <p class="normal">The <code class="inlineCode">numpy()</code> function retrieves the NumPy array from the TensorFlow Tensor object. The full code is as below. All the code examples in this chapter will be available in the <code class="inlineCode">tensorflow_introduction.ipynb</code> file in the <code class="inlineCode">ch2</code> folder:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">layer</span>(<span class="hljs-params">x, W, b</span>):    
    <span class="hljs-comment"># Building the graph</span>
    h = tf.nn.sigmoid(tf.matmul(x,W) + b) <span class="hljs-comment"># Operation to be performed</span>
    <span class="hljs-keyword">return</span> h
x = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]], dtype=np.float32) 
<span class="hljs-comment"># Variable</span>
init_w = tf.initializers.RandomUniform(minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>)(shape=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>])
W = tf.Variable(init_w, dtype=tf.float32, name=<span class="hljs-string">'W'</span>) 
<span class="hljs-comment"># Variable</span>
init_b = tf.initializers.RandomUniform()(shape=[<span class="hljs-number">5</span>])
b = tf.Variable(init_b, dtype=tf.float32, name=<span class="hljs-string">'b'</span>) 
h = layer(x,W,b)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"h =</span> <span class="hljs-subst">{h.numpy()}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">For future reference, let’s call our example <em class="italic">the sigmoid example</em>.</p>
    <p class="normal">As you can already<a id="_idIndexMarker121"/> see, defining a TensorFlow computational graph and executing that is very “Pythonic”. This is because TensorFlow executes its operations “eagerly”, or immediately after the <code class="inlineCode">layer()</code> function is called. This is a special mode<a id="_idIndexMarker122"/> in TensorFlow known as <em class="italic">eager execution</em> mode. This was an optional mode for TensorFlow 1, but has been made the default in TensorFlow 2. </p>
    <p class="normal">Also note that the next two sections will be somewhat complex and technical. However, don’t worry if you don’t understand everything completely because the explanation will be supplemented with a more digestible, and thorough, real-world example that explains how an order is fulfilled in our new-and-improved restaurant, <em class="italic">Café Le TensorFlow 2</em>.</p>
    <h2 id="_idParaDest-35" class="heading-2">TensorFlow 2 architecture – What happens during graph build?</h2>
    <p class="normal">Let’s now understand what TensorFlow <a id="_idIndexMarker123"/>does when you execute TensorFlow operations.</p>
    <p class="normal">When you call a function decorated by <code class="inlineCode">tf.function()</code>, such as the <code class="inlineCode">layer()</code> function, there is quite a bit happening in the background. First, TensorFlow will trace all the TensorFlow operations taking place in the function and build the computational graph automatically. </p>
    <p class="normal">In fact, <code class="inlineCode">tf.function()</code> will return a function that executes the built dataflow graph<a id="_idIndexMarker124"/> when invoked. Therefore, <code class="inlineCode">tf.function()</code> is a multi-stage process, where it first builds the dataflow graph and then executes it. Additionally, since TensorFlow traces each line in the function, if something goes wrong, TensorFlow can point to the exact line that is causing the issue. </p>
    <p class="normal">In our sigmoid example, the computational, or dataflow, graph would look like<em class="italic"> Figure 2.2</em>. A single element<a id="_idIndexMarker125"/> or vertex of the graph is called a <strong class="keyWord">node. </strong>There are two main types of objects in this graph: <em class="italic">operations </em>and <em class="italic">tensors</em>. In the preceding example, <code class="inlineCode">tf.nn.sigmoid</code> is an operation and <code class="inlineCode">h</code> is a tensor:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_01.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg"/></figure>
    <p class="packt_figref">Figure 2.2: A computational graph of the client</p>
    <p class="normal">The preceding graph shows the order of operations as well as how inputs flow through them.</p>
    <div class="note">
      <p class="normal">Keep in mind that <code class="inlineCode">tf.function()</code> or AutoGraph is not a silver bullet that turns any arbitrary Python function using TensorFlow operations into a computational graph; it has its limitations. For example, the current version cannot handle recursive calls. To see a full list of the eager mode capabilities, refer to the following link: <a href="https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md"><span class="url">https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md</span></a>.</p>
    </div>
    <p class="normal">Now we know that TensorFlow<a id="_idIndexMarker126"/> is skilled at creating a nice computational graph, with all the dependencies and operations so that it knows exactly how, when, and where the data flows. However, we did not quite answer how this graph is executed. In fact, TensorFlow does quite a bit behind the scenes. For example, the graph might be divided into subgraphs, and subsequently into even finer pieces, to achieve parallelization. These subgraphs or pieces will then be assigned to workers that will perform the assigned task. </p>
    <h2 id="_idParaDest-36" class="heading-2">TensorFlow architecture – what happens when you execute the graph?</h2>
    <p class="normal">The computational graph<a id="_idIndexMarker127"/> uses the <code class="inlineCode">tf.GraphDef</code> protocol to canonicalize the dataflow graph and send it to the distributed master. The distributed master would perform the actual operation execution and parameter updates in a single-process setting. In a distributed setting, the master would delegate these tasks to worker processes/devices and manage these worker processes. <code class="inlineCode">tf.GraphDef</code> is a standardized representation of the graph specific to TensorFlow. The distributed master sees all computations in the graph and divides the computations into different devices (for example, different GPUs and CPUs). TensorFlow operations have multiple kernels. A kernel is a device-specific implementation of a certain operation. For example, the <code class="inlineCode">tf.matmul()</code> function will be implemented differently to run on the CPU or GPU since, on a GPU, you can achieve much better performance due to more parallelization.</p>
    <p class="normal">Next, the computational graph will be broken into subgraphs and pruned by the distributed master. Although decomposing the computational graph in <em class="italic">Figure 2.2</em> appears too trivial in our example, the computational graph can exponentially grow in real-world solutions with many hidden layers. Additionally, it becomes important to break the computational graph into multiple pieces and shave off any redundant computations in order to get results faster (for example, in a multi-device setting). </p>
    <p class="normal">Executing the graph or a subgraph (if the graph is divided into subgraphs) is called a single <em class="italic">task</em>, where each task is allocated<a id="_idIndexMarker128"/> to a single worker (which could be a single process or an entire device). These workers can run as a single process in a multi-process device (for example, a multi-processing CPU), or run on different devices (for example, CPUs and GPUs). In a distributed<a id="_idIndexMarker129"/> setting, we would have multiple workers executing tasks (for example, multiple workers training the model on different batches of data). On the contrary, we have only one set of parameters. So how do multiple workers manage to update the same set of parameters?</p>
    <p class="normal">To solve this, there is one worker that is considered the parameter server and will hold the main copy of the parameters. The workers will copy the parameters over, update them, and send them back to the parameter server. Typically, the parameter server will define some resolution strategy to resolve multiple updates coming from multiple workers (for example, taking the mean). These details were provided so you can understand the complexity that has gone into TensorFlow. However, our book will be based on using TensorFlow in a single-process/worker setting. In this setting, the organization of the distributed master, workers, and the parameter server is much more straightforward and is absorbed mostly by a special session implementation used by TensorFlow. This general workflow of a TensorFlow client is depicted in <em class="italic">Figure 2.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_03.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_02.jpg"/></figure>
    <p class="packt_figref">Figure 2.3: The generic execution of a TensorFlow client. A TensorFlow client starts with a graph that gets sent to the distributed master. The master spins up worker processes to perform actual tasks and parameter updates</p>
    <p class="normal">Once the calculation is done, the session brings back the updated data to the client from the parameter server. The architecture of TensorFlow is shown in <em class="italic">Figure 2.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_04.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_04.png"/></figure>
    <p class="packt_figref">Figure 2.4: TensorFlow framework architecture. This explanation is based on the official TensorFlow documentation found at: <a href="https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md"><span class="url">https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md</span></a></p>
    <p class="normal">Most of the changes<a id="_idIndexMarker130"/> introduced in TensorFlow 2 can be attributed to front-end changes. That is, how the dataflow graph is built and when the graph is executed. The way the graph is executed remains more or less the same in TensorFlow 1 and 2.</p>
    <p class="normal">Now we know what happens end-to-end from the moment you execute <code class="inlineCode">tf.function()</code>, but this was a very technical explanation, and nothing explains something better than a good analogy. Therefore, we will try to understand TensorFlow 2 with an analogy to our new and improved Café Le TensorFlow 2.</p>
    <h2 id="_idParaDest-37" class="heading-2">Café Le TensorFlow 2 – understanding TensorFlow 2 with an analogy</h2>
    <p class="normal">Let’s say the owners renovated<a id="_idIndexMarker131"/> our previous Café Le TensorFlow (this is an analogy from the first edition) and reopened it as Café Le TensorFlow 2. The word around the town is that it’s much more opulent than it used to be. Remembering the great experience you had before, you book a table instantly and go there to grab a seat. </p>
    <p class="normal">You want to order a <em class="italic">chicken burger with extra cheese and no tomatoes</em>. And you realize the café is indeed fancy. There’re no waiters here, but a voice-enabled tablet for each table into which you say what you want. This will get converted to a standard format that the chefs will understand (for example, table number, menu item ID, quantity, and special requirements). </p>
    <p class="normal">Here, you represent<a id="_idIndexMarker132"/> the TensorFlow 2 program. The ability of the voice-enabled tablet that converts your voice (or TensorFlow operations) to the standard format (or GraphDef format) is analogous to the AutoGraph feature.</p>
    <p class="normal">Now comes the best part. As soon as you start speaking, a manager will be looking at your order and assigning various tasks to chefs. The manager is responsible for making sure things happen as quickly as possible. The kitchen manager makes decisions, such as how many chefs are required to make the dish and which chefs are the best candidates for the job. The kitchen manager represents the distributed master.</p>
    <p class="normal">Each chef has a cook whose responsibility it is to provide the chef with the right ingredients, equipment, and so forth. So, the kitchen manager takes the order to a single chef and a cook (a burger is not that hard to prepare) and asks them to prepare the dish. The chef looks at the order and tells the cook what is needed. So, the cook first finds the things that will be required (for example, buns, patties, and onions) and keeps them close to fulfill the chef’s requests as soon as possible. Moreover, the chef might also ask to keep the intermediate results (for example, cut vegetables) of the dish temporarily until the chef needs it back again. In our example, the chef is the operation executor, and the cook is the parameter server.</p>
    <p class="normal">This café is full of surprises. As you are speaking out your order (that is, invoking Python functions that have TensorFlow operations), you see it getting prepared in real time through the tablet on your table (that is, eager execution).</p>
    <p class="normal">The best thing about this video feed is that, if you see that the chef did not put enough cheese, you know exactly why the burger wasn’t as good as expected. So, you can either order another one or provide specific feedback. This is a great improvement over how TensorFlow 1 did things, where they would take your order and you would not see anything until the full burger had been prepared. This process is shown in <em class="italic">Figure 2.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.5: The restaurant analogy illustrated</p>
    <p class="normal">Let’s now have a look back at how TensorFlow 1 used to work.</p>
    <h2 id="_idParaDest-38" class="heading-2">Flashback: TensorFlow 1</h2>
    <p class="normal">We said numerous<a id="_idIndexMarker133"/> times that TensorFlow 2 is very different from TensorFlow 1. But we still don’t know what it used to be like. Therefore, let’s now do a bit of time traveling to see how the same sigmoid computation could have been implemented in TensorFlow 1.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Warning</strong></p>
      <p class="normal">You will not be able to execute the following code in TensorFlow 2.x as it stands. </p>
    </div>
    <p class="normal">First, we’ll define a <code class="inlineCode">graph</code> object, which we will populate with operations and variables later:</p>
    <pre class="programlisting code"><code class="hljs-code">graph = tf.Graph() <span class="hljs-comment"># Creates a graph</span>
session = tf.InteractiveSession(graph=graph) <span class="hljs-comment"># Creates a session</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">graph</code> object contains the computational graph that connects the various inputs and outputs we define in our program to get the final desired output. This is the same graph we discussed earlier. Also, we’ll define a <code class="inlineCode">session</code> object that takes the defined graph as the input, which executes the graph. In other words, compared to TensorFlow 2, the <code class="inlineCode">graph</code> object and the <code class="inlineCode">session</code> object do what happens when you invoke them decorated by <code class="inlineCode">tf.function()</code>.</p>
    <p class="normal">Now we’ll define a few tensors, namely <code class="inlineCode">x</code>, <code class="inlineCode">W</code>, <code class="inlineCode">b</code>, and <code class="inlineCode">h</code>. There are several different ways that you can<a id="_idIndexMarker134"/> define tensors in TensorFlow 1. Here, we will look at three such different approaches:</p>
    <ul>
      <li class="bulletList">First, <code class="inlineCode">x</code> is a placeholder. Placeholders, as the name suggests, are not initialized with any value. Rather, we will provide the value on the fly at the time of the graph execution. If you remember from the TensorFlow 2 sigmoid exercise, we fed <code class="inlineCode">x</code> (which was a NumPy array) directly to the function <code class="inlineCode">layer(x, w, b)</code>. Unlike in TensorFlow 2, you cannot feed NumPy arrays directly to TensorFlow 1 graphs or operations. </li>
      <li class="bulletList">Next, we have the variables <code class="inlineCode">W</code> and <code class="inlineCode">b</code>. Variables are defined similarly to TensorFlow 2 with some minor changes in the syntax. </li>
      <li class="bulletList">Finally, we have <code class="inlineCode">h</code>, which is an immutable tensor produced by performing some operations on <code class="inlineCode">x</code>, <code class="inlineCode">W</code>, and <code class="inlineCode">b</code>. Note that you will not see the value of <code class="inlineCode">h</code> immediately as you needed to manually execute the graph in TensorFlow 1.</li>
    </ul>
    <p class="normal">These tensors are defined as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.placeholder(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">10</span>],dtype=tf.float32,name=<span class="hljs-string">'x'</span>)
W = tf.Variable(tf.random_uniform(shape=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>], minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>, dtype=tf.float32),name=<span class="hljs-string">'W'</span>)
b = tf.Variable(tf.zeros(shape=[<span class="hljs-number">5</span>],dtype=tf.float32),name=<span class="hljs-string">'b'</span>) h = tf.nn.sigmoid(tf.matmul(x,W) + b)
</code></pre>
    <div class="note">
      <p class="normal">The lifetime of variables in TensorFlow 1 was managed by the session object, meaning that variables lived in memory for as long as the session lived (even after losing references to them in the code). However, in TensorFlow 2, variables are removed soon after the variables are not referenced in the code, just like in Python.</p>
    </div>
    <p class="normal">Next, we’ll run an initialization operation that initializes the variables in the graph, <code class="inlineCode">W</code> and <code class="inlineCode">b</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.global_variables_initializer().run()
</code></pre>
    <p class="normal">Now, we will execute the graph to obtain the final output we need, <code class="inlineCode">h</code>. This is done by running <code class="inlineCode">session.run(...)</code>, where we provide the value to the placeholder as an argument of the <code class="inlineCode">session.run()</code> command:</p>
    <pre class="programlisting code"><code class="hljs-code">h_eval = session.run(h,feed_dict={x: np.random.rand(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)})
</code></pre>
    <p class="normal">Finally, we close the session, releasing<a id="_idIndexMarker135"/> any resources held by the <code class="inlineCode">session</code> object:</p>
    <pre class="programlisting code"><code class="hljs-code">session.close()
</code></pre>
    <p class="normal">Here is the full code of this TensorFlow 1 example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-comment"># Defining the graph and session graph = tf.Graph() # Creates a graph</span>
session = tf.InteractiveSession(graph=graph) <span class="hljs-comment"># Creates a session</span>
<span class="hljs-comment"># Building the graph</span>
<span class="hljs-comment"># A placeholder is an symbolic input</span>
x = tf.placeholder(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">10</span>],dtype=tf.float32,name=<span class="hljs-string">'x'</span>)
<span class="hljs-comment"># Variable</span>
W = tf.Variable(tf.random_uniform(shape=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>], minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>, dtype=tf.float32),name=<span class="hljs-string">'W'</span>) 
b = tf.Variable(tf.zeros(shape=[<span class="hljs-number">5</span>],dtype=tf.float32),name=<span class="hljs-string">'b'</span>)
h = tf.nn.sigmoid(tf.matmul(x,W) + b) <span class="hljs-comment"># Operation to be performed</span>
<span class="hljs-comment"># Executing operations and evaluating nodes in the graph</span>
tf.global_variables_initializer().run()<span class="hljs-comment"> # Initialize the variables</span>
<span class="hljs-comment"># Run the operation by providing a value to the symbolic input x h_eval = </span>session.run(h,feed_dict={x: np.random.rand(1,10)})
<span class="hljs-comment"># Closes the session to free any held resources by the session</span>
session.close()
</code></pre>
    <p class="normal">As you can see, before TensorFlow 2 the user had to:</p>
    <ul>
      <li class="bulletList">Define the computational graph using various TensorFlow data structures (for example, <code class="inlineCode">tf.placeholder</code>) and operations (for example, <code class="inlineCode">tf.matmul()</code>)</li>
      <li class="bulletList">Execute the required part of the graph using <code class="inlineCode">session.run()</code> to fetch the results by feeding the correct data into the session</li>
    </ul>
    <p class="normal">In conclusion, TensorFlow<a id="_idIndexMarker136"/> 1.x had several limitations:</p>
    <ul>
      <li class="bulletList">Coding with TensorFlow 1 did not provide the same intuitive “Pythonic” feeling as you needed to define the computational graph<a id="_idIndexMarker137"/> first and then invoke the execution of it. This is known as declarative programming.</li>
      <li class="bulletList">The design in TensorFlow 1 made it very hard to break the code down into manageable functions as the user needed to define the graph fully, before doing any computations. This resulted in very large functions or pieces of code containing very large computational graphs. </li>
      <li class="bulletList">It was very difficult to do real-time debugging of the code as TensorFlow had its own runtime that used <code class="inlineCode">session.run()</code>.</li>
      <li class="bulletList">But also, it was not without some advantages, such as the efficiency brought about by declaring the full computational graph upfront. Knowing all the computations in advance meant TensorFlow 1 could perform all sorts of optimizations (for example, graph pruning) to run the graph efficiently. </li>
    </ul>
    <p class="normal">In this part of the chapter, we discussed our first example in TensorFlow2 and the architecture of TensorFlow. Finally, we compared and contrasted TensorFlow 1 and 2. Next, we will discuss the various building blocks of TensorFlow 2.</p>
    <h1 id="_idParaDest-39" class="heading-1">Inputs, variables, outputs, and operations</h1>
    <p class="normal">Now we are returning from our journey into TensorFlow 1 and stepping back to TensorFlow 2. Let’s proceed to the most common elements that comprise a TensorFlow 2 program. If you read any of the millions of TensorFlow clients available on the internet, the TensorFlow-related code all falls into one of these buckets:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Inputs</strong>: Data used to train<a id="_idIndexMarker138"/> and test our algorithms</li>
      <li class="bulletList"><strong class="keyWord">Variables</strong>: Mutable tensors, mostly<a id="_idIndexMarker139"/> defining the parameters of our algorithms</li>
      <li class="bulletList"><strong class="keyWord">Outputs</strong>: Immutable tensors storing<a id="_idIndexMarker140"/> both terminal and intermediate outputs</li>
      <li class="bulletList"><strong class="keyWord">Operations</strong>: Various transformations<a id="_idIndexMarker141"/> for inputs to produce the desired outputs</li>
    </ul>
    <p class="normal">In our earlier sigmoid example, we can find instances of all these categories. We list the respective TensorFlow elements and the notation used in the sigmoid example in <em class="italic">Table 2.1</em>:</p>
    <table id="table001" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">TensorFlow element</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Value from example client</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Inputs</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">x</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Variables</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">W</code> and <code class="inlineCode">b</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Outputs</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">h</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Operations</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">tf.matmul(...)</code>, <code class="inlineCode">tf.nn.sigmoid(...)</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 2.1: The different types of TensorFlow primitives we have encountered so far</p>
    <p class="normal">The following subsections explain each of these TensorFlow elements listed in the table in more detail.</p>
    <h2 id="_idParaDest-40" class="heading-2">Defining inputs in TensorFlow</h2>
    <p class="normal">There are three different<a id="_idIndexMarker142"/> ways you can feed data<a id="_idIndexMarker143"/> to a TensorFlow program:</p>
    <ul>
      <li class="bulletList">Feeding data as NumPy arrays</li>
      <li class="bulletList">Feeding data as TensorFlow tensors</li>
      <li class="bulletList">Using the <code class="inlineCode">tf.data</code> API to create an input pipeline</li>
    </ul>
    <p class="normal">Next, we will discuss a few different ways you can feed data to TensorFlow operations.</p>
    <h3 id="_idParaDest-41" class="heading-3">Feeding data as NumPy arrays</h3>
    <p class="normal">This is the simplest way to feed data<a id="_idIndexMarker144"/> into a TensorFlow program. Here, you pass a NumPy array<a id="_idIndexMarker145"/> as an input to the TensorFlow operation and the result is executed immediately. This is exactly what we did in the sigmoid example. If you look at <code class="inlineCode">x</code>, it is a NumPy array.</p>
    <h3 id="_idParaDest-42" class="heading-3">Feeding data as tensors</h3>
    <p class="normal">The second method<a id="_idIndexMarker146"/> is like the first one, but the type<a id="_idIndexMarker147"/> of data is different. Here, we are defining <code class="inlineCode">x</code> as a TensorFlow tensor.</p>
    <p class="normal">To see this in action, let’s modify our sigmoid example. Remember that we defined <code class="inlineCode">x</code> as:</p>
    <pre class="programlisting code"><code class="hljs-code">x = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]], dtype=np.float32)
</code></pre>
    <p class="normal">Instead, let’s define this as a tensor that contains specific values:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant(value=[[<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>,<span class="hljs-number">0.4</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.6</span>,<span class="hljs-number">0.7</span>,<span class="hljs-number">0.8</span>,<span class="hljs-number">0.9</span>,<span class="hljs-number">1.0</span>]],
dtype=tf.float32,name=<span class="hljs-string">'x'</span>)
</code></pre>
    <p class="normal">Also, the full code<a id="_idIndexMarker148"/> would become<a id="_idIndexMarker149"/> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">layer</span>(<span class="hljs-params">x, W, b</span>):    
    <span class="hljs-comment"># Building the graph</span>
    h = tf.nn.sigmoid(tf.matmul(x,W) + b) <span class="hljs-comment"># Operation to be performed</span>
    <span class="hljs-keyword">return</span> h
<span class="hljs-comment"># A pre-loaded input</span>
x = tf.constant(value=[[<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>,<span class="hljs-number">0.4</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.6</span>,<span class="hljs-number">0.7</span>,<span class="hljs-number">0.8</span>,<span class="hljs-number">0.9</span>]],dtype=tf.float32,name=<span class="hljs-string">'x'</span>) 
<span class="hljs-comment"># Variable</span>
init_w = tf.initializers.RandomUniform(minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>)(shape=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>])
W = tf.Variable(init_w, dtype=tf.float32, name=<span class="hljs-string">'W'</span>) 
<span class="hljs-comment"># Variable</span>
init_b = tf.initializers.RandomUniform()(shape=[<span class="hljs-number">5</span>])
b = tf.Variable(init_b, dtype=tf.float32, name=<span class="hljs-string">'b'</span>) 
h = layer(x,W,b)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"h =</span> <span class="hljs-subst">{h}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"h is of type</span> <span class="hljs-subst">{</span><span class="hljs-built_in">type</span>(h)<span class="hljs-subst">}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">Let’s now discuss how we can define data pipelines in TensorFlow.</p>
    <h3 id="_idParaDest-43" class="heading-3">Building a data pipeline using the tf.data API</h3>
    <p class="normal"><code class="inlineCode">tf.data</code> provides you with a convenient way<a id="_idIndexMarker150"/> to build data pipelines in TensorFlow. Input pipelines<a id="_idIndexMarker151"/> are designed for more heavy-duty programs that need to process a lot of data. For example, if you have a small dataset (for example, the MNIST dataset) that fits into the memory, input pipelines would be excessive. However, when working with complex data or problems, where you might need to work with large datasets that do not fit in memory, augment the data (for example, for adjusting image contrast/brightness), numerically transform it (for example, standardize), and so on. The <code class="inlineCode">tf.data</code> API provides convenient functions that can be used to easily load and transform your data. Furthermore, it streamlines<a id="_idIndexMarker152"/> your data ingestion code with the model<a id="_idIndexMarker153"/> training. </p>
    <p class="normal">Additionally, the <code class="inlineCode">tf.data</code> API offers various options to enhance the performance of your data pipeline, such as multi-processing and pre-fetching data. Pre-fetching refers to bringing data into the memory before it’s required and keeping it ready. We will discuss these methods in more detail as they are used in the upcoming chapters.</p>
    <p class="normal">When creating an input pipeline, we intend to perform the following:</p>
    <ul>
      <li class="bulletList">Source the data from a data source (for example, an in-memory NumPy array, CSV file on disk, or individual files such as images).</li>
      <li class="bulletList">Apply various transformations to the data (for example, cropping/resizing image data).</li>
      <li class="bulletList">Iterate the resulting dataset element/batch-wise. Batching is required as deep learning models are trained on randomly sampled batches of data. As the datasets these models are trained on are large, they typically do not fit in memory.</li>
    </ul>
    <p class="normal">Let’s write an input pipeline using TensorFlow’s <code class="inlineCode">tf.data</code> API. In this example, we have three text files (<code class="inlineCode">iris.data.1</code>, <code class="inlineCode">iris.data.2</code>, and <code class="inlineCode">iris.data.3</code>) in CSV format, each file having 50 lines and each line having 4 floating-point numbers (in other words, various lengths associated with a flower) and a string label separated by commas (an example line would be <code class="inlineCode">5.6,2.9,3.6,1.3,Iris-versicolor</code>). We will now use the <code class="inlineCode">tf.data</code> API to read data from these files. We also know that some of this data is corrupted (as with any real-life machine learning project). In our case, some data points have negative lengths. So, let’s first write a pipeline to go through the data row by row and print the corrupted outputs. </p>
    <div class="note">
      <p class="normal">For more information, refer to the official TensorFlow<a id="_idIndexMarker154"/> page on importing data at <a href="https://www.tensorflow.org/guide/data"><span class="url">https://www.tensorflow.org/guide/data</span></a>.</p>
    </div>
    <p class="normal">First, let’s import a few important libraries as before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Next, we will define a list containing the filenames:</p>
    <pre class="programlisting code"><code class="hljs-code">filenames = [<span class="hljs-string">f"./iris.data</span>.<span class="hljs-subst">{i}</span><span class="hljs-string">"</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)]
</code></pre>
    <p class="normal">Now we will use one of the dataset readers provided in TensorFlow. The dataset reader takes in a list of filenames<a id="_idIndexMarker155"/> and another list that specifies the data types<a id="_idIndexMarker156"/> of each column in the dataset. As we saw previously, we have four floating numbers and one string:</p>
    <pre class="programlisting code"><code class="hljs-code">dataset = tf.data.experimental.CsvDataset(filenames, [tf.float32, tf.float32, tf.float32, tf.float32, tf.string])
</code></pre>
    <p class="normal">Now we will organize our data into inputs and labels as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x1,x2,x3,x4,y: (tf.stack([x1,x2,x3,x4]), y))
</code></pre>
    <p class="normal">We are using lambda functions to separate out <code class="inlineCode">x1,x2,x3,x4</code> into one dataset and <code class="inlineCode">y</code> to another dataset, along with the <code class="inlineCode">dataset.map()</code> function.</p>
    <div class="note">
      <p class="normal">Lambda functions<a id="_idIndexMarker157"/> are a special type of function that allow you to define some computations succinctly. With lambda functions, you don’t need to name your function, which can be quite handy if you are using a certain function only once in your code. The format of the lambda function looks like:</p>
      <p class="center"><code class="inlineCode"> lambda &lt;arguments&gt;: &lt;result returned after the computation&gt; </code></p>
      <p class="normal">For example, if you need to write a function that adds two numbers, simply write:</p>
      <p class="center"><code class="inlineCode"> lambda x, y: x+y</code></p>
    </div>
    <p class="normal">Here, <code class="inlineCode">tf.stack()</code> stacks individual tensors (here, the individual feature) to a single tensor. When using the <code class="inlineCode">map</code> function, you first need to visualize what needs to be done to a single item in the dataset (a single item in our case is a single row from the dataset), and write the transformation.</p>
    <div class="note">
      <p class="normal">The map function<a id="_idIndexMarker158"/> is very simple but powerful. All it does is transform a set of given inputs into a new set of values. For example, if you have a list, <code class="inlineCode">xx</code>, that contains a list of numbers and want to convert them to power 2 element-wise, you can write something like <code class="inlineCode">xx_pow = map(lambda x: x**2, xx)</code>. And this can be very easily parallelized as there’s no dependency between items.</p>
    </div>
    <p class="normal">Next, you can iterate through<a id="_idIndexMarker159"/> this dataset, examining individual data<a id="_idIndexMarker160"/> points, as you would iterate through a normal Python list. Here, we are printing out all the corrupted items:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> next_element <span class="hljs-keyword">in</span> dataset:
    x, y = next_element[<span class="hljs-number">0</span>].numpy(), next_element[<span class="hljs-number">1</span>].numpy().decode(<span class="hljs-string">'ascii'</span>)
    <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">min</span>(x)&lt;<span class="hljs-number">0.0</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"(corrupted) X =&gt;</span> <span class="hljs-subst">{x}</span>\tY =&gt; <span class="hljs-subst">{y}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">Since you don’t want those corrupted inputs in your dataset, you can use the <code class="inlineCode">dataset.filter()</code> function to filter out those corrupted entries as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x,y: tf.reduce_min(x)&gt;<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Here we are checking whether the minimum element in <code class="inlineCode">x</code> is greater than zero; if not, those elements will be filtered out of the dataset.</p>
    <p class="normal">Another useful function is <code class="inlineCode">dataset.batch()</code>. When training deep neural networks, we often traverse the dataset in batches, not individual items. <code class="inlineCode">dataset.batch()</code> provides a convenient way to do that:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">5</span>
dataset = dataset.batch(batch_size=batch_size)
</code></pre>
    <p class="normal">Now, if you print the shape of a single element in your dataset, you should get the following: </p>
    <pre class="programlisting code"><code class="hljs-code">x.shape = (<span class="hljs-number">5</span>, <span class="hljs-number">4</span>), y.shape = (<span class="hljs-number">5</span>,)
</code></pre>
    <p class="normal">Now that we have examined the three different methods you can use to define inputs in TensorFlow, let’s see how we can define variables in TensorFlow.</p>
    <h2 id="_idParaDest-44" class="heading-2">Defining variables in TensorFlow</h2>
    <p class="normal">Variables play an important<a id="_idIndexMarker161"/> role in TensorFlow. A variable is essentially a tensor<a id="_idIndexMarker162"/> with a specific shape defining how many dimensions the variable will have and the size of each dimension. However, unlike a regular TensorFlow tensor, variables are <em class="italic">mutable</em>; meaning that the value of the variables can change after they are defined. This is an ideal property to have to implement the parameters of a learning model (for example, neural network weights), where the weights change slightly after each step of learning. For example, if you define a variable with <code class="inlineCode">x = tf.Variable(0,dtype=tf.int32)</code>, you can change the value of that variable using a TensorFlow operation such as <code class="inlineCode">tf.assign(x,x+1)</code>. However, if you define a tensor such as <code class="inlineCode">x = tf.constant(0,dtype=tf.int32)</code>, you cannot change the value of the tensor, as you could for a variable. It should stay <code class="inlineCode">0</code> until the end of the program execution.</p>
    <p class="normal">Variable creation is quite simple. In our sigmoid example, we already created two variables, <code class="inlineCode">W</code> and <code class="inlineCode">b</code>. When creating a variable, a few things are extremely important. We will list them here and discuss each in detail in the following paragraphs:</p>
    <ul>
      <li class="bulletList">Variable shape</li>
      <li class="bulletList">Initial values</li>
      <li class="bulletList">Data type</li>
      <li class="bulletList">Name (optional)</li>
    </ul>
    <p class="normal">The variable shape is a list of the <code class="inlineCode">[x,y,z,...]</code> format. Each value in the list indicates how large the corresponding dimension or axis is. For instance, if you require a 2D tensor with 50 rows and 10 columns as the variable, the shape would be equal to <code class="inlineCode">[50,10]</code>.</p>
    <p class="normal">The dimensionality of the variable (that is, the length of the <code class="inlineCode">shape</code> vector) is recognized as the rank of the tensor in TensorFlow. Do not confuse this with the rank of a matrix.</p>
    <div class="note">
      <p class="normal">Tensor rank in TensorFlow indicates the dimensionality of the tensor; for a two-dimensional matrix, <em class="italic">rank</em> = 2.</p>
    </div>
    <p class="normal">Next, a variable requires an <em class="italic">initial </em>value to be initialized with. TensorFlow provides several different initializers for our convenience, including constant initializers and normal distribution initializers. Here are a few popular TensorFlow initializers you can use to initialize variables:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">tf.initializers.Zeros</code></li>
      <li class="bulletList"><code class="inlineCode">tf.initializers.Constant</code></li>
      <li class="bulletList"><code class="inlineCode">tf.initializers.RandomNormal</code></li>
      <li class="bulletList"><code class="inlineCode">tf.initializers.GlorotUniform</code></li>
    </ul>
    <p class="normal">The shape of the variable can be provided as a part of the initializer as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">tf.initializers.RandomUniform(minval=-</code><span class="hljs-number">0.1</span><code class="inlineCode">, maxval=</code><span class="hljs-number">0.1</span><code class="inlineCode">)(shape=[</code><span class="hljs-number">10</span><code class="inlineCode">,</code><span class="hljs-number">5</span><code class="inlineCode">])</code>
</code></pre>
    <p class="normal">The data type plays an important <a id="_idIndexMarker163"/>role in determining the size of a variable. There are many<a id="_idIndexMarker164"/> different data types, including the commonly used <code class="inlineCode">tf.bool</code>, <code class="inlineCode">tf.uint8</code>, <code class="inlineCode">tf.float32</code>, and <code class="inlineCode">tf.int32</code>. Each data type has a number of bits required to represent a single value with that type. For example, <code class="inlineCode">tf.uint8</code> requires 8 bits, whereas <code class="inlineCode">tf.float32</code> requires 32 bits. It is common practice to use the same data types for computations, as doing otherwise can lead to data type mismatches. So, if you have two different data types for two tensors that you need to transform, you have to explicitly convert one tensor to the other tensor’s type using the <code class="inlineCode">tf.cast(...)</code> operation.</p>
    <p class="normal">The <code class="inlineCode">tf.cast(...)</code> operation is designed to cope with such situations. For example, if you have an <code class="inlineCode">x</code> variable with the <code class="inlineCode">tf.int32</code> type, which needs to be converted to <code class="inlineCode">tf.float32</code>, employ <code class="inlineCode">tf.cast(x,dtype=tf.float32)</code> to convert <code class="inlineCode">x</code> to <code class="inlineCode">tf.float32</code>.</p>
    <p class="normal">Finally, the <em class="italic">name </em>of the variable will be used as an ID to identify that variable in the graph. If you ever visualize the computational graph, the variable will appear by the argument passed to the <code class="inlineCode">name</code> keyword. If you do not specify a name, TensorFlow will use the default naming scheme.</p>
    <div class="note">
      <p class="normal">Note that the Python variable <code class="inlineCode">tf.Variable</code> is assigned to is not known by the computational graph, and is not a part of TensorFlow variable naming. Consider this example where you specify a TensorFlow variable as follows:</p>
      <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">a = tf.Variable(tf.zeros([</code><span class="hljs-number">5</span><code class="inlineCode">]),name=</code><span class="hljs-string">'b'</span><code class="inlineCode">)</code>
</code></pre>
      <p class="normal">Here, the TensorFlow graph will know this variable by the name <code class="inlineCode">b</code> and not <code class="inlineCode">a</code>.</p>
    </div>
    <p class="normal">Moving on, let’s talk about how to define TensorFlow outputs.</p>
    <h2 id="_idParaDest-45" class="heading-2">Defining outputs in TensorFlow</h2>
    <p class="normal">TensorFlow outputs<a id="_idIndexMarker165"/> are usually tensors, and the result of a transformation<a id="_idIndexMarker166"/> to either an input, or a variable, or both. In our example, <code class="inlineCode">h</code> is an output, where <code class="inlineCode">h = tf.nn.sigmoid(tf.matmul(x,W) + b)</code>. It is also possible to give such outputs to other operations, forming a chained set of operations. Furthermore, they do not necessarily have to be TensorFlow operations. You also can use standard Python arithmetic with TensorFlow. Here is an example:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.matmul(w,A) 
y = x + B
</code></pre>
    <p class="normal">Below, we explain various operations available in TensorFlow and how to use them.</p>
    <h2 id="_idParaDest-46" class="heading-2">Defining operations in TensorFlow</h2>
    <p class="normal">An operation in TensorFlow<a id="_idIndexMarker167"/> takes one or more inputs and produces<a id="_idIndexMarker168"/> one or more outputs. If you take a look at the TensorFlow API at <a href="https://www.tensorflow.org/api_docs/python/tf"><span class="url">https://www.tensorflow.org/api_docs/python/tf</span></a>, you will see that TensorFlow has a massive collection<a id="_idIndexMarker169"/> of operations available. Here, we will take a look at a selected few of the myriad TensorFlow operations.</p>
    <h3 id="_idParaDest-47" class="heading-3">Comparison operations</h3>
    <p class="normal">Comparison operations<a id="_idIndexMarker170"/> are useful for comparing two tensors. The following<a id="_idIndexMarker171"/> code example includes a few useful comparison operations. </p>
    <p class="normal">To understand the working of these operations, let’s consider two example tensors, <code class="inlineCode">x</code> and <code class="inlineCode">y</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Let's assume the following values for x and y </span>
<span class="hljs-comment"># x (2-D tensor) =&gt; [[1,2],[3,4]]</span>
<span class="hljs-comment"># y (2-D tensor) =&gt; [[4,3],[3,2]]</span>
x = tf.constant([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]], dtype=tf.int32)
y = tf.constant([[<span class="hljs-number">4</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">2</span>]], dtype=tf.int32)
<span class="hljs-comment"># Checks if two tensors are equal element-wise and returns a boolean</span>
<span class="hljs-comment"># tensor</span>
<span class="hljs-comment"># x_equal_y =&gt; [[False,False],[True,False]] </span>
x_equal_y = tf.equal(x, y, name=<span class="hljs-literal">None</span>)
<span class="hljs-comment"># Checks if x is less than y element-wise and returns a boolean tensor</span>
<span class="hljs-comment"># x_less_y =&gt; [[True,True],[False,False]]</span>
x_less_y = tf.less(x, y, name=<span class="hljs-literal">None</span>)
<span class="hljs-comment"># Checks if x is greater or equal than y element-wise and returns a</span>
<span class="hljs-comment"># boolean tensor</span>
<span class="hljs-comment"># x_great_equal_y =&gt; [[False,False],[True,True]]</span>
x_great_equal_y = tf.greater_equal(x, y, name=<span class="hljs-literal">None</span>)
<span class="hljs-comment"># Selects elements from x and y depending on whether, # the condition is satisfied (select elements from x) # or the condition failed (select elements from y)</span>
condition = tf.constant([[<span class="hljs-literal">True</span>,<span class="hljs-literal">False</span>],[<span class="hljs-literal">True</span>,<span class="hljs-literal">False</span>]],dtype=tf.<span class="hljs-built_in">bool</span>)
<span class="hljs-comment"># x_cond_y =&gt; [[1,3],[3,2]]</span>
x_cond_y = tf.where(condition, x, y, name=<span class="hljs-literal">None</span>)
</code></pre>
    <p class="normal">Next, let’s look<a id="_idIndexMarker172"/> at some<a id="_idIndexMarker173"/> mathematical operations.</p>
    <h3 id="_idParaDest-48" class="heading-3">Mathematical operations</h3>
    <p class="normal">TensorFlow allows you to perform<a id="_idIndexMarker174"/> math operations on tensors <a id="_idIndexMarker175"/>that range from the simple to the complex. We will discuss a few of the mathematical operations made available in TensorFlow. The complete<a id="_idIndexMarker176"/> set of operations is available at <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math"><span class="url">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math</span></a>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Let's assume the following values for x and y</span>
<span class="hljs-comment"># x (2-D tensor) =&gt; [[1,2],[3,4]]</span>
<span class="hljs-comment"># y (2-D tensor) =&gt; [[4,3],[3,2]]</span>
x = tf.constant([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]], dtype=tf.float32)
y = tf.constant([[<span class="hljs-number">4</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">2</span>]], dtype=tf.float32)
<span class="hljs-comment"># Add two tensors x and y in an element-wise fashion</span>
<span class="hljs-comment"># x_add_y =&gt; [[5,5],[6,6]]</span>
x_add_y = tf.add(x, y)
<span class="hljs-comment"># Performs matrix multiplication (not element-wise)</span>
<span class="hljs-comment"># x_mul_y =&gt; [[10,7],[24,17]]</span>
x_mul_y = tf.matmul(x, y)
<span class="hljs-comment"># Compute natural logarithm of x element-wise # equivalent to computing ln(x)</span>
<span class="hljs-comment"># log_x =&gt; [[0,0.6931],[1.0986,1.3863]]</span>
log_x = tf.log(x)
<span class="hljs-comment"># Performs reduction operation across the specified axis</span>
<span class="hljs-comment"># x_sum_1 =&gt; [3,7]</span>
x_sum_1 = tf.reduce_sum(x, axis=[<span class="hljs-number">1</span>], keepdims=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># x_sum_2 =&gt; [[4,6]]</span>
x_sum_2 = tf.reduce_sum(x, axis=[<span class="hljs-number">0</span>], keepdims=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Segments the tensor according to segment_ids (items with same id in</span>
<span class="hljs-comment"># the same segment) and computes a segmented sum of the data</span>
data = tf.constant([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>], dtype=tf.float32)
segment_ids = tf.constant([<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span> ], dtype=tf.int32)
<span class="hljs-comment"># x_seg_sum =&gt; [6,9,40]</span>
x_seg_sum = tf.segment_sum(data, segment_ids)
</code></pre>
    <p class="normal">Now, we<a id="_idIndexMarker177"/> will look<a id="_idIndexMarker178"/> at the scatter operation.</p>
    <h3 id="_idParaDest-49" class="heading-3">Updating (scattering) values in tensors</h3>
    <p class="normal">A scatter operation, which refers<a id="_idIndexMarker179"/> to changing the values<a id="_idIndexMarker180"/> at certain indices of a tensor, is very common in scientific computing problems. This functionality was originally provided through an intimidating <code class="inlineCode">tf.scatter_nd()</code> function, which can be difficult to understand. </p>
    <p class="normal">However, in recent TensorFlow versions, you can perform scatter operations via array indexing and slicing using NumPy-like syntax. Let’s see a few examples. Say you have the TensorFlow variable v, which is a [3,2] matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">v = tf.Variable(tf.constant([[</code><span class="hljs-number">1</span><code class="inlineCode">,</code><span class="hljs-number">9</span><code class="inlineCode">],[</code><span class="hljs-number">3</span><code class="inlineCode">,</code><span class="hljs-number">10</span><code class="inlineCode">],[</code><span class="hljs-number">5</span><code class="inlineCode">,</code><span class="hljs-number">11</span><code class="inlineCode">]],dtype=tf.float32),name=</code><span class="hljs-string">'ref'</span><code class="inlineCode">)</code>
</code></pre>
    <p class="normal">You can change the 0<sup class="superscript">th</sup> row of this tensor with:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">v[</code><span class="hljs-number">0</span><code class="inlineCode">].assign([-</code><span class="hljs-number">1</span><code class="inlineCode">, -</code><span class="hljs-number">9</span><code class="inlineCode">])</code>
</code></pre>
    <p class="normal">which results in:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=
array([[-1., -9.],
       [ 3., 10.],
       [ 5., 11.]], dtype=float32)&gt;
</code></pre>
    <p class="normal">You can change<a id="_idIndexMarker181"/> the value<a id="_idIndexMarker182"/> at index [1,1] with:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">v[</code><span class="hljs-number">1</span><code class="inlineCode">,</code><span class="hljs-number">1</span><code class="inlineCode">].assign(-</code><span class="hljs-number">10</span><code class="inlineCode">)</code>
</code></pre>
    <p class="normal">which results in:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=
array([[  1.,   9.],
       [  3., -10.],
       [  5.,  11.]], dtype=float32)&gt;
</code></pre>
    <p class="normal">You can perform row slicing with:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">v[</code><span class="hljs-number">1</span><code class="inlineCode">:,</code><span class="hljs-number">0</span><code class="inlineCode">].assign([-</code><span class="hljs-number">3</span><code class="inlineCode">,-</code><span class="hljs-number">5</span><code class="inlineCode">])</code>
</code></pre>
    <p class="normal">which results in:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=
array([[ 1.,  9.],
       [-3., 10.],
       [-5., 11.]], dtype=float32)&gt;
</code></pre>
    <div class="note">
      <p class="normal">It is important to remember that the scatter operation (performed via the <code class="inlineCode">assign()</code> operation) can only be performed on <code class="inlineCode">tf.Variables</code>, which are mutable structures. Remember that <code class="inlineCode">tf.Tensor</code>/<code class="inlineCode">tf.EagerTensor</code> are immutable objects.</p>
    </div>
    <h3 id="_idParaDest-50" class="heading-3">Collecting (gathering) values from a tensor</h3>
    <p class="normal">A gather operation<a id="_idIndexMarker183"/> is very similar to a scatter operation. Remember<a id="_idIndexMarker184"/> that scattering is about assigning values to tensors, whereas gathering retrieves the values of a tensor. Let’s understand this through an example. Say you have a TensorFlow tensor, <code class="inlineCode">t</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">t = tf.constant([[</code><span class="hljs-number">1</span><code class="inlineCode">,</code><span class="hljs-number">9</span><code class="inlineCode">],[</code><span class="hljs-number">3</span><code class="inlineCode">,</code><span class="hljs-number">10</span><code class="inlineCode">],[</code><span class="hljs-number">5</span><code class="inlineCode">,</code><span class="hljs-number">11</span><code class="inlineCode">]],dtype=tf.float32)</code>
</code></pre>
    <p class="normal">You can obtain the 0<sup class="superscript">th</sup> row of <code class="inlineCode">t</code> with:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">t[</code><span class="hljs-number">0</span><code class="inlineCode">].numpy()</code>
</code></pre>
    <p class="normal">which will return:</p>
    <pre class="programlisting con"><code class="hljs-con">[1. 9.]
</code></pre>
    <p class="normal">You can also perform row-slicing with:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">t[</code><span class="hljs-number">1</span><code class="inlineCode">:,</code><span class="hljs-number">0</span><code class="inlineCode">].numpy()</code>
</code></pre>
    <p class="normal"> which will return:</p>
    <pre class="programlisting con"><code class="hljs-con">[3. 5.]
</code></pre>
    <p class="normal">Unlike the scatter<a id="_idIndexMarker185"/> operation, the gather operation<a id="_idIndexMarker186"/> works both on <code class="inlineCode">tf.Variable</code> and <code class="inlineCode">tf.Tensor</code> structures.</p>
    <h2 id="_idParaDest-51" class="heading-2">Neural network-related operations</h2>
    <p class="normal">Now, let’s look at several useful<a id="_idIndexMarker187"/> neural network-related operations that we will use heavily in the following chapters. The operations we will discuss here range from simple element-wise transformations (that is, activations) to computing partial derivatives of a set of parameters with respect to another value. We will also implement a simple neural network as an exercise.</p>
    <h3 id="_idParaDest-52" class="heading-3">Nonlinear activations used by neural networks</h3>
    <p class="normal">Nonlinear activations<a id="_idIndexMarker188"/> enable neural networks to perform well at numerous tasks. Typically, there is a nonlinear activation transformation (that is, activation layer) after each layer output in a neural network (except for the last layer). A nonlinear transformation helps a neural network to learn various nonlinear patterns that are present in data. This is very useful for complex real-world problems, where data often has more complex nonlinear patterns, in contrast to linear patterns. If not for the nonlinear activations between layers, a deep neural network would be a bunch of linear layers stacked on top of each other. Also, a set of linear layers can essentially be compressed to a single bigger linear layer. </p>
    <p class="normal">In conclusion, if not for the nonlinear activations, we cannot create a neural network with more than one layer.</p>
    <div class="note">
      <p class="normal">Let’s observe the importance of nonlinear activation through an example. First, recall the computation for the neural networks we saw in the sigmoid example. If we disregard b, it will be this:</p>
      <p class="center"><code class="inlineCode">h = sigmoid(W*x)</code></p>
      <p class="normal">Assume a three-layer neural network (having <code class="inlineCode">W1</code>, <code class="inlineCode">W2</code>, and <code class="inlineCode">W3</code> as layer weights) where each layer does the preceding computation; we can summarize the full computation as follows:</p>
      <p class="center"><code class="inlineCode">h = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))</code></p>
      <p class="normal">However, if we remove the nonlinear activation (that is, sigmoid), we get this:</p>
      <p class="center"><code class="inlineCode">h = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*x</code></p>
      <p class="normal">So, without the nonlinear activations, the three layers can be brought down to a single linear layer.</p>
    </div>
    <p class="normal">Now we’ll list two commonly used nonlinear activations<a id="_idIndexMarker189"/> in neural networks (in other words, sigmoid and ReLU) and how they can be implemented in TensorFlow:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Sigmoid activation of x is given by 1 / (1 + exp(-x))</span>
tf.nn.sigmoid(x,name=<span class="hljs-literal">None</span>)
<span class="hljs-comment"># ReLU activation of x is given by max(0,x) </span>
tf.nn.relu(x, name=<span class="hljs-literal">None</span>)
</code></pre>
    <p class="normal">The functional form of these computations is visualized in<em class="italic"> Figure 2.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.6: The functional forms of sigmoid (left) and ReLU (right) activations</p>
    <p class="normal">Next, we will discuss the convolution operation.</p>
    <h3 id="_idParaDest-53" class="heading-3">The convolution operation</h3>
    <p class="normal">A convolution operation is a widely<a id="_idIndexMarker190"/> used signal-processing technique. For images, convolution<a id="_idIndexMarker191"/> is used to produce different effects (such as blurring), or extract features (such as edges) from an image. An example of edge detection using convolution is shown in <em class="italic">Figure 2.7</em>. This is achieved by shifting a convolution filter on top of an image to produce a different output at each location (see <em class="italic">Figure</em><em class="italic"> 2.8 </em>later in this section). Specifically, at each location, we do element-wise multiplication of the elements in the convolution filter with the image patch (the same size as the convolution filter) that overlaps with the convolution filter and takes the sum of the multiplication:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_07.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_06.png"/></figure>
    <p class="packt_figref">Figure 2.7: Using the convolution operation for edge detection in an image (Source: <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)"><span class="url">https://en.wikipedia.org/wiki/Kernel_(image_processing)</span></a>)</p>
    <p class="normal">The following is the implementation of the convolution operation:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant(
    [[
        [[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>]],
        [[<span class="hljs-number">4</span>],[<span class="hljs-number">3</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">1</span>]],
        [[<span class="hljs-number">5</span>],[<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>],[<span class="hljs-number">8</span>]],
        [[<span class="hljs-number">8</span>],[<span class="hljs-number">7</span>],[<span class="hljs-number">6</span>],[<span class="hljs-number">5</span>]]
    ]],
    dtype=tf.float32)
x_filter = tf.constant(
    [ [ [[<span class="hljs-number">0.5</span>]],[[<span class="hljs-number">1</span>]] ],
      [ [[<span class="hljs-number">0.5</span>]],[[<span class="hljs-number">1</span>]] ] 
    ],
    dtype=tf.float32)
x_stride = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]
x_padding = <span class="hljs-string">'VALID'</span>
x_conv = tf.nn.conv2d(
    <span class="hljs-built_in">input</span>=x, filters=x_filter, strides=x_stride, padding=x_padding
)
</code></pre>
    <p class="normal">Here, the apparently excessive<a id="_idIndexMarker192"/> number of square brackets<a id="_idIndexMarker193"/> used might make you think that the example can be made easy to follow by getting rid of these redundant brackets. Unfortunately, that is not the case. For the <code class="inlineCode">tf.nn.conv2d(...)</code> operation, TensorFlow requires <code class="inlineCode">input</code>, <code class="inlineCode">filters</code>, and <code class="inlineCode">strides</code> to be of an exact format. We will now go through each argument in <code class="inlineCode">tf.conv2d(input, filters, strides, padding)</code> in more detail:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">input</strong>: This is typically a 4D tensor where the dimensions should be ordered as <code class="inlineCode">[batch_size, height, width, channels]</code>:<ul>
          <li class="bulletList"><strong class="keyWord">batch_size</strong>: This is the amount of data (for example, inputs such as images, and words) in a single batch of data. We normally process data in batches as large datasets are used for learning. At a given training step, we randomly sample a small batch of data that approximately represents the full dataset. And doing this for many steps allows us to approximate the full dataset quite well. This <code class="inlineCode">batch_size</code> parameter is the same as the one we discussed in the TensorFlow input pipeline example.</li>
          <li class="bulletList"><strong class="keyWord">height and width</strong>: This is the height and the width of the input.</li>
          <li class="bulletList"><strong class="keyWord">channels</strong>: This is the depth of an input (for example, for an RGB image, the number of channels will be 3—a channel for each color).</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">filters</strong>: This is a 4D tensor that represents the convolution window of the convolution operation. The filter dimensions should be <code class="inlineCode">[height, width, in_channels, out_channels]</code>:<ul>
          <li class="bulletList"><strong class="keyWord">height and width</strong>: This is the height and the width of the filter (often smaller than that of the input)</li>
          <li class="bulletList"><strong class="keyWord">in_channels</strong>: This is the number of channels of the input to the layer</li>
          <li class="bulletList"><strong class="keyWord">out_channels</strong>: This is the number of channels to be produced in the output of the layer</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">strides</strong>: This is a list with four elements, where the elements are <code class="inlineCode">[batch_stride, height_stride, width_stride, channels_stride]</code>. The <code class="inlineCode">strides</code> argument denotes how many elements to skip during a single shift of the convolution window on the input. Usually, you don’t have to worry about <code class="inlineCode">batch_stride</code> and <code class="inlineCode">channels_stride</code>. If you do not completely understand what <code class="inlineCode">strides</code> is, you can use the default value of <code class="inlineCode">1</code>.</li>
      <li class="bulletList"><strong class="keyWord">padding</strong>: This can be one of <code class="inlineCode">['SAME', 'VALID']</code>. It decides how to handle the convolution operation near the boundaries of the input. The <code class="inlineCode">VALID</code> operation performs the convolution without padding. If we were to convolve an input of <em class="italic">n</em> length with a convolution window of size <em class="italic">h</em>, this will result in an output of size (<em class="italic">n-h+1 &lt; n</em>). The diminishing of the output size can severely limit the depth of neural networks. <code class="inlineCode">SAME</code> pads zeros to the boundary such that the output will have the same height and width as the input.</li>
    </ul>
    <p class="normal">To gain a better understanding<a id="_idIndexMarker194"/> of what filter size, stride, and padding<a id="_idIndexMarker195"/> are, refer to <em class="italic">Figure 2.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_08.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_07.png"/></figure>
    <p class="packt_figref">Figure 2.8: The convolution operation. Note how the kernel is moved over the input to compute values at each position</p>
    <p class="normal">Next, we will discuss the pooling operation.</p>
    <h3 id="_idParaDest-54" class="heading-3">The pooling operation</h3>
    <p class="normal">A pooling operation<a id="_idIndexMarker196"/> behaves similarly to the convolution<a id="_idIndexMarker197"/> operation, but the final output is different. Instead of outputting the sum of the element-wise multiplication of the filter and the image patch, we now take the maximum element of the image patch for that location (see <em class="italic">Figure 2.9</em>):</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant(
    [[
        [[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>]],
        [[<span class="hljs-number">4</span>],[<span class="hljs-number">3</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">1</span>]],
        [[<span class="hljs-number">5</span>],[<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>],[<span class="hljs-number">8</span>]],
        [[<span class="hljs-number">8</span>],[<span class="hljs-number">7</span>],[<span class="hljs-number">6</span>],[<span class="hljs-number">5</span>]]
    ]],
    dtype=tf.float32)
x_ksize = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]
x_stride = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]
x_padding = <span class="hljs-string">'VALID'</span>
x_pool = tf.nn.max_pool2d(
    <span class="hljs-built_in">input</span>=x, ksize=x_ksize,
    strides=x_stride, padding=x_padding
)
<span class="hljs-comment"># Returns (out) =&gt; [[[[ 4.],[ 4.]],[[ 8.],[ 8.]]]]</span>
</code></pre>
    <figure class="mediaobject"><img src="../Images/B14070_02_09.png" alt="C:\Users\gauravg\Desktop\14070\CH02\B08681_02_08.png"/></figure>
    <p class="packt_figref">Figure 2.9: The max-pooling operation</p>
    <h3 id="_idParaDest-55" class="heading-3">Defining loss</h3>
    <p class="normal">We know that, for a neural network<a id="_idIndexMarker198"/> to learn something useful, a loss needs<a id="_idIndexMarker199"/> to be defined. The loss represents how close or far away the predictions are from actual targets. There are several functions for automatically calculating the loss in TensorFlow, two of which are shown in the following code. The <code class="inlineCode">tf.nn.l2_loss</code> function is the mean squared error loss, and <code class="inlineCode">tf.nn.softmax_cross_entropy_with_logits</code> is another type of loss that actually gives better performance in classification tasks. And by logits here, we mean the unnormalized output of the neural network (that is, the linear output of the last layer of the neural network):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Returns half of L2 norm of t given by sum(t**2)/2</span>
x = tf.constant([[<span class="hljs-number">2</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">6</span>,<span class="hljs-number">8</span>]],dtype=tf.float32)
x_hat = tf.constant([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]],dtype=tf.float32)
<span class="hljs-comment"># MSE = (1**2 + 2**2 + 3**2 + 4**2)/2 = 15</span>
MSE = tf.nn.l2_loss(x-x_hat)
<span class="hljs-comment"># A common loss function used in neural networks to optimize the network</span>
<span class="hljs-comment"># Calculating the cross_entropy with logits (unnormalized outputs of the last layer)</span>
<span class="hljs-comment"># instead of probabilsitic outputs leads to better numerical stabilities</span>
y = tf.constant([[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],dtype=tf.float32)
y_hat = tf.constant([[<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">5</span>]],dtype=tf.float32)
<span class="hljs-comment"># This function alone doesn't average the cross entropy losses of all data points,</span>
<span class="hljs-comment"># You need to do that manually using reduce_mean function</span>
CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=y))
</code></pre>
    <p class="normal">Here, we discussed several<a id="_idIndexMarker200"/> important operations<a id="_idIndexMarker201"/> intertwined with neural networks, such as the convolution operation and the pooling operation. We will now discuss how a sub-library in TensorFlow known as Keras can be used to build models.</p>
    <h1 id="_idParaDest-56" class="heading-1">Keras: The model building API of TensorFlow</h1>
    <p class="normal">Keras was developed<a id="_idIndexMarker202"/> as a separate library that provides high-level building blocks to build models conveniently. It was initially platform-agnostic and supported many softwares (for example, TensorFlow and Theano). </p>
    <p class="normal">However, TensorFlow acquired Keras and now is an integral part of TensorFlow for building models effortlessly.</p>
    <p class="normal">Keras’s primary focus is model building. For that, Keras<a id="_idIndexMarker203"/> provides several different APIs with varying degrees of flexibility and complexity. Choosing the right API for the job will require sound knowledge of the limitations of each API as well as experience. The APIs provided by Keras are:</p>
    <ul>
      <li class="bulletList">Sequential API – The most easy-to-use API. In this<a id="_idIndexMarker204"/> API, you simply stack layers<a id="_idIndexMarker205"/> on top of each other to create a model.</li>
      <li class="bulletList">Functional API – The functional API provides<a id="_idIndexMarker206"/> more flexibility by allowing<a id="_idIndexMarker207"/> you to define custom models that can have multiple input layers/multiple output layers.</li>
      <li class="bulletList">Sub-classing API – The sub-classing API enables<a id="_idIndexMarker208"/> you to define custom reusable layers/models as Python classes. This is the most flexible API, but it requires strong familiarity with the API and raw TensorFlow operations<a id="_idIndexMarker209"/> to use it correctly.</li>
    </ul>
    <div class="note">
      <p class="normal">Do not confuse the<a id="_idIndexMarker210"/> Keras TensorFlow sub-module (<a href="https://www.tensorflow.org/api_docs/python/tf/keras"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras</span></a>) with the external Keras library (<a href="https://keras.io/"><span class="url">https://keras.io/</span></a>). They share roots <a id="_idIndexMarker211"/>in terms of where they’ve come from, but they are not the same. You will run into strange issues if you treat them as the same during your development. In this book, we exclusively use <code class="inlineCode">tf.keras</code>.</p>
    </div>
    <p class="normal">One of the most innate concepts in Keras is that a model is composed of one or more layers connected in a specific way. Here, we will briefly go through what the code looks like, using different APIs to develop models. You are not expected to fully understand the code below. Rather, focus on the code style to spot any differences between the three methods.</p>
    <h2 id="_idParaDest-57" class="heading-2">Sequential API</h2>
    <p class="normal">When using the Sequential API, you simply<a id="_idIndexMarker212"/> define your model as a list of layers. Here, the first element<a id="_idIndexMarker213"/> in the list is the closest to the input, where the last is the output layer:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Sequential([
        tf.keras.layers.Dense(<span class="hljs-number">500</span>, activation=<span class="hljs-string">'relu'</span>, shape=(<span class="hljs-number">784</span>, )),
        tf.keras.layers.Dense(<span class="hljs-number">250</span>, activation=<span class="hljs-string">'relu'</span>),
        tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)
    ])
</code></pre>
    <p class="normal">In the preceding code, we have three layers. The first layer has 500 output nodes and takes in a vector of 784 elements as the input. The second layer is automatically connected to the first one, whereas <a id="_idIndexMarker214"/>the last layer is connected<a id="_idIndexMarker215"/> to the second layer. All of these layers are fully-connected layers, where all input nodes are connected to all output nodes.</p>
    <h2 id="_idParaDest-58" class="heading-2">Functional API</h2>
    <p class="normal">In the Functional API, we do things<a id="_idIndexMarker216"/> differently. We first define one or more input layers, and other layers<a id="_idIndexMarker217"/> that carry computations. Then we connect the inputs to outputs ourselves, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">inp = tf.keras.layers.Input(shape=(<span class="hljs-number">784</span>,))
out_1 = tf.keras.layers.Dense(<span class="hljs-number">500</span>, activation=<span class="hljs-string">'relu'</span>)(inp)
out_2 = tf.keras.layers.Dense(<span class="hljs-number">250</span>, activation=<span class="hljs-string">'relu'</span>)(out_1)
out = tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)(out_2)
model = tf.keras.models.Model(inputs=inp, outputs=out)
</code></pre>
    <p class="normal">In the code, we start with an input layer that accepts a 784 element-long vector. The input is passed to a Dense layer that has 500 nodes. The output of that layer is assigned to <code class="inlineCode">out_1</code>. Then <code class="inlineCode">out_1</code> is passed to another Dense layer, which outputs <code class="inlineCode">out_2</code>. Next, a Dense layer with 10 nodes outputs the final output. Finally, the model is defined as a <code class="inlineCode">tf.keras.models.Model</code> object that takes two arguments: </p>
    <ul>
      <li class="bulletList">inputs – One or more input layers</li>
      <li class="bulletList">outputs – One or more outputs produced by any <code class="inlineCode">tf.keras.layers</code> type object</li>
    </ul>
    <p class="normal">The model is identical<a id="_idIndexMarker218"/> to what was defined in the previous section. One of the benefits<a id="_idIndexMarker219"/> of the Functional API is that you can create far more complex models as you’re not bounded to have layers as a list. Because of this freedom, you can have multiple inputs connecting to many layers in many different ways and potentially produce many outputs as well.</p>
    <h2 id="_idParaDest-59" class="heading-2">Sub-classing API</h2>
    <p class="normal">Finally, we will use the sub-classing API<a id="_idIndexMarker220"/> to define a model. With sub-classing, you define<a id="_idIndexMarker221"/> your model as a Python object that inherits from the base object, <code class="inlineCode">tf.keras.Model</code>. When using sub-classing, you need to define two important functions: <code class="inlineCode">__init__()</code>, which will specify any special parameters, layers, and so on required to successfully perform the computations, and <code class="inlineCode">call()</code>, which defines the computations that need to happen in the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span>(tf.keras.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_classes</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.hidden1_layer = tf.keras.layers.Dense(<span class="hljs-number">500</span>, activation=<span class="hljs-string">'relu'</span>)
        self.hidden2_layer = tf.keras.layers.Dense(<span class="hljs-number">250</span>, activation=<span class="hljs-string">'relu'</span>)
        self.final_layer = tf.keras.layers.Dense(num_classes, activation=<span class="hljs-string">'softmax'</span>)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs</span>):
        h = self.hidden1_layer(inputs)
        h = self.hidden2_layer(h)
        y = self.final_layer(h)
        <span class="hljs-keyword">return</span> y
    
    
model = MyModel(num_classes=<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">Here, you can see that our model<a id="_idIndexMarker222"/> has three layers, just like all the previous models we defined. Next, the call function<a id="_idIndexMarker223"/> defines how these layers connect to produce the final output. The sub-classing API is considered the most difficult to master, mainly due to the freedom allowed by the method. However, the rewards are immense once you learn the API as it enables you to define very complex models/layers as unit computations that can be reused later. Now that you understand how each API works, let’s implement a neural network using Keras and train it on a dataset.</p>
    <h1 id="_idParaDest-60" class="heading-1">Implementing our first neural network</h1>
    <p class="normal">Great! Now that you’ve learned the architecture<a id="_idIndexMarker224"/> and foundations of TensorFlow, it’s high time that we move on and implement something slightly more complex. Let’s implement a neural network. Specifically, we will implement a fully connected neural network model (FCNN), which we discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Natural Language Processing</em>.</p>
    <p class="normal">One of the stepping stones to the introduction of neural networks is to implement a neural network that is able to classify digits. For this task, we will be using<a id="_idIndexMarker225"/> the famous MNIST dataset made available at <a href="http://yann.lecun.com/exdb/mnist/"><span class="url">http://yann.lecun.com/exdb/mnist/</span></a>.</p>
    <p class="normal">You might feel a bit skeptical regarding our using a computer vision task rather than an NLP task. However, vision tasks can be implemented with less preprocessing and are easy to understand.</p>
    <p class="normal">As this is our first encounter with neural networks, we will see how to implement this model using Keras. Keras is the high-level submodule that provides a layer of abstraction over TensorFlow. Therefore, you can implement neural networks with much less effort with Keras than using TensorFlow’s raw<a id="_idIndexMarker226"/> operations. To run the examples end to end, you can find the full exercise in the <code class="inlineCode">tensorflow_introduction.ipynb</code> file in the <code class="inlineCode">Ch02-Understanding-TensorFlow</code> folder. The next step is to prepare the data.</p>
    <h2 id="_idParaDest-61" class="heading-2">Preparing the data</h2>
    <p class="normal">First, we need to download<a id="_idIndexMarker227"/> the dataset. TensorFlow out of the box provides convenient functions to download data and MNIST is one of those supported datasets. We will be performing four important steps during the data preparation:</p>
    <ul>
      <li class="bulletList">Downloading the data and storing it as <code class="inlineCode">numpy.ndarray</code> objects. We will create a folder named data within our <code class="inlineCode">ch2</code> directory and store the data there.</li>
      <li class="bulletList">Reshaping the images so that 2D grayscale images in the dataset will be converted to 1D vectors.</li>
      <li class="bulletList">Standardizing<a id="_idIndexMarker228"/> the images to have a zero-mean and unit-variance (also known as <strong class="keyWord">whitening</strong>).</li>
      <li class="bulletList">One-hot encoding the integer class labels. One-hot encoding<a id="_idIndexMarker229"/> refers to the process of representing integer class labels as a vector. For example, if you have 10 classes and a class label of 3 (where labels range from 0-9), your one-hot encoded vector will be <code class="inlineCode">[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</code>.</li>
    </ul>
    <p class="normal">The following code performs these functions for us:</p>
    <pre class="programlisting code"><code class="hljs-code">os.makedirs(<span class="hljs-string">'data'</span>, exist_ok=<span class="hljs-literal">True</span>)
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(
    path=os.path.join(os.getcwd(), <span class="hljs-string">'data'</span>, <span class="hljs-string">'mnist.npz'</span>)
)
<span class="hljs-comment"># Reshaping x_train and x_test tensors so that each image is represented</span>
<span class="hljs-comment"># as a 1D vector</span>
x_train = x_train.reshape(x_train.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)
x_test = x_test.reshape(x_test.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)
<span class="hljs-comment"># Standardizing x_train and x_test tensors</span>
x_train = (
    x_train - np.mean(x_train, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)
)/np.std(x_train, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)
x_test = (
    x_test - np.mean(x_test, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)
)/np.std(x_test, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># One hot encoding y_train and y_test</span>
y_onehot_train = np.zeros((y_train.shape[<span class="hljs-number">0</span>], num_labels), dtype=np.float32)
y_onehot_train[np.arange(y_train.shape[<span class="hljs-number">0</span>]), y_train] = <span class="hljs-number">1.0</span>
y_onehot_test = np.zeros((y_test.shape[<span class="hljs-number">0</span>], num_labels), dtype=np.float32)
y_onehot_test[np.arange(y_test.shape[<span class="hljs-number">0</span>]), y_test] = <span class="hljs-number">1.0</span>
</code></pre>
    <p class="normal">You can see that we are using the <code class="inlineCode">tf.keras.datasets.mnist.load_data()</code> function provided by TensorFlow<a id="_idIndexMarker230"/> to download the training and testing data. It will be downloaded to a folder named <code class="inlineCode">data</code> within the <code class="inlineCode">Ch02-Understanding-TensorFlow</code> folder. This will provide four output tensors:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">x_train</code> – A 60000 x 28 x 28 sized tensor where each image is 28 x 28</li>
      <li class="bulletList"><code class="inlineCode">y_train</code> – A 60000 sized vector, where each element is a class label between 0-9</li>
      <li class="bulletList"><code class="inlineCode">x_test</code> – A 10000 x 28 x 28 sized tensor</li>
      <li class="bulletList"><code class="inlineCode">y_test</code> – A 10000 sized vector</li>
    </ul>
    <p class="normal">Once the data is downloaded, we reshape the 28 x 28 sized images into a 1D vector. This is because we will be implementing a fully connected neural network. Fully connected neural networks take a 1D vector as the input. Therefore, all the pixels in the image will be arranged as a sequence of pixels in order to feed into the model. Finally, if you look at the range of values present in the <code class="inlineCode">x_train</code> and <code class="inlineCode">x_test</code> tensors, they will be in the range of 0-255 (typical grayscale range). We would bring these values to a zero mean unit-variance range by subtracting the mean of each image and dividing by the standard deviation.</p>
    <h2 id="_idParaDest-62" class="heading-2">Implementing the neural network with Keras</h2>
    <p class="normal">Let’s now examine how to implement<a id="_idIndexMarker231"/> the type of neural network <a id="_idIndexMarker232"/>we discussed in <em class="chapterRef">Chapter 1, Introduction to Natural Language Processing</em>, with Keras. The network is a fully connected neural network with 3 layers having 500, 250, and 10 nodes, respectively. The first two layers will use ReLU activation, whereas the last layer uses softmax. To implement this, we are going to use the simplest of the Keras APIs available to us – the Sequential API. </p>
    <p class="normal">You can find the full exercise in the <code class="inlineCode">tensorflow_introduction.ipynb</code> file in the <code class="inlineCode">Ch02-Understanding-TensorFlow</code> folder:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Sequential([
        tf.keras.layers.Dense(<span class="hljs-number">500</span>, activation=<span class="hljs-string">'relu'</span>),
        tf.keras.layers.Dense(<span class="hljs-number">250</span>, activation=<span class="hljs-string">'relu'</span>),
        tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)
    ])
</code></pre>
    <p class="normal">You can see that all it takes<a id="_idIndexMarker233"/> is a single line in the Keras Sequential API to define the model we just defined. Keras provides various types of layers. You can<a id="_idIndexMarker234"/> see the full list of layers available to you at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/layers</span></a>. For a fully connected network, we only need Dense layers that mimic the computations of a hidden layer in a fully connected network. With the model defined, you need to compile this model with an appropriate loss function, an optimizer, and, optionally, performance metrics:</p>
    <pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.RMSprop()
loss_fn = tf.keras.losses.CategoricalCrossentropy()
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer, loss=loss_fn, metrics=[<span class="hljs-string">'acc'</span>])
</code></pre>
    <p class="normal">With the model defined and compiled, we can now train our model on the prepared data.</p>
    <h3 id="_idParaDest-63" class="heading-3">Training the model</h3>
    <p class="normal">Training a model<a id="_idIndexMarker235"/> could not be easier in Keras. Once the data is prepared, all you need to do is call the <code class="inlineCode">model.fit()</code> function with the required arguments:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">100</span>
num_epochs = <span class="hljs-number">10</span>
train_history = model.fit(
    x=x_train, 
    y=y_onehot_train, 
    batch_size=batch_size, 
    epochs= num_epochs, 
    validation_split=<span class="hljs-number">0.2</span>
)
</code></pre>
    <p class="normal"><code class="inlineCode">model.fit()</code> accepts several important arguments. We will go through them in more detail here:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">x </code>– An input tensor. In our case, this is a 60000 x 784 sized tensor.</li>
      <li class="bulletList"><code class="inlineCode">y</code> – The one-hot encoded <a id="_idIndexMarker236"/>label tensor. In our case, this is a 60000 x 10 sized tensor.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> – Deep learning models are trained with batches of data (in other words, stochastically) as opposed to feeding the full dataset at once. The batch size defines how many examples are included in a single batch. The larger the batch size, the better the accuracy of your model would be generally.</li>
      <li class="bulletList"><code class="inlineCode">epochs</code> – Deep learning models iterate through the dataset in batches several times. The number of times iterated through the dataset is known as the number of epochs. In our example, this is set to 10.</li>
      <li class="bulletList"><code class="inlineCode">validation_split</code> – When training deep learning models, a validation set is used to monitor performance, where the validation set acts as a proxy for real-world performance. <code class="inlineCode">validation_split</code> defines how much of the full dataset is to be used as the validation subset. In our example, this is set to 20% of the total dataset size.</li>
    </ul>
    <p class="normal">Here’s what the training loss and validation accuracy look like over the number of epochs we trained the model (<em class="italic">Figure 2.10</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_02_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.10: Training loss and validation accuracy over 10 epochs as the model is trained</p>
    <p class="normal">Next up is testing our model on some unseen data.</p>
    <h3 id="_idParaDest-64" class="heading-3">Testing the model</h3>
    <p class="normal">Testing the model<a id="_idIndexMarker237"/> is also straightforward. During testing, we measure the loss and the accuracy of the model on the test dataset. In order to evaluate the model on a dataset, Keras models provide a convenient function called <code class="inlineCode">evaluate()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">test_res = model.evaluate(
    x=x_test, 
    y=y_onehot_test, 
    batch_size=batch_size
)
</code></pre>
    <p class="normal">The arguments expected by the <code class="inlineCode">model.evaluate()</code> function are already covered during our discussion of <code class="inlineCode">model.fit()</code>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">x</code> – An input tensor. In our case, this is a 10000 x 784 sized tensor.</li>
      <li class="bulletList"><code class="inlineCode">y</code> – The one-hot encoded label tensor. In our case, this is a 10000 x 10 sized tensor.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> – Batch size defines how many examples are included in a single batch. The larger the batch size, the better the accuracy of your model would be generally.</li>
    </ul>
    <p class="normal">You will get a loss of 0.138 and an accuracy of 98%. You will not get the exact same values due to various randomness present in the model, as well as during training.</p>
    <p class="normal">In this section, we went through an end-to-end example of training a neural network. We prepared the data, trained the model on that data, and finally tested it on some unseen data.</p>
    <h1 id="_idParaDest-65" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, you took your first steps to solving NLP tasks by understanding the primary underlying platform (TensorFlow) on which we will be implementing our algorithms. First, we discussed the underlying details of TensorFlow architecture. Next, we discussed the essential ingredients of a meaningful TensorFlow program. We got to know some new features in TensorFlow 2, such as the AutoGraph feature, in depth. We then discussed more exciting elements in TensorFlow such as data pipelines and various TensorFlow operations. </p>
    <p class="normal">Specifically, we discussed the TensorFlow architecture by lining up the explanation with an example TensorFlow program; the sigmoid example. In this TensorFlow program, we used the AutoGraph feature to generate a TensorFlow graph; that is, using the <code class="inlineCode">tf.function()</code> decorator over the function that performs the TensorFlow operations. Then, a <code class="inlineCode">GraphDef</code> object was created representing the graph and sent to the distributed master. The distributed master looked at the graph, decided which components to use for the relevant computation, and divided it into several subgraphs to make the computations faster. Finally, workers executed subgraphs and returned the result immediately.</p>
    <p class="normal">Next, we discussed the various elements that comprise a typical TensorFlow client: inputs, variables, outputs, and operations. Inputs are the data we feed to the algorithm for training and testing purposes. We discussed three different ways of feeding inputs: using NumPy arrays, preloading data as TensorFlow tensors, and using <code class="inlineCode">tf.data</code> to define an input pipeline. Then we discussed TensorFlow variables, how they differ from other tensors, and how to create and initialize them. Following this, we discussed how variables can be used to create intermediate and terminal outputs. </p>
    <p class="normal">Finally, we discussed several available TensorFlow operations, including mathematical operations, matrix operations, and neural network-related operations that will be used later in the book.</p>
    <p class="normal">Later, we discussed Keras, a sub-module in TensorFlow that supports building models. We learned that there are three different APIs for building models: the Sequential API, the Functional API, and the Sub-classing API. We learned that the Sequential API is the easiest to use, whereas the Sub-classing API takes much more effort. However, the Sequential API is very restrictive in terms of the type of models that can be implemented with it.</p>
    <p class="normal">Finally, we implemented a neural network using all the concepts learned previously. We used a three-layer neural network to classify a MNIST digit dataset, and we used Keras (a high-level sub-module in TensorFlow) to implement this model.</p>
    <p class="normal">In the next chapter, we will see how to use the fully connected neural network we implemented in this chapter for learning the semantic, numerical word representation of words.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"> <img src="../Images/QR_Code5143653472357468031.png" alt=""/></figure>
  </div>
</body></html>