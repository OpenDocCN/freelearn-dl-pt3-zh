- en: Semantic Segmentation and Custom Dataset Builder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll analyze semantic segmentation and the challenges that
    come with it. Semantic segmentation is the challenging problem of classifying
    every single pixel of an image with the correct semantic label. The first part
    of this chapter presents the problem itself, why it is important, and what are
    the possible applications. At the end of the first part, we will discuss the well-known
    U-Net architecture for semantic segmentation, and we will implement it as a Keras
    model in pure TensorFlow 2.0 style. The model implementation is preceded by the
    introduction of the deconvolution operation required to implement semantic segmentation
    networks successfully.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this chapter starts with dataset creation—since, at the time
    of writing, there is no `tfds` builder for semantic segmentation, we take advantage
    of this to introduce the TensorFlow Datasets architecture and show how to implement
    a custom DatasetBuilder. After getting the data, we'll perform the training process
    of U-Net step by step, showing how straightforward it is to train this model using
    Keras and Keras callbacks. This chapter ends with the usual exercise section,
    perhaps the most crucial part of this whole chapter. The only way to understand
    a concept is to get your hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a TensorFlow DatasetBuilder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Different from object detection, where the goal is to detect objects in rectangular
    regions, and image classification, which has the purpose of classifying the whole
    image with a single label, semantic segmentation is a challenging computer vision
    task, the goal of which is to assign the correct label to every pixel of the input
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f1d9dcf-dd97-40ee-af55-939fb21f97ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of semantically annotated images from the CityScapes dataset. Every
    single pixel of the input image has a corresponding pixel-label. (Source: [https://www.cityscapes-dataset.com/examples/](https://www.cityscapes-dataset.com/examples/))
  prefs: []
  type: TYPE_NORMAL
- en: The applications of semantic segmentation are countless, but perhaps the most
    important ones are in the autonomous driving and medical imaging domains.
  prefs: []
  type: TYPE_NORMAL
- en: Automated guided vehicles and self-driving cars can take advantage of semantic
    segmentation results, getting a complete understanding of the whole scene captured
    by the cameras mounted on the vehicle. For example, having the pixel-level information
    of the road can help the driving software have better control of the position
    of the car. Localizing the road using a bounding box is far less accurate than
    having a pixel-level classification that localizes the road pixels independently
    from the perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In the medical imaging domain, the bounding boxes predicted by an object detector
    are sometimes useful and other times not. In fact, if the task is the detection
    of a specific type of cell, a bounding box can give the user enough information.
    But if, instead, the task is to localize blood vessels, then using a bounding
    box is not enough. As it is easy to imagine, a fine-grained classification is
    not an easy task, and there are several challenges to face from both the theoretical
    and practical points of view.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the tough challenges is to get the correct data. There are several enormous
    datasets of labeled images since the process of classifying an image by its main
    content is relatively fast. A team of professional annotators can easily label
    thousands of images a day since the task only consists of looking at the picture
    and selecting a label.
  prefs: []
  type: TYPE_NORMAL
- en: There are also a lot of object detection datasets, where multiple objects have
    been localized and classified. The process requires more annotation time with
    respect to the classification alone, but as it is something that does not require
    extreme accuracy, it is a relatively fast process.
  prefs: []
  type: TYPE_NORMAL
- en: The semantic segmentation dataset, instead, requires specialized software and
    very patient annotators that are extremely accurate in their work. In fact, the
    process of labeling with pixel-level accuracy is perhaps the most time-consuming
    process of all of the annotation types. For this reason, the number of semantic
    segmentation datasets is low, and their number of images is limited. As we will
    see in the next section, dedicated to dataset creation, PASCAL VOC 2007, which
    contains 24,640 annotated objects for the image classification and localization
    task, only contains approximately 600 labeled images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another challenge that semantic segmentation brings is technical. Classifying
    every single pixel of an image requires designing convolutional architectures
    in a different way with respect to the ones seen so far. All of the architectures
    described so far followed the same structure:'
  prefs: []
  type: TYPE_NORMAL
- en: One input layer, which defines the input resolution expected by the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature extractor part, which is a stack of several convolution operations
    with different strides or with pooling operations in between, which, layer-by-layer,
    reduce the spatial extent of the feature maps until it is reduced to a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classification part which, given the feature vector produced by the feature
    extractor, is trained to classify this low-dimensional representation to a fixed
    number of classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, a regression head, which uses the same features to produce a set
    of four coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task of semantic segmentation, however, cannot follow this structure since,
    if the feature extractor only reduces the input resolution layer-by-layer, how
    can the network produce a classification for every pixel in the input image?
  prefs: []
  type: TYPE_NORMAL
- en: One of the proposed solutions is the deconvolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: Deconvolution – transposed convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start this section by saying that the term deconvolution is misleading.
    In fact, in mathematics and engineering, the deconvolution operation exists but
    has very little in common with what deep learning practitioners intend with this
    term.
  prefs: []
  type: TYPE_NORMAL
- en: In this domain, a deconvolution operation is a transposed convolution operation,
    or even an image resize, followed by a standard convolution operation. Yes, two
    different implementations are named in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: The deconvolution operation in deep learning just guarantees that, if a feature
    map is a result of a convolution between an input map and a kernel with a certain
    size and stride, the deconvolution operation will produce a feature map with the
    same spatial extent of the input, if applied with the same kernel size and stride.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, a standard convolution is performed with a pre-processed input
    where a zero padding is added not only at the borders but also within the feature
    map cells. The following diagram should help to clarify the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b009c6eb-460e-4357-97de-2b631cbba889.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image and caption source: A guide to convolution arithmetic for deep learning—Vincent
    Dumoulin and Francesco Visin'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow, through the `tf.keras.layers` package, offers a ready-to-use deconvolution
    operation: `tf.keras.layers.Conv2DTranspose`.
  prefs: []
  type: TYPE_NORMAL
- en: Another possible way of performing the deconvolution is to resize the input
    to the desired resolution and make this operation learnable by adding a standard
    2D convolution with same padding on top of the resized image.
  prefs: []
  type: TYPE_NORMAL
- en: In short, what really matters in the deep learning context is creating a learnable
    layer that reconstructs the original spatial resolution and performs a convolution.
    This is not the mathematical inverse of the convolution operation, but the practice
    has shown that it is enough to achieve good results.
  prefs: []
  type: TYPE_NORMAL
- en: One of the semantic segmentation architectures that used the deconvolution operation
    extensively and achieved impressive results in the task of the segmentation of
    medical images is the U-Net architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: U-Net is a convolutional architecture for semantic segmentation introduced by
    Olaf Ronnerberg et al. in *Convolutional Networks for Biomedical Image Segmentation*
    with the explicit goal of segmenting biomedical images.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture revealed itself to be general enough to be applied in every
    semantic segmentation task since it has been designed without any constraints
    about the datatypes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The U-Net architecture follows the typical encoder-decoder architectural pattern
    with skip connections. This way of designing the architecture has proven to be
    very effective when the goal is to produce an output with the same spatial resolution
    of the input since it allows the gradients to propagate between the output and
    the input layer in a better way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6befbb85-9184-4b00-9814-eece8624ae3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The U-Net architecture. The blue boxes are the feature maps produced by the
    blocks, denoted with their shapes. The white boxes are copied and cropped feature
    maps. Different arrows indicate different operations. Source: Convolutional Networks
    for Biomedical Image Segmentation—Olaf Ronnerberg et al.'
  prefs: []
  type: TYPE_NORMAL
- en: The left side of the U-Net architecture is an encoder that, layer-by-layer,
    reduces the input size from 572 x 572 to 32 x 32 in the lowest resolution. The
    right side contains the encoder part of the architecture, which mixes information
    extracted from the encoding part to the information learned by the up-convolution
    (deconvolution) operations.
  prefs: []
  type: TYPE_NORMAL
- en: The original U-Net architecture does not produce an output with the same resolution
    of the input, but it has been designed to produce a slightly lower resolution
    output. A final 1 x 1 convolution is used as the final layer to map each feature
    vector (with a depth of 64) to the desired number of classes. For a complete assessment
    of the original architecture, carefully read the original U-Net paper by Olaf
    Ronnerberg et al. in *Convolutional Networks for Biomedical Image Segmentation*.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of implementing the original U-net architecture, we are going to show
    how to implement a slightly modified U-Net that produces an output with the same
    resolution of the input and that follows the same original block organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from the screenshot of the architecture, there are two main
    blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoding blocks**: There are three convolutions followed by a downsampling
    operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoding blocks**: This is a deconvolution operation, followed by the concatenation
    of its output with the corresponding input feature, and two convolution operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible and really easy to use the Keras functional API to define this
    model and connect these logical blocks. The architecture we are going to implement
    differs a little from the original one, since this is a custom U-Net variation,
    and it shows how Keras allows the use of models as layers (or building blocks).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `upsample` and `downsample` functions are implemented as a `Sequential`
    model, which is nothing but a convolution or deconvolution operation, with a stride
    of `2`, followed by an activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The model definition function supposes a minimum input resolution of 256 x
    256, and it implements the encoding, decoding, and concatenate (skip connection)
    blocks of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using Keras, it is possible to visualize not only the tabular summary of the
    model (by using the `summary()` method of a Keras model), but also to get a graphical
    representation of the created model, which is often a blessing when designing
    complex architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'These three lines of code, generate this great graphical representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0aa2463f-1c2b-4ae3-9734-7b552846b844.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical representation of the U-Net-like structure defined. Keras allows this
    kind of visualization to help the architecture design process.
  prefs: []
  type: TYPE_NORMAL
- en: The generated image looks like the horizontally flipped version of the U-net
    architecture, and this is the architecture we are going to use to tackle the semantic
    segmentation problem in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the problem and defined a deep architecture, we
    can move forward and gather the required data.
  prefs: []
  type: TYPE_NORMAL
- en: Create a TensorFlow DatasetBuilder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the same way as any other machine learning problem, the first step is getting
    the data. Since semantic segmentation is a supervised learning task, we need a
    classification dataset of images and corresponding labels. The peculiarity is
    that the label in itself is an image.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, there is no semantic dataset ready to use in TensorFlow
    Datasets. For this reason, we use this section not only to create `tf.data.Dataset`
    with the data that we need, but also to have a look at the process required to
    develop a `tfds` DatasetBuilder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since, in the previous section dedicated to the object detection, we used the
    PASCAL VOC 2007 dataset, we are going to reuse the downloaded files to create
    the semantic segmentation version of the PASCAL VOC 2007 dataset. The following
    screenshot shows how the dataset is provided. Each picture has a corresponding
    label, where the pixel color identifies a different class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee25a449-6ddc-4c91-a7b9-5461e524bcb5.png)'
  prefs: []
  type: TYPE_IMG
- en: A pair (image, label) sampled from the dataset. The top image is the original
    image, while the bottom image contains the semantic segmentation class of the
    known objects. Every not know class is marked as background (color black), while
    the objects are delimited using the color white.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset previously downloaded comes not only with the annotated bounding
    boxes but also with the semantic segmentation annotation for many images. TensorFlow
    Datasets downloaded the raw data in the default directory (`~/tensorflow_datasets/downloads/`)
    and placed the extracted archive in the `extracted` subfolder. We can, therefore,
    re-use the downloaded data to create a new dataset for semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Before doing it, it is worth looking at the TensorFlow dataset organization
    to understand what we need to do to achieve our goal.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical organization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole TensorFlow Datasets API has been designed to be as extensible as
    possible. To do that, the architecture of TensorFlow Datasets is organized in
    several abstraction layers that transform the raw dataset data to the `tf.data.Dataset`
    object. The following diagram, from the TensorFlow Dataset GitHub page ([https://github.com/tensorflow/datasets/](https://github.com/tensorflow/datasets/)),
    shows the logical organization of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21ca6bd5-559f-4e4a-b421-e134598022f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The logical organization of the TensorFlow Datasets project. The raw data flows
    from several abstraction layers that apply transformation and standardizations,
    in order to define the TFRecord structure and obtain a tf.data.Dataset object
    at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the `FeatureConnector` and `FileFormatAdapter` classes are ready to
    use, while the `DatasetBuilder` class must be correctly implemented since it is
    the data-specific part of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each dataset creation pipeline starts from a subclass of a `DatasetBuilder`
    object that must implement the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_info` is used to build the `DatasetInfo` object that describes the dataset
    (and produces the human-readable representation that is extremely useful to have
    a complete understanding of the data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_download_and_prepare` is used to download the data from a remote location
    (if any) and do some basic preprocessing (such as extracting the compressed archives).
    Moreover, it creates the serialized (TFRecord) representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_as_dataset`: This is the final step, to produce a `tf.data.Dataset` object
    from the serialized data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subclassing directly, the `DatasetBuilder` class is often not needed since `GeneratorBasedBuilder`
    is a ready-to-use subclass of `DatasetBuilder` that simplifies the dataset definition.
    The methods to implement by subclassing it are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_info` is the same method of `DatasetBuilder` (see the `_info` method description
    of the previous bullet list).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_split_generators` is used to download the raw data and do some basic preprocessing
    but without the need to worry about TFRecord creation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_generate_examples` is used to create a Python iterator. This method yields
    examples in the dataset from the raw data, where every example will be automatically
    serialized as a row in a TFRecord.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, by subclassing `GeneratorBasedBuilder`, there are only three simple
    methods to implement, and we can hence start implementing them.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset class and DatasetInfo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Subclassing a model and implementing the required methods is straightforward.
    The first step is to define the skeleton of our class and then start by implementing
    the methods in the order of complexity. Moreover, since our goal is to create
    a dataset for semantic segmentation that uses the same downloaded files of the
    PASCAL VOC 2007 dataset, we can override the methods of the `tfds.image.Voc2007`
    DatasetBuilder to reuse all of the information already present in the parent class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The most straightforward, but perhaps the most important method, to implement
    is `_info`, which contains all of the dataset information and the definition of
    the structure of a single example.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are extending the `tfds.image.Voc2007` dataset, it is possible to reuse
    certain common information. The only thing to note is that the semantic segmentation
    requires a label, which is a single-channel image (and not a color image as we
    are used to seeing).
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the `_info` method is hence straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It is worth noting that TensorFlow Datasets already comes with a predefined
    set of feature connectors that have been used to define `FeatureDict`. For example,
    the correct way of defining an image feature with a fixed depth (4 or 1) and an
    unknown height and width is to use `tfds.features.Image(shape=(None, None, depth))`.
  prefs: []
  type: TYPE_NORMAL
- en: The `description`, `urls`, and `citation` fields have been kept from the parent,
    although this is not fully correct since the description and citation fields of
    the parent are about the object detection and classification challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The second method to implement is `_split_generators`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the dataset splits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `_split_generators` method is used to download the raw data and do some
    basic preprocessing without needing to worry about TFRecord creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are inheriting from `tfds.image.Voc2007`, there is no need to reimplement
    it, but it is, instead, required to have a look at the parent source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The source code is from [https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/voc.py](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/voc.py),
    released under the Apache License, 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: As it can be easily seen, the method uses a `dl_manager` object to download
    (and cache) and extract the archive from some remote location. The dataset split
    definition in `"train"`, `"test"`, and `"val"` is performed in the return line.
  prefs: []
  type: TYPE_NORMAL
- en: The most important part of every `tfds.core.SplitGeneratro` call is the `gen_kwargs`
    parameter. In fact, at this line, we are instructing how the `_generate_exaples`
    function is going to be called.
  prefs: []
  type: TYPE_NORMAL
- en: In short, this function creates three splits, by calling the `_generate_examples` function,
    passing the `data_path` parameters set to the current dataset path (`test_path`
    or `trainval_path`), and setting `set_name` to the correct dataset name.
  prefs: []
  type: TYPE_NORMAL
- en: The `set_name` parameter value comes from the PASCAL VOC 2007 directory and
    file organization. As we will see in the next section, where the `_generate_example`
    method is implemented, knowing the dataset structure and content is needed to
    create the splits correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `_generate_example` method can be defined with any signature. This method
    is called only by the `_split_generators` method, and therefore, it is up to this
    method to correctly invoke `_generate_example` with the correct parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Since we haven't overwritten the parent `_split_generators` method, we have
    to use the same signature required by the parent. Hence, we have the `data_path`
    and `set_name` parameters to use, in addition to all of the other information
    that is available in the PASCAL VOC 2007 documentation.
  prefs: []
  type: TYPE_NORMAL
- en: To goal of `_generate_examples` is to yield an example every time it is invoked
    (behaving like a standard Python iterator).
  prefs: []
  type: TYPE_NORMAL
- en: From the dataset structure, we know that, inside `VOCdevkit/VOC2007/ImageSets/Segmentation/`,
    there are three text files—one for each split: `"train"`, `"test"`, and `"val"`.
    Every file contains the name of the labeled image for every split.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is straightforward to use the information contained in these files
    to create the three splits. We only have to open the file and read it line-by-line
    to know which are the images to read.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets constrains us from using the Python file operations, but
    it explicitly requires the usage of the `tf.io.gfile` package. This constraint
    is necessary, since there are datasets that are too huge to be processed on a
    single machine, and `tf.io.gfile` can be easily used by TensorFlow Datasets to
    read and process remote and distributed datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the PASCAL VOC 2007 documentation, we can also extract a **Look-Up Table**
    (**LUT**) to create a mapping between the RGB values and the scalar labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After creating this look-up table, we can use only TensorFlow operations to
    read the images, check for their existence (because there is no guarantee that
    the raw data is perfect and we have to prevent failures during the dataset creation),
    and create the single-channel image that contains the numerical value associated
    with the RGB color.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the source code carefully since it may be hard to understand the first
    read. In particular, the loop over the look-up table where we look for correspondences
    between the RGB colors and the colors available may be not easy to understand
    at first glance. The following code not only creates the single-channel image
    with the numerical values associated with the RGB colors, using `tf.Variable`,
    but also checks whether the RGB values are correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `_generate_examples` method not only yields single examples, but it must
    yield a pair, `(id, example)`, where `id`—in this case, `image_id`—should uniquely
    identify the record; this field is used to shuffle the dataset globally and to
    avoid having repeated elements in the generated dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Having implemented this last method, everything is correctly set up, and we
    can use the brand-new Voc2007Semantic loader.
  prefs: []
  type: TYPE_NORMAL
- en: Use the builder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow Datasets can automatically detect whether a class visible in the
    current scope is a `DatasetBuilder` object. Therefore, having implemented the
    class by subclassing an existing `DatasetBuilder`, the `"voc2007_semantic"` builder
    is already ready to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: At the first execution, the splits are created and the `_generate_examples`
    method is classed three times to create the TFRecord representation of the examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'By inspecting the `info` variable, we can see some dataset statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The features are described by implementing the `_info` method, and the dataset
    size is relatively small, containing only 207 images each for train and test split
    and 211 for the validation split.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing `DatasetBuilder` is a relatively straightforward operation that
    you are invited to do every time you start working with a new dataset—in this
    way, a high-efficiency pipeline can be used during the training and evaluation
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the network architecture is not that of an image classifier and the
    labels are not scalars, semantic segmentation can be seen as a traditional classification
    problem and therefore the training and evaluation processes can be the same.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, instead of writing a custom training loop, we can use the `compile`
    and `fit` Keras models to build the training loop and execute it respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use the Keras `fit` model, the `tf.data.Dataset` object should generate tuples
    in the `(feature, label)` format, where `feature` is the input image and `label`
    is the image label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it is worth defining some functions that can be applied to the elements
    produced by `tf.data.Dataset`, which transforms the data from a dictionary to
    a tuple, and, at the same time, we can apply some useful preprocessing for the
    training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now easy to get the validation and training sets from the `dataset` object
    obtained from the `tfds.load` call and apply to them the required transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The datasets are ready to be used in the `fit` method, and since we are developing
    a pure Keras solution, we can configure the hidden training loop using the Keras
    callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Training loop and Keras callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `compile` method is used to configure the training loop. We can specify
    the optimizer, the loss, the metrics to measure, and some useful callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks are functions that are executed at the end of every training epoch.
    Keras comes with a long list of predefined callbacks that are ready to use. In
    the next code snippet, two of the most common are going to be used, the `ModelCheckpoint`
    and `TensorBoard` callbacks. As it can be easily guessed, the former saves a checkpoint
    at the end of the epoch, while the latter logs the metrics using `tf.summary`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since semantic segmentation can be treated as a classification problem, the
    loss used is `SparseCategoricalCrossentropy`, configured to apply the sigmoid
    to the output layer of the network when computing the loss value (in the depth
    dimension), as stated by the `from_logits=True` parameter. This configuration
    is required since we haven''t added an activation function to the last layer of
    the custom U-Net:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The datasets and the callbacks are passed to the `fit` method, which performs
    the effective training loop for the desired number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The training loop will train the model for 50 epochs, measuring the loss and
    the accuracy during the training and, at the end of every epoch, the accuracy
    and loss value on the validation set. Moreover, having passed two callbacks, we
    have a checkpoint with the model parameters logged in the `ckpt` directory, and
    we have the logging of the metrics not only on the standard output (that is, the
    Keras default) but also on TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During the training, we can open TensorBoard and look at the plots of the losses
    and metrics. At the end of the 50^(th) epoch, we get the plots shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b8b8a58-807d-45af-8d93-4ea5b8ee0513.png)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy and loss values on the training set (orange) and validation set
    (blue). The summary usage and configuration is hidden to the user by Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, since we have all of the parameters of the model in our `model` variable,
    we can try to feed it an image downloaded from the internet and see whether the
    segmentation works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that we downloaded the following image from the internet and
    saved it as `"author.jpg"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d236e4a-e3eb-4f1c-8a88-fa0c03d1e59f.png)'
  prefs: []
  type: TYPE_IMG
- en: Greetings!
  prefs: []
  type: TYPE_NORMAL
- en: We expect the model to produce a segmentation of the only known class contained
    in this image, that is, `"person"`, while producing the `"background"` label everywhere
    else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve downloaded the image, we convert it into the same format expected
    by the model (a float with values between *[0,1]*) and resize it to *512*. Since
    the model works on a batch of images, a unary dimension to the `sample` variable
    has to be added. Now, running the inference is as easy as `model(sample)`. After
    that, we use the `tf.argmax` function on the last channel to extract the predicted
    labels for every pixel position:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the `pred_image` tensor, we have the dense predictions that are pretty much
    useless for visualization. In fact, this tensor has values in the *[0, 21]* range,
    and these values are indistinguishable once visualized (they all look black).
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we can use the LUT created for the dataset to apply the inverse mapping
    from label to color. In the end, we can use the TensorFlow `io` package to convert
    the image and JPEG format and store it on the disk for easy visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the result of the segmentation after training a simple model for only
    50 epochs on a small dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e4d9cd8-400b-4084-a0cf-62a107ab4d83.png)'
  prefs: []
  type: TYPE_IMG
- en: The result of the segmentation after mapping the predicted labels to the corresponding
    colors.
  prefs: []
  type: TYPE_NORMAL
- en: Although coarse, because the architecture hasn't been optimized, model selection
    hasn't been performed, and the dataset size is small, the segmentation results
    already look promising!
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to inspect the predicted labels by counting the number of matches
    per label. In the `pixels_per_label` list, we saved the pair (`label`, `match_count`) and
    by printing it, we can verify if the predicted class is `"person"` (id 15) as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'That produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is as expected. Of course, there is still room for improvement, and this
    is left to the reader as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the problem of semantic segmentation and implemented
    U-Net: a deep encoder-decoder architecture used to tackle this problem. A short
    introduction about the possible use cases and the challenges this problem poses
    has been presented, followed by an intuitive introduction of the deconvolution
    (transposed convolution) operation, used to build the decoder part of the architecture.
    Since, at the time of writing, there is not a dataset for semantic segmentation
    that''s ready to use in TensorFlow Datasets, we took the advantage of this to
    show the architecture of TensorFlow Datasets and show how to implement a custom
    `DatasetBuilder`. Implementing it is straightforward, and it is something that''s
    recommended to every TensorFlow user since it is a handy way of creating a high-efficiency
    data input pipeline (`tf.data.Dataset`). Moreover, by implementing the `_generate_examples`
    method, the user is forced to "have a look" at the data, and this is something
    that''s highly recommended when doing machine learning and data science.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, we learned the implementation of the training loop for the semantic
    segmentation network by treating this problem as a classification problem. This
    chapter showed how to use the Keras `compile` and `fit` methods and presented
    how to customize the training loop using Keras callbacks. This chapter ended with
    a quick example of how to use the trained model for inference and how to save
    the resulting image using only the TensorFlow methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, `Chapter 9`, *Generative Adversarial Networks*, an introduction
    to **Generative Adversarial Networks** (**GANs**) and the adversarial training
    process is shown, and, obviously, we explain how to implement them using TensorFlow
    2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following exercises are of fundamental importance and you are invited to
    answer to every theoretical question and solve all of the code challenges presented:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the semantic segmentation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is semantic segmentation a difficult problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is deconvolution? Is the deconvolution operation in deep learning a real
    deconvolution operation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is possible to use Keras models as layers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it possible to use a single Keras `Sequential` model to implement a model
    architecture with skip connections?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Describe the original U-Net architecture: what are the differences between
    the custom implementation presented in this chapter and the original one?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement, using Keras, the original U-Net architecture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a DatasetBuilder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the hierarchical organization of TensorFlow Datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `_info` method contains the description of every single example of the dataset.
    How is this description related to the `FeatureConnector` object?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the `_generate_splits` and `_generate_examples` methods. Explain how
    these methods are connected and the role of the `gen_kwargs` parameter of `tfds.core.SplitGenerator`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a LUT? Why is it a useful data structure when creating a dataset for
    semantic segmentation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why it is required to use `tf.io.gfile` when developing a custom DatasetBuilder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(Bonus): Add a missing dataset for semantic segmentation to the TensorFlow
    Datasets project! Submit a Pull Request to [https://github.com/tensorflow/datasets](https://github.com/tensorflow/datasets),
    and in the message, feel free to share this exercise section and this book.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the modified U-Net architecture as shown in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the loss function and add a term of reconstruction loss, where the goal
    of the minimization process is to both minimize the cross-entropy and make the
    predicted label similar to the ground truth label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the mean intersection over union using the Keras callback. The Mean
    IOU is already implemented in the `tf.metrics` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to improve the model's performance, on the validation set, by adding dropout
    layers in the encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the training, start by dropping neurons with a probability of 0.5 and,
    at every epoch, increase this value of 0.1\. Stop the training when the validations
    mean IOU stops increasing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the trained model to run inference on a random image downloaded from the
    internet. Postprocess the result segmentation in order to detect a bounding box
    around different elements of different classes. Draw the bounding boxes, using
    TensorFlow, on the input image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
