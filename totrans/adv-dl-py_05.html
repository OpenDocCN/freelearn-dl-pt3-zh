<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Convolutional Networks</h1>
                </header>
            
            <article>
                
<p>In <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <em>Understanding Convolutional Networks</em>, we discussed the building blocks of <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) and some of their properties. In this chapter, we'll go a step further and talk about some of the most popular CNN architectures. These networks usually combine multiple primitive convolution and/or pooling operations in a novel building block that serves as a base for a complex architecture. This allows us to build very deep (and sometimes wide) networks with high representational power that perform well on complex tasks such as ImageNet classification, image segmentation, speech recognition, and so on. Many of these models were first released as participants in the ImageNet challenge, which they usually won. To simplify our task, we'll discuss all architecture within the context of image classification. We'll still discuss more complex tasks, but we'll do it in <a href="9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml">Chapter 4</a>, <em>Object Detection and Image Segmentation</em>.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Introducing AlexNet</li>
<li>An introduction to Visual Geometry Group</li>
<li>Understanding residual networks</li>
<li><span>Understanding </span>Inception networks</li>
<li>Introducing Xception</li>
<li><span>Introducing MobileNet</span></li>
<li>An introduction to DenseNets</li>
<li>The workings of neural architecture search</li>
<li>Introducing capsule networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing AlexNet</h1>
                </header>
            
            <article>
                
<p><span>The first model we'll discuss is the winner of the 2012 <strong>ImageNet Large Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>, or simply</span> ImageNet<span>). It's nicknamed AlexNet (<em>ImageNet Classification with Deep Convolutional Neural Networks</em>, </span><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a><span>), after one of its authors, Alex Krizhevsky. Although this model is rarely used nowadays, it's an important milestone in contemporary deep learning.</span></p>
<p><span>The following diagram shows the network architecture:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1236 image-border" src="assets/ec08175c-5282-4be2-b6e7-6f2d99272166.png" style="width:59.83em;height:27.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The AlexNet architecture. The original model was split in two, so it can fit on the memory of two GPUs</div>
<p>The model has five cross-correlated convolutional layers, three overlapping max pooling layers, three fully connected layers, and ReLU activations. The output is a 1,000-way softmax (one for each <span>ImageNet</span> class). The first and second convolutional layers use local response normalization<span>—</span>a type of normalization, somewhat similar to batch normalization. The fully connected layers have a dropout rate of 0.5. To prevent overfitting, the network was trained using random 227<span>×</span>227 crops of the 256<span>×</span>256 input images. The network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%.</p>
<p>In the next section, we'll discuss an NN architecture that was introduced by <span>Oxford's Visual Geometry Group in 2014, when it became a runner-up in the ImageNet challenge of that year. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to Visual Geometry Group </h1>
                </header>
            
            <article>
                
<p>The next architecture we're going to<span> </span>discuss is <strong>Visual Geometry Group</strong> (<strong>VGG</strong>) (from Ox<span>ford's Visual Geometry Group, <em>Very Deep Convolutional Networks for Large-Scale Ima</em></span>g<em>e</em> <em>Recognition</em>, <span><a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a><a href="https://arxiv.org/abs/1409.1556">)</a></span>. The VGG family of networks remains popular today and is often used as a benchmark against newer architectures. Prior to VGG (for example, LeNet-5:<span> </span><a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a> and AlexNet<span>),</span> the initial convolutional layers of a network used<span> </span>filters with large receptive fields, such as 11<span>×</span>11. Additionally, the networks usually<span> </span>had alternating single convolutional and pooling layers. The authors of the paper observed that a convolutional layer with a large filter size can be replaced with a stack of two or more convolutional layers with smaller filters (factorized convolution). For example, we can replace one 5<span>×</span>5 layer with a stack of two 3<span>×</span>3 layers, or a 7<span>×</span>7 layer with a stack of three 3<span>×</span>3 layers.</p>
<p>This structure has several advantages, as follows:</p>
<ul>
<li>The neurons of the last of the stacked layers have the equivalent receptive field size of a single layer with a large filter.</li>
<li>The number of weights and operations of stacked layers is smaller, compared to a single layer with a large filter size. Let's assume we want to replace one 5<span>×</span>5 layer with two 3<span>×</span>3 layers. Let's also assume that all layers have an equal number of input and output channels (slices),<span> </span><em>M</em>. The total number of weights (excluding biases) of the 5<span>×</span>5 layer is<span> </span><em>5*5*M*M = 25*M<sup>2</sup></em>. On the other hand, the total weights of a single 3<span>×</span>3 layer is<span> </span><em><span>3*3*M*M = 9*M</span><sup>2</sup></em>, and simply<span> </span><em>2<span>*</span>(<span>3*3*M*M) = 18*M</span><sup>2</sup></em><span> </span>for two layers, which makes this arrangement 28% more efficient (18/25 = 0.72). The efficiency will increase further with larger filters.</li>
<li>Stacking multiple layers makes the decision function more discriminative.</li>
</ul>
<p>The VGG networks consist of multiple blocks of two, three, or four stacked convolutional layers combined with a max pooling layer. We can see the two most popular variants, <strong>VGG16</strong> and <strong>VGG19</strong>, in the following table:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-1237 image-border" src="assets/ab51fa0e-ccbf-4a89-b3c7-f76561637925.png" style="width:12.42em;height:33.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Architecture of the VGG16 and VGG19 networks, named after the number of weighted layers in each network</span></div>
<p>As the depth of the VGG network increases, so does the width (the number of filters) in the convolutional layers. We have multiple pairs of cross-channel convolutions with a volume depth of 128/256/512 connected to other layers with the same depth. In addition, we also have two<span> </span><span>4,096-unit fully connected layers, followed by a 1000-unit fully connected layer and a softmax (one for each ImageNet class). Because of this, the VGG networks have a large number of parameters (weights), which makes them memory-inefficient, as well as computationally expensive.</span><span> </span>Still, this is a popular and straightforward network architecture, which has been further improved by the addition of batch normalization.</p>
<p>In the next section, we'll use VGG as an example of how to load pretrained network models with TensorFlow and PyTorch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VGG with PyTorch and TensorFlow</h1>
                </header>
            
            <article>
                
<p>Both PyTorch and TensorFlow have pretrained<span> </span>VGG models. Let's see how to use them.</p>
<p>Keras is an official part of TensorFlow 2, therefore, we'll use it to load the model:</p>
<pre><span>import </span>tensorflow <span>as </span>tf<br/><br/><span># VGG16<br/></span>vgg16 = tf.keras.applications.vgg16.VGG16(<span>include_top</span>=<span>True,<br/></span><span>                                          </span><span>weights</span>=<span>'imagenet'</span><span>,<br/></span><span>                                          </span><span>input_tensor</span>=<span>None,<br/></span><span>                                          </span><span>input_shape</span>=<span>None,<br/></span><span>                                          </span><span>pooling</span>=<span>None,<br/></span><span>                                          </span><span>classes</span>=<span>1000</span>)<br/><br/><span># VGG19 <br/></span>vgg19 = tf.keras.applications.vgg19.VGG19(<span>include_top</span>=<span>True,<br/></span><span>                                          </span><span>weights</span>=<span>'imagenet'</span><span>,<br/></span><span>                                          </span><span>input_tensor</span>=<span>None,<br/></span><span>                                          </span><span>input_shape</span>=<span>None,<br/></span><span>                                          </span><span>pooling</span>=<span>None,<br/></span><span>                                          </span><span>classes</span>=<span>1000</span>)</pre>
<ol start="2"/>
<p><span>By setting the <kbd>weights='imagenet'</kbd> parameter, the network will be loaded with pretrained ImageNet weights (they </span>will<span> be downloaded automatically). Yo</span>u can set <kbd>include_top</kbd> to<span> </span><kbd>False</kbd><span>, which</span><span> </span>will <span>exclude</span><span> </span>the <span>fully connected layers for a transfer learning scenario. In this case, you can also use an arbitrary input size by setting a t</span>uple value to <kbd>input_shape</kbd><span>—</span>the c<span>onvolutional layers will automatically scale to match the desired input shape. This is possible because the convolution filter is shared along the whole feature map. Therefore, we can use the same filter on feature maps with different sizes.</span></p>
<p>We'll continue with PyTorch, where you can choose whether you want to use a pretrained model (again, with automatic download):</p>
<pre style="padding-left: 60px"><span>import </span>torchvision.models <span>as </span>models<br/>model = models.vgg16(pretrained=True)</pre>
<p>You can<span> </span><span>try other pretrained models</span>, using the same procedures we described. To avoid repetition, we won't include the same code examples for the other architectures in this section.</p>
<p>In the next section, we'll discuss one of the most popular CNN architectures, which was released after VGG.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding residual networks</h1>
                </header>
            
            <article>
                
<p>Residual networks (<strong>ResNets</strong>, <em>Deep Residual Learning for Image Recognition</em>,<span> </span><a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>) were released in 2015, when they won all five categories of the ImageNet challenge that year. In<span> </span><a href="b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml">Chapter 1</a>, <span><em>The Nuts and Bolts of Neural</em></span> <em>Networks</em>, we mentioned that the layers of a neural network are not restricted to sequential order, but form a graph instead. This is the first architecture we'll learn, which takes advantage of this flexibility. This is also the first network architecture that has successfully trained a network with a depth of more than 100 layers.</p>
<p>Thanks to better weight initializations, new<span> </span>activation functions, as well as normalization layers, it's now possible to train deep networks. But, t<span>he authors of the paper conducted some experiments and observed that a network with 56 layers had higher training and testing errors compared to a network with 20 layers. They argue that this should not be the case. In theory, we can take a shallow network and stack identity layers (these are layers whose output just repeats the input) on top of it to produce a deeper network that behaves in exactly the same way as the shallow one. Yet, their experiments have been unable to match the performance of the shallow network.</span></p>
<p>To solve this problem, they proposed a network constructed of residual blocks. A residual block consists of two or three sequential convolutional layers and a separate parallel identity (repeater) shortcut connection, which connects the input of the first layer and the output of the last one.<span> </span><span>We can see three types of residual blocks</span><span> </span>in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-1238 image-border" src="assets/9629b25c-e912-469b-83c7-9d1f4058a249.png" style="width:34.08em;height:21.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">From left to right: original residual block; original bottleneck residual block; pre-activation residual block; pre-activation bottleneck residual block</div>
<p><span>Each block has two parallel paths. The left-hand path is similar to the other networks we've seen, and consists of sequential convolutional layers + batch normalization. The right path contains the identity shortcut connection (also known as the skip connection). The two paths are merged via an element-wise sum. That is, the left and right tensors have the same shape and an element of the first tensor is added to the element in the same position in the second tensor. The output is a single tensor with the same shape as the input. In effect, we propagate forward the features learned by the block, but also the original unmodified signal. In this way, we can get closer to the original scenario, as described by the authors. The network can decide to skip some of the convolutional layers thanks to the skip connections, in effect reducing its own depth. The residual blocks use padding in such a way that the input and the output of the block have the same dimensions. Thanks to this, we can stack any number of blocks for a network with an arbitrary depth.</span></p>
<p>And now, let's see how the blocks in the diagram differ:</p>
<ul>
<li>The first block contains two 3<span>×</span>3 convolutional layers. This is the original residual block, but if the layers are wide, stacking multiple blocks becomes computationally expensive.</li>
<li>The second block is equivalent to the first, but it uses the so-called bottleneck layer. First, we use a 1<span>×</span>1 convolution to downsample the input volume depth (we discussed this in <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>,<em> Understanding Convolutional Networks</em>). Then, we apply a 3<span>×</span>3 (bottleneck) convolution to the reduced input. Finally, we expand the output back to the desired depth with another 1<span>×</span>1 convolution. This layer is less computationally expensive than the first.</li>
<li>The third block is the latest revision of the idea, published in 2016 by the same authors <span>(<em>Identity Mappings in Deep Residual Networks</em>, </span><a href="https://arxiv.org/abs/1603.05027">https://arxiv.org/abs/1603.05027</a><span>)</span>. It uses pre-activations, and the batch normalization and the activation function come before the convolutional layer. This may seem strange at first, but thanks to this design, the skip connection path can run uninterrupted throughout the network. This is contrary to the other residual blocks, where at least one activation function is on the path of the skip connection. A combination of stacked residual blocks still has the layers in the right order.</li>
<li>The fourth block is the bottleneck version of the third layer. It follows the same principle as the bottleneck residual layer v1. </li>
</ul>
<p>In the following table, we can see the family of networks proposed by the authors of the paper:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1239 image-border" src="assets/3bf80ec8-ffb0-4f87-964a-3db43e6b09d0.png" style="width:62.92em;height:35.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The family of the most popular residual networks.<span> </span><span>The residual blocks are represented by rounded rectangles</span></div>
<p><span> </span>Some of their properties are as follows:</p>
<ul>
<li>They start with a 7<span>×</span>7 convolutional layer with stride 2, followed by 3<span>×</span>3 max-pooling. This layer also serves as a downsampling step<span>—</span>the rest of the network starts with a much smaller slice of 56<span>×</span>56, compared to <span>224×224 of</span><span> </span>the input.</li>
<li>Downsampling in the rest of the network is implemented with a modified residual block with stride 2.</li>
<li>Average pooling downsamples the output after all residual blocks and before the 1,000-unit fully connected softmax layer.</li>
</ul>
<p><span>The ResNet family of networks is popular not only because of their accuracy, but also because of their relative simplicity and the versatility of the residual blocks. As we mentioned, the input and output shape of the residual block can be the same due to the padding. We can stack residual blocks in different configurations to solve various problems with wide-ranging training set sizes and input dimensions. Because of this universality, we'll implement an example of ResNet in the next section</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing residual blocks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we'll implement a pre-activation ResNet to classify the CIFAR-10 images using PyTorch 1.3.1 and <kbd>torchvision</kbd> 0.4.2. Let's start:</p>
<ol>
<li class="mce-root"><span>As usual, we'll start with the imports. Note that we'll use the shorthand</span> <kbd>F</kbd> <span>for the PyTorch functional module (</span><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a><span>):</span></li>
</ol>
<pre style="padding-left: 60px"><span>import </span>matplotlib.pyplot <span>as </span>plt<br/><span>import </span>torch<br/><span>import </span>torch.nn <span>as </span>nn<br/><span>import </span>torch.nn.functional <span>as </span>F<br/><span>import </span>torch.optim <span>as </span>optim<br/><span>import </span>torchvision<br/><span>from </span>torchvision <span>import </span>transforms</pre>
<ol start="2">
<li>Next, let's define the pre-activation regular (non-bottleneck) residual block. We'll implement it as <kbd>nn.Module</kbd><span>—</span>the base class for all neural network modules. Let's st<span>art with the class definition and the <kbd>__init__</kbd> method:</span></li>
</ol>
<pre style="padding-left: 60px">class PreActivationBlock(nn.Module):<br/>    expansion = 1<br/>    def __init__(self, in_slices, slices, stride=1):<br/>        super(PreActivationBlock, self).__init__()<br/><br/>        self.bn_1 = nn.BatchNorm2d(in_slices)<br/>            <br/>                                out_channels=slices,kernel_size=3, <br/>                                stride=stride, padding=1,<br/>                                bias=False)<br/><br/>        self.bn_2 = nn.BatchNorm2d(slices)<br/>        self.conv_2 = nn.Conv2d(in_channels=slices, <br/>                                out_channels=slices,kernel_size=3, <br/>                                stride=1, padding=1,<br/>                                bias=False)<br/><br/>        # if the input/output dimensions differ use convolution for <br/>        the shortcut<br/>        if stride != 1 or in_slices != self.expansion * slices:<br/>            self.shortcut = nn.Sequential(<br/>                nn.Conv2d(in_channels=in_slices,<br/>                          out_channels=self.expansion * slices,<br/>                          kernel_size=1,<br/>                          stride=stride,<br/>                          bias=False)<br/>            )</pre>
<p style="padding-left: 60px">We will define only the learnable block components in the <kbd>__init__</kbd> method—these include the convolution and batch normalization operations. Also, note the way we implement the <kbd>shortcut</kbd> connection. If the input dimensions are the same as the output dimensions, we can directly use the input tensor as the shortcut. However, if the dimensions differ, we have to transform the input with the help of a 1<span>×</span>1 convolution with the same stride and output channels as the one in the main path. The dimensions may differ either by height/width (<kbd>stride != 1</kbd>) or by depth (<kbd>in_slices != self.expansion * slices</kbd>). <kbd>self.expansion</kbd> is a hyper-parameter, which was included in the original ResNet implementation. It allows us to expand the output depth of the residual block. </p>
<ol start="3">
<li>The actual data propagation is implemented in the <kbd>forward</kbd> method (please mind the indentation, as it's a member of <kbd>PreActivationBlock</kbd>):</li>
</ol>
<pre style="padding-left: 60px">def forward(self, x):<br/>    out = F.relu(self.bn_1(x))<br/><br/>    # reuse bn+relu in downsampling layers<br/>    shortcut = self.shortcut(out) if hasattr(self, 'shortcut')<br/>    else x<br/><br/>    out = self.conv_1(out)<br/><br/>    out = F.relu(self.bn_2(out))<br/>    out = self.conv_2(out)<br/><br/>    out += shortcut<br/><br/>    return out</pre>
<p style="padding-left: 60px"><span>We use the functional </span><kbd>F.relu</kbd><span> for the activation function, as it doesn't have learnable parameters. Then,</span> if the shortcut connection is a convolution and not an identity (that is, the input/output dimensions of the block differ), we'll reuse <kbd>F.relu(self.bn_1(x))</kbd> to add non-linearity and batch normalization to the shortcut. Otherwise, we'll just repeat the input. </p>
<ol start="4">
<li>Then, let's implement the bottleneck version of the residual block. We'll use the same blueprint as in the non-bottleneck implementation. We'll start with the class definition and the <kbd>__init__</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"><span>class </span>PreActivationBottleneckBlock(nn.Module):<br/>    expansion = 4<br/>    def __init__(self, in_slices, slices, stride=1):<br/>        super(PreActivationBottleneckBlock, self).__init__()<br/><br/>        self.bn_1 = nn.BatchNorm2d(in_slices)<br/>        self.conv_1 = nn.Conv2d(in_channels=in_slices, <br/>                                out_channels=slices, kernel_size=1,<br/>                                bias=False)<br/><br/>        self.bn_2 = nn.BatchNorm2d(slices)<br/>        self.conv_2 = nn.Conv2d(in_channels=slices, <br/>                                out_channels=slices, kernel_size=3, <br/>                                stride=stride, padding=1,<br/>                                bias=False)<br/><br/>        self.bn_3 = nn.BatchNorm2d(slices)<br/>        self.conv_3 = nn.Conv2d(in_channels=slices,<br/>                                out_channels=self.expansion * <br/>                                slices,<br/>                                kernel_size=1,<br/>                                bias=False)<br/><br/>        # if the input/output dimensions differ use convolution for the shortcut<br/>        if stride != 1 or in_slices != self.expansion * slices:<br/>            self.shortcut = nn.Sequential(<br/>                nn.Conv2d(in_channels=in_slices,<br/>                          out_channels=self.expansion * slices,<br/>                          kernel_size=1, stride=stride,<br/>                          bias=False)<br/>            )</pre>
<p style="padding-left: 60px"><span>The </span><kbd>expansion</kbd><span> parameter is <kbd>4</kbd> after the original implementation. The <kbd>self.conv_1</kbd> convolution operation represents the 1×1 downsampling bottleneck connection, <kbd>self.conv_2</kbd> is the actual convolution, and <kbd>self.conv_3</kbd> is the upsampling 1×1 convolution. The shortcut mechanism follows the same logic as in <kbd>PreActivationBlock</kbd>.</span></p>
<ol start="5">
<li>Next, let's implement the <kbd>PreActivationBottleneckBlock.forward</kbd> method. Once again, it follows the same logic as the one in <kbd>PreActivationBlock</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>forward</span>(<span>self</span><span>, </span>x):<br/>    out = F.relu(<span>self</span>.bn_1(x))<br/><br/>    <span>#  reuse bn+relu in downsampling layers<br/></span><span>    </span>shortcut = <span>self</span>.shortcut(out) <span>if </span><span>hasattr</span>(<span>self</span><span>, </span><span>'shortcut'</span>) <br/>    <span>else </span>x<br/><br/>    out = <span>self</span>.conv_1(out)<br/><br/>    out = F.relu(<span>self</span>.bn_2(out))<br/>    out = <span>self</span>.conv_2(out)<br/><br/>    out = F.relu(<span>self</span>.bn_3(out))<br/>    out = <span>self</span>.conv_3(out)<br/><br/>    out += shortcut<br/><br/>    <span>return </span>out</pre>
<ol start="6">
<li>Next, let's implement the residual network itself. We'll start with the class definition (it inherits <kbd>nn.Module</kbd>) and the <kbd>__init__</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">class PreActivationResNet(nn.Module):<br/>    def __init__(self, block, num_blocks, num_classes=10):<br/>        """<br/>        :param block: type of residual block (regular or <br/>        bottleneck)<br/>        :param num_blocks: a list with 4 integer values.<br/>            Each value reflects the number of residual blocks in <br/>            the group<br/>        :param num_classes: number of output classes<br/>        """<br/><br/>        super(PreActivationResNet, self).__init__()<br/><br/>        self.in_slices = 64<br/><br/>        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=64,<br/>                                kernel_size=3, stride=1, padding=1,<br/>                                bias=False)<br/><br/>        self.layer_1 = self._make_group(block, 64, num_blocks[0], <br/>        stride=1)<br/>        self.layer_2 = self._make_group(block, 128, num_blocks[1], <br/>        stride=2)<br/>        self.layer_3 = self._make_group(block, 256, num_blocks[2], <br/>        stride=2)<br/>        self.layer_4 = self._make_group(block, 512, num_blocks[3], <br/>        stride=2)<br/>        self.linear = nn.Linear(512 * block.expansion, num_classes)</pre>
<p style="padding-left: 60px"><span>The network contains four groups of residual blocks, just like the original implementation. The number of blocks of each group is specifie</span>d by the <kbd>num_blocks</kbd> para<span>meter. The initial convolution uses a 3×3 filter with stride 1, as opposed to a 7×7 with stride 2 of the original implementation. This is because the </span><span>32×32</span><span> CIFAR-10 images are much smaller than the </span><span>224×224</span><span> ImageNet ones, and the downsampling is unnecessary.</span></p>
<ol start="7">
<li>Then, we'll implement the <kbd>PreActivationResNet._make_group</kbd> method, which creates one residual block group. All blocks in the group have stride 1, except for the first, where <kbd>stride</kbd> is supplied as a parameter:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>_make_group</span>(<span>self</span><span>, </span>block<span>, </span>slices<span>, </span>num_blocks<span>, </span>stride):<br/>    <span>"""Create one residual group"""<br/></span><span><br/></span><span>    </span>strides = [stride] + [<span>1</span>] * (num_blocks - <span>1</span>)<br/>    layers = []<br/>    <span>for </span>stride <span>in </span>strides:<br/>        layers.append(block(<span>self</span>.in_slices<span>, </span>slices<span>, </span>stride))<br/>        <span>self</span>.in_slices = slices * block.expansion<br/><br/>    <span>return </span>nn.Sequential(*layers)</pre>
<ol start="8">
<li>Next, we'll implement the <span><kbd>PreActivationResNet.forward</kbd> method, which propagates the data through the network. We can see the downsampling average pooling before the fully connected final layer:</span></li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>forward</span>(<span>self</span><span>, </span>x):<br/>    out = <span>self</span>.conv_1(x)<br/>    out = <span>self</span>.layer_1(out)<br/>    out = <span>self</span>.layer_2(out)<br/>    out = <span>self</span>.layer_3(out)<br/>    out = <span>self</span>.layer_4(out)<br/>    out = F.avg_pool2d(out<span>, </span><span>4</span>)<br/>    out = out.view(out.size(<span>0</span>)<span>, </span>-<span>1</span>)<br/>    out = <span>self</span>.linear(out)<br/><br/>    <span>return </span>out</pre>
<ol start="9">
<li>Once we're done with the network, we can implement several ResNet configurations. The following is <kbd>ResNet34</kbd> with 34 convolution layers, grouped in <kbd>[3, 4, 6, 3]</kbd> non-bottleneck residual blocks:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>PreActivationResNet34</span>():<br/>    <span>return </span>PreActivationResNet(<span>block</span>=PreActivationBlock<span>,<br/></span><span>                               </span><span>num_blocks</span>=[<span>3</span><span>, </span><span>4</span><span>, </span><span>6</span><span>, </span><span>3</span>])</pre>
<ol start="10">
<li>Finally, we can train the network. We'll start by defining the train and test datasets. We won't go into much detail about the implementation, as we've already looked at a similar scenario, in <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <em>Understanding Convolutional Networks</em>. We'll augment the training set by padding the samples with four pixels, and then we'll take random 32<span>×</span>32 crops out of it. The following is the implementation:</li>
</ol>
<pre style="padding-left: 60px"><span># training data transformation<br/></span>transform_train = transforms.Compose([<br/>    transforms.RandomCrop(<span>32</span><span>, </span><span>padding</span>=<span>4</span>)<span>,<br/></span><span>    </span>transforms.RandomHorizontalFlip()<span>,<br/></span><span>    </span>transforms.ToTensor()<span>,<br/></span><span>    </span>transforms.Normalize((<span>0.4914</span><span>, </span><span>0.4821</span><span>, </span><span>0.4465</span>)<span>, </span>(<span>0.2470</span><span>, </span><span>0.2435</span><span>, <br/>    </span><span>0.2616</span>))<br/>])<br/><br/><span># training data loader<br/></span>train_set = torchvision.datasets.CIFAR10(<span>root</span>=<span>'./data'</span><span>, </span><span>train</span>=<span>True,<br/>                                        </span><span>download</span>=<span>True, <br/>                                        </span><span>transform</span>=transform_train)<br/><br/>train_loader = torch.utils.data.DataLoader(<span>dataset</span>=train_set<span>, <br/>                                        </span><span>batch_size</span>=<span>100</span><span>,<br/>                                        </span><span>shuffle</span>=<span>True, <br/>                                        </span><span>num_workers</span>=<span>2</span>)<br/><br/><span># test data transformation<br/></span>transform_test = transforms.Compose([<br/>    transforms.ToTensor()<span>,<br/></span><span>    </span>transforms.Normalize((<span>0.4914</span><span>, </span><span>0.4821</span><span>, </span><span>0.4465</span>)<span>, </span>(<span>0.2470</span><span>, </span><span>0.2435</span><span>, <br/>    </span><span>0.2616</span>))<br/>])<br/><br/><span># test data loader<br/></span>testset = torchvision.datasets.CIFAR10(<span>root</span>=<span>'./data'</span><span>, </span><span>train</span>=<span>False,<br/>                                        </span><span>download</span>=<span>True, <br/>                                        </span><span>transform</span>=transform_test)<br/><br/>test_loader = torch.utils.data.DataLoader(<span>dataset</span>=testset<span>, <br/>                                        </span><span>batch_size</span>=<span>100</span><span>,<br/>                                        </span><span>shuffle</span>=<span>False,<br/>                                        </span><span>num_workers</span>=<span>2</span>)</pre>
<ol start="11">
<li>Then, we'll instantiate the network model and the training parameters<span>—</span>cross-entropy loss and the Adam optimizer:</li>
</ol>
<pre style="padding-left: 60px"><span># load the pretrained model<br/></span>model = PreActivationResNet34()<br/><br/><span># select gpu 0, if available<br/></span><span># otherwise fallback to cpu<br/></span>device = torch.device(<span>"cuda:0" </span><span>if </span>torch.cuda.is_available() <span>else </span><span>"cpu"</span>)<br/><br/><span># transfer the model to the GPU<br/></span>model = model.to(device)<br/><br/><span># loss function<br/></span>loss_function = nn.CrossEntropyLoss()<br/><br/><span># We'll optimize all parameters<br/></span>optimizer = optim.Adam(model.parameters())</pre>
<ol start="12">
<li>We can now train the network for <kbd>EPOCHS</kbd> epochs. The <kbd>train_model</kbd>, <kbd>test_model</kbd>, and <kbd>plot_accuracy</kbd> functions are the same as the ones we defined in the <em>Implementing transfer learning with PyTorch</em> section of <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <em>Understanding Convolutional Networks</em>, and we won't repeat their implementation here. The following is the code:</li>
</ol>
<pre style="padding-left: 60px"><span># train<br/></span>EPOCHS = <span>15<br/></span><span><br/></span>test_acc = <span>list</span>()  <span># collect accuracy for plotting<br/></span><span>for </span>epoch <span>in </span><span>range</span>(EPOCHS):<br/>    <span>print</span>(<span>'Epoch {}/{}'</span>.format(epoch + <span>1</span><span>, </span>EPOCHS))<br/><br/>    train_model(model<span>, </span>loss_function<span>, </span>optimizer<span>, </span>train_loader)<br/>    _<span>, </span>acc = test_model(model<span>, </span>loss_function<span>, </span>test_loader)<br/>    test_acc.append(acc)<br/><br/>plot_accuracy(test_acc)</pre>
<p style="padding-left: 60px">And, in the following graph, we can see the test accuracy in 15 iterations (the training might take a while):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1240 image-border" src="assets/ace8fd47-9786-401b-87fc-75cc65bba7e9.png" style="width:26.58em;height:13.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">ResNet34 CIFAR accuracy in 15 epochs</div>
<div class="packt_infobox"><span>The code in this section is partially based on the pre-activation ResNet implementation in </span><a href="https://github.com/kuangliu/pytorch-cifar">https://github.com/kuangliu/pytorch-cifar</a><span>.</span></div>
<p>In this section, we discussed the various types of ResNets, and then we implemented one with PyTorch. In the next section, we'll discuss Inception networks—yet another family of networks, which elevate the use of parallel connections to a new level. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Inception networks</h1>
                </header>
            
            <article>
                
<p>Inception networks (<em>Going Deeper with Convolutions</em>, <a href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</a>) were introduced in 2014, when they won the ImageNet challenge of<span> </span>that year (there seems to be a pattern here). Since then, the authors have released multiple improvements (versions) of the architecture.</p>
<div class="packt_tip">Fun fact: the name Inception comes in part from the <strong>We need to go deeper</strong> internet meme, related to the movie<span> </span><em>Inception</em>. </div>
<p>The idea behind Inception networks started from the basic premise that the objects in an image have different scales. A distant object might take up a small region of the image, but the same object, once nearer, might take up the majority of the image. This presents a difficulty for standard CNNs, where the neurons in the different layers have a fixed receptive field size as imposed on the input image. A regular network might be a good detector of objects at a certain scale, but could miss them otherwise. To solve this problem,<span> </span><span>the authors of the paper</span><span> </span>proposed a novel architecture: one composed of Inception blocks. An Inception block starts with a common input, and then splits it into different parallel paths (or towers). Each path contains either convolutional layers with a different-sized filter, or a pooling layer. In this way, we apply different receptive fields on the same input data.<span> </span><span>At the end of the Inception block, the outputs of the different paths are concatenated. In the next few sections, we'll discuss the different variations of Inception networks.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inception v1</h1>
                </header>
            
            <article>
                
<p><span>The following diagram shows the first </span>version <span>of the Inception block, part of the GoogLeNet network architecture (<a href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</a>). GoogLeNet contains nine such Inception blocks:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1241 image-border" src="assets/532d7c33-b7ba-4743-a7f2-c44ef6b9c4e5.png" style="width:43.83em;height:13.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Inception v1 block, inspired by https://arxiv.org/abs/1409.4842</div>
<p>The v1 block has four paths:</p>
<ul>
<li>1<span>×</span>1 convolution, which acts as a kind of repeater to the input</li>
<li>1<span>×</span>1 convolution, followed by a 3<span>×</span>3 convolution</li>
<li>1<span>×</span>1 convolution, followed by a 5<span>×</span>5 convolution</li>
<li>3<span>×</span>3 max pooling with stride 1</li>
</ul>
<p>The layers in the block use padding in such a way that the input and the output have the same shape (but different depths). The padding is also necessary, because each path would produce an output with a different shape, depending on the filter size. This is valid for all versions of Inception blocks.</p>
<p>The other major innovation of this Inception block is the use of downsampling 1<span>×</span>1 convolutions. They are needed because the output of all paths is concatenated to produce the final output of the block. The result of the concatenation is an output with a quadrupled depth. If another Inception block followed the current one, its output depth would quadruple again. To avoid such exponential growth, the block uses 1<span>×</span>1 convolutions to reduce the depth of each path, which in turn reduces the output depth of the block. This makes it possible to create deeper networks, without running out of resources.</p>
<p>GoogLeNet also utilizes auxiliary classifiers—that is, it has two additional classification outputs (with the same groundtruth labels) at various intermediate layers. During training,<span> </span><span>the</span><span> </span>total value of the loss<span> </span><span>is a</span><span> </span>weighted sum<span> </span><span>of the</span><span> </span>auxiliary losses<span> </span><span>and the</span><span> </span>real loss<span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inception v2 and v3</h1>
                </header>
            
            <article>
                
<p>Inception v2 and v3 were released together<span> </span>and propose several improvements over the original Inception block<span> </span><span>(<em>Rethinking the Inception Architecture for Computer Vision</em>, </span><a href="https://arxiv.org/abs/1512.00567">https://arxiv.org/abs/1512.00567</a><span>)</span>. The first is the factorization of the 5<span>×</span>5 convolution in two stacked 3<span>×</span>3 convolutions. We discussed the advantages of this in the<span> <em>Introduction to Visual Geometry Group</em> </span>section.</p>
<p>We can see the new Inception block in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1242 image-border" src="assets/26826db8-c95e-41ee-a113-970221a8668e.png" style="width:43.25em;height:15.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Inception block A, inspired by https://arxiv.org/abs/1512.00567</div>
<p>The next improvement is the factorization of an<span> </span><em>n</em><span>×</span><em>n</em><span> </span>convolution in two stacked asymmetrical<span> </span>1<span>×</span><em>n</em><span> </span>and<span> </span><em>n</em><span>×</span>1<span> </span>convolutions. For example, we can split a single 3<span>×</span>3 convolution into two 1<span>×</span>3 and 3<span>×</span>1 convolutions, where the 3<span>×</span>1 convolution is applied over the output of the 1<span>×</span>3 convolution. In the first case, the filter size would be 3*3 = 9, while in the second case, we would have a combined size of (3*1) + (1*3) = 3 + 3 = 6, resulting in 33% efficiency, as seen in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1243 image-border" src="assets/023a33c1-b7ee-4535-ab68-18ab39d21038.png" style="width:22.17em;height:13.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Factorization of a 3<span>×</span>3 convolution in 1<span>×</span>3 and 3<span>×</span>1 convolutions.<span> Inspired by </span>https://arxiv.org/abs/1512.00567</div>
<p>The authors introduced two new blocks, which utilizes factorized convolutions. The first of these blocks (and the second in total) is equivalent of Inception block A:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1244 image-border" src="assets/43e66db9-3ccb-4855-b1df-0890467a8f60.png" style="width:15.58em;height:18.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Inception block B. When n=3, it is equivalent to block A. <span>Inspired by </span>https://arxiv.org/abs/1512.00567</div>
<p>The second (third in total) block is similar, but the asymmetrical convolutions are parallel, resulting in a higher output depth (more concatenated paths).<span> </span><span>The hypothesis here is that the more features (different filters) the network has, the faster it learns (we also discussed the need for more filters in</span> <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <span><em>Understanding Convolutional Networks</em>). On the other hand, the wider layers take more memory and computation time. As a compromise, this block</span><span> </span><span>is only used in the deeper part of the network, after the other blocks:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1245 image-border" src="assets/e74a4669-e728-4122-ad8d-a5607c4ba63e.png" style="width:19.50em;height:14.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Inception block C, i<span>nspired by </span>https://arxiv.org/abs/1512.00567</div>
<p><span>Using these new blocks, the authors proposed two new Inception networks: v2 and v3. Another</span><span> </span>major improvement in this version is the use of batch normalization, which was<span> </span>introduced by the same authors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inception v4 and Inception-ResNet</h1>
                </header>
            
            <article>
                
<p>In the latest revision of Inception networks, the<span> </span>authors introduced three new streamlined Inception blocks that build upon the idea of the previous versions<span> </span><span>(<em>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</em>, </span><a href="https://arxiv.org/abs/1602.07261">https://arxiv.org/abs/1602.07261</a><span>)</span>. They introduced 7<span>×</span>7 asymmetric factorized convolutions, and average pooling instead of max pooling. More importantly, they created a residual/Inception hybrid network known as Inception-ResNet, where the Inception blocks also include residual connections. We can see the schematic of one such block in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1246 image-border" src="assets/c073e89b-8b95-4fb6-b486-aaa22c26193b.png" style="width:15.67em;height:15.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">An Inception block with a residual skip connection</div>
<p>In this section, we discussed different types of Inception networks and the different principles used in the various Inception blocks. Next, we'll talk about a newer CNN architecture, which takes the Inception concept to a new depth (or width, as it should be).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Xception</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>All Inception blocks so far start by splitting the input into several parallel paths. Each path continues with a dimensionality-reduction</span><span> </span><span>1×1 cross-channel</span><span> </span><span>convolution, followed by regular cross-channel convolutions. On one hand, the 1×1 connection maps cross-channel correlations, but not spatial ones (because of the 1×1 filter size). On the other hand, the subsequent cross-channel convolutions map both types of correlations. </span><span>Let's recall that in</span><span> </span><a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <em>Understanding Convolutional Networks</em><span>, we introduced <strong>depthwise separable convolutions</strong> (<strong>DSC</strong>), which</span><span> combine the following two operations:</span></p>
<ul>
<li class="mce-root"><span><strong>A depthwise convolution</strong>: In a depthwise convolution, a single input slice produces a single output slice, therefore it only maps spatial (and not cross-channel) correlations.</span></li>
<li class="mce-root"><span><strong>A 1×1 cross-channel convolution</strong>: With 1×1 convolutions, we have the opposite, that is, they only map cross-channel correlations.</span></li>
</ul>
<p>The author of Xception (<em>Xception: Deep Learning with Depthwise Separable Convolutions</em>, <a href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a>) argues that, in fact, we can think of <span>DSC</span> as an extreme (hence the name) version of an Inception block, where each depthwise input/output slice pair represents one parallel path. We have as many parallel paths as the number of input slices. <span>The following diagram shows a simplified Inception block and its transformation to an Xception block:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1247 image-border" src="assets/41ef8798-793d-4b06-8b80-d4fd09bf4d0c.png" style="width:51.33em;height:13.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Left: simplified Inception module. Right: Xception block. Inspired by https://arxiv.org/abs/1610.02357</div>
<p>The Xception block and the DSC have two differences:</p>
<ul>
<li>In Xception, the 1<span>×</span>1 convolution comes first, instead of last as in DSC. But, these operations are meant to be stacked anyway, and we can assume that the order is of no significance.</li>
<li>The Xception block uses ReLU activations after each convolution, while the DSC doesn't use non-linearity after the cross-channel convolution. According to the author's experiments, networks with absent non-linearity depthwise convolution converged faster and were more accurate.</li>
</ul>
<p><span>The following diagram </span>depicts <span>the architecture of the Xception network:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1248 image-border" src="assets/c75638b5-9f2a-4871-874c-c3013d8f80ee.png" style="width:59.25em;height:38.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">From left to right: Entry flow; Middle flow, repeated eight times; Exit flow. Source: https://arxiv.org/abs/1610.02357</div>
<p>It is built of linearly stacked DSCs and some of its properties are as follows:</p>
<ul>
<li>The network contains 36 convolutional layers, structured into 14 modules, all of which have linear residual connections around them, except for the first and last modules. The modules are grouped in <span>three sequential virtual flows—entry, middle, and exit.</span></li>
<li>Downsampling with 3<span>×</span>3 max pooling in the entry and exit flows; no downsampling in the middle flow; <span>global average pooling before the fully connected layers.</span></li>
<li>All convolutions and DSCs are followed by batch normalization.</li>
<li>All DSCs have a depth multiplier of 1 (no depth expansion).</li>
</ul>
<p>This section concludes the series of Inception-based models. In the next section, we'll focus on a special model, which prioritizes a small footprint and computational efficiency. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing MobileNet</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss <span>a lightweight CNN model called</span> MobileNet (<em>MobileNetV2: Inverted Residuals and Linear Bottlenecks</em>, <a href="https://arxiv.org/abs/1801.04381">https://arxiv.org/abs/1801.04381</a>). We'll focus on the second revision of this idea (MobileNetV1 was introduced in <em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</em>, <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>).</p>
<p>MobileNet is aimed at devices with limited memory and computing power, such as mobile phones (the name kind of gives it away). To reduce its footprint, the network uses DSC, linear bottlenecks, and inverted residuals.</p>
<p>We are already familiar with DSC, so let's discuss the other two:</p>
<ul>
<li><strong>Linear bottlenecks</strong>: To understand this concept, we'll quote the paper:</li>
</ul>
<div style="padding-left: 60px" class="packt_quote">"Consider a deep neural network consisting of <em>n</em> layers <em>L<sub>i</sub></em> each of which has an activation tensor of dimensions <sub><img class="fm-editor-equation" src="assets/684623c0-9307-4ec3-9630-699e64599285.png" style="width:6.00em;height:1.17em;"/></sub>. Throughout this section we will be discussing the basic properties of these activation tensors, which we will treat as containers of <sub><img class="fm-editor-equation" src="assets/d2c0077c-fecb-48f2-9e46-06527c760674.png" style="width:3.67em;height:1.17em;"/></sub> "pixels" with <em>d<sub>i</sub></em> dimensions. Informally, for an input set of real images, we say that the set of layer activations (for any layer <em>L<sub>i</sub></em>) forms a "<em>manifold of interest"</em>. It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual <em>d</em>-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which can be embedded into a low-dimensional subspace."</div>
<p style="padding-left: 60px">One way to do this is with <span>1×1 </span><span>bottleneck convolutions. But, the authors of the paper argue that if this convolution is followed by non-linearity like ReLU, this might lead to a loss of manifold information. If the ReLU input is larger than 0, then the output of this unit is equivalent to the linear transformation of the input. But, if the input is smaller, then the ReLU collapses and the information of that unit is lost. Because of this, MobileNet uses 1×1 bottleneck convolution without non-linear activation.</span></p>
<ul>
<li><strong>Inverted residuals</strong>: In the <em>Residual networks</em> section, we introduced the bottleneck residual block, where the data flow in the non-shortcut path is <strong>input -&gt; 1<span>×</span>1 bottleneck conv -&gt; 3<span>×</span>3 conv -&gt; 1<span>×</span>1 unsampling conv</strong>. In other words, it follows a <strong>wide -&gt; narrow -&gt; wide</strong> data representation. The authors argue that <em>the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor</em>. Because of this, they propose having shortcut connections between the bottleneck connections instead.</li>
</ul>
<p>Based on these properties, the MobileNet model is composed of the following building blocks:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1249 image-border" src="assets/f84ccba9-bb59-4d84-bcd9-3701fd3b9e62.png" style="width:44.50em;height:12.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Top: inverted residual block with stride 1. Bottom: stride 2 block</div>
<p><span>The model uses ReLU6 non-linearity: </span>ReLU6 = min(max(input, 0),6). <span>The maximum activation value is limited to 6—</span><span>in this way, the non-linearity is more robust in low-precision floating-point computations. That's because </span><span>6 can take at most 3 bits, leaving the rest for the floating-point portion of the number. </span></p>
<p>Besides stride, the blocks are described by an expansion factor, <em>t</em>, which determines the expansion ratio of the bottleneck convolution.</p>
<p>The following table shows the relationship between the input and output dimensions of the blocks:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1250 image-border" src="assets/deac5fcf-f98e-40f8-b066-5e2f656d142a.png" style="width:29.33em;height:7.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The input and output dimensions relationship. Source: https://arxiv.org/abs/1801.04381<a href="https://arxiv.org/abs/1801.04381"/></div>
<p>In the preceding table, <strong>h</strong><span> and </span><strong>w</strong><span> are the input height and width, </span><strong>s</strong><span> is the stride, and</span><span> </span><strong>k</strong><span> and </span><strong>k'</strong><span> are the input and output number of channels.</span></p>
<p>Finally, here is the full model architecture: </p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1251 image-border" src="assets/f4f0756c-e7fb-4b20-a32a-683e628747be.png" style="width:21.17em;height:15.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The MobileNetV2 architecture. Source: https://arxiv.org/abs/1801.04381</div>
<p>Each line describes a group of one or more identical blocks, repeated <em>n</em> times. All layers in the same group have the same number, <strong>c</strong>, of output channels. The first layer of each sequence has a stride, <strong>s</strong>, and all others use stride 1. All spatial convolutions use 3 × 3 kernels. The expansion factor, <strong>t</strong>, is always applied to the input size, as described in the preceding table.</p>
<p>The next model we'll discuss is a network model with a new type of building block, where all layers are interconnected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to DenseNets</h1>
                </header>
            
            <article>
                
<p>DenseNet <span>(<em>Densely Connected Convolutional Networks</em>, </span><a href="https://arxiv.org/abs/1608.06993">https://arxiv.org/abs/1608.06993</a><span>) </span>try to alleviate the vanishing gradient problem and improve feature propagation, while reducing the number of network parameters. We've already seen how ResNets introduce residual blocks with skip connections to solve this. DenseNets take some inspiration from this idea and take it even further with the introduction of dense blocks. A dense block consists of sequential convolutional layers, where any layer has a direct connection to all subsequent layers. In other words, a network layer, <em>l</em>, will receive input, <strong>x</strong><em><sub>l</sub></em>, from all preceding network layers:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6c100639-f0e1-4f6e-8fd0-f7df5a9cacad.png" style="width:11.67em;height:1.17em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/72b34058-eb37-45f3-b58e-6714361f0560.png" style="width:7.42em;height:1.17em;"/> are the <strong>concatenated </strong>output feature maps of the preceding network layers. <span>This is unlike </span><span>ResNets, where we combine different layers with the element-wi</span>se sum. <em>H<sub>l</sub></em> is a <span>composite function, which defines three types of DenseNet blocks (only two are displayed):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1252 image-border" src="assets/e47bdbbc-aaad-41b4-9371-2498f5bbe3c8.png" style="width:49.08em;height:18.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A dense block: the dimensionality-reduction layers (dashed lines) are part of the DenseNet-B architecture, while DenseNet-A doesn't have them. DenseNet-C is not displayed</div>
<p><span>Let's define them:</span></p>
<ul>
<li><strong>DenseNet-A</strong>: This is the base block, where <em>H<sub>l</sub></em> consists of batch normalization, followed by activation, and a 3<span>×</span>3 convolution:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2934a1ea-ee3a-404e-8913-e08533fd1c18.png" style="width:19.08em;height:1.17em;"/></p>
<ul>
<li><strong>DenseNet-B</strong>: <span>The authors also introduced a second type of dense block, DenseNet-B, which applies a dimensionality-reduction 1×1 convolution after each concatenation:</span></li>
</ul>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><em><img class="fm-editor-equation" src="assets/7f2113f2-433e-409a-bca9-51c15c338392.png" style="width:57.42em;height:1.50em;"/></em></p>
<ul>
<li><strong>DenseNet-C</strong>: A further modification, which adds a downsampling 1<span>×</span>1 convolution after each dense block. The combination of B and C is referred to as DenseNet-BC. </li>
</ul>
<p><span>A dense block is specified by its number of convolutional layers and the output volume depth of each layer, which is called the <strong>growth rate</strong> in this context. Let's assume </span>that <span>the input of the dense block has a volume depth of <em>k<sub>0</sub></em> and the output volume depth of each convolutional layer is <em>k</em>. Then, because of the concatenation, the input volume depth for the <em>l</em>-th layer will be <em>k<sub>0</sub>+k<sub>x</sub>(l − 1)</em>. </span>Although the later layers of a dense block have a large input volume depth (because of the many concatenations), DenseNets can work with growth rate values as low as 12, which reduces the total number of parameters. To understand why this works, let's think of the feature maps as the <strong>collective knowledge</strong> (or global state) of the network. Each layer adds its own <em>k</em> feature maps to this state, and the growth rate determines the amount of information the layer contributes to it. Because of the dense structure, the global state can be accessed from everywhere within the network (hence the term global). In other words, there is no need to replicate it from one layer to the next as in traditional network architectures, which allows us to start with a smaller number of feature maps.</p>
<p>To make concatenation possible, dense blocks use padding in such a way that the height and width of all output slices are the same throughout the block. But because of this, downsampling is not possible within a dense block. Therefore, <span>a dense network consists of multiple sequential dense blocks, separated by downsampling pooling operations. </span></p>
<p>The authors of the paper have proposed a family of DenseNets, whose overall architecture resembles ResNet:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8eed3144-e67e-4370-ac24-9c13d1bdbcd1.png" style="width:56.42em;height:27.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The family of DenseNet networks. Source: https://arxiv.org/abs/1608.06993</div>
<p>They have the following properties:</p>
<ul>
<li>Start with a 7<span>×</span>7 stride 2 downsampling convolution. </li>
<li>A further downsampling 3<span>×</span>3 max pooling with stride 2.</li>
<li>Four groups of DenseNet-B blocks. The family of networks differs by the number of dense blocks within each group. </li>
<li>Downsampling is handled by a transition layer of a 2<span>×</span>2 pooling operation with stride 2 between the dense groups.</li>
<li>The transition layer contains a further 1<span>×</span>1 bottleneck convolution to reduce the number of feature maps. The compression ratio of this convolution is specified by a hyper-parameter, <em>θ</em>, where <em>0</em> &lt; <em>θ</em> ≤ <em>1</em>. If the number of input feature maps is <em>m</em>, then the number of output feature maps is <em>θm.</em></li>
<li>The dense blocks end up with a 7<span>×</span>7 global average pooling, followed by a 1,000-unit fully connected softmax layer.</li>
</ul>
<p>The authors of DenseNet have also released an improved DenseNet model called MSDNet (<em>Multi-Scale Dense Networks for Resource Efficient Image Classification</em>, <a href="https://arxiv.org/abs/1703.09844">https://arxiv.org/abs/1703.09844</a>), which (as the name suggests) uses multi-scale dense blocks. </p>
<p>With DenseNet, we conclude our discussion about conventional CNN architectures. In the next section, we'll discuss whether it's possible to automate the process of finding the optimal NN architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The workings of neural architecture search</h1>
                </header>
            
            <article>
                
<p>The NN models we've discussed so far were designed by their authors. But, what if we could make the computer itself design the NN? Enter <strong>neural architecture search</strong> (<strong>NAS</strong>)—a technique that automates the design of NNs.</p>
<div class="packt_infobox">Before we continue, let's see what the network architecture consists of:
<ul>
<li>The graph of operations, which represents the network. As we discussed in <a href="b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml">Chapter 1</a>, <em>The Nuts and bolts of Neural Networks</em>, the operations include (but are not limited to) convolutions, activation functions, fully connected layers, normalization, and so on.</li>
<li>The parameters of each operation. For example, the convolution parameters are: type (cross-channel, depthwise, and so on), input dimensions, number of input and output slices, stride, and padding.</li>
</ul>
<span>This set of architecture parameters are a subset of all hyperparameters of the NN ML algorithm. Other parameters include learning rate, mini-batch size, optimization algorithm (for example, Adam or SGD). Therefore, we can think of NAS as a type of hyperparameter optimization problem (the task of selecting optimal hyperparameters for an ML algorithm). Hyperparameter optimization itself is one of the components of automated machine learning (AutoML). This is a</span> broader process, w<span>hich aims to automate all steps of an ML solution. A</span>n AutoML algorithm will first select the type of algorithm (for example, decision tree or NN) and then it will proceed with the hyperparameter optimization of the selected algorithm. Since our book is focused on NNs, we'll assume that we've already selected the algorithm (how convenient!) and now we want to design our network.</div>
<p><span>In this section, we'll discuss gradient-based NAS with reinforcement learning (<em>Neural Architecture Search with Reinforcement Learning</em>, </span><a href="https://arxiv.org/abs/1611.01578">https://arxiv.org/abs/1611.01578</a><span>). At this point, we won't discuss reinforcement learning, and we'll focus on the algorithm instead. It starts with the premise that we can represent the network definition as a string (a sequence of tokens). Let's assume that we'll generate a sequential CNN, which consists only of convolutions. </span></p>
<p><span>Then, part of the string definition will look like this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1683 image-border" src="assets/dd4ee96f-aa67-4c50-b853-a98045890dbd.png" style="width:49.17em;height:5.92em;"/></p>
<p><span>We don't have to specify the layer type, because we only use convolutions.</span> <span>We exclude padding for the sake of simplicity. The subscript text on the first line is included for clarity, but won't be included in the algorithmic version. </span></p>
<p><span>We can see the algorithm overview in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1254 image-border" src="assets/9c0eb578-01b6-4814-881f-79cb79301681.png" style="width:28.08em;height:11.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">NAS overview. Source: https://arxiv.org/abs/1611.01578</div>
<p class="CDPAlignLeft CDPAlign">Let's start with the controller. It is an RNN, whose task is to generate new network architectures. Although we haven't yet discussed RNNs (this honor won't come until <a href="379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml">Chapter 7</a>, <em>Understanding Recurrent Networks</em>), we'll try to explain how it works nevertheless. In <a href="b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml">Chapter 1</a>,<span> </span><em>The Nuts and Bolts of Neural Networks</em>, we mentioned that an RNN maintains an internal state—a summary of all its previous inputs. Based on that internal state and the latest input sample, the network generates a new output, updates its internal state, and waits for the next input.</p>
<p class="CDPAlignLeft CDPAlign">Here, the controller will generate the string sequence, which describes the network architecture. The controller output is a single token of the sequence. This could be filter height/width, stride width/height, or the number of output filters. The type of token depends on the length of the <span>currently </span>generated architecture. Once we have this token, we feed it back to the RNN controller as input. Then, the network generates the next token of the sequence.</p>
<p class="CDPAlignLeft CDPAlign">This process is depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1255 image-border" src="assets/36bb6544-b95c-41a3-879b-78160ed1fd28.png" style="width:44.17em;height:15.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Generating a network architecture with the RNN controller. The output token is fed back to the controller as input to generate the next token. Source: https://arxiv.org/abs/1611.01578</div>
<p>The white vertical squares in the diagram represent the RNN controller, which consists of a two-layer <strong>Long short-term memory</strong> (<strong>LSTM</strong>) cell (along the <em>y</em>-axis). Although the diagram shows multiple instances of the RNN (along the <em>x</em>-axis), it is in fact the same network; it's just <strong>unfolded</strong> in time, to represent the process of sequence generation. That is, each step along the <em>x</em>-axis represents a single token of the network definition. A token prediction at step <em>t</em> is carried out by a softmax classifier and then fed as controller input at step <em>t+1</em>. <span>We continue this process until the length of the generated network reaches a certain value. Initially, this value is small (a short network), but it gradually increases (a longer network) as the training progresses.</span></p>
<p>To better understand NAS, let's see a step-by-step execution of the algorithm:</p>
<ol>
<li>The controller generates a new architecture, <em>A</em>.</li>
<li>It builds and <strong>trains</strong> a new network with said architecture until it converges.</li>
<li>It tests the new network on a withheld part of the training set and measures the error, <em>R</em>.</li>
<li>It uses this error to update the controller parameters, <em>θ<sub>c</sub></em>. As our controller is RNN, this means training the network and adjusting its weights. The model parameters are updated in such a way as to reduce the error, <em>R</em>, of the future architectures. This is made possible by a reinforcement learning algorithm called REINFORCE, which is beyond the scope of this section.</li>
<li>It repeats these steps until the <span>error, </span><em>R</em>, of the generated network falls below a certain threshold. </li>
</ol>
<p>The controller can generate network architectures with some restrictions. As we mentioned earlier in this section, the most severe is that the generated network only consists of convolutional layers. <span>To simplify things, each convolutional layer automatically includes batch normalization and ReLU activation.</span> But in theory, the controller could generate more complex architectures with other layers such as pooling or normalization. We could implement this by adding additional controller steps in the architecture sequence for the layer type.</p>
<p>The authors of the paper implemented a technique that allows us to add residual skip connections to the generated architecture. It works with a special type of controller step called an anchor point. The anchor point at layer <em>N</em> has content-based sigmoids. The output of a sigmoid <em>j (j = 1, 2, 3, ..., N-1)</em> represents the probability that the current layer has a residual connection to layer <em>j</em>.</p>
<p>The modified controller is depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1256 image-border" src="assets/6365b957-b098-48ef-b4fc-95a9c8460a5c.png" style="width:43.17em;height:17.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">RNN controller with anchor points for the residual connections. Source: https://arxiv.org/abs/1611.01578</div>
<p>If one layer has many input layers, all inputs are concatenated along the channel (depth) dimension. Skip connections could create some issues in the network design:</p>
<ul>
<li>The first hidden layer of the network (that is, the one not connected to any other input layer) uses the input image as an input layer.</li>
<li>At the end of the network, all layer outputs that have not been connected are concatenated in a final hidden state, which is sent to the classifier.</li>
<li>It may happen that the outputs to be concatenated have different sizes. In that case, the smaller feature maps are padded with zeros to match the size of the bigger ones.</li>
</ul>
<p>In their experiment, the authors used a controller with a 2-layer LSTM cell with 35 units in each layer. For every convolution, the controller has to select a filter height and width from the values {1, 3, 5, 7}, and a number of filters to be one of {24, 36, 48, 64}. Additionally, they performed 2 sets of experiments—one where the controller was allowed to select strides in {1, 2, 3} and another with a fixed stride of 1.</p>
<p>Once the controller generates an architecture, the new network is trained for 50 epochs on 45,000 images of the CIFAR-10 dataset. The remaining 5,000 images are used for validation.<span> During training, the controller starts with an architecture depth of 6 layers and then increases the depth by 2 layers on every 1,600 iterations. </span>The best performing model has a<span> validation accuracy of 3.65%. It </span>was discovered after 12,800 architectures using 800 GPUs (wow!). The reason for these steep computational requirements is that each new network is trained from scratch just to produce one accuracy value, which can then be used to train the controller. More recently, the new ENAS algorithm (<em>Efficient Neural Architecture Search via Parameter Sharing</em>, <a href="https://arxiv.org/abs/1802.03268">https://arxiv.org/abs/1802.03268</a>) has made it possible to significantly reduce the computational resources of NAS by sharing the weights among the generated models.</p>
<p>In the next section, we'll discuss a novel type of NN, which tries to overcome some of the limitations of the CNNs we talked about so far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing capsule networks</h1>
                </header>
            
            <article>
                
<p>Capsule networks <span>(<em>Dynamic Routing Between Capsules</em>, <a href="https://arxiv.org/abs/1710.09829">https://arxiv.org/abs/1710.09829</a>)</span><span> </span>were introduced as a way to overcome some of the limitations of standard CNNs. To understand the idea behind capsule networks, we need to understand their limitations first, which we will see in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The limitations of convolutional networks</h1>
                </header>
            
            <article>
                
<p>Let's start with a quote from professor<span> </span>Hinton himself:</p>
<div class="packt_quote">"The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster."</div>
<p>As we mentioned in <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <em>Understanding Convolutional Networks</em>, CNNs are <strong>translation-invariant</strong>. Let's<span> </span>imagine a picture with a face, located in the right half of the picture. Translation invariance means that a CNN is very good at telling us that the picture contains a face, but it cannot tell us whether the face is in the left or right part of the image. The main culprit for this behavior is the pooling layers. Every pooling layer introduces a little translation invariance. For example, the max pooling routes forward the activation of only one of the input neurons, but the subsequent layers don't have any knowledge of which neuron is routed.</p>
<p>By stacking multiple pooling layers, we gradually increase the receptive field size. But, the detected object could be anywhere in the new receptive field, because none of the pooling layers relay such information. Therefore, we also increase the translation invariance. At first, this might seem to be a good thing, because the final labels have to be translation-invariant. But, it poses a problem, as CNNs cannot identify the position of one object relative to another. A CNN would identify both of the following images as a face, because they both contain the components of a face (a nose, mouth, and eyes) regardless of their relative positions to one another.</p>
<p>This is also known as the<span> </span><span><strong>Picasso problem</strong>, as demonstrated in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1257 image-border" src="assets/2b264a39-059f-4898-8bbc-b3d5b423b1ff.png" style="width:27.00em;height:11.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A convolutional network would identify both of these images as a face</div>
<p>But, that's not all. A CNN would be confused even if the face had a different<span> </span><strong>orientation</strong>, for example, if it was turned upside down. One way to overcome this is with data augmentation (rotation) during training. But, this only shows the limitations of the network. We have to explicitly show the object in different orientations and tell the CNN that this is, in fact, the same object.</p>
<p>So far, we've seen that a CNN discards the translation information (transitional invariance) and doesn't understand the orientation of an object. In computer vision, the combination of<span> </span><span>translation</span><span> </span>and orientation is known as the<span> </span><strong>pose</strong>. The pose is enough to uniquely identify the object's<span> </span><span>properties in the coordinate system.</span><span> </span><span>Let's use</span><span> </span><span>computer graphics to illustrate this. A 3D object, say a cube, is entirely defined by its pose and the edge length.</span><span> </span><span>The process of transforming the representation of a 3D object into an image on the screen is called rendering. Knowing just its pose and the edge length of the cube,</span><span> </span><span>we can render it from any point of view we like.</span></p>
<p><span>Therefore, if we can somehow train a network to understand these properties, we won't have to feed it with multiple augmented versions of the same object. A CNN cannot do that, because its internal data representation doesn't contain information about the object's pose (only about its type). In contrast, capsule networks <strong>preserve information</strong> for both the type and the pose of an object. Therefore, they can detect objects that can transform into each other, which is known as <strong>equivariance</strong>. We can also think of this as <strong>reverse graphics</strong>, that is, a reconstruction of the object's </span>properties <span>according to its rendered image.</span></p>
<p><span>To solve these problems, the authors of the paper propose a new type of network building block, called a</span><span> </span><strong>capsule</strong><span>, instead of the neuron. Let's discuss it in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capsules</h1>
                </header>
            
            <article>
                
<p><span>The output of a capsule is a vector, compared to </span>the output of a<span> </span>neuron, which is a scalar value. The capsule output vector carries the following meaning:</p>
<ul>
<li>The elements of the vector represent the pose and other properties of the object.</li>
<li>The length of the vector is in the (0, 1) range and represents the probability of detecting the feature at that location. As a reminder, the length of a vector is <sub><img class="fm-editor-equation" src="assets/0164c9bd-513d-44ac-a282-cf9fc9a2293c.png" style="width:6.67em;height:2.00em;"/></sub>, where<span> </span><em>v<sub>i</sub></em><span> </span>are the vector elements.</li>
</ul>
<p>Let's consider a capsule that detects faces. If we start moving a face across an image, the values of the capsule vector will change to reflect the change in the position. However, its length will always stay the same, because the probability of the face doesn't change with the location.</p>
<p><span>Capsules are organized in interconnected layers, just like a regular network.</span><span> </span>The capsules in one layer serve as input to the capsules in the next. And, like a CNN, the shallower layers detect basic features, and the deeper layers combine them in more abstract and complex ones. But now, the capsules also relay positional information, instead of just detected objects. This allows the deeper capsules to analyze not only the presence of features, but also their relationship. For example, a capsule layer may detect a mouth, face, nose, and eyes. The subsequent capsule layer will be able to not only verify the presence of these features, but also whether they have the correct spatial relationship. Only if both conditions are true can the subsequent layer verify that a face is present. This is a high-level overview of capsule networks. Now, let's see how exactly capsules work.</p>
<p><span>We can see the schematic of a capsule i</span>n the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1258 image-border" src="assets/17593e60-cd8e-4ffd-8916-b875625f79c7.png" style="width:20.58em;height:10.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A capsule</div>
<p>Let's analyze it in the following steps:</p>
<ol>
<li>The capsule inputs are the output vectors,<span> </span><strong>u</strong><em><sub>1</sub>,</em> <strong>u</strong><em><sub>2</sub>, ...</em> <strong>u</strong><em><sub>n</sub></em>, from the capsules of the previous layer.</li>
<li>We multiply each vector,<span> </span><strong>u</strong><em><sub>i</sub></em>, by its corresponding weight matrix,<span> </span><em>W<sub>ij</sub></em>, to produce<span> </span><strong>pred</strong><strong>iction vectors</strong>, <sub><img class="fm-editor-equation" src="assets/af47eced-a6b2-4d2c-9b72-529fc22462c1.png" style="width:5.75em;height:1.17em;"/></sub>. The<span> </span>weight matrices,<span> </span><strong>W</strong>, encode spatial and other relationships between the lower-level features, coming from the capsules of the previous layer, and the high-level ones in the current layer. For example, imagine that the capsule in the current layer detects faces and the capsules from the previous layer detect the mouth (<strong>u</strong><em><sub>1</sub></em>),<span> </span><span>eyes (<strong>u</strong><em><sub>2</sub></em>),</span><span> </span>and nose (<strong>u</strong><em><sub>3</sub></em>). Then, <sub><img class="fm-editor-equation" src="assets/b4c26187-9be6-4b8e-8eb0-bc91e7ad1715.png" style="width:6.00em;height:1.17em;"/></sub> is the predicted position of the face, given where the location of the mouth is. In the same way, <sub><img class="fm-editor-equation" src="assets/dcaec96e-5891-4d32-9a83-430790d1ff04.png" style="width:6.00em;height:1.17em;"/></sub> predicts the location of the face based on the detected location of the eyes, and <sub><img class="fm-editor-equation" src="assets/309fb5d1-833a-4b43-b136-62fdd584760a.png" style="width:6.00em;height:1.17em;"/></sub> predicts the location of the face based on the location of the nose. If all three lower-level capsule vectors agree on the same location, then the current capsule can be confident that a face is indeed present. We only used location for this example, but the vectors could encode other types of relationships between the features, such as scale and orientation. The weights,<span> </span><strong>W</strong>, are learned with backpropagation.</li>
</ol>
<ol start="3">
<li>Next, we multiply the <sub><img class="fm-editor-equation" src="assets/2970c647-8b48-4b8e-960d-c8616516740c.png" style="width:1.50em;height:1.33em;"/></sub> vectors by the scalar coupling coefficients,<span> </span><em>c<sub>ij</sub></em>. These coefficients are a separate set of parameters, apart from the weight matrices. They exist between any two capsules, and indicate which high-level capsules will receive input from a lower-level capsule. But, unlike weight matrices, which are adjusted via backpropagation, coupling coefficients are computed on the fly during the forward pass via a process called<span> </span><strong>dynamic routing</strong>. We'll describe it in the next section.</li>
<li>Then, we perform the sum of the weighted input vectors.<span> </span><span>This step is </span>similar <span>to the weighted sum in neurons, but with vectors:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ba2e77a3-d557-4965-a7cd-c66f0513762e.png" style="width:8.75em;height:3.17em;"/></p>
<ol start="5">
<li>Finally, we'll compute the output of the capsule,<span> </span><strong>v</strong><em><sub>j</sub></em>, by squashing the vector,<span> </span><strong>s</strong><em><sub>j</sub></em>. In this context, squashing means transforming the vector in such a way that its length comes in the (0, 1) range, without changing its direction. As mentioned, the length of the capsule vector represents the probability of the detected feature and squashing it in the (0, 1) range reflects that. To do this, the authors propose a novel formula:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/45aaa75e-ef3c-4427-8b4f-6ad373e78a0c.png" style="width:10.00em;height:3.83em;"/></p>
<p>Now that we know the structure of the capsules, <span>in the following section, we'll</span> describe the algorithm to compute the coupling coefficients between capsules of different layers. That is, the mechanism by which they relay signals between one another.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic routing</h1>
                </header>
            
            <article>
                
<p>Let's describe the dynamic routing process to<span> </span>compute the<span> </span><span>coupling coefficients,</span><span> </span><em>c<sub>ij</sub></em><span>, displayed in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3433e46b-78fc-42f1-a1f8-d187d0824843.png" style="width:39.17em;height:21.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Dynamic routing example. The grouped dots indicate lower-level capsules that agree with each other</div>
<p><span>We have a lower-level capsule, <em>I</em>, that has to decide whether to send its output to one of two higher-level capsules, <em>J</em> and <em>K</em>. The dark and light dots represent prediction vectors, <img class="fm-editor-equation" src="assets/52b505fd-dae9-4348-8af6-c50be798ceab.png" style="width:1.67em;height:1.33em;"/>and <img class="fm-editor-equation" src="assets/d8f4d069-59c4-418d-8641-0644a4c1a5c5.png" style="width:1.83em;height:1.33em;"/>, which <em>J</em> and <em>K</em> have already received from other lower-level capsules. The arrows from the <em>I</em> capsule to the <em>J</em> and <em>K</em> capsules point to the <img class="fm-editor-equation" src="assets/18952087-30c0-49a3-a1de-6641307e87f4.png" style="width:1.58em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/017be278-853c-4f1b-936c-0dfcbc6e49b3.png" style="width:1.67em;height:1.33em;"/> prediction v</span>ectors from <em>I</em> to <em>J</em> and <em>K</em>. The clustered prediction vectors (lighter dots) indicate lower-level capsules that agree with each other with regard to the high-level feature. For example, if the<span> </span><em>K</em><span> </span>capsule describes a face, then the clustered predictions would indicate lower-level features, such as a mouth, nose, and eyes. Conversely, the dispersed (darker) dots indicate disagreement. If the<span> </span><em>I</em><span> </span>capsule predicts a vehicle tire, it would disagree with the clustered predictions in<span> </span><em>K</em>.</p>
<p>However, if the clustered predictions in<span> </span><em>J</em><span> </span>represent features such as headlights, windshield, or fenders, then the prediction of<span> </span><em>I</em><span> </span>would be in agreement with them. The lower-level capsules have a way of determining whether they fall into the clustered or dispersed group of each higher-level capsule. If they fall into the clustered group, they will increase the corresponding coupling coefficient with that capsule and will route their vector in that direction. Conversely, if they fall into the dispersed group, the coefficient will decrease.</p>
<p>Let's formalize this knowledge with a step-by-step algorithm, introduced by the authors:</p>
<ol>
<li>For all<span> </span><em>i</em><span> </span>capsules in the<span> </span><em>l</em><span> </span>layer, and<span> </span><em>j</em><span> </span>capsules in the<span> </span><em>(l + 1)</em><span> </span>layer, we'll initialize<span> </span><sub><img class="fm-editor-equation" src="assets/d0af0750-02cf-491b-9bcd-1965a7670f14.png" style="width:3.17em;height:1.17em;"/></sub>, where<span> </span><em><span>b</span><sub>ij</sub></em><span> </span>is a temporary variable equivalent to<span> </span><em>c<sub>ij</sub></em>. The vector representation of all<span> </span><em><span>b</span><sub>ij</sub></em><span> </span>is <strong>b</strong><sub><em>i</em></sub>. At the start of the algorithm, the<span> </span><em>i</em><span> </span>capsule has an equal chance of routing its output to any of the capsules of the<span> </span><em>(l + 1)</em><span> </span>layer.</li>
</ol>
<ol start="2">
<li>Repeat for<span> </span><em>r</em><span> </span>iterations, where<span> </span><em>r</em><span> </span>is a parameter:
<ol>
<li>For all<span> </span><em>i</em><span> </span>capsules in the<span> </span><em>l</em><span> </span>layer: <sub><img class="fm-editor-equation" src="assets/ac9bd249-5a93-4e77-9f99-f9583cef4ac8.png" style="width:7.92em;height:1.17em;"/></sub>.<span> </span><span>The sum of all outgoing coupling coefficients, <em>c<sub>i</sub></em>, of a capsule amounts to 1 (they have a probabilistic nature), hence the softmax.</span></li>
<li>For all<span> </span><em>j</em><span> </span>capsules in the<span> </span><em>(l + 1)</em><span> </span>layer: <sub><span><img class="fm-editor-equation" src="assets/5c2be571-0a64-4f10-be1c-3c7574a40c67.png" style="width:6.75em;height:1.33em;"/></span></sub>. That is, we'll compute all non-squashed output vectors of the<span> </span><em>(l + 1)</em><span> </span>layer.</li>
<li><span>For all <em>j</em> capsules in the <em>(l + 1)</em> layer, we'll </span>compute <span>the squashed vectors: <sub><img class="fm-editor-equation" src="assets/c045dd46-4691-40a5-b186-a2ad864ccd04.png" style="width:8.00em;height:1.33em;"/></sub>.</span></li>
<li>For all<span> </span><em>i</em><span> </span>capsules in the<span> </span><em>l</em><span> </span>layer, and<span> </span><em>j</em><span> </span>capsules in the<span> </span><em>(l + 1)</em><span> </span>layer: <sub><span><img class="fm-editor-equation" src="assets/022f06a1-e7e3-446a-bbf5-8c55e5f72cfa.png" style="width:8.42em;height:1.33em;"/></span></sub>. Here, <sub><span><img class="fm-editor-equation" src="assets/3768d012-866d-44cb-8efd-bb6d3210fa24.png" style="width:3.17em;height:1.33em;"/></span></sub>is the dot product of the prediction vector of the low-level<span> </span><em>i</em><span> </span>capsule and the output vector of the high-level<span> </span><em>j</em><span> </span>capsule vectors. If the dot product is high, then the<span> </span><em>i</em><span> </span>capsule is in agreement with the other low-level capsules, which route their output to the<span> </span><em>j</em><span> </span>capsule, and the coupling coefficient increases.</li>
</ol>
</li>
</ol>
<p>The authors have recently released an updated dynamic routing algorithm using a clustering technique called expectation-maximization. You can read more about it in the original paper,<strong><span> </span></strong><em>Matrix capsules with EM routing</em><span> </span>(<a href="https://ai.google/research/pubs/pub46653">https://ai.google/research/pubs/pub46653</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The structure of the capsule network</h1>
                </header>
            
            <article>
                
<p>In this section, we'll describe the structure of the<span> </span>capsule network, which the authors used to classify the MNIST dataset. The input of the network is the 28<span>×</span>28 MNIST grayscale images, and the following are the steps:</p>
<ol>
<li>We'll start with a single convolutional layer with 256 9<span>×</span>9 filters, stride 1, and ReLU activation. The shape of the output volume is (256, 20, 20).</li>
<li>We have another convolutional layer with 256 9<span>×</span>9 filters and stride 2. The shape of the output volume is (256, 6, 6).</li>
<li>Use the output of the layer as a foundation for the first capsule layer, called <kbd>PrimaryCaps</kbd>. Take the (256, 6, 6) output volume and split it into 32 separate (8, 6, 6) blocks. That is, each of the 32 blocks contains eight 6<span>×</span>6 slices. Take one activation value with the same coordinates from each slice and combine these values in a vector. For example, we can take activation (3, 7) of slice 1, (3, 7) of slice 2, and so on and combine them in a vector with a length of 8. We'll have 36 of these vectors. Then, we'll <strong>transform</strong> each vector into a capsule for a total of 36 capsules. The shape of the output volume of the <kbd>PrimaryCaps</kbd> layer is (32, 8, 6, 6).</li>
</ol>
<ol start="4">
<li>The second capsule layer is called <kbd>DigitCaps</kbd>. It contains 10 capsules (1 per digit), whose output is a vector with length 16.<span> </span><span>The shape of the output volume of the <kbd>DigitCaps</kbd> layer is (10, 16). During inference, we compute the length of each <kbd>DigitCaps</kbd> capsule vector. We then take the capsule with the longest vector as the prediction result of the network.</span></li>
<li>During training, the network includes three additional, fully connected layers after <kbd>DigitCaps</kbd>, the last of which has 784 neurons (28<span>×</span>28). In the forward training pass, the longest capsule vector serves as input to these layers. They try to reconstruct the original image, starting from that vector. Then, the reconstructed image is compared to the original one and the difference serves as additional regularization loss for the backward pass.</li>
</ol>
<p>Capsule networks are a new and promising approach to computer vision. However, they are not widely adopted yet and don't have an official implementation in any of the deep learning libraries discussed in this book, but you can find multiple third-party implementations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed some popular CNN architectures: we started with the classics, AlexNet and VGG. Then, we paid special attention to ResNets, as one of the most well-known network architectures. We also discussed the various incarnations of Inception networks and the Xception and MobileNetV2 models, which are related to them. We also talked about the exciting new ML area of neural architecture search. Finally, we discussed capsule networks—a new type of CV network, which tries to overcome some of the inherent CNN limitations. </p>
<p>We've already seen how to apply these models in <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>, <em>Understanding Convolutional Networks</em>, where we employed ResNet and MobileNet in a transfer learning scenario for a classification task. <span>In the next chapter, we'll see how to apply some of them to more complex tasks such as</span> <span>object detection and image segmentation. </span></p>


            </article>

            
        </section>
    </body></html>