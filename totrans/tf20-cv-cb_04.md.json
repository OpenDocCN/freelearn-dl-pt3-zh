["```\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras import Model\n    from tensorflow.keras.applications.inception_v3 import *\n    ```", "```\n    class DeepDreamer(object):\n        def __init__(self,\n                     octave_scale=1.30,\n                     octave_power_factors=None,\n                     layers=None):\n    ```", "```\n            self.octave_scale = octave_scale\n            if octave_power_factors is None:\n                self.octave_power_factors = [*range(-2, 3)]\n            else:\n                self.octave_power_factors = \n                           octave_power_factors\n            if layers is None:\n                self.layers = ['mixed3', 'mixed5']\n            else:\n                self.layers = layers\n    ```", "```\n            self.base_model = InceptionV3(weights='imagenet',\n                                         include_top=False)\n            outputs = [self.base_model.get_layer(name).output\n                      for name in self.layers]\n            self.dreamer_model = Model(self.base_model.input,\n                                       outputs)\n    ```", "```\n        def _calculate_loss(self, image):\n            image_batch = tf.expand_dims(image, axis=0)\n            activations = self.dreamer_model(image_batch)\n            if len(activations) == 1:\n                activations = [activations]\n            losses = []\n            for activation in activations:\n                loss = tf.math.reduce_mean(activation)\n                losses.append(loss)\n            total_loss = tf.reduce_sum(losses)\n            return total_loss\n    ```", "```\n        @tf.function\n        def _gradient_ascent(self, image, steps, step_size):\n            loss = tf.constant(0.0)\n            for _ in range(steps):\n                with tf.GradientTape() as tape:\n                    tape.watch(image)\n                    loss = self._calculate_loss(image)\n                gradients = tape.gradient(loss, image)\n                gradients /= tf.math.reduce_std(gradients) \n                                              + 1e-8\n                image = image + gradients * step_size\n                image = tf.clip_by_value(image, -1, 1)\n            return loss, image\n    ```", "```\n        def _deprocess(self, image):\n            image = 255 * (image + 1.0) / 2.0\n            image = tf.cast(image, tf.uint8)\n            image = np.array(image)\n            return image\n    ```", "```\n        def _dream(self, image, steps, step_size):\n            image = preprocess_input(image)\n            image = tf.convert_to_tensor(image)\n            step_size = tf.convert_to_tensor(step_size)\n            step_size = tf.constant(step_size)\n            steps_remaining = steps\n            current_step = 0\n            while steps_remaining > 0:\n                if steps_remaining > 100:\n                    run_steps = tf.constant(100)\n                else:\n                    run_steps = \n                         tf.constant(steps_remaining)\n                steps_remaining -= run_steps\n                current_step += run_steps\n                loss, image = self._gradient_ascent(image,\n                                               run_steps,\n                                               step_size)\n            result = self._deprocess(image)\n            return result\n    ```", "```\n        def dream(self, image, steps=100, step_size=0.01):\n            image = tf.constant(np.array(image))\n            base_shape = tf.shape(image)[:-1]\n            base_shape = tf.cast(base_shape, tf.float32)\n            for factor in self.octave_power_factors:\n                new_shape = tf.cast(\n                    base_shape * (self.octave_scale ** \n                                   factor),\n                                   tf.int32)\n                image = tf.image.resize(image, \n                                       new_shape).numpy()\n                image = self._dream(image, steps=steps,\n                                    step_size=step_size)\n            base_shape = tf.cast(base_shape, tf.int32)\n            image = tf.image.resize(image, base_shape)\n            image = tf.image.convert_image_dtype(image / \n                                                  255.0,\n                                           dtype=tf.uint8)\n            image = np.array(image)\n            return np.array(image)\n    ```", "```\n    import matplotlib.pyplot as plt\n    from tensorflow.keras.preprocessing.image import *\n    from ch4.recipe1.deepdream import DeepDreamer\n    ```", "```\n    def load_image(image_path):\n        image = load_img(image_path)\n        image = img_to_array(image)\n        return image\n    ```", "```\n    def show_image(image):\n        plt.imshow(image)\n        plt.show()\n    ```", "```\n    original_image = load_image('road.jpg')\n    show_image(original_image / 255.0)\n    ```", "```\n    dreamy_image = DeepDreamer().dream(original_image)\n    show_image(dreamy_image)\n    ```", "```\n    dreamy_image = (DeepDreamer(layers=['mixed2',\n                                        'mixed5',\n                                        'mixed7'])\n                    .dream(original_image))\n    show_image(dreamy_image)\n    ```", "```\n    dreamy_image = (DeepDreamer(octave_power_factors=[-3, -1,\n                                                      0, 3])\n                    .dream(original_image))\n    show_image(dreamy_image)                \n    ```", "```\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras import Model\n    from tensorflow.keras.applications.vgg19 import *\n    ```", "```\n    class StyleTransferrer(object):\n        def __init__(self,\n                     content_layers=None,\n                     style_layers=None):\n    ```", "```\n            self.model = VGG19(weights='imagenet',\n                               include_top=False)\n            self.model.trainable = False\n    ```", "```\n            self.style_weight = 1e-2\n            self.content_weight = 1e4\n            if content_layers is None:\n                self.content_layers = ['block5_conv2']\n            else:\n                self.content_layers = content_layers\n            if style_layers is None:\n                self.style_layers = ['block1_conv1',\n                                     'block2_conv1',\n                                     'block3_conv1',\n                                     'block4_conv1',\n                                     'block5_conv1']\n            else:\n                self.style_layers = style_layers\n    ```", "```\n            outputs = [self.model.get_layer(name).output\n                       for name in\n                       (self.style_layers + \n                       self.content_layers)]\n            self.style_model = Model([self.model.input], \n                                    outputs)\n    ```", "```\n        def _gram_matrix(self, input_tensor):\n            result = tf.linalg.einsum('bijc,bijd->bcd',\n                                      input_tensor,\n                                      input_tensor)\n            input_shape = tf.shape(input_tensor)\n            num_locations = np.prod(input_shape[1:3])\n            num_locations = tf.cast(num_locations,tf.float32)\n            result = result / num_locations\n            return result\n    ```", "```\n        def _calc_outputs(self, inputs):\n            inputs = inputs * 255\n            preprocessed_input = preprocess_input(inputs)\n            outputs = self.style_model(preprocessed_input)\n            style_outputs = outputs[:len(self.style_layers)]\n            content_outputs = \n                        outputs[len(self.style_layers):]\n            style_outputs = \n           [self._gram_matrix(style_output)\n                             for style_output in \n                                style_outputs]\n            content_dict = {content_name: value\n                            for (content_name, value)\n                            in zip(self.content_layers,\n                                   content_outputs)}\n            style_dict = {style_name: value\n                          for (style_name, value)\n                          in zip(self.style_layers,\n                                 style_outputs)}\n            return {'content': content_dict,\n                    'style': style_dict}\n    ```", "```\n        @staticmethod\n        def _clip_0_1(image):\n            return tf.clip_by_value(image,\n                                    clip_value_min=0.0,\n                                    clip_value_max=1.0)\n    ```", "```\n        @staticmethod\n        def _compute_loss(outputs, targets):\n            return tf.add_n([\n                tf.reduce_mean((outputs[key] - \n                               targets[key]) ** 2)\n                for key in outputs.keys()\n            ])\n    ```", "```\n        def _calc_total_loss(self,\n                             outputs,\n                             style_targets,\n                             content_targets):\n            style_outputs = outputs['style']\n            content_outputs = outputs['content']\n            n_style_layers = len(self.style_layers)\n            s_loss = self._compute_loss(style_outputs,\n                                        style_targets)\n            s_loss *= self.style_weight / n_style_layers\n            n_content_layers = len(self.content_layers)\n            c_loss = self._compute_loss(content_outputs,\n                                        content_targets)\n            c_loss *= self.content_weight / n_content_layers\n            return s_loss + c_loss\n    ```", "```\n        @tf.function()\n        def _train(self,\n                   image,\n                   s_targets,\n                   c_targets,\n                   epochs,\n                   steps_per_epoch):\n            optimizer = \n                  tf.optimizers.Adam(learning_rate=2e-2,\n                                           beta_1=0.99,\n                                           epsilon=0.1)\n            for _ in range(epochs):\n                for _ in range(steps_per_epoch):\n                    with tf.GradientTape() as tape:\n                        outputs = \n                             self._calc_outputs(image)\n                        loss = \n                          self._calc_total_loss(outputs,\n                                               s_targets,\n                                              c_targets)\n                    gradient = tape.gradient(loss, image)\n                    optimizer.apply_gradients([(gradient, \n                                                image)])\n                    image.assign(self._clip_0_1(image))\n            return image \n    ```", "```\n        @staticmethod\n        def _tensor_to_image(tensor):\n            tensor = tensor * 255\n            tensor = np.array(tensor, dtype=np.uint8)\n            if np.ndim(tensor) > 3:\n                tensor = tensor[0]\n            return tensor\n    ```", "```\n        def transfer(self, s_image, c_image, epochs=10,\n                     steps_per_epoch=100):\n            s_targets = self._calc_outputs(s_image)['style']\n            c_targets = \n              self._calc_outputs(c_image)['content']\n            image = tf.Variable(c_image)\n            image = self._train(image,\n                                s_targets,\n                                c_targets,\n                                epochs,\n                                steps_per_epoch)\n            return self._tensor_to_image(image)\n    ```", "```\n    import matplotlib.pyplot as plt\n    import tensorflow as tf\n    from chapter4.recipe3.styletransfer import StyleTransferrer\n    ```", "```\n    tf.config.experimental_run_functions_eagerly(True)\n    ```", "```\n    def load_image(image_path):\n        dimension = 512\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.convert_image_dtype(image, \n                                             tf.float32)\n        shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n        longest_dimension = max(shape)\n        scale = dimension / longest_dimension\n        new_shape = tf.cast(shape * scale, tf.int32)\n        image = tf.image.resize(image, new_shape)\n        return image[tf.newaxis, :]\n    ```", "```\n    def show_image(image):\n        if len(image.shape) > 3:\n            image = tf.squeeze(image, axis=0)\n        plt.imshow(image)\n        plt.show()\n    ```", "```\n    content = load_image('bmw.jpg')\n    show_image(content)\n    ```", "```\n    style = load_image(art.jpg')\n    show_image(style)\n    ```", "```\n    stylized_image = StyleTransferrer().transfer(style, \n                                                 content)\n    show_image(stylized_image)\n    ```", "```\n    stylized_image = StyleTransferrer().transfer(style, \n                                                 content,\n                                               epochs=100)\n    show_image(stylized_image)\n    ```", "```\n$> pip install tensorflow-hub\n```", "```\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow_hub import load\n    ```", "```\n    def load_image(image_path):\n        dimension = 512\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.convert_image_dtype(image, \n                                             tf.float32)\n        shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n        longest_dimension = max(shape)\n        scale = dimension / longest_dimension\n        new_shape = tf.cast(shape * scale, tf.int32)\n        image = tf.image.resize(image, new_shape)\n        return image[tf.newaxis, :]\n    ```", "```\n    def tensor_to_image(tensor):\n        tensor = tensor * 255\n        tensor = np.array(tensor, dtype=np.uint8)\n        if np.ndim(tensor) > 3:\n            tensor = tensor[0]\n        return tensor\n    ```", "```\n    def show_image(image):\n        if len(image.shape) > 3:\n            image = tf.squeeze(image, axis=0)\n        plt.imshow(image)\n        plt.show()\n    ```", "```\n    module_url = ('https://tfhub.dev/google/magenta/'\n                  'arbitrary-image-stylization-v1-256/2')\n    hub_module = load(module_url)\n    ```", "```\n    image = load_image('bmw.jpg')\n    show_image(image)\n    ```", "```\n    style_image = load_image('art4.jpg')\n    show_image(style_image)\n    ```", "```\n    results = hub_module(tf.constant(image),\n                         tf.constant(style_image))\n    stylized_image = tensor_to_image(results[0])\n    show_image(stylized_image)\n    ```", "```\n$> pip install Pillow\n```", "```\n    import pathlib\n    from glob import glob\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from PIL import Image\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def build_srcnn(height, width, depth):\n        input = Input(shape=(height, width, depth))\n        x = Conv2D(filters=64, kernel_size=(9, 9),\n                   kernel_initializer='he_normal')(input)\n        x = ReLU()(x)\n        x = Conv2D(filters=32, kernel_size=(1, 1),\n                   kernel_initializer='he_normal')(x)\n        x = ReLU()(x)\n        output = Conv2D(filters=depth, kernel_size=(5, 5),\n                        kernel_initializer='he_normal')(x)\n        return Model(input, output)\n    ```", "```\n    def resize_image(image_array, factor):\n        original_image = Image.fromarray(image_array)\n        new_size = np.array(original_image.size) * factor\n        new_size = new_size.astype(np.int32)\n        new_size = tuple(new_size)\n        resized = original_image.resize(new_size)\n        resized = img_to_array(resized)\n        resized = resized.astype(np.uint8)\n        return resized\n    ```", "```\n    def tight_crop_image(image):\n        height, width = image.shape[:2]\n        width -= int(width % SCALE)\n        height -= int(height % SCALE)\n        return image[:height, :width]\n    ```", "```\n    def downsize_upsize_image(image):\n        scaled = resize_image(image, 1.0 / SCALE)\n        scaled = resize_image(scaled, SCALE / 1.0)\n        return scaled\n    ```", "```\n    def crop_input(image, x, y):\n        y_slice = slice(y, y + INPUT_DIM)\n        x_slice = slice(x, x + INPUT_DIM)\n        return image[y_slice, x_slice]\n    ```", "```\n    def crop_output(image, x, y):\n        y_slice = slice(y + PAD, y + PAD + LABEL_SIZE)\n        x_slice = slice(x + PAD, x + PAD + LABEL_SIZE)\n        return image[y_slice, x_slice]\n    ```", "```\n    SEED = 999\n    np.random.seed(SEED)\n    ```", "```\n    file_patten = (pathlib.Path.home() / '.keras' / \n                   'datasets' /\n                   'dogscats' / 'images' / '*.png')\n    file_pattern = str(file_patten)\n    dataset_paths = [*glob(file_pattern)]\n    ```", "```\n    SUBSET_SIZE = 1500\n    dataset_paths = np.random.choice(dataset_paths, \n                                     SUBSET_SIZE)\n    ```", "```\n    SCALE = 2.0\n    INPUT_DIM = 33\n    LABEL_SIZE = 21\n    PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0)\n    STRIDE = 14\n    ```", "```\n    data = []\n    labels = []\n    for image_path in dataset_paths:\n        image = load_img(image_path)\n        image = img_to_array(image)\n        image = image.astype(np.uint8)\n        image = tight_crop_image(image)\n        scaled = downsize_upsize_image(image)\n        height, width = image.shape[:2]\n        for y in range(0, height - INPUT_DIM + 1, STRIDE):\n            for x in range(0, width - INPUT_DIM + 1, STRIDE):\n                crop = crop_input(scaled, x, y)\n                target = crop_output(image, x, y)\n                data.append(crop)\n                labels.append(target)\n    data = np.array(data)\n    labels = np.array(labels)\n    ```", "```\n    EPOCHS = 12\n    optimizer = Adam(lr=1e-3, decay=1e-3 / EPOCHS)\n    model = build_srcnn(INPUT_DIM, INPUT_DIM, 3)\n    model.compile(loss='mse', optimizer=optimizer)\n    ```", "```\n    BATCH_SIZE = 64\n    model.fit(data, labels, batch_size=BATCH_SIZE, \n              epochs=EPOCHS)\n    ```", "```\n    image = load_img('dogs.jpg')\n    image = img_to_array(image)\n    image = image.astype(np.uint8)\n    image = tight_crop_image(image)\n    scaled = downsize_upsize_image(image)\n    ```", "```\n    plt.title('Low resolution image (Downsize + Upsize)')\n    plt.imshow(scaled)\n    plt.show()\n    ```", "```\n    output = np.zeros(scaled.shape)\n    height, width = output.shape[:2]\n    ```", "```\n    for y in range(0, height - INPUT_DIM + 1, LABEL_SIZE):\n        for x in range(0, width - INPUT_DIM + 1, LABEL_SIZE):\n            crop = crop_input(scaled, x, y)\n            image_batch = np.expand_dims(crop, axis=0)\n            prediction = model.predict(image_batch)\n            new_shape = (LABEL_SIZE, LABEL_SIZE, 3)\n            prediction = prediction.reshape(new_shape)\n            output_y_slice = slice(y + PAD, y + PAD + \n                                   LABEL_SIZE)\n            output_x_slice = slice(x + PAD, x + PAD + \n                                  LABEL_SIZE)\n            output[output_y_slice, output_x_slice] = \n                                     prediction\n    ```", "```\n    plt.title('Super resolution result (SRCNN output)')\n    plt.imshow(output / 255)\n    plt.show()\n    ```"]