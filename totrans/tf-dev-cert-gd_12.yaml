- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Time Series, Sequences, and Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Time series** cut across various industries, sectors, and aspects of our
    lives. Finance, healthcare, social sciences, physics – you name it, time series
    data is there. It’s in sensors monitoring our environment, social media platforms
    tracking our digital footprint, online transactions recording our financial behavior,
    and many more avenues. This sequential data represents dynamic processes that
    evolve over time, and as we increasingly digitize our planet, the volume, and
    thereby the importance, of this data type is set to grow exponentially.'
  prefs: []
  type: TYPE_NORMAL
- en: Time series follow a chronological order, capturing events as they occur in
    time. This temporal nature of time series bestows a unique quality that differentiates
    it from cross-sectional data. When we turn on the searchlight on time series data,
    we can observe attributes such as trends, seasonality, noise, cyclicity, and autocorrelations.
    These unique characteristics endow time series data with rich information, but
    they also present us with a unique set of challenges that we must overcome to
    harness the gains inherent in this data type. With frameworks such as **TensorFlow**,
    we can leverage patterns from the past to make informed decisions about the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis – characteristics, applications, and forecasting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical techniques for forecasting time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for forecasting with neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sales forecasting with neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained theoretical insight and hands-on
    experience in building, training, and evaluating time series forecasting models
    using statistical and deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis – characteristics, applications, and forecasting techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know time series data is defined by the ordering of data points in a sequence
    over time. Imagine we are forecasting energy consumption patterns in London. Over
    the years, there has been a growing increase in energy consumption, perhaps due
    to urbanization – this signifies a positive upward trend. During winter each year,
    we expect energy consumption to rise as more people will need to heat up their
    homes and offices to stay warm. This seasonal change in the weather also accounts
    for seasonality in energy utilization. Again, we could also witness an unusual
    surge in energy consumption due to a major sporting event, leading to a large
    influx of guests during the period. This causes noise in the data as such events
    are one-offs or occur at irregular intervals.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, let us explore the characteristics of time series,
    types, applications, and techniques for modeling time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To effectively build efficient forecasting models, we need to gain a clear understanding
    of the underlying nature of time series data. We may find ourselves working with
    time series data that has a positive upward trend, monthly seasonality, noise,
    and autocorrelation, while the next time series we work on may have yearly seasonality
    and noise but no visible sign of autocorrelation or trends in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding these data properties of time series data arms us with the requisite
    details to make informed preprocessing decisions. For example, if we are working
    with a dataset with high volatility, our knowledge of this may inform our decision
    to apply smoothing during our preprocessing steps. Later in this chapter, where
    we will be building statistical and deep learning models for forecasting time
    series, we will see how our understanding of the properties of time series data
    will guide our decisions with respect to engineering new features, choosing optimal
    hyperparameter values, and making model selection decisions. Let us examine the
    characteristics of time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trend**: Trends refer to the general direction in which a time series is
    moving over the long term. We could look at a trend as the overall big picture
    of our time series data. Trends can be linear, as shown in *Figure 12**.1*, or
    nonlinear (quadratic and exponential); they can also be in a positive (upward)
    or negative (downward) direction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.1 – A plot showing a positive stock price trend over time](img/B18118_12_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – A plot showing a positive stock price trend over time
  prefs: []
  type: TYPE_NORMAL
- en: Trend analysis empowers data professionals, businesses, and policymakers to
    make informed decisions about the future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Seasonality**: Seasonality refers to repetitive cycles occurring at regular
    intervals over a specific period, such as on a daily, weekly, monthly, or yearly
    basis. These variations are often byproducts of seasonal fluctuations; for example,
    a retail store in a residential area might get higher sales on weekends compared
    to weekdays (weekly seasonality). The same store could also witness a surge in
    sales during the holiday season in December and a drop in sales shortly after
    the festive period (annual seasonality), as illustrated by *Figure 12**.2*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.2 – A plot showing annual seasonality for a retail store](img/B18118_12_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – A plot showing annual seasonality for a retail store
  prefs: []
  type: TYPE_NORMAL
- en: '**Cyclicality**: Cyclicality refers to irregular cycles that occur in a time
    series over a long period of time. Unlike seasonality, these cycles are long-term
    in nature and their duration and magnitude are irregular, making them harder to
    predict when compared to seasonality. Economic cycles are a good example of cyclicity.
    These cycles are influenced by several factors such as inflation, interest rates,
    and government policies. Due to its irregular nature, predicting the timing, duration,
    and magnitude of cycles can be quite challenging. Advanced statistical and machine
    learning models are often required to model and forecast cyclic patterns accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autocorrelation**: Autocorrelation is a statistical concept that refers to
    the correlation between a time series and the lagged version of itself, as shown
    in *Figure 12**.3*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.3 – A plot showing autocorrelation](img/B18118_12_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – A plot showing autocorrelation
  prefs: []
  type: TYPE_NORMAL
- en: It is often referred to as serial correlation and measures the degree to which
    a data point is related to its past values. Autocorrelations can be positive or
    negative, with values ranging from -1 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise**: Noise is an inherent part of any real-world data. It refers to the
    random fluctuations in data that cannot be explained by the model, nor can it
    be explained by any known underlying factors, patterns, or structural influences.
    These fluctuations can result from various sources such as measurement errors
    or unexpected events. For example, in the financial markets, an unpredicted event
    such as a political announcement can create noise that deviates stock prices from
    their underlying trends. Noise displays randomness and unexplained variations
    that we may have to smooth out when forecasting time series data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have discussed some important characteristics that can occur individually
    or together in time series data. Next, let us look at the types of time series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Types of time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time series can be classified as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Stationary and non-stationary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate and multivariate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stationary and non-stationary time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A stationary time series is a time series whose statistical properties (mean,
    variance, and autocorrelation) remain constant over time. It is a series that
    displays recurring patterns and behaviors that are likely to replicate themselves
    in the future. A non-stationary time series is the opposite. It is not stationary,
    and we typically find these types of time series in many real-world scenarios
    where the series may display trends or seasonality. For example, we will expect
    the monthly sales of a ski resort to reach their peak during winter and dip during
    off-seasons. This seasonal component has an impact on the statistical properties
    of the series.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate and multivariate time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A univariate time series is a type of time series where we track just one metric
    over time. For example, we could use a smartwatch to track the number of steps
    we take on a daily basis. On the other hand, when we track more than one metric
    over time, we have a multivariate time series, as shown in *Figure 12**.4*. In
    the chart, we see the interaction between inflation and wage growth in the UK.
    Over time, we see that inflation persistently outpaces wage growth, leaving everyday
    people with lowered real income and purchasing power:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – A multivariate time series showing the relationship between
    inflation and wage growth](img/B18118_12_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – A multivariate time series showing the relationship between inflation
    and wage growth
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate time series analysis lets us account for the dependencies and interactions
    between several variables over time. Next, let us delve into the importance of
    time series data and various applications of time series.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have discussed the types and properties of time series data. By applying
    machine learning techniques, we can leverage the wealth of information in these
    data types. Let us examine some of the important applications of machine learning
    in time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forecasting**: We can apply machine learning models to forecast time series;
    for instance, we may want to forecast the future sales of a retail store to inform
    our inventory decisions. If we analyze the sales record of the business, we may
    find some patterns, such as increased sales during holiday seasons or lowered
    sales during specific months. We can train our models with these patterns to make
    informed predictions about the future sales of the store, allowing key stakeholders
    to effectively plan for the expected demand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imputed data**: Missing values can pose a significant challenge when we are
    working on analyzing or forecasting time series data. A good solution is the application
    of imputation, allowing us to fill the missing data points with substitute values.
    For example, in *Figure 12**.5 (a)*, we see a plot of temperature values over
    a year. We quickly notice that some temperature recordings are missing. With the
    aid of imputation, we can estimate those values using adjacent days, as shown
    in *Figure* *12**.5 (b)*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.5 – A plot displaying temperature over time (a) with missing values
    (b) with no missing values](img/B18118_12_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – A plot displaying temperature over time (a) with missing values
    (b) with no missing values
  prefs: []
  type: TYPE_NORMAL
- en: By filling in the gaps, we now have a complete dataset that can be better utilized
    for analysis and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Anomalies are data points that deviate significantly
    from the general norm. We can apply time series analysis to detect anomalies and
    potentially identify significant issues. For example, in credit card transactions,
    as illustrated in *Figure 12**.6*, an unusually large transaction might indicate
    fraudulent activity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.6 – A plot showing spikes in transaction values](img/B18118_12_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – A plot showing spikes in transaction values
  prefs: []
  type: TYPE_NORMAL
- en: By using time series analysis to identify such anomalies, the bank can swiftly
    take action to mitigate potential damages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Trend analysis**: Understanding trends can provide valuable insights into
    the underlying phenomena. For example, the international energy agencies show
    that 14% of all new cars sold in 2022 were electric vehicles. This trend could
    indicate that people are moving toward a more sustainable option of transportation
    (see [https://www.iea.org/reports/global-ev-outlook-2023/executive-summary](https://www.iea.org/reports/global-ev-outlook-2023/executive-summary)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality analysis**: Another useful application of time series is in seasonality
    analysis. This could prove useful in guiding energy consumption planning and infrastructural
    expansion needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now looked at some important applications of time series data. Next,
    let us take a look at some important techniques for forecasting time series.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for forecasting time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this book, we will examine two main techniques for forecasting time series:
    statistical methods and machine learning methods. Statistical methods use mathematical
    models to capture the trend, seasonality, and other components of the time series
    data, with popular models being **autoregressive integrated moving average** (**ARIMA**)
    and **seasonal and trend decomposition using LOESS** (**STL**). However, these
    methods are beyond the scope of this book. Here, we will be using simpler statistical
    methods such as **naïve forecasting** and **moving averages** to establish our
    baseline, after which we will apply different machine learning methods. In this
    chapter, we will focus on using **deep neural networks** (**DNNs**), and in the
    next chapter, we will apply **recurrent neural networks** (**RNNs**) and **long
    short-term memory** **networks** (**LSTMs**).'
  prefs: []
  type: TYPE_NORMAL
- en: Each approach has its pros and cons, and the best approach to forecasting time
    series is largely dependent on the specific characteristics of the data and the
    problem at hand. It is important thatwe highlight here that time series forecasting
    is a broad field, and there are other methods that fall outside the scope of this
    book that you may explore at a later stage. Before we move into modeling time
    series problems, let us examine how we can evaluate this type of data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating time series forecasting techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To effectively evaluate a time series forecasting model, we must gauge its performance
    with appropriate metrics. In [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065),
    *Linear Regression with TensorFlow,* we explored several regression metrics such
    as MAE, MSE, RMSE, and MAPE. We can apply these metrics to evaluate time series
    forecasting models. However, in this chapter, we will concentrate on the application
    of MAE and MSE in line with the exam requirements. We use MAE to compute the average
    of the absolute difference between the predicted and true values. This way, we
    have a sense of how wrong our predictions are. A smaller MAE indicates a better
    model fit. Imagine you are a stock market analyst trying to forecast the future
    price of a specific stock. Using MAE as your evaluation metric, you would get
    a clear understanding of how much, on average, your forecasts differ from the
    actual stock prices. This information can help you refine your model to make more
    accurate predictions, minimizing potential financial risks.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, MSE takes the average of squared discrepancies between predictions
    and actuals. By squaring the errors, MSE is more sensitive to large errors compared
    to MAE, making it useful where large discrepancies are unfavorable, such as when
    working with a power grid where precise load forecasting is of top priority. With
    this in mind, let’s now turn our attention to a sales use case and apply our learnings
    to forecast future sales.
  prefs: []
  type: TYPE_NORMAL
- en: Retail store forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are working as a machine learning engineer and your company just
    landed a new project. A rapidly growing superstore in Florida wants your help.
    They want to predict future reviews, as this will serve as a guide in planning
    the expansion of their stores to meet the expected demand. You have been saddled
    with the responsibility of building a forecasting model with the available historical
    data provided by the Tensor superstore. Let’s jump in and see how you can solve
    this problem, as your company is counting on you. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the necessary libraries for our project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we import `numpy` and `matplotlib` for numerical analysis and visualization
    purposes and `pandas` for data transformations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we load the time series data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we load the data and use the `head` function to get a snapshot of the
    first five rows of the data. When we run the code, we see that the first day of
    sales from the data given to us is `2013-01-01`. Next, let us look at some statistics
    to get a sense of the data we have in hand.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the following code to check the data type and summary statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run the code, it returns the data type as `float64` and key summary
    statistics for our sales data. Using the `describe` function, we get a count of
    3,653 data points. This points to daily data over a 10-year period. We also see
    the mean sales per day come to around $75, giving us a sense of the central tendency.
    We see decent variability in the daily sales with a standard deviation of `20.2`.
    The minimum and maximum values reveal a range from $22 to $128 in sales, signaling
    some significant fluctuations occurring. The 25th and 75th percentiles are `60.27`
    and `89.18`, respectively, showing that lower-volume days see sales ofaround $60
    while higher-volume days see around $90\. Let us continue to explore the data
    by looking at it on a plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s visualize our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code returns the plot shown in *Figure 12**.7*, which represents the company’s
    sales over a 10-year period:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.7 – A plot showing sales over time](img/B18118_12_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – A plot showing sales over time
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in *Figure 12**.7*, we can observe an overall positive upward
    trend, potentially indicative of economic growth or successful business strategies,
    such as new product releases and effective marketing. A clear yearly seasonality
    also emerges; this may suggest that the company deals in seasonal goods with annual
    demand fluctuations. Also, we observe some noise exists. This could be a result
    of weather variability, random events, or the entry of competitors. The upward
    trend in the data demonstrates promising performance and growth in sales over
    time. However, the seasonal effects and noise elements showcase complex dynamics
    underneath the aggregate trend. Next, let’s explore how to data partition the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Data partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In time series forecasting, we typically divide our dataset into distinct sections:
    a training period for training our machine learning models, a validation period
    for model tuning and evaluation, and a test period for assessing performance on
    unseen data. This process is known as fixed partitioning. An alternate method
    is roll-forward partitioning, which we will be discussing shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: Our sales data demonstrates seasonality; hence, we have to split the data in
    such a way that each partition captures entire seasonal cycles. We do this to
    ensure we do not omit important seasonal patterns in one or more of our partitions.
    While this method diverges from typical data partitioning, we see that when working
    with other machine learning problems where random samples are taken to form training,
    validation, and testing sets, the fundamental purpose remains the same. We train
    our model on the training data, fine-tune it using the validation data, and evaluate
    it on the test data. We can then incorporate the validation data into the training
    data to benefit from the most recent information and forecast future data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is generally a good idea to ensure that your training set is large enough
    to capture all relevant patterns in the data, including any seasonal behavior
    of the data. When setting the size of your validation set, you must strike a balance.
    While a larger validation set gives a more reliable estimate of model performance,
    it also reduces the size of the training set. You should also remember to retrain
    your final model on the entire dataset (combining the training and validation
    sets) before making final predictions. This strategy maximizes the amount of data
    the model learns from, likely improving its predictive performance on future unseen
    data. Again, avoid shuffling the data before splitting, as it would disrupt the
    temporal order, leading to misleading results. In fixed partitioning, we usually
    use a chronological split such that the training data should be from the earliest
    timestamps, followed sequentially by the validation set, and finally, the test
    set containing the latest timestamps. Let’s split our sales data into training
    and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we split the data into training and validation sets. We take 80% of the
    data (`len(df) * 0.8`), which, in this case, is 8 years of data for training and
    the last 2 years for validation. We use the `int` function to ensure that the
    split time is an integer for indexing purposes. We set up our training and validation
    sets, using everything before the split time for training and everything after
    the split time for validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us plot our training and validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code displays the partitioning of sales data into a training set and a
    validation set, marking them in green and blue colors respectively, with dates
    along the *x* axis and sales along the *y* axis. For readability, we set our *x*
    ticks marks to every 180 days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – A plot showing fixed partitioning](img/B18118_12_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – A plot showing fixed partitioning
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 12**.8*, we split our sales data into 8 years for training and 2
    years for validation. In this scenario, our test set will be data from the future.
    This is done to ensure that the model is trained on the earliest part of our series,
    validated on the recent past, and tested on the future. Another method of partitioning
    time series data is called roll-forward partitioning, or “walk-forward” validation.
    In this method, we start with a short training period and gradually increase it.
    For each training period, the following period is used as the validation set.
    This mirrors a real-life situation where we continually retrain our model as new
    data comes in and use it to predict the next period. Let’s discuss our first method
    of forecasting, called naïve forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naïve forecasting is one of the simplest methods for forecasting in time series
    analysis. The principle behind naïve forecasting is to simply set all forecasts
    to be the value of the last observed point. This is why it’s referred to as “naïve.”
    It is a method that assumes that the future value is likely to be the same as
    the current one. Despite its simplicity, naïve forecasting can often serve as
    a good baseline for time series forecasting; however, its performance can vary
    depending on the characteristics of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see how to implement this in code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement naïve forecasting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To implement the naïve forecasting method, each forecasted value is simply
    set to the actual observed value from the previous time step, achieved by shifting
    the `Sales` column by one unit. We use `df.head()` to display the first five rows
    of the DataFrame, providing a quick overview of the sales data and the naïve forecast:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.9 – A snapshot of the DataFrame showing the sales and naïve forecasts](img/B18118_12_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – A snapshot of the DataFrame showing the sales and naïve forecasts
  prefs: []
  type: TYPE_NORMAL
- en: From the table in *Figure 12**.9*, we see that the first forecast is not available.
    This is because this method simply takes all the values in the series starting
    from one step before the validation data until the second-last value of the series.
    This effectively shifts the time series by one time step into the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us create a function for plotting purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We construct a utility plot to generate the graph of the true and predicted
    validation values. This function takes in the predicted and true values of our
    validation data along with the start date, end date, plot title, and plot label.
    This function gives us the flexibility to drill down into different areas of interest
    in the plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s plot the naïve forecast:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We pass in the required parameters and run the code to generate the following
    plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.10 – A time series forecast using the naïve method](img/B18118_12_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – A time series forecast using the naïve method
  prefs: []
  type: TYPE_NORMAL
- en: The plot in *Figure 12**.10* displays the forecasted values and the true values
    for the validation data. Because the plot looks a bit clustered due to the closeness
    in values, let us zoom in to help us visually investigate how our forecasting
    is doing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a specific time range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set the start date and end date parameters to `2022-01-01` and `2022-06-30`,
    respectively. The resulting plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Zoomed-in time series forecasting using the naïve method](img/B18118_12_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – Zoomed-in time series forecasting using the naïve method
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in *Figure 12**.11*, we can see that it starts the forecast one
    step later because the naïve forecast is shifted one step into the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us evaluate the performance of the naïve method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the `metrics` functions provided by the `.numpy()` to convert the result
    from a TensorFlow tensor to a NumPy array. When we run the code, we get an MSE
    for the naïve forecast as `45.22` and an MAE for the naïve forecast as `5.43`.
    Recall that for the MSE and MAE values, lower values are always better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s keep this in mind as we explore other forecasting techniques. Naïve forecasting
    can serve as a baseline to compare the performance of more complex models. Next,
    let’s examine another statistical method, called moving average.
  prefs: []
  type: TYPE_NORMAL
- en: Moving average
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving average is a technique for smoothing time series data by replacing each
    point with an average of the neighboring points. In this process, we generate
    a new series in which the data points are averages of the raw data in our original
    series. The key parameter in this method is the **window width**; this determines
    the number of consecutive raw data points included in the average calculation.
  prefs: []
  type: TYPE_NORMAL
- en: The term “moving” refers to the sliding of the window along the time series
    to compute average values, thereby generating a new series.
  prefs: []
  type: TYPE_NORMAL
- en: Types of moving averages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Two main types of moving averages are commonly used in time series analysis
    – the centered moving average and the trailing moving average:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centered moving average**: A centered moving average calculates the average
    around a central point (*t*). It uses data from both before and after the time
    of interest for visualization, as illustrated in *Figure 12**.12*. Centered moving
    averages can give a well-balanced view of data trends, but since they require
    future data, they’re not suitable for forecasting as we do not have access to
    future values when making forecasts. Centered moving averages are good for visualization
    and time series analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.12 – A plot showing acentered moving average](img/B18118_12_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – A plot showing acentered moving average
  prefs: []
  type: TYPE_NORMAL
- en: Centered moving averages can give a well-balanced view of data trends, but since
    they require future data, they’re not suitable for forecasting as we do not have
    access to future values when making forecasts. Centered moving averages are good
    for visualization and time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Trailing moving average**: A trailing moving average, also known as a rolling
    or running average, calculates the average using the most recent *n* data points.
    This method solely requires past data points, as illustrated in *Figure 12**.13*,
    making it ideal for forecasting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.13 – A plot showing atrailing moving average](img/B18118_12_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 – A plot showing atrailing moving average
  prefs: []
  type: TYPE_NORMAL
- en: To compute a trailing moving average, the first step is choosing the window
    width (*W*). This selection can depend on various factors such as the series’
    patterns and how much smoothing you want to achieve. A smaller window width will
    track quick changes closely, but this will happen at the risk of including more
    noise. On the other hand, a larger window width will provide a smoother line but
    might miss out on some short-term fluctuations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to implement moving averages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The code uses pandas’ `rolling` function to calculate the moving average of
    the `Sales` data over a 30-day window, then shifts the outcome one step forward
    to simulate a forecast for the subsequent time step, storing the result in a new
    `Moving_Average_Forecast` column. You can think of it as using a window of 30
    days of sales to predict the sales on the 31st day.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the `plot_forecast` function to plot both the validation data and the
    moving average forecast data. We can see the resulting plot in *Figure 12**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14 – A time series forecast using the moving average method](img/B18118_12_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.14 – A time series forecast using the moving average method
  prefs: []
  type: TYPE_NORMAL
- en: In the plot in *Figure 12**.14*, the moving average forecast is computed using
    the past 30 days of sales data. A smaller window size, such as a 7-day window,
    will follow the actual sales more closely than the 30-day moving average, but
    it might also capture more of the noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next logical step will be to evaluate our model by using the `metric` function
    again. This time, we pass in the moving average forecast against the true validation
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: When we run the code, we achieve an MSE of `55.55` and an MAE of `6.05` for
    the moving average forecast. The MAE and MSE are much worse than our baseline.
    If you change the window size to 7 days, we end up with a much lower MSE and MAE
    of `48.57` and `5.61,` respectively. This is a much better result, but slightly
    worse than our naïve approach. You could experiment with a smaller window size
    and see whether your results will surpass the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: However, we need to note that the underlying assumption when using moving average
    is stationarity. And we know that our time series has both trend and seasonality,
    so how do we achieve stationarity with this data? And will this help us to achieve
    a much lower MAE? To achieve stationarity, we use a concept called **differencing**.
    Let us discuss differencing next, and see how to apply it and whether it will
    help us achieve a much lower MAE and MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Differencing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Differencing is a method used to achieve stationarity in our time series. It
    works by calculating the difference between consecutive observations. The logic
    here is that, though the original series may have a trend and is non-stationary,
    the difference between the values of the series can be stationary. By differencing
    the data, we can remove the trend and seasonality, making the series stationary
    and thus suitable for a moving average forecast model. This can significantly
    improve the accuracy of the model and, therefore, the reliability of the forecast.
    Let us see this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block applies differencing to our time series data. We begin
    by generating a new series where each value is different between a value and the
    value 365 days earlier. We do this because we know our data has yearly seasonality.
    Next, we plot our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – A plot showing the sales time series after differencing](img/B18118_12_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 – A plot showing the sales time series after differencing
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the plot in *Figure 12**.15* has no trend or seasonality. Hence,
    we have achieved the stationarity required in the underlying assumption when using
    moving averages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now restore the trend and seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we incorporate seasonality back into our time series data after it has
    been differenced, then apply moving average forecasting on this restored data.
    We use a 7-day window for the moving average computation. After restoring the
    trend and seasonality by adding the shifted sales data to the differenced sales
    data, we compute a moving average of these restored sales over our chosen window
    size, and then shift the resulting series one step ahead for forecasting. We then
    split the data again into training and validation sets. The same split time is
    used to ensure consistency with the earlier split. This is crucial to ensure we
    are evaluating our model correctly. Finally, we prepare our forecasted and true
    sales values for evaluation by extracting the `Restored_Moving_Average_Forecast`
    and `Sales` values from the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – A sales forecast after restoring seasonality and trend](img/B18118_12_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 – A sales forecast after restoring seasonality and trend
  prefs: []
  type: TYPE_NORMAL
- en: In the plot in *Figure 12**.16*, the orange line represents the forecasts after
    we’ve added the past values back in to restore the trend and seasonality. Recall
    that we are using a window size of `7` with which we achieved lower MAE and MSE
    values. Now, we have essentially used the forecasts from the differenced series
    to forecast the changes from one year to the next and then added these changes
    onto the values from a year ago to get our final forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: When we compute the MSE for forecasting with restored seasonality and trend,
    we get `48.57`, and for theMAE for forecast with restored seasonality and trend,
    we get `5.61`, both of which are significantly lower than without using differencing.
    We can also try to smooth out the noise in the data to improve our MAE and MSE
    scores. Next, let us see how we can perform forecasting with machine learning,
    and in particular, with neural networks with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Time series forecasting with machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have examined statistical methods with reasonable success. Now,
    we will proceed with modeling time series data using deep learning techniques.
    We will begin with mastering how to set up a window dataset. We will also cover
    ideas such as shuffling and batching, and see how we can build and train a neural
    network for our sales forecasting problem. Let’s begin by mastering how we can
    prepare time series data for modeling using the windowed dataset method with the
    aid of TensorFlow utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the libraries required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we will be using NumPy and TensorFlow to prepare and manipulate our data
    into the required structure for modeling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let us create a simple dataset. Here, we are assuming the data consists of
    temperature values for two weeks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we print out the temperature, we get the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We get an array of values 1-14, where we are assuming the temperature rises
    from 1 on the first day to 14 on the 14th day. Odd, but let’s assume this is the
    case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create windowed data. Now that we have our data, we need to make a “window”
    of data points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `window_size` parameter refers to the window of data under consideration.
    If we set the window size to `3`, this means we will use 3 consecutive days’ temperature
    values to predict the next day’s temperature. The batch size determines how many
    samples are processed in each iteration during training, and `shuffle_buffer`
    specifies the number of elements from which TensorFlow randomly samples when shuffling
    the data. We shuffle to avoid sequential bias; we will expand on this in the next
    chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Creating a dataset works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This line of code is used to create a TensorFlow `Dataset` object from our temperature
    data. This `Dataset` API is a high-level TensorFlow API for reading data and transforming
    it into a form that a machine learning model can use. Next, we iterate over the
    dataset and print each element. We use `numpy()` to convert a TensorFlow object
    into a NumPy array. When we run the code, we get the numbers 1-14; however, they
    are now ready to be windowed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s transform our temperature data into a “windowed” dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We apply the `window` method to create a dataset of windows, where each window
    is a dataset itself. The `window_size + 1` parameter means that we are considering
    the `window_size` elements as input and the next one as a label. The `shift=1`
    parameter means that the window moves one step at a time. The `drop_remainder=True`
    parameter means that we will drop the last few elements if they can’t form a complete
    window.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When we print out our `window_data`, we get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see that we now have the window size that we set to 3 values and the (`+1`)
    that will serve as our label. Because we set the shift value to `1`, the next
    window starts from the second value in our series, which in this case is `2`.
    Next, the window will move one more step until we make all the windowed data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Flattening the data works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It’s easy to view each window created in the last step as a separate dataset.
    With this code, we flatten the data so that each window of data is packaged as
    a single batch in the main dataset. We use `flat_map` to flatten it back into
    a dataset of tensors and `window.batch(window_size + 1)` to convert each window
    dataset into a batched tensor. When we run the code, we get the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see the windowed data is now put into batched tensors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Shuffle the data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this code block, we shuffle the data. This is an important step as shuffling
    is used to ensure that the model doesn’t accidentally learn patterns from the
    order in which examples are presented during training. When we run the code block,
    we get the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the mini datasets in the main dataset have been shuffled. However,
    take note that the features (window) and the label are unchanged in the shuffled
    dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Mapping features and labels works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `map` method applies a function to each element of the dataset. Here, we
    are splitting each window into features and labels. The features are all but the
    last element of the `(window[:-1])` window, and the label is the last element
    of the `(window[-1])` window. When we run the code block, we see the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From our print result, we see the features are made up of three observations
    in an order and the next value is our label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Batching and prefetching the data works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `batch()` function groups the dataset into batches of `batch_size`. In
    this case, we’re creating batches of size `2`. The `prefetch(1)` performance optimization
    function makes sure that TensorFlow always has one batch ready to go while it’s
    processing the current one. After these transformations, the dataset is ready
    to be used for training a machine learning model. Let’s print out the batch:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see that each element of the dataset is a batch of features and label pairs,
    where the features are arrays of the `window_size` values from the original series,
    and the label is the next value that we want to predict. We have seen how to prepare
    our time series data for modeling; we have sliced, windowed, batched, shuffled,
    and split it into features and labels for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us use what we have learned here on our synthetic sales data to forecast
    future sales values.
  prefs: []
  type: TYPE_NORMAL
- en: Sales forecasting using neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s return to the sales data that we created to forecast sales using both
    naïve and moving average methods. Let us now use a neural network; here, we will
    use a DNN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extracting the data works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, we extract the `Date` and `Sales` data from the `Sales` DataFrame.
    `Date` is converted into datetime format and `Sales` is converted into a NumPy
    array.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Splitting the data works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For uniformity, we split data into a training set and a validation set using
    the same 80:20 split by using `split_time` of 80%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Creating the windowed dataset works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We create the `windowed_dataset` function; this function takes in a series,
    a window size, a batch size, and a shuffle buffer. It creates windows of data
    for training, with each window containing a `window_size + 1` data point. These
    windows are then shuffled and mapped to features and labels, where the features
    are all data points in a window, except the last one, and the label is the last
    data point. The windows are then batched and prefetched for efficient data loading.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Building the model works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we use a simple **feedforward neural network** (**FFN**) for modeling.
    This model contains two dense layers with ReLU activation, followed by a dense
    output layer with a single neuron. The model is compiled with MSE loss and **stochastic
    gradient descent** (**SGD**) as the optimizer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Training the model works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model is trained on the windowed dataset for 100 epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generating predictions works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we generate predictions in batches to improve computational efficiency.
    Only the predictions for the validation period are kept.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluating the model works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The MSE and MAE between the true validation data and the predicted data are
    calculated. Here, we achieved an MSE of `34.51` and an MAE of `4.72`, which surpasses
    all our simple statistical methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualizing the results works as follows. The true validation data and the
    predicted data are plotted over time to visualize the model’s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.17 – A time series forecast using a simple FFN](img/B18118_12_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.17 – A time series forecast using a simple FFN
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in *Figure 12**.17*, we see that our forecasted values map closely
    to the ground truth on the validation set, less the noisy spikes. Our FFN has
    demonstrated notable achievements in this experiment. Compared to the traditional
    statistical methods, our model exhibits a significant improvement in performance.
    You could tune the hyperparameters as well as implement callbacks to improve performance.
    However, our job is done here.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s meet in the final chapter, where we will be predicting app stock prices.
    See you there.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the concept of time series, examined the core characteristics
    and types of time series, and looked at some well-known applications of time series
    in machine learning. We also covered concepts such as trailing and centered windows
    and examined how to prepare time series for modeling with neural networks with
    the aid of utilities from TensorFlow. In our case study, we applied both statistical
    and deep learning techniques in order to build a sales forecasting model for a
    fictional company.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will extend our modeling using more complex architectures
    such as RNNs, CNNs, and CNN-LSTM architecture in forecasting time series data.
    Also, we will explore concepts such as learning rate scheduler and Lambda layers.
    To conclude the final chapter of this book, we will build a forecasting model
    for Apple’s closing stock price.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply the principle of naïve forecasting to the “*Air Passenger*” dataset using
    the exercise notebook provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the moving average technique on the same dataset and evaluate its
    performance by calculating the MAE and MSE values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, introduce the method of differencing to your moving average model. Again,
    assess the accuracy of your forecast by determining the MAE and MSE values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the sample temperature dataset at hand, demonstrate how to create meaningful
    features and labels from this data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, experiment with the simple FFN model on the dataset and observe its
    performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
