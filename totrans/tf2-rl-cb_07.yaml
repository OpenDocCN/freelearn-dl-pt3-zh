- en: '*Chapter 7*: Deploying Deep RL Agents to the Cloud'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cloud has become the *de facto* platform of deployment for AI-based products
    and solutions. Deep learning models running in the cloud are becoming increasingly
    common. The deployment of reinforcement learning-based agents to the cloud is,
    however, still very limited for a variety of reasons. This chapter contains recipes
    to equip yourself with tools and details to get ahead of the curve and build cloud-based
    Simulation-as-a-Service and Agent/Bot-as-a-Service applications using deep RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following recipes are discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the RL agent’s runtime components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building RL environment simulators as a service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training RL agents using a remote simulator service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing/evaluating RL agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging RL agents for deployment – a trading bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying RL agents to the cloud – a trading Bot-as-a-Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04
    and should work with later versions of Ubuntu if Python 3.6+ is available. With
    Python 3.6+ installed along with the necessary Python packages as listed before
    the start of each of the recipes, the code should run fine on Windows and macOS
    X too. It is advised to create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Miniconda or Anaconda installation
    for Python virtual environment management is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the RL agent’s runtime components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have looked at several agent algorithm implementations in the previous chapters.
    You may have noticed from recipes in the previous chapters (especially [*Chapter
    3*](B15074_03_ePub_AM.xhtml#_idTextAnchor090), *Implementing Advanced Deep RL
    Algorithms*), where we implemented RL agent training code, that some parts of
    the agent code were conditionally executed. For example, the experience replay
    routine was only run when a certain condition (such as the number of samples in
    the replay memory) was met, and so on. That begs the question: what are the essential
    components in an agent that is required, especially when we do not aim to train
    it further and only execute a learned policy?'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will help you distill the implementation of the **Soft Actor-Critic**
    (**SAC**) agent down to the minimal set of components – those that are absolutely
    necessary for the runtime of your agent.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repository. WebGym is built on top of the miniwob-plusplus benchmark ([https://github.com/stanfordnlp/miniwob-plusplus](https://github.com/stanfordnlp/miniwob-plusplus)),
    which is also made available as part of this book’s code repository for ease of
    use. If the following `import` statements run without issues, you are ready to
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps provide the implementation details for the minimal runtime
    necessary to utilize an SAC agent. Let’s jump right into the details:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s implement the actor component, which is going to be a TensorFlow
    2.x model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s implement the critic component, which is also going to be a TensorFlow
    2.x model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s implement a utility function to update the weights of a target model
    given a source TensorFlow 2.x model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can begin our SAC agent runtime class implementation. We will split
    our implementation into the following steps. Let’s start with the class implementation
    and define the constructor arguments in this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now initialize the state/observation shapes, action shapes, and action
    limits/bounds for the agent and also initialize a deque to store the agent’s memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, let’s define and initialize the actor component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now define and initialize the critic components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, let’s initialize the temperature and target entropy for the SAC
    agent based on the `auto_alpha` flag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s complete the constructor implementation by setting the hyperparameters
    and initializing the training progress summary dictionary for TensorBoard logging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the constructor implementation completed, let’s now move on to implement
    the `process_action` function, which takes the raw action from the agent and processes
    it so that it can be executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This step is crucial. We are going to implement the `act` method, which will
    take as input the state and generate and return the action to be executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s implement utility methods to load the actor and critic model
    weights from previously trained models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes the implementation of all the necessary runtime components for
    the SAC RL agent!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented the essential runtime components for the SAC
    agent. The runtime components include the actor and critic model definitions,
    a mechanism to load weights from previously trained agent models, and an agent
    interface to generate actions given states using the actor’s prediction and to
    process the prediction to generate an executable action.
  prefs: []
  type: TYPE_NORMAL
- en: The runtime components for other actor-critic-based RL agent algorithms, such
    as A2C, A3C, and DDPG, as well as their extensions and variants, will be very
    similar, if not the same.
  prefs: []
  type: TYPE_NORMAL
- en: It’s now time to move on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Building RL environment simulators as a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will walk you through the process of converting your RL training
    environment/simulator into a service. This will allow you to offer Simulation-as-a-Service
    for training RL agents!
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have trained several RL agents in a variety of environments using
    different simulators depending on the task to be solved. The training scripts
    used the Open AI Gym interface to talk to the environment running in the same
    process, or locally in a different process. This recipe will guide you through
    the process of converting any OpenAI Gym-compatible training environment (including
    your custom RL training environments) into a service that can be deployed locally
    or remotely as a service. Once built and deployed, an agent training client can
    connect to the sim server and train one or more agents remotely.
  prefs: []
  type: TYPE_NORMAL
- en: As a concrete example, we will take our `tradegym` library, which is a collection
    of the RL training environments for cryptocurrency and stock trading that we built
    in the previous chapters, and expose them through a **RESTful HTTP interface**
    for training RL agents.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repository.
  prefs: []
  type: TYPE_NORMAL
- en: We will also need to create a new Python module called `tradegym`, which contains
    the `crypto_trading_env.py`, `stock_trading_continuous_env.py`, `trading_utils.py`,
    and other custom trading environments we implemented in the previous chapters.
    You will find the `tradegym` module with its contents in the book’s code repository
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our implementation will contain two core modules – the `tradegym` server and
    the `tradegym` client, which are built based on the OpenAI Gym HTTP API. The recipe
    will focus on the customizations and the core components of the HTTP service interface.
    We will first define a minimum set of custom environments exposed as part of the
    `tradegym` library and then build the server and client modules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first make sure the minimal contents of the `tradegym` library’s `__init__.py`
    file exists so that we can import these environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now begin our `tradegym` server implementation as `tradegym_http_server.py`.
    We will finalize the implementation in the following several steps. Let’s begin
    by importing the necessary Python modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will import the `tradegym` module to register the available environments
    with the Gym registry:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now look at the skeleton of the environment container class with comments
    describing what each method does. You can refer to the full implementation of
    `tradegym_http_server.py` in the book’s code repository under `chapter7`. We will
    start with the class definition in this step and complete the skeleton in the
    following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will look at the two helper methods that are useful for managing
    the environment instances. They enable look up and delete operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will look at a few other methods that help with the environment management
    operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The methods discussed in this step enable the core operation of the RL environment,
    which have 1-1 correspondences with the core Gym API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the preceding skeleton (and implementation), we can look at a few samples
    to expose these operations as REST APIs using the **Flask** Python library. We
    will discuss the core server application setup and the route setup for the create,
    reset, and step methods in the following steps. Let’s look at the server application
    setup that exposes the endpoint handlers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now look at the REST API route definition for the `v1/envs`. This takes
    in an `env_id`, which should be a valid Gym environment ID (such as our custom
    `StockTradingContinuous-v0` or `MountainCar-v0`, which is available in the Gym
    registry) and returns an `instance_id`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will look at the REST API route definition for the HTTP POST endpoint
    at `v1/envs/<instance_id>/reset`, where `<instance_id>` can be any of the IDs
    returned by the `env_create()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will look at the route definition for the endpoint at `v1/envs/<instance_id>/step`,
    which is the endpoint that will likely be called the most number of times in an
    RL training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the remaining route definitions on the `tradegym` server, refer to the
    book’s code repository. We will implement a `__main__` function in the `tradegym`
    server script to launch the server when executed (which we will be using later
    in this recipe to test):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now move on to understand the implementation of the `tradegym` client.
    The full implementation is available in `tradegym_http_client.py` in the book’s
    code repository under `chapter7`. We will begin by importing the necessary Python
    modules in this step and continue to implement the client wrapper in the following
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The client class provides a Python wrapper to interface with the `tradegym`
    HTTP server. The constructor of the client class takes in the server’s address
    (IP and port information) to connect. Let’s look at the constructor implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reproducing all the standard Gym HTTP client methods here will not be a sensible
    use of the space available for this book and therefore, we will focus on the core
    wrapper methods, such as `env_create`, `env_reset`, and `env_step`, which we will
    use extensively in our agent training script. For a complete implementation, please
    refer to the book’s code repository. Let’s look at the `env_create` wrapper for
    creating an instance of an RL simulation environment on the remote `tradegym`
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will look at the wrapper method that calls the `reset` method
    on a specific environment using the unique `instance_id` returned by the `tradegym`
    server when the `env_create` call is made:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The most frequently used method on the `tradegym` client’s `Client` class is
    the `step` method. Let’s look at the implementation, which should look straightforward
    to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the other client wrapper methods in place, we can implement the `__main__`
    routine to connect to the `tradegym` server and call a few methods as an example
    to test whether everything is working as expected. Let’s write out the `__main__`
    routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now start to actually create a client instance and check the `tradegym`
    service! First, we need to launch the `tradegym` server by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can launch the `tradegym` client by running the following command in
    a separate terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see an output similar to the following in the terminal where you
    launched the `tradegym_http_client.py` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes the recipe! Let’s briefly recap on how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `tradegym` server provides an environment container class and exposes the
    environment interface through a REST API. The `tradegym` client provides Python
    wrapper methods to interact with the RL environment through the REST API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Envs` class acts as a manager for the environments instantiated on the
    `tradegym` server. It also acts as a container for several environments as a client
    can send a request to create multiple (same or different) environments. When the
    `tradegym` client requests the `tradegym` server using the REST API to create
    a new environment, the server creates an instance of the requested environment
    and returns a unique instance ID (example: `8kdi4289`). From that point on, the
    client can refer to specific environments using the instance ID. This allows the
    client and the agent training code to interact with multiple environments simultaneously.
    Thus, the `tradegym` server acts as a true service with a RESTful interface over
    HTTP.'
  prefs: []
  type: TYPE_NORMAL
- en: Ready for the next recipe? Let’s do it.
  prefs: []
  type: TYPE_NORMAL
- en: Training RL agents using a remote simulator service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at how we can utilize a remote simulator service
    to train our agent. We will be reusing the SAC agent implementation from one of
    the previous chapters and will focus on how we can train the SAC, or any of your
    RL agents for that matter, using an RL simulator that is running elsewhere (on
    the cloud, for example) as a service. We will leverage the `tradegym` server we
    built in the previous recipe to provide us with the RL simulator service for this
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, and to ensure that you have the latest version, you
    will first need to activate the `tf2rl-cookbook` Python/conda virtual environment.
    Make sure to update the environment to match the latest conda environment specification
    file (`tfrl-cookbook.yml`) in the cookbook’s code repository. If the following
    `import` statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Let’s get right into it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will implement the core parts of the training script and leave out command-line
    configuration and other non-essential functionalities to keep the script concise.
    Let’s name our script `3_training_rl_agents_using_remote_sims.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create an application-level child logger, add a stream handler
    to it, and then set the logging level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s create a TensorFlow `SummaryWriter` to log the agent’s training
    progress:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now get to the core of our implementation. Let’s start the implementation
    of the `__main__` function and continue the implementation in the following steps.
    Let’s first set up the client to connect to the sim service using the server address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s ask the server to create our desired RL training environment to
    train our agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s initialize our agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to configure our training using a few hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we are ready to start our outer training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now handle the case where an episode has ended and `done` is set to `True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now for the crucial steps! Let’s use the agent’s `act` and `train` methods
    to collect experience by acting (take actions) and training the agent using the
    collected experience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now update the variables to prepare for the next step in the episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our training loop. It was simple, wasn''t it? Let’s not forget
    to save the agent’s model after training so that we can use the trained model
    when it’s time for deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now proceed and run the script using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Did we forget something? Which sim server is the client connecting to? Is the
    sim server running?! If you get a long error on the command line that ends with
    a line that looks like the following, it certainly means that the sim server is
    not running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s do it correctly this time! Let’s make sure our sim server is running
    by launching the `tradegym` server using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now launch the agent training script using the following command (same
    as before):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see an output similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes our script for training RL agents using remote sims!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have been directly using the `gym` library to interact with the sim
    since we were running the RL environment simulators as part of the agent training
    scripts. While this will be good enough for CPU-bound local simulators, as we
    start to use advanced simulators or simulators that we don’t own, or even those
    cases where we don’t want to run or manage the simulator instances, we can leverage
    the client wrapper we built using our previous recipe in this chapter and talk
    to tradegym-like RL environments that expose a REST API for interfacing. In this
    recipe, the agent training script utilizes the `tradegym` client module to interact
    with a remote `tradegym` server to complete the RL training loop.
  prefs: []
  type: TYPE_NORMAL
- en: With that, let’s move on to the next recipe to see how we can evaluate previously
    trained agents.
  prefs: []
  type: TYPE_NORMAL
- en: Testing/evaluating RL agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume that you have trained the SAC agent in one of the trading environments
    using the training script (previous recipe) and that you have several versions
    of the trained agent models, each with different policy network architectures
    or hyperparameters or your own tweaks and customizations to improve its performance.
    When you want to deploy an agent, you want to make sure that you pick the best
    performing agent, don’t you?
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will help you build a lean script to evaluate a given pre-trained
    agent model locally so that you can get a quantitative performance assessment
    and compare several trained models before choosing the right agent model for deployment.
    Specifically, we will use the `tradegym` module and the `sac_agent_runtime` module
    that we built earlier in this chapter to evaluate the agent models that we train.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repository. If the following `import` statements run without issues, you
    are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s focus on creating a simple, but complete, agent evaluation script:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the `tradegym` module for the training environment and
    the SAC agent runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s create a command-line argument parser to handle command-line configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now add support for the `--env` argument to specify the RL environment
    ID and `–-num-episodes` to specify the number of episodes for evaluating the agent.
    Let’s have some sensible default values for both the arguments so that we can
    run the script even without any arguments for quick (or lazy?!) testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also add support for `–-trained-models-dir` to specify the directory
    containing the trained models, and the `–-model-version` flag to specify the specific
    version of the model in that trained directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to finalize the argument parsing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s begin our implementation of the `__main__` method in this step and continue
    its implementation in the following steps. Let’s start by creating a local instance
    of the RL environment in which we want to evaluate the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now initialize the agent class. For now, we only support the SAC agent,
    but it is quite easy to add support for other agents we have discussed in this
    book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s load the trained agent models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to evaluate the agent using the trained models in the test
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now try to evaluate the agent in the `StockTradingContinuous-v0` environment.
    Note that the market data source for the stocks trading environment in `data/MSFT.csv`
    and `data/TSLA.csv` can be different from the market data used for training! After
    all, we want to evaluate how well the agent has learned to trade! Run the following
    command to launch the agent evaluation script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Depending on how well your trained agents are, you will see output on the console
    similar to the following (reward values will vary):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That’s it!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We initialized an SAC agent with only the runtime components that are required
    to evaluate the agent using the `sac_agent_runtime` module and then loaded previously
    trained model versions (for both the actor and the critic), which are both customizable
    using the command-line arguments. We then created a local instance of the `StockTradingContinuousEnv-v0`
    environment using the `tradegym` library and evaluated our agents to get the cumulative
    reward as one of the quantitative indicators of the performance of the trained
    agent models.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to evaluate and pick the best performing agent, let’s move
    on to the next recipe to understand how to package the trained agent for deployment!
  prefs: []
  type: TYPE_NORMAL
- en: Packaging RL agents for deployment – a trading bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the crucial recipes of this chapter, where we will be discussing
    how to package the agent so that we can deploy on the cloud (next recipe!) as
    a service. We will be implementing a script that takes our trained agent models
    and exposes the `act` method as a RESTful service. We will then package the agent
    and the API script into a **Docker** container that is ready to be deployed to
    the cloud! By the end of this recipe, you will have built a deployment-ready Docker
    container with your trained RL agent that is ready to create and offer your Agent/Bot-as-a-Service!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s jump into the details.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repository. If the following `import` statements run without issues, you
    are ready to undertake the next step, which is to set up Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: For this recipe, you will need Docker installed. Please follow the official
    setup instructions to install Docker for your platform. You can find the instructions
    at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first implement the script to expose the agent’s `act` method as a
    REST service and then move on to create the Dockerfile for containerizing the
    agent:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the `sac_agent_runtime` that we built earlier in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s create a handler for the command-line argument and `–-agent` as
    the first supported argument to allow specification of the agent algorithm we
    want to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s add arguments to allow specification of the IP address and the
    port of the host server where our agent will be deployed. We will set and use
    the defaults for now and can change them from the command line when we need to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s add support for arguments to specify the directory containing the
    trained agent models and the specific model version to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final set of supported arguments, let’s add arguments to allow specification
    of the observation shape and the action space specifications based on the trained
    model configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now finalize the argument parser and start the implementation of the
    `__main__` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, let’s load the runtime configurations for the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s create an instance of the agent and load the weights for the agent’s
    actor and critic networks from the pre-trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now set up the service endpoint using Flask, which is going to be as
    simple as shown in the following lines of code. Note that we are exposing the
    agent’s `act` method at the `/v1/act` endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we just have to add the line that launches the Flask application to
    start the service when executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our REST API implementation for the agent is ready. We can now focus on creating
    a Docker container for the agent service. We will start to implement the Dockerfile
    by specifying the base image to be `nvidia/cuda:*` so that we have the GPU drivers
    that are necessary to utilize the GPU on the server where we will be deploying
    the agent. The following code lines in this and the following steps will go into
    a file named `Dockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now install a few system-level packages that are necessary and clean
    up the files to save disk space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To execute our agent runtime with all the necessary packages installed, we
    will make use of the conda Python environment. So, let’s go ahead and go through
    the instructions to download and set up `miniconda` in the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now copy the source code of this chapter into the container and create
    the conda environment using the list of packages specified in the `tfrl-cookbook.yml`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we just have to set the `ENTRYPOINT` for the container and the `CMD`,
    which will be passed as arguments to the `ENTRYPOINT` when the container is launched:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our Dockerfile and we are now ready to package our agent by
    building the Docker container. You can run the following command to build the
    Docker container as per the instructions in the Dockerfile and tag it with a container
    image name of your choice. Let’s use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you are running the preceding command for the first time, it may take quite
    some time for Docker to build the container. Subsequent runs or updates will run
    faster as intermediate layers might already have been cached when run for the
    first time. When things are running smoothly, you will see an output similar to
    the following (note that most of the layers are cached since I had already built
    the container previously):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the layers that involve COPY/ADD file operations from the disk, the instructions
    will be run as they cannot be cached. For example, you will see that the following
    steps from *step 9* will continue to be run fresh without using any cache. This
    is normal even if you have already built the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, when the Docker container is built, you will see something like the
    following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations on successfully packing your RL agent for deployment!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We leveraged the `sac_agent_runtime` we built earlier in this chapter to create
    and initialize an SAC agent instance. We then loaded pre-trained agent models
    for both the actor and the critic. After that, we exposed the SAC agent’s `act`
    method as a REST API with an HTTP POST endpoint for taking the observations as
    the POST message and returning actions as the response to the POST request. Finally,
    we launched the script as a Flask application to start serving.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the recipe, we packaged the agent actioa-serving application
    as a Docker container and prepared it for deployment!
  prefs: []
  type: TYPE_NORMAL
- en: We are now on the verge of deploying the agent to the cloud! March on to the
    next recipe and find out how.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying RL agents to the cloud – a trading Bot-as-a-Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ultimate goal of training an RL agent is to use it for taking actions given
    new observations. In the case of our stock trading SAC agent, we have so far learned
    to train, evaluate, and package the best performing agent model to build our trading
    bot. While we focused on one particular application (autonomous trading bot),
    you can see how easy it is to change the training environment or agent algorithms
    based on the recipes in earlier chapters of this book. This recipe will walk you
    through the steps to deploy the Docker containerized/packaged RL agent to the
    cloud and run the Bot-as-a-Service.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will need access to a cloud service such as Azure,
    AWS, GCP, Heroku or another cloud service provider that allows you to host and
    run your Docker container. If you are a student, you can make use of GitHub’s
    student developer pack ([https://education.github.com/pack](https://education.github.com/pack)),
    which, as of 2020, allows you to avail yourself of several benefits for free,
    including $100 worth of Microsoft Azure credits or $50 platform credit on DigitalOcean
    if you are a new user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several guides exist on how to push your Docker container to the cloud and
    to deploy/run the container as a service. For example, if you have an Azure account,
    you can following the official guide here: [https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quickstart](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quickstart).'
  prefs: []
  type: TYPE_NORMAL
- en: The guide walks you through the various options (the CLI, Portal, PowerShell,
    ARM templates, and the Docker CLI) to deploy your Docker container-based agent
    service.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first deploy the trading bot locally and test it out. After that, we
    can deploy it to your cloud service of choice. As an example, this recipe will
    walk you through the steps to deploy it to Heroku ([https://heroku.com](https://heroku.com)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first build our Docker container containing our trading bot using the
    following command. Note that if you have already built the container by following
    the previous recipe in this chapter, the following command may finish executing
    faster depending on the cached layers and your changes to the Dockerfile:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the Docker container with the bot has been built successfully, we can
    launch the bot using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If all goes well, you should see console output somewhat similar to the following,
    indicating that the bot is up and running and ready to act:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that you have deployed the trading bot locally (on your own server), let’s
    create a simple script to utilize the Bot-as-a-Service that you have built. Create
    a file named `test_agent_service.py` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can execute the script using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that your bot container still needs to be running. Once you execute the
    preceding command, you will see an output similar to the following line on the
    console output of your bot, indicating that a new POST message was received at
    the `/v1/act` endpoint, which was served with an HTTP response status of 200,
    indicating success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will also notice that your test script will print out an output similar
    to the following on its console window, indicating that it received an action
    from the trading bot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It’s time to deploy your trading bot to a cloud platform so that you or others
    can access it over the internet! As discussed in the *Getting started* section,
    you have several options in terms of choosing the cloud provider where you want
    to host your Docker container image and deploy the RL agent Bot-as-a-Service.
    We will use Heroku as an example as it offers free hosting and a simple CLI. First,
    you need to install the Heroku CLI. Follow the official instructions listed at
    [https://devcenter.heroku.com/articles/heroku-cli](https://devcenter.heroku.com/articles/heroku-cli)
    to install the Heroku CLI for your platform (Linux/Windows/macOS X). On Ubuntu
    Linux, we can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the Heroku CLI is installed, you can log in to the Heroku container registry
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, run the following command from the directory containing the Dockerfile
    for the agent; for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you have not logged in to Heroku, you will be prompted to log in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you are logged in, you will get an output similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That’s the address of your container registry on Heroku. You can now build
    your bot container and push it to Heroku using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once that process completes, you can release the bot container image to your
    Heroku app using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have just deployed your bot to the cloud! You can now access
    your bot at the new address, such as [https://salty-fortress-4191.herokuapp.com/](https://salty-fortress-4191.herokuapp.com/)
    in the sample used in the preceding code. You should be able to send observations
    to your bot and get the actions in return! Congratulations on deploying your Bot-as-a-Service!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are now ready to wrap up the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first built and launched the Docker container locally on your machine by
    using the `docker run` command and specifying that you want the local port `5555`
    to be mapped with the container’s port `5555`. This will allow the host (your
    machine) to communicate with your container using that port as if it were a local
    port on the machine. Following deployment, we used a test script that uses the
    Python `request` library to create a POST request with sample data for the observation
    values and sent it to the bot running in the container. We observed how the bot
    responded to the request via the command-line status output followed by a returned
    success response containing the bot’s trading action.
  prefs: []
  type: TYPE_NORMAL
- en: We then deployed the same container with the bot to the cloud (Heroku). Following
    a successful deployment, the bot was accessible over the web using the public
    `herokuapp` URL created automatically by Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: That completes the recipe and the chapter! I hope you enjoyed working through
    it. See you in the next chapter.
  prefs: []
  type: TYPE_NORMAL
