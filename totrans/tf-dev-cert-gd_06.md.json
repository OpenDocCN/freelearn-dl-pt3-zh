["```\n#Step 1:  Model configuration\nmodel=keras.Sequential([\n    keras.layers.Flatten(input_shape=(28,28)),\n    keras.layers.Dense(100, activation=»relu»),\n    keras.layers.Dense(10,activation=»softmax»)\n])\n#Step 2: Compiling the model, we add the loss, optimizer and evaluation metrics here\nmodel.compile(optimizer='adam',\n    loss=›sparse_categorical_crossentropy›,\n    metrics=[‹accuracy›])\n#Step 3: We fit our data to the model\nhistory= model.fit(train_images, train_labels, epochs=40)\n```", "```\nEpoch 36/40\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.1356 - accuracy: 0.9493\nEpoch 37/40\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.1334 - accuracy: 0.9503\nEpoch 38/40\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1305 - accuracy: 0.9502\nEpoch 39/40\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1296 - accuracy: 0.9512\nEpoch 40/40\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1284 - accuracy: 0.9524\n```", "```\ntest_loss, test_acc=model.evaluate(test_images,test_labels)\nprint('Test Accuracy: ', test_acc)\n```", "```\n    from tensorflow.keras.callbacks import EarlyStopping\n    ```", "```\n    callbacks = EarlyStopping(monitor='val_loss',\n    ```", "```\n        patience=5, verbose=1, restore_best_weights=True)\n    ```", "```\n    #Step 1:  Model configuration\n    ```", "```\n    model=keras.Sequential([\n    ```", "```\n        keras.layers.Flatten(input_shape=(28,28)),\n    ```", "```\n        keras.layers.Dense(100, activation=»relu»),\n    ```", "```\n        keras.layers.Dense(10,activation=»softmax»)\n    ```", "```\n    ])\n    ```", "```\n    #Step 2: Compiling the model, we add the loss, optimizer and evaluation metrics here\n    ```", "```\n    model.compile(optimizer='adam',\n    ```", "```\n        loss=›sparse_categorical_crossentropy›,\n    ```", "```\n        metrics=[‹accuracy›])\n    ```", "```\n    #Step 3: We fit our data to the model\n    ```", "```\n    history= model.fit(train_images, train_labels,\n    ```", "```\n        epochs=100, callbacks=[callbacks],\n    ```", "```\n        validation_split=0.2)\n    ```", "```\nEpoch 14/100\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.2197 - accuracy: 0.9172 - val_loss: 0.3194 - val_accuracy: 0.8903\nEpoch 15/100\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.2133 - accuracy: 0.9204 - val_loss: 0.3301 - val_accuracy: 0.8860\nEpoch 16/100\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.2064 - accuracy: 0.9225 - val_loss: 0.3267 - val_accuracy: 0.8895\nEpoch 17/100\n1500/1500 [==============================] - 3s 2ms/step - loss: 0.2018 - accuracy: 0.9246 - val_loss: 0.3475 - val_accuracy: 0.8844\nEpoch 18/100\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.1959 - accuracy: 0.9273 - val_loss: 0.3203 - val_accuracy: 0.8913\nEpoch 19/100\n1484/1500 [============================>.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9282 Restoring model weights from the end of the best epoch: 14.\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.1928 - accuracy: 0.9281 - val_loss: 0.3347 - val_accuracy: 0.8912\nEpoch 19: early stopping\n```", "```\n    test_loss, test_acc = model.evaluate(test_images,\n    ```", "```\n        test_labels)\n    ```", "```\n    print('Test Accuracy: ', test_acc)\n    ```", "```\nclass EarlyStop(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy') > 0.85):\n            print(\"\\n\\n85% validation accuracy has been reached.\")\n            self.model.stop_training = True\ncallback = EarlyStop()\n```", "```\nEpoch 1/100\n1490/1500 [============================>.] - ETA: 0s - loss: 0.5325 - accuracy: 0.8134/n/n 85% validation accuracy has been reached\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.5318 - accuracy: 0.8138 - val_loss: 0.4190 - val_accuracy: 0.8538\n```", "```\ndef train_model(hidden_neurons, train_images, train_labels, callbacks=None, validation_split=0.2, epochs=100):\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(hidden_neurons, activation=»relu»),\n        keras.layers.Dense(10, activation=»softmax»)\n    ])\n    model.compile(optimizer=›adam›,\n        loss=›sparse_categorical_crossentropy›,\n        metrics=[‹accuracy›])\n    history = model.fit(train_images, train_labels,\n        epochs=epochs, callbacks=[callbacks] if callbacks else None,\n        validation_split=validation_split)\n    return model, history\n```", "```\nneuron_values = [1, 500]\nfor neuron in neuron_values:\n    model, history = train_model(neurons, train_images,\n        train_labels, callbacks=callbacks)\n    print(f»Trained model with {neurons} neurons in the hidden layer»)\n```", "```\nEpoch 36/40\n1500/1500 [==============================] - 3s 2ms/step - loss: 1.2382 - accuracy: 0.4581 - val_loss: 1.2705 - val_accuracy: 0.4419\nEpoch 37/40\n1500/1500 [==============================] - 2s 1ms/step - loss: 1.2360 - accuracy: 0.4578 - val_loss: 1.2562 - val_accuracy: 0.4564\nEpoch 38/40\n1500/1500 [==============================] - 2s 1ms/step - loss: 1.2340 - accuracy: 0.4559 - val_loss: 1.2531 - val_accuracy: 0.4507\nEpoch 39/40\n1500/1500 [==============================] - 2s 1ms/step - loss: 1.2317 - accuracy: 0.4552 - val_loss: 1.2553 - val_accuracy: 0.4371\nEpoch 40/40\n1500/1500 [==============================] - 2s 1ms/step - loss: 1.2292 - accuracy: 0.4552 - val_loss: 1.2523 - val_accuracy: 0.4401\nend of experiment with 1 neuron\n```", "```\nEpoch 11/40\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.2141 - accuracy: 0.9186 - val_loss: 0.3278 - val_accuracy: 0.8878\nEpoch 12/40\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.2057 - accuracy: 0.9220 - val_loss: 0.3169 - val_accuracy: 0.8913\nEpoch 13/40\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.1976 - accuracy: 0.9258 - val_loss: 0.3355 - val_accuracy: 0.8860\nEpoch 14/40\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.1893 - accuracy: 0.9288 - val_loss: 0.3216 - val_accuracy: 0.8909\nEpoch 15/40\n1499/1500 [============================>.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9303Restoring model weights from the end of the best epoch: 10.\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.1826 - accuracy: 0.9303 - val_loss: 0.3408 - val_accuracy: 0.8838\nEpoch 15: early stopping\nend of experiment with 500 neurons\n```", "```\ndef learning_rate_test(learning_rate):\n    #Step 1:  Model configuration\n    model=keras.Sequential([\n        keras.layers.Flatten(input_shape=(28,28)),\n        keras.layers.Dense(64, activation=»relu»),\n        keras.layers.Dense(10,activation=»softmax»)\n])\n    #Step 2: Compiling the model, we add the loss,\n         #optimizer and evaluation metrics here\n    model.compile(optimizer=tf.keras.optimizers.Adam(\n        learning_rate=learning_rate),\n        loss='sparse_categorical_crossentropy',\n        metrics=[‹accuracy›])\n    #Step 3: We fit our data to the model\n    callbacks = EarlyStopping(monitor='val_loss',\n        patience=5, verbose=1, restore_best_weights=True)\n    history=model.fit(train_images, train_labels,\n        epochs=50, validation_split=0.2,\n        callbacks=[callbacks])\n    score=model.evaluate(test_images, test_labels)\n    return score[1]\n```", "```\n# Try out different learning rates\nlearning_rates = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001,\n    0.000001]\n# Create an empty list to store the accuracies\naccuracies = []\n# Loop through the different learning rates\nfor learning_rate in learning_rates:\n    # Get the accuracy for the current learning rate\n    accuracy = learning_rate_test(learning_rate)\n    # Append the accuracy to the list\n    accuracies.append(accuracy)\n```", "```\ndf = pd.DataFrame(list(zip(learning_rates, accuracies)),\n    columns =[‹Learning_rates›, ‹Test_Accuracy›])\ndf\n```"]