- en: Generating Uncertainty in Traffic Signs Classifier Using Bayesian Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As humans, we love the uncertainty that comes with predictions. For example,
    we always want to know what the chances are of it raining before we leave the
    house. However, with traditional deep learning, we only have a point prediction
    and no notion of uncertainty. Predictions from these networks are assumed to be
    accurate, which is not always the case. Ideally, we would like to know the level
    of confidence of predictions from neural networks before making a decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, having uncertainty in the model could have potentially avoided
    the following disastrous consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: In May 2016, the Tesla Model S crashed in northern Florida into a truck that
    was turning left in front of it. According to the official Tesla blog ([https://www.tesla.com/en_GB/blog/tragic-loss](https://www.tesla.com/en_GB/blog/tragic-loss)),
    *Neither Autopilot nor the driver noticed the white side of the tractor trailer
    against a brightly lit sky, so the brake was not applied*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In July 2015, two African Americans were classified as gorillas by Google's
    image classifier, raising concerns of racial discrimination. Here ([https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/](https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/))
    is the press release.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The thing that is pretty clear from these examples is that quantifying uncertainty
    in predictions could have avoided these disasters. The question now is: If it''s
    so obvious, why didn''t Tesla or Google implement it in the first place?'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian algorithms (like Gaussian processes) can quantify uncertainty, but
    cannot scale to large datasets such as images and videos, whereas deep learning
    is able to produce much better accuracy—except that it lacks any notion of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the concept of Bayesian neural networks, which
    combines the concepts of deep learning and Bayesian learning with model uncertainty
    in the predictions of deep neural networks. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Bayesian deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Bayesian neural networks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Bayesian neural network with the German Traffic Sign Image dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Bayesian deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've all understood the basics of Bayes' rule, as explained in Chapter 6, *Predicting
    Stock Prices using Gaussian Process Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Bayesian machine learning, we use the same formula as Bayes'' rule to learn
    model parameters (![](img/f5d0830f-24cd-4bde-a545-131fa87abb6c.png)) from the
    given data, ![](img/839c249a-4c94-4cc9-9f42-a5eefc53e68f.png). The formula, then,
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ca7cadc-8435-47da-9f98-f28d27c21001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/b4360234-8459-431a-8283-360b9fd20244.png) or the probability of
    observed data is also called evidence. This is always difficult to compute. One
    brute-force way is to integrate out ![](img/a534dbd7-1960-4d32-9cb1-d54dabe39401.png) for
    all the values of model parameters, but this is obviously too expensive to evaluate.![](img/610582f1-80be-46ca-b3d8-625b3794d4a0.png) is
    the prior on parameters, which is nothing but some randomly initialized value
    of parameters in most cases. Generally, we don't care about setting the priors
    perfectly as we expect the inference procedure to converge to the right value
    of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/614510dd-708d-4882-a251-6cb8cfec80bf.png) is known as the likelihood
    of data, given the modeling parameters. Effectively, it shows how likely it is
    to obtain the given observations in the data when given the model parameters.
    We use likelihood as a measure to evaluate different models. The higher the likelihood,
    the better the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ![](img/18494e17-11f3-433a-ab12-e82727d65da9.png), a posterior, is
    what we want to calculate. It's a probability distribution over model parameters
    that's obtained from the given data. Once we obtain the uncertainty in model parameters,
    we can use them to quantify the uncertainty in model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, in machine learning, we use **Maximum Likelihood estimation** (**MLE**)
    ([https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf))
    to get the estimates of model parameters. However, in the case of Bayesian deep
    learning, we estimate a posterior from the prior and the procedure is known as
    **Maximum a posteriori** (**MAP**) estimation ([https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' rule in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Traditionally, neural networks produce a point estimate by optimizing weights
    and biases to minimize a loss function, such as the mean squared error in regression
    problems. As mentioned earlier, this is similar to finding parameters using the
    Maximum likelihood estimation criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df3f26a9-ede4-43c3-886b-97655eb0ffda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Typically, we obtain the best parameters through backpropagation in neural
    networks. To avoid overfitting, we introduce a regularizer of ![](img/9346cd9b-1b59-4039-aa0f-1d75ac2bada1.png) norm
    over weights. If you are not aware of regularization, please refer to the following
    Andrew Ng video: [http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning).
    It has been shown that ![](img/16aa6075-3e33-4e93-a7f5-6c6beeb2117c.png) normalization
    is equivalent to placing a normal prior on weights ![](img/b2ee0746-0cee-44a6-8dbc-fc6269a8f8f7.png).
    With a prior on weights, the MLE estimation problem can be framed as a MAP estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88c60ffd-1803-4bce-b8de-88ee81a6a1b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using Bayes'' rule, the preceding equation can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/989188bb-023e-4889-a734-f91ebb74ebe7.png)'
  prefs: []
  type: TYPE_IMG
- en: The exact proof of equivalence of regularization to the Bayesian framework is
    outside the scope of this chapter. If you are interested, you can read more about
    it at the following MIT lecture: [http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf](http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: From this, we can observe that traditional neural networks with regularization
    can be framed as a problem of inference using Bayes' rule. Bayesian neural networks
    aim to determine the posterior distribution using Monte Carlo or Variational inference
    techniques. In the rest of this chapter, we will look at how to build a Bayesian
    neural network using TensorFlow Probability.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TensorFlow probability, variational inference, and Monte Carlo
    methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow Probability** (**tfp** in code – [https://www.tensorflow.org/probability/overview#layer_2_model_building](https://www.tensorflow.org/probability/overview#layer_2_model_building))
    was recently released by Google to perform probabilistic reasoning in a scalable
    manner. It provides tools and functionalities to define distributions, build neural
    networks with prior on weights, and perform probabilistic inference tasks such
    as Monte Carlo or Variational Inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some of the functions/utilities we will be using for
    building our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tfp.distributions.categorical**: This is a standard categorical distribution
    that''s characterized by probabilities or log-probabilities over K classes. In
    this project, we have Traffic Sign images from 43 different traffic signs. We
    will define a categorical distribution over 43 classes in this project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probabilistic layers**: Built on top of the TensorFlow layers implementation,
    probabilistic layers incorporate uncertainty over the functions they represent.
    Effectively, they incorporate uncertainty in the weights of the neural networks.
    They have the functionality to forward pass through the inputs by sampling from
    the posterior of weight distributions (![](img/3dc23fbc-1c62-49e1-afff-1457bbdbb861.png)).
    Specifically, we will use the Convolutional2DFlipout ([https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout))
    layer, which can compute the forward pass by sampling from the posterior of the
    weight parameters of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kullback-leibler (KL) divergence**: If we want to measure the difference
    between two numbers, we just subtract them. What if we want to obtain a difference
    between two probability distributions? What is the equivalent of subtraction in
    this case? Often in the case in probability and statistics, we will replace the
    observed data or complex distributions with a simpler, approximating distribution.
    KL Divergence helps us measure just how much information we lose when we choose
    an approximation. Essentially, it is a measure of one probability distribution
    from others. A KL divergence of 0 indicates that two distributions are identical.
    If you want to know more about the mathematics of KL divergence, please refer
    to a great explanation from MIT open courseware, which can be found at [https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational inference**: Variational inference is a machine learning method
    that''s used to approximate complex, intractable integrals in Bayesian learning
    through optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we know, our aim in Bayesian learning is to calculate the posterior ![](img/13cfd9ef-c6bd-486c-b10f-32f04f36f217.png), given
    prior ![](img/580456b9-ca75-4a74-8ff2-2d7062f886e1.png) and data ![](img/f1181935-aeb9-47fc-95b9-606593c46334.png).
    A prerequisite for computing the posterior is the computation of distribution
    of ![](img/eafed642-dc26-40cf-b555-99b5c617f851.png) (data) in order to obtain ![](img/f0ce5197-f81a-4652-95e4-f2a4f361b607.png), or
    *evidence*. As mentioned earlier, the distribution of X is intractable as it is
    too expensive to compute using a brute-force approach. To address this problem,
    we will use something called Variational Inference (VI). In VI, we define a family
    of distributions ![](img/787fd8b6-1a02-4263-a9b9-4cb5b05afa8f.png), parameterized
    by ![](img/8f23cfcf-a85c-4cc1-b76c-7261bd3f8d4f.png). The core idea is to optimize
    ![](img/740b2ea1-bf88-4e90-a83a-406a68d690f4.png) so that the approximate distribution
    is as close to the true posterior as possible. We measure the difference between
    two distributions using KL divergence. As it turns out, it is not easy to minimize
    the KL divergence. We can show you that this KL divergence is always positive
    and that it comprises two parts using the following mathematical formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7644d761-4949-4895-b510-906255b0c046.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **ELBO** is **Evidence Lower Bound** ([https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Monte Carlo method: **Monte Carlo methods are computational methods which
    rely on repeated random sampling to obtain the statistical behavior of some phenomenon
    (behavior). They are typically used to model uncertainties or generate what-if
    scenarios for business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's say you commute by train every day to work. You are thinking about whether
    to take the company shuttle to the office instead. Now, there are many random
    variables associated with a bus ride, such as time of arrival, traffic, number
    of passengers boarding the bus, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: One way that we can look at this what-if scenario is if we take the mean of
    these random variables and calculate the arrival time. However, that will be too
    naive as it doesn't take into account variance in these variables. Another way
    is to sample from these random variables (somehow, if you are able to do that!)
    and generate what-if scenarios of reaching the office.
  prefs: []
  type: TYPE_NORMAL
- en: To make a decision, you will need an acceptable criteron. For instance, if you
    observe that in 80% of these what-if scenarios you reach office on or before time,
    you can continue forward. This approach is also known as the Monte Carlo simulation.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we will model the weights of neural networks as random variables.
    To determine the final prediction, we will sample from the distributions of these
    weights repeatedly to obtain the distribution of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have skipped some mathematical details. Feel free to read more
    about Variational Inference ([https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)[)](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Building a Bayesian neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, we will use the German Traffic Sign Dataset ([http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset))
    to build a Bayesian neural network. The training dataset contains 26,640 images
    in 43 classes. Similarly, the testing dataset contains 12,630 images.
  prefs: []
  type: TYPE_NORMAL
- en: Please read the `README.md` file in this book's repository before executing
    the code to install the appropriate dependencies and for instructions on how to
    run the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an image that''s present in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bca6d66c-486e-4b01-8b71-8501f9a16c56.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that there are different kinds of traffic sign depicted by different
    classes in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by pre-processing our dataset and making it conform to the requirements
    of the learning algorithm. This is done by reshaping the images to a uniform size
    via histogram equalization, which is used to enhance contrast, and cropping them
    to only focus on the traffic signs in the image. Also, we convert the images to
    grayscale as traffic signs are identified by shape and not color in the image.
  prefs: []
  type: TYPE_NORMAL
- en: For modeling, we define a standard Lenet model ([http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)),
    which was developed by Yann Lecun. Lenet is one of the first convolutional neural
    networks that was designed. It is small and easy to understand, yet large enough
    to provide interesting results.
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard Lenet model has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Three convolutional layers with increasing filter sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four fully connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No dropout layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rectified linear** (**ReLU**) after every fully connected or convolutional
    layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max pooling after every convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We train this model to minimize the negative of ELBO loss that is defined in
    the *Understanding TensorFlow Probability, Variational Inference, and Monte Carlo
    methods* section of this chapter. Specifically, we define ELBO loss as a combination
    of two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Expected log likelihood or cross entropy that can be estimated through the Monte
    Carlo method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KL divergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the model is trained, we evaluate the predictions on the hold-out dataset.
    One of the major differences in Bayesian neural network evaluation is that there
    is no single set of parameters (weights of the model) that we can obtain from
    training. Instead, we obtain a distribution of all the parameters. For evaluation,
    we will have to sample values from the distribution of each parameter to obtain
    the accuracy on the testing set. We will sample the parameters of the model multiple
    times to obtain a confidence interval on our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will show some uncertainty in our predictions in sample images from
    the testing dataset and also plot the distribution of the weight parameters we
    obtain.
  prefs: []
  type: TYPE_NORMAL
- en: Defining, training, and testing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download both the training and testing datasets from [http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).
    Let''s look at the steps to build the project after you have downloaded the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by transforming the images present in the dataset using histogram equalization.
    This is essential as each image in the dataset may have a different scale of illumination.
    You can see from the following two images how images of the same traffic sign
    have very different illumination. Histogram equalization helps to normalize these
    differences and makes the training data more consistent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/564c6534-4274-47d0-844e-7337c1db8f30.png)![](img/fabef10e-3b86-4de1-a49d-3f0d9f745e8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have performed the equalization, crop the image to focus on just the
    sign, and resize the image to 32 x 32 as desired by our learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use 32 x 32 as the shape of images for training as it is big enough
    to preserve the nuances of the image for detection and small enough to train the
    model faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create dictionaries with label and image information for the train/test dataset
    and store them as pickle files so that we don''t have to run the pre-processing
    code every time we run the model. This means that we essentially pre-process the
    transformed data to create our train and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will use grayscale images in our project as our task throughout this project
    is to classify traffic sign images into one of the 43 classes and provide a measure
    of uncertainty in our classification. We do not care about the color of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model using the LeNet architecture in Keras. Finally, we will assign
    the 43 sized vector of outputs from the LeNet model into a categorical distribution
    function (`tfd.categorical`) from TensorFlow probability. This will help us generating
    the uncertainty in predictions afterwards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the loss to minimize the KL divergence up to ELBO. Compute the ELBO
    loss that is defined in the *Understanding TensorFlow Probability, Variational
    Inference, and Monte Carlo methods* section of this chapter. As you can see, we
    use the `model.losses` attribute to compute the KL divergence. This is because
    the `losses` attribute of a TensorFlow Keras Layer represents a side-effect computation
    such as regularizer penalties. Unlike regularizer penalties on specific TensorFlow
    variables, here the `losses` represent the KL divergence computation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the Adam optimizer, as defined in [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml),
    *Sentiment Analysis in your browser using TensorFlow.js*, to optimize the ELBO
    loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using the Adam optimizer because it generally performs better
    than other optimizers with default parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model with the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Epochs = 1,000
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size = 128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning rate = 0.001:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is trained, each weight in the Bayesian neural network will
    have a distribution and not a fixed value. Sample each weight multiple times (50,
    in the code) and obtain different predictions for each sample. Sampling, although
    useful, is expensive. Therefore, we should only use Bayesian neural networks where
    we require some measure of uncertainty in our predictions. Here is the code for
    Monte Carlo sampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the samples, obtain the mean probability for each image in the
    test dataset and compute the mean accuracy like in usual machine learning classifiers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mean accuracy we obtain for this dataset is ~ 89% for 1,000 Epochs. You
    can tune the parameters further or create a deeper model to obtain better accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here is the code for getting the mean accuracy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to calculate the distribution of accuracy for each Monte Carlo
    sample of each test image. For that, compute the predicted class and compare it
    with the test label. The predicted class can be obtained by assigning the label
    to the class with the maximum probability for a given network parameter sample.
    This way, you can get the range of accuracies and can also plot those accuracies
    on a histogram. Here is the code for obtaining accuracy and generating a histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram that''s generated will look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/467b468e-0ee2-4b1f-a89b-c39554f745f6.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have a distribution of accuracies. This distribution can
    help us obtain the confidence interval over the accuracy of our model on the test
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the plot might look differently when you run the code, since it is
    obtained through random sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a few images from the test dataset and see their predictions for different
    samples in Monte Carlo. Use the following function `plot_heldout_prediction` to
    generate the histogram of predictions from different samples in Monte Carlo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at some of the images and their predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0032af64-7c08-4a41-a99e-77a001e86ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the preceding image, all of the predictions belonged to the correct class
    02, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce216508-385a-4f51-bf4b-56aec0ed1850.png)'
  prefs: []
  type: TYPE_IMG
- en: In the following two cases, although our mean prediction was correct, some samples
    in Monte Carlo predicted the wrong class. You can imagine how quantifying uncertainty
    in such cases can make a self-driving car make better decisions on the road.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1: **'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68be4b03-477a-477c-95c0-9947f85ef0cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding image, some of the Monte Carlo predictions belonged to the
    wrong class, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2070f84-669b-4fcd-affa-82c0922332c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Case 2: **'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/232995e8-3458-4c34-a5b7-05ddc7da72ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding image, some of the Monte Carlo predictions belonged to the
    wrong class, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfd0ab71-71a0-45b9-81b6-8c389e0b4cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Case 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following case, average prediction is incorrect, but some samples were correctly
    predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01afa093-37b6-469d-b53f-02db3dd06cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the preceding image, we obtained the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efffb17d-c64b-4d65-a4cd-621a1eb9fff3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Case 4:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, we will get cases where we didn''t predict correctly for any sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1775ee39-0175-48a1-961c-99fa49b20639.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For this image, we obtained the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06c3c145-42c9-4d7e-b3f6-40442e7fbc36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, visualize the posterior of weights in the network. In the following
    plot we are showing both the posterior mean and standard deviation of the different
    weights in the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ec78307f-11b7-4edc-a408-17e9f7d78b43.png)'
  prefs: []
  type: TYPE_IMG
- en: Having a distribution on weights enables us to develop predictions for the same
    image, which is extremely useful in developing a confidence interval around our
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks, as we know, are great for point predictions, but can't help
    us identify the uncertainty in their predictions. On the other hand, Bayesian
    learning is great for quantifying uncertainty, but doesn't scale well in multiple
    dimensions or problems with big unstructured datasets such as images.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we looked at how we can combine neural networks with Bayesian
    learning using Bayesian neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the dataset of German Traffic Signs to develop a Bayesian neural network
    classifier using Google''s recently released tool: TensorFlow probability. TF
    probability provides high-level APIs and functions to perform Bayesian modeling
    and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: We trained the Lenet model on the dataset. Finally, we used Monte Carlo to sample
    from the posterior of the parameters of the network to obtain predictions for
    each sample of the test dataset to quantify uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: However, we have only scratched the surface in terms of the complexity of Bayesian
    neural networks. If we want to develop safe AI, then understanding uncertainty
    in our predictions is of the utmost importance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about a new concept in machine learning known
    as autoencoders. We will look at how to detect credit card fraud using them.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is TensorFlow probability?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Variational inference and why is it important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is KL divergence?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do we mean by prior and posterior on the weights of neural networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
