["```\n# create the virtualenv in the current folder (tf1)\npipenv --python 3.7\n# run a new shell that uses the just created virtualenv\npipenv shell\n# install, in the current virtualenv, tensorflow\npip install tensorflow==1.15\n#or for GPU support: pip install tensorflow-gpu==1.15\n```", "```\n# create the virtualenv in the current folder (tf2)\npipenv --python 3.7\n# run a new shell that uses the just created virtualenv\npipenv shell\n# install, in the current virtualenv, tensorflow\npip install tensorflow==2.0\n#or for GPU support: pip install tensorflow-gpu==2.0\n```", "```\nimport tensorflow as tf\n\n# Build the graph\nA = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\nx = tf.constant([[0, 10], [0, 0.5]])\nb = tf.constant([[1, -1]], dtype=tf.float32)\ny = tf.add(tf.matmul(A, x), b, name=\"result\") #y = Ax + b\n\nwriter = tf.summary.FileWriter(\"log/matmul\", tf.get_default_graph())\nwriter.close()\n```", "```\ntensorboard --logdir log/matmul\n```", "```\nimport tensorflow as tf\n```", "```\ng1 = tf.Graph()\ng2 = tf.Graph()\n\nwith g1.as_default():\n    A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    x = tf.constant([[0, 10], [0, 0.5]])\n    b = tf.constant([[1, -1]], dtype=tf.float32)\n    y = tf.add(tf.matmul(A, x), b, name=\"result\")\n\nwith g2.as_default():\n    with tf.name_scope(\"scope_a\"):\n        x = tf.constant(1, name=\"x\")\n        print(x)\n    with tf.name_scope(\"scope_b\"):\n        x = tf.constant(10, name=\"x\")\n        print(x)\n    y = tf.constant(12)\n    z = x * y\n```", "```\nwriter = tf.summary.FileWriter(\"log/two_graphs/g1\", g1)\nwriter = tf.summary.FileWriter(\"log/two_graphs/g2\", g2)\nwriter.close()\n```", "```\nTensor(\"scope_a/x:0\", shape=(), dtype=int32)\nTensor(\"scope_b/x:0\", shape=(), dtype=int32)\n```", "```\n# Original example, using only API calls\ny = tf.add(tf.matmul(A, x), b, name=\"result\")\n\n# Using overloaded operators\ny = A @ x + b\n```", "```\n/job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX>\n```", "```\nimport tensorflow as tf\n```", "```\nwith tf.device(\"/CPU:0\"):\n    A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    x = tf.constant([[0, 10], [0, 0.5]])\n    b = tf.constant([[1, -1]], dtype=tf.float32)\n```", "```\nwith tf.device(\"/GPU:0\"):\n    mul = A @ x\n```", "```\ny = mul + b\n```", "```\nwriter = tf.summary.FileWriter(\"log/matmul_optimized\", tf.get_default_graph())\nwriter.close()\n```", "```\n# The context manager opens the session\nwith tf.Session() as sess:\n    # Use the session to execute operations\n    sess.run(...)\n# Out of the context, the session is closed and the resources released\n```", "```\n# the IP and port of the TensorFlow server\nip = \"192.168.1.90\"\nport = 9877\nwith tf.Session(f\"grpc://{ip}:{port}\") as sess:\n    sess.run(...)\n```", "```\nimport tensorflow as tf\nimport numpy as np\n\nA = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\nx = tf.constant([[0, 10], [0, 0.5]])\nb = tf.constant([[1, -1]], dtype=tf.float32)\ny = tf.add(tf.matmul(A, x), b, name=\"result\")\n\nwriter = tf.summary.FileWriter(\"log/matmul\", tf.get_default_graph())\nwriter.close()\n\nwith tf.Session() as sess:\n    A_value, x_value, b_value = sess.run([A, x, b])\n    y_value = sess.run(y)\n\n    # Overwrite\n    y_new = sess.run(y, feed_dict={b: np.zeros((1, 2))})\n\nprint(f\"A: {A_value}\\nx: {x_value}\\nb: {b_value}\\n\\ny: {y_value}\")\nprint(f\"y_new: {y_new}\")\n```", "```\nA: [[1\\. 2.]\n    [3\\. 4.]]\nx: [[ 0\\. 10\\. ]\n    [ 0\\. 0.5]]\nb: [[ 1\\. -1.]]\ny: [[ 1\\. 10.]\n    [ 1\\. 31.]]\n```", "```\ny_new: [[ 0\\. 11.]\n        [ 0\\. 32.]]\n```", "```\nw = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\nb = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n```", "```\ndef conv2D(input, size_in, size_out, name=\"conv\"):\n\"\"\"Define a 2D convolutional layer + max pooling.\nArgs:\n    input: Input tensor: 4D.\n    size_in: it could be inferred by the input (input.shape[-1])\n    size_out: the number of convolutional kernel to learn\n    name: the name of the operation, using name_scope.\nReturns:\n    The result of the convolution as specified + a max pool operation\n    that halves the spatial resolution.\n\"\"\"\n    with tf.name_scope(name):\n        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n        act = tf.nn.relu(conv + b)\n        tf.summary.histogram(\"w\", w)\n        tf.summary.histogram(\"b\", b)\n        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n```", "```\ndef fc(input, size_in, size_out, name=\"fc\"):\n\"\"\"Define a fully connected layer.\nArgs:\n    input: Input tensor: 2D.\n    size_in: it could be inferred by the input (input.shape[-1])\n    size_out: the number of output neurons kernel to learn\n    name: the name of the operation, using name_scope.\nReturns:\n    The linear neurons output.\n\"\"\"\n```", "```\nwith tf.name_scope(name):\n    w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n    act = tf.matmul(input, w) + b\n    tf.summary.histogram(\"w\", w)\n    tf.summary.histogram(\"b\", b)\n    return act\n```", "```\nwith tf.variable_scope(\"scope\"):\n    a = tf.get_variable(\"v\", [1]) # a.name == \"scope/v:0\"\nwith tf.variable_scope(\"scope\"):\n    b = tf.get_variable(\"v\", [1]) # ValueError: Variable scope/v:0 already exists\nwith tf.variable_scope(\"scope\", reuse=True):\n    c = tf.get_variable(\"v\", [1]) # c.name == \"scope/v:0\"\n```", "```\ndef conv2D(input, size_in, size_out):\n    w = tf.get_variable(\n        'W', [5, 5, size_in, size_out],\n        initializer=tf.truncated_normal_initializer(stddev=0.1))\n    b = tf.get_variable(\n        'B', [size_out], initializer=tf.constant_initializer(0.1))\n    conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n    act = tf.nn.relu(conv + b)\n    tf.summary.histogram(\"w\", w)\n    tf.summary.histogram(\"b\", b)\n    return tf.nn.max_pool(\n        act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\ndef fc(input, size_in, size_out):\n    w = tf.get_variable(\n        'W', [size_in, size_out],\n        initializer=tf.truncated_normal_initializer(stddev=0.1))\n    b = tf.get_variable(\n        'b', [size_out], initializer=tf.constant_initializer(0.1))\n    act = tf.matmul(input, w) + b\n    tf.summary.histogram(\"w\", w)\n    tf.summary.histogram(\"b\", b)\n    return act\n```", "```\ninput = tf.placeholder(tf.float32, (None, 28,28,1))\nwith tf.variable_scope(\"first)\":\n    conv1 = conv2d(input, input.shape[-1].value, 10)\nwith tf.variable_scope(\"second\"): #no conflict, variables under the second/ scope\n    conv2 = conv2d(conv1, conv1.shape[-1].value, 1)\n# and so on...\n```", "```\ndef define_cnn(x, n_classes, reuse, is_training):\n    \"\"\"Defines a convolutional neural network for classification.\n    Args:\n        x: a batch of images: 4D tensor.\n        n_classes: the number of classes, hence, the number of output neurons.\n        reuse: the `tf.variable_scope` reuse parameter.\n        is_training: boolean variable that indicates if the model is in training.\n    Returns:\n        The output layer.\n    \"\"\"\n    with tf.variable_scope('cnn', reuse=reuse):\n        # Convolution Layer with 32 learneable filters 5x5 each\n        # followed by max-pool operation that halves the spatial extent.\n        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n\n        # Convolution Layer with 64 learneable filters 3x3 each.\n        # As above, max pooling to halve.\n        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n```", "```\n        shape = (-1,conv2.shape[1].value * conv2.shape[2].value * conv2.shape[3].value)\n        fc1 = tf.reshape(conv2, shape)\n\n        # Fully connected layer\n        fc1 = tf.layers.dense(fc1, 1024)\n        # Apply (inverted) dropout when in training phase.\n        fc1 = tf.layers.dropout(fc1, rate=0.5, training=is_training)\n\n        # Prediction: linear neurons\n        out = tf.layers.dense(fc1, n_classes)\n\n    return out\n\ninput = tf.placeholder(tf.float32, (None, 28, 28, 1))\nlogits = define_cnn(input, 10, reuse=False, is_training=True)\n```", "```\n# Input placeholders: input is the cnn input, labels is the loss input.\ninput = tf.placeholder(tf.float32, (None, 28, 28, 1))\nlabels = tf.placeholder(tf.int32, (None,))\n\nlogits = define_cnn(input, 10, reuse=False, is_training=True)\n# Numerically stable loss\nloss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n# Instantiate the Optimizer and get the operation to use to minimize the loss\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\n\n# As in the baseline example, log the graph\nwriter = tf.summary.FileWriter(\"log/graph_loss\", tf.get_default_graph())\nwriter.close()\n```", "```\nfrom tensorflow.keras.datasets import fashion_mnist\n\n(train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n# Scale input in [-1, 1] range\ntrain_x = train_x / 255\\. * 2 - 1\ntest_x = test_x / 255\\. * 2 - 1\n# Add the last 1 dimension, so to have images 28x28x1\ntrain_x = np.expand_dims(train_x, -1)\ntest_x = np.expand_dims(test_x, -1)\n```", "```\nepochs = 10\nbatch_size = 32\nnr_batches_train = int(train_x.shape[0] / batch_size)\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Number of batches per epoch: {nr_batches_train}\")\n```", "```\n# Define the accuracy operation over a batch\npredictions = tf.argmax(logits, 1)\n# correct predictions: [BATCH_SIZE] tensor\ncorrect_predictions = tf.equal(labels, predictions)\naccuracy = tf.reduce_mean(\n    tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n\n# Define the scalar summarie operation that once executed produce\n# an input for the tf.train.SummaryWriter.\n\naccuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\nloss_summary = tf.summary.scalar(\"loss\", loss)\n```", "```\nwriter = tf.summary.FileWriter(\"log/graph_loss\", tf.get_default_graph())\nvalidation_summary_writer = tf.summary.FileWriter(\n    \"log/graph_loss/validation\")\n```", "```\ndef train():\n    input = tf.placeholder(tf.float32, (None, 28, 28, 1))\n    labels = tf.placeholder(tf.int64, (None,))\n    logits = define_cnn(input, 10, reuse=False, is_training=True)\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n    global_step = tf.train.get_or_create_global_step()\n    train_op = tf.train.AdamOptimizer().minimize(loss, global_step)\n\n    writer = tf.summary.FileWriter(\"log/graph_loss\", tf.get_default_graph())\n    validation_summary_writer = tf.summary.FileWriter(\n        \"log/graph_loss/validation\")\n\n    init_op = tf.global_variables_initializer()\n\n    predictions = tf.argmax(logits, 1)\n    # correct predictions: [BATCH_SIZE] tensor\n    correct_predictions = tf.equal(labels, predictions)\n    accuracy = tf.reduce_mean(\n        tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n\n    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n    loss_summary = tf.summary.scalar(\"loss\", loss)\n    # Input preprocessing a Python stuff\n    (train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n    # Scale input in [-1, 1] range\n    train_x = train_x / 255\\. * 2 - 1\n    train_x = np.expand_dims(train_x, -1)\n    test_x = test_x / 255\\. * 2 - 1\n    test_x = np.expand_dims(test_x, -1)\n\n    epochs = 10\n    batch_size = 32\n    nr_batches_train = int(train_x.shape[0] / batch_size)\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Number of batches per epoch: {nr_batches_train}\")\n\n    validation_accuracy = 0\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        for epoch in range(epochs):\n            for t in range(nr_batches_train):\n                start_from = t * batch_size\n                to = (t + 1) * batch_size\n\n                loss_value, _, step = sess.run(\n                    [loss, train_op, global_step],\n                    feed_dict={\n                        input: train_x[start_from:to],\n                        labels: train_y[start_from:to]\n                    })\n                if t % 10 == 0:\n                    print(f\"{step}: {loss_value}\")\n            print(\n                f\"Epoch {epoch} terminated: measuring metrics and logging summaries\"\n            )\n\n            saver.save(sess, \"log/graph_loss/model\")\n            start_from = 0\n            to = 128\n            train_accuracy_summary, train_loss_summary = sess.run(\n                [accuracy_summary, loss_summary],\n                feed_dict={\n                    input: train_x[start_from:to],\n                    labels: train_y[start_from:to]\n                })\n\n            validation_accuracy_summary, validation_accuracy_value, validation_loss_summary = sess.run(\n                [accuracy_summary, accuracy, loss_summary],\n                feed_dict={\n                    input: test_x[start_from:to],\n                    labels: test_y[start_from:to]\n                })\n\n            # save values in TensorBoard\n            writer.add_summary(train_accuracy_summary, step)\n            writer.add_summary(train_loss_summary, step)\n\n            validation_summary_writer.add_summary(validation_accuracy_summary,\n                                                  step)\n            validation_summary_writer.add_summary(validation_loss_summary, step)\n\n            validation_summary_writer.flush()\n            writer.flush()\n\n            # model selection\n            if validation_accuracy_value > validation_accuracy:\n                validation_accuracy = validation_accuracy_value\n                saver.save(sess, \"log/graph_loss/best_model/best\")\n\n    writer.close()\n```"]