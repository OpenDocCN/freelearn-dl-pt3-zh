<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Modern Neural Networks</h1>
                </header>
            
            <article>
                
<p><span class="">In <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, we presented how recent neural networks, which are more suitable for image processing, surpassed previous computer vision methods of the past decade. However, limited by how much we can reimplement from scratch, we only covered basic architectures. Now, with TensorFlow's powerful APIs at our fingertips, it is time to discover what <strong>convolutional neural networks</strong> <span>(</span><strong>CNNs</strong><span>)</span> are, and how these modern methods are trained to further improve their robustness.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li><span>CNNs and their re</span><span>levance to computer vision</span></li>
<li><span>Implementing these modern networks with TensorFlow and Keras</span></li>
<li>Advanced optimizers and how to train CNNs efficiently</li>
<li>Regularization methods and how to avoid overfitting</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The main resources of this chapter are implemented with TensorFlow. The Matplotlib package (<a href="https://matplotlib.org">https://matplotlib.org</a>) and the scikit-image package (<a href="https://scikit-image.org">https://scikit-image.org</a>) are also used, though only to display some results or to load example images.</p>
<p>As in previous chapters, Jupyter notebooks illustrating the concepts covered in this chapter can be found in the following GitHub folder: <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter03">github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter03</a><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discovering convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>In the first part of this chapter, we will present CNNs, also known as <strong>ConvNets</strong>,<strong> </strong>and explain why they have become omnipresent in vision tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks for multidimensional data</h1>
                </header>
            
            <article>
                
<p>CNNs were introduced to solve some of the shortcomings of the original neural networks. In this section, we will address these issues and present how CNNs deal with them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problems with fully connected networks</h1>
                </header>
            
            <article>
                
<p>Through our introductory experiment in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, and <a href="c7c49010-458f-47ef-a538-96118f9cd892.xhtml">Chapter 2</a>, <em>TensorFlow Basics and Training a Model</em>, we have already highlighted the following two main drawbacks of basic networks when dealing with images:</p>
<ul>
<li>An explosive number of parameters</li>
<li>A lack of spatial reasoning<br/></li>
</ul>
<p>Let's discuss each of these here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An explosive number of parameters</h1>
                </header>
            
            <article>
                
<p>Images are complex structures with a large number of values (that is, <em>H</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">× </span><em>W</em> <span class="js-about-item-abstr">× </span><em>D</em></span> values with <em>H</em> indiacting the image's height, <span class="ui_qtext_rendered_qtext"><em>W</em></span> its width, and <span class="ui_qtext_rendered_qtext"><em>D</em></span> its depth/number of channels, such as <span class="ui_qtext_rendered_qtext"><em>D</em></span> = 3 for RGB images). Even the small, single-channel images we used as examples in the first two chapters represent input vectors of size <em>28 </em><span class="js-about-item-abstr">×</span> <em>28 </em><span class="js-about-item-abstr"><span class="texhtml">×</span></span> <em>1 = 784</em> values each. For the first layer of the basic neural network we implemented, this meant a weight matrix of shape (784, 64). This equates to 50,176 (784 <span class="js-about-item-abstr"><span class="texhtml">×</span></span><span> </span>64) parameter values to optimize, just for this variable!</p>
<p>This number of parameters simply explodes when we consider larger RGB images or deeper networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A lack of spatial reasoning</h1>
                </header>
            
            <article>
                
<p>Because their neurons receive all the values from the previous layer without any distinction (they are <em>fully connected</em>), these neural networks do not have a notion of <em>distance</em>/<em>spatiality</em>. Spatial relations in the data are lost. Multidimensional data, such as images, could also be anything from column vectors to dense layers because their operations do not take into account the data dimensionality nor the positions of input values. More precisely, this means that the notion of proximity between pixels is lost to <strong>fully connected</strong> (<strong>FC</strong>) layers, as all pixel values are combined by the layers with no regard for their original positions.</p>
<div class="packt_infobox">As it does not change the behavior of dense layers, to simplify their computations and parameter representations, it is common practice to <em>flatten</em> multidimensional inputs before passing them to these layers (that is, to reshape them into column vectors).</div>
<p>Intuitively, neural layers would be much smarter if they could take into account <strong>spatial information</strong>; that is, that some input values belong to the same pixel (channel values) or to the same image region (neighbor pixels).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing CNNs</h1>
                </header>
            
            <article>
                
<p>CNNs offer simple solutions to these shortcomings. While they work the same way as the networks we introduced <span>previously </span>(such as feed-forward and backpropagation), some clever changes were brought to their architecture.</p>
<p>First of all, CNNs can handle multidimensional data. For images, a CNN takes as input three-dimensional data (height <span class="js-about-item-abstr">×</span> width <span class="js-about-item-abstr">×</span> depth) and has its own neurons arranged in a similar volume (refer to <em>Figure 3.1</em>). This leads to the second novelty of CNNs—unlike fully connected networks, where neurons are connected to all elements from the previous layer, each neuron in CNNs only has access to some elements in the neighboring region of the previous layer. This region (usually square and spanning all channels) is called the <strong>receptive field</strong> of the neurons (or the filter size):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/adc1e504-6290-43f6-b749-aa6409ffa9be.png" style="width:45.17em;height:20.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.1: CNN representation, showing the <em>receptive fields</em> of the top-left neurons from the first layer to the last (further explanations can be found in the following subsections)</div>
<p>By linking neurons only to their neighboring ones in the previous layer, CNNs not only drastically reduce the number of parameters to train, but also preserve the localization of image features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNN operations</h1>
                </header>
            
            <article>
                
<p>With this architecture paradigm, several new types of layers were also introduced, efficiently taking advantage of <em>multidimensionality</em> and <em>local connectivity</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers</h1>
                </header>
            
            <article>
                
<p>CNNs get their name from <em>convolutional layers</em>, which are at the core of their architecture. In these layers, the number of parameters is further reduced by sharing the same weights and bias among all neurons connected to the same output channel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Concept</h1>
                </header>
            
            <article>
                
<p>These specific neurons with shared weights and bias can also be <span><span>thought of </span></span>as a single neuron sliding over the whole input matrix with <em>spatially limited connectivity</em>. At each step, <span class="ui_qtext_rendered_qtext">this neuron is only spatially connected to the local region in the input volume (<em>H</em> <span class="js-about-item-abstr">× </span><em>W</em> <span class="js-about-item-abstr">×</span> <em>D</em>) it is currently sliding over. Given this limited input of dimensions, <em>k<sub>H</sub></em> <span class="js-about-item-abstr">× </span><em>k<sub>W</sub></em> <span class="js-about-item-abstr">×</span> <em>D</em> for a neuron with a filter size (<em>k<sub>H</sub></em>, <em>k<sub>W</sub></em>), the neuron still works like the ones modeled in our first chapter—it linearly combines the input values (<em>k<sub>H</sub></em> <span class="texhtml"><span class="js-about-item-abstr">× </span></span><em>k<sub>W</sub></em> <span class="texhtml"><span class="js-about-item-abstr">× </span></span><em>D</em> values) before applying an activation function to the sum (a linear or non-linear function).</span> <span class="ui_qtext_rendered_qtext">Mathematically, the response, <em>z<sub>i,j</sub></em></span>, <span class="ui_qtext_rendered_qtext">of the neuron when presented with the input patch starting at position <em>(i,</em> <em>j</em>) can be expressed as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2928d949-f803-4a05-af2b-f58df1cee1b8.png" style="width:26.08em;height:4.33em;"/></p>
<p><img class="fm-editor-equation" src="assets/55a0a8c2-4a60-41e8-a9da-0c3bc0154704.png" style="width:7.75em;height:1.33em;"/> is the neuron's weights (that is, a two-dimensional matrix of shape <span class="ui_qtext_rendered_qtext"><em>k<sub>H</sub></em> <span class="js-about-item-abstr">×</span> <em>k<sub>W</sub></em> <span class="js-about-item-abstr">×</span> <em>D</em></span><span class="ui_qtext_rendered_qtext">), </span><img class="fm-editor-equation" src="assets/895581ac-c2d3-4936-8811-a3f896e59e00.png" style="width:2.67em;height:1.00em;"/> is the neuron's bias, and <img class="fm-editor-equation" src="assets/4f85cb73-d794-47a7-a737-750799a3cc72.png" style="width:0.83em;height:0.83em;"/> is the activation function (for instance, <em>sigmoid</em>). <span class="ui_qtext_rendered_qtext">Repeating this operation for each position that the neuron can take over the input data, we obtain its complete response matrix, 𝑧</span>, <span class="ui_qtext_rendered_qtext">of dimensions <em>H</em><sub>o</sub> <span class="js-about-item-abstr">×</span> <em>W</em><sub>o</sub>, with <em>H</em><sub>o</sub> and <em>W</em><sub>o</sub> being the number of times the neuron can slide vertically and horizontally (respectively) over the input tensor.</span></p>
<div class="packt_infobox">In practice, most of the time, square filters are used, meaning that they have a size (<em>k,</em> <em>k</em>) with <span class="ui_qtext_rendered_qtext"><em>k =</em></span> <span class="ui_qtext_rendered_qtext"><em>k<sub>H</sub></em> = <em>k<sub>W</sub></em></span><span class="ui_qtext_rendered_qtext">. For the rest of this chapter, we will only consider square filters to simplify the explanations, though it is good to remember that their height and width may vary.<br/></span></div>
<p><span class="ui_qtext_rendered_qtext">As a convolutional layer can still have <em>N</em> sets of different neurons (that is, <em>N</em> sets of neurons with shared parameters), their response maps are stacked together into an output tensor of shape <em>H</em><sub>o</sub> <span class="js-about-item-abstr">×</span> <em>W</em><sub>o</sub> <span class="js-about-item-abstr">×</span> <em>N</em>.<br/></span></p>
<p>In the same way that we applied matrix multiplication to fully connected layers, the <strong>convolution operation</strong> can be used here to compute all the response maps at once (hence the name of these layers). Those familiar with this operation may have recognized it as soon as we mentioned <em>sliding filters over the input matrix</em>. For those who are unfamiliar with the operation, the results of a convolution are indeed obtained by sliding a filter, <em>w</em>, over the input matrix, <em>x</em>, and computing, at each position, the dot product of the filter and the patch of <em>x</em> starting at the current position. This operation is illustrated in <em>Figure 3.2</em> (an input tensor with a single channel is used to keep the diagram easy to understand):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6a7df36e-fb8c-46ea-91bc-ccbb59354477.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.2: A convolution illustrated</div>
<p>In <em>Figure 3.2</em>, please note that the input, <em>x,</em> h<span>as been </span><em>padded</em><span> with zeros, which is commonly done in convolutional layers; for instance, when we want the output to be the same size as the original input (a size of 3 × 3 in this example). The notion of padding is further developed later in this chapter.</span><span> </span></p>
<div class="packt_tip packt_infobox">The proper mathematical term for this operation is actually <em>cross-correlation</em>, though <em>convolution </em>is commonly used in the machine learning community. The cross-correlation of a matrix, <em>x</em>, with a filter, <em>w</em>, is <img class="fm-editor-equation" src="assets/bc0a22cf-8707-490f-9dcf-d6799fadb04f.png" style="width:17.25em;height:1.33em;"/>:<br/>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7679d42b-e3ac-4046-b758-00bd8baab798.png" style="width:19.92em;height:4.33em;"/></p>
<p>Notice the correspondence with our equation for <em>z</em>. On the other hand, the actual mathematical convolution of a matrix, <em>x</em>, with a filter, <em>w</em>, is for all valid positions (<em>i</em>, <em>j</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cd2644fe-7b8b-45ee-b290-56ce8f40cbba.png" style="width:19.92em;height:4.33em;"/></p>
As we can see, both operations are quite similar in this setup, and convolution results can be obtained from the cross-correlation operation by simply <em>flipping </em>the filters before it.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Properties</h1>
                </header>
            
            <article>
                
<div>
<p><span class="ui_qtext_rendered_qtext">A convolutional layer with <em>N</em> sets of different neurons is</span> <span class="ui_qtext_rendered_qtext">thus defined by <em>N</em></span> weight matrices (also called <strong>filters</strong> or <strong>kernels</strong>) of shape <span class="ui_qtext_rendered_qtext"><em>D</em></span> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">× </span><em>k</em> <span class="js-about-item-abstr">×</span> <em>k</em> (when the filters are square),</span> and <span class="ui_qtext_rendered_qtext"><em>N</em></span> bias values. Therefore, this layer only has <span class="ui_qtext_rendered_qtext"><em>N</em></span> <span class="ui_qtext_rendered_qtext"><span class="texhtml"><span class="js-about-item-abstr">×</span></span></span> (<span class="ui_qtext_rendered_qtext"><em>D</em><span class="texhtml"><em>k</em><sup>2</sup></span></span> <span class="ui_qtext_rendered_qtext">+ 1)</span> values to train. A fully connected layer with similar input and output dimensions would need (<span class="ui_qtext_rendered_qtext"><em>H</em> <span class="js-about-item-abstr">×</span> <em>W</em> <span class="texhtml"><span class="js-about-item-abstr">×</span></span> <em>D</em>) <span class="js-about-item-abstr">×</span> (<em>H</em><sub>o</sub> <span class="js-about-item-abstr">×</span> <em>W</em><sub>o</sub> <span class="texhtml"><span class="js-about-item-abstr">× <em>N</em></span></span>) parameters instead. As we demonstrated previously, the number of parameters for fully connected layers is influenced by the dimensionality of the data, whereas this does not affect the parameter numbers for convolutional layers.</span></p>
<p><span class="ui_qtext_rendered_qtext">This property makes convolutional layers really powerful tools in computer vision for two reasons. First, as implied in the previous paragraph, it means we can train networks for larger input images without impacting the number of parameters we would need to tune. Second, this also means that convolutional layers can be applied to any images,</span> irrespective of their dimensions<span class="ui_qtext_rendered_qtext">! Unlike networks with fully connected layers, purely convolutional ones do not need to be adapted and retrained for inputs of different sizes.</span><span class="ui_qtext_rendered_qtext"> </span></p>
</div>
<div class="packt_tip">When applying a CNN to images of various sizes, you still need to be careful when sampling the input batches. Indeed, a subset of images can be stacked together into a normal batch tensor only if they all have the same dimensions. Therefore, in practice, you should either sort the images before batching them (mostly done during the training phase) or simply process each image separately (usually during the testing phase). However, both to simplify data processing and the network's task, people usually preprocess their images so they are all the same size (through scaling and/or cropping).</div>
<p>Besides those computational optimizations, convolutional layers also have interesting properties related to image processing. With training, the layer's filters become really good at reacting to specific <em>local features</em> (a layer with <span class="ui_qtext_rendered_qtext"><em>N</em></span> filters means the possibility to react to <em>N</em> different features). Each kernel of the first convolutional layer in a CNN would, for instance, learn to activate for a specific low-level feature, such as a specific line orientation or color gradient. Then, deeper layers would use these results to localize more abstract/advanced features, such as the shape of a face, and the contours of a particular object. Moreover, each filter (that is, each set of shared neurons) would respond to a specific image feature, whatever its location(s) in the image. More formally, convolutional layers are invariant to translation in the image coordinate space.</p>
<p>The response map of a filter over the input image can be described as a map representing the locations where the filter responded to its target feature. For this reason, those intermediary results in CNNs are commonly called <strong>feature maps</strong>. A layer with <span class="ui_qtext_rendered_qtext"><em>N</em></span> filters will, therefore, return <span class="ui_qtext_rendered_qtext"><em>N</em></span> feature maps, each corresponding to the detection of a particular feature in the input tensors. The stack of <span class="ui_qtext_rendered_qtext"><em>N</em></span> feature maps returned by a layer is commonly called a <strong>feature volume</strong> (with a <span>shape of </span><span class="ui_qtext_rendered_qtext"><em>H</em><sub>o</sub> <span class="js-about-item-abstr">×</span> <em>W</em><sub>o</sub> <span class="texhtml"><span class="js-about-item-abstr">× <em>N</em></span></span></span>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameters</h1>
                </header>
            
            <article>
                
<p>A convolutional layer is first defined by its number of filters, <em>N</em>, by its input depth, <em>D</em> <span class="ui_qtext_rendered_qtext">(that is, the number of input channels)</span>, and by its filter/kernel size, (<em>k<sub>H</sub></em>, <em>k<sub>W</sub></em>). As square filters are commonly used, the size is usually simply defined by <em>k</em> <span class="ui_qtext_rendered_qtext">(though, as mentioned earlier, non-square filters are sometimes considered)</span>.</p>
<p>However, as mentioned previously, convolutional layers actually differ from the homonym mathematical operation. The operation between the input and their filters can take several additional hyperparameters, affecting the way the filters are <em>sliding</em> over the images.</p>
<p>First, we can apply different <em>strides</em> with which the filters are sliding. The stride hyperparameter thus defines whether the dot product between the image patches and the filters should be computed at every position when sliding (<em>stride = 1</em>), or every <em>s</em> position (<em>stride = s</em>). The larger the stride, the sparser the resulting feature maps.</p>
<p>Images can also be <em>zero-padded</em> before convolution; that is, their sizes can be synthetically increased by adding rows and columns of zeros around their original content. As shown in <em>Figure 3.2</em>, this padding increases the number of positions the filters can take over the images. We can thus specify the padding value to be applied (that is, the number of empty rows and columns to be added on each side of the inputs).</p>
<div class="packt_tip"><span class="ui_qtext_rendered_qtext">The letter <em>k</em> is commonly used for the filter/kernel size (<em>k</em> for <em>kernel</em>). Similarly, <em>s</em> is commonly used for the stride, and <em>p</em> for the padding. Note that, as with the filter size, the same values are usually used for the horizontal and vertical strides <em>(s = s<sub>H</sub> = s<sub>W</sub>),</em> as well as for the horizontal and vertical padding; though, for some specific use cases, they may have different values.<br/></span></div>
<p>All these parameters (the number of kernels, <em>N;</em> kernel size, <em>k;</em> stride, <em>s;</em> and padding, <span class="ui_qtext_rendered_qtext"><em>p</em></span>) not only affect the layer's operations, but also its output shape. Until now, we defined this shape as (<span class="ui_qtext_rendered_qtext"><em>H</em><sub>o</sub>, <em>W</em><sub>o</sub>, <em>N</em></span>), <span class="ui_qtext_rendered_qtext">with <em>H</em><sub>o</sub> and <em>W</em><sub>o</sub></span> the number of times the neuron can slide vertically and horizontally over the inputs. So, what actually <span>are </span><span class="ui_qtext_rendered_qtext"><em>H</em><sub>o</sub> and <em>W</em><sub>o</sub>? Formally, they can be computed as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea6641bb-f197-4660-bd66-92dec120b447.png" style="width:25.33em;height:2.67em;"/></p>
<p>While we invite you to pick some concrete examples to better grasp these formulas, we can intuitively understand the logic behind them. Filters of size <span class="ui_qtext_rendered_qtext">𝑘</span> can take a maximum of <em><span class="ui_qtext_rendered_qtext">H</span> - <span class="ui_qtext_rendered_qtext">k + 1</span></em> <span class="ui_qtext_rendered_qtext">different vertical positions and <em>W - k + 1</em> horizontal ones in images of size <em>H</em> <span class="js-about-item-abstr">×</span> <em>W</em>. Additionally, this number of positions increases to <em>H - k + 2p + 1 </em>(with respect to <em>W - k + 2p + 1</em>) if these images are padded by <em>p</em> on every side. Finally, increasing the stride, <em>s,</em> basically means considering only one position out of <em>s</em>, explaining the division (note that it is an integer division).</span></p>
<p><span class="ui_qtext_rendered_qtext">With these hyperparameters, we can easily control the layer's output sizes. This is particularly convenient for applications such as object segmentation; that is, when we want the output segmentation mask to be the same size as the input image.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow/Keras methods</h1>
                </header>
            
            <article>
                
<p>Available in the low-level API, <kbd>tf.nn.conv2d()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d</a>) is the default choice for image convolution. Its main parameters are as follows:</p>
<ul>
<li><kbd>input</kbd>: The batch of input images, of shape <em>(B, H, W, D</em>), with <em>B </em>being the batch size.</li>
<li><kbd>filter</kbd>: The <em>N</em> filters stacked into a tensor of shape (<em>k<sub>H</sub>, k<sub>W</sub>, D, N</em>).</li>
<li><kbd>strides</kbd>: A list of four integers representing the stride for each dimension of the batched input. Typically, you would use <em>[1, s<sub>H</sub>, s<sub>W</sub>, 1</em>] (that is, applying a custom stride only for the two spatial dimensions of the image).</li>
<li><kbd>padding</kbd>: Either a list of <em>4 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> 2</em> integers representing the padding before and after each dimension of the batched input, or a string defining which predefined padding case to use; that is, either <kbd>VALID</kbd> or <kbd>SAME</kbd> (explanations follow).</li>
<li><kbd>name</kbd>: The name to identify this operation (useful for creating clear, readable graphs).</li>
</ul>
<p>Note that <kbd>tf.nn.conv2d()</kbd> accepts some other more advanced parameters, which we will not cover yet (refer to the documentation). <em>Figures 3.3</em> and<em> 3.4</em> illustrate the effects of two convolutional operations with different arguments:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a9c4782-cf35-48eb-b204-c14d5a29ecb9.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.3: Example of a convolution performed on an image with TensorFlow. The kernel here is a well-known one, commonly used to apply <em>Gaussian blur</em> to images</div>
<div>
<p>In the following screenshot, a kernel that's well known in computer vision is applied:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0451566f-c965-4ee1-8b51-c38f95f12463.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.4: Example of another TensorFlow convolution, with a larger stride. This specific kernel <span>is commonly used to extract edges/contours in images</span></div>
</div>
<p>Regarding padding, TensorFlow developers made the choice to provide two different pre-implemented modes so that users do not have to figure out which value, <em>p,</em> they need for usual cases. <kbd>VALID</kbd> means the images won't be padded (<em>p</em> = 0), and the filters will slide only over the default <em>valid</em> positions. When opting for <kbd>SAME</kbd>, TensorFlow will calculate the value, <em>p,</em> so that the convolution outputs have the <em>same</em> height and width as the inputs for a stride of <kbd>1</kbd> (that is, solving <span class="ui_qtext_rendered_qtext"><em>H</em><sub>o</sub> = <em>H</em><sub>o</sub> and <em>W</em><sub>o</sub> = <em>W</em> given the equations presented in the previous section, temporarily setting <em>s</em> to 1).<br/></span></p>
<div class="packt_tip">Sometimes, you may want to pad with something more complex than zeros. In those cases, it is recommended to use the <kbd>tf.pad()</kbd> method (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/pad">https://www.tensorflow.org/api_docs/python/tf/pad</a>) instead, and then simply instantiate a convolution operation with <kbd>VALID</kbd> <span>padding.</span><br/>
<br/>
TensorFlow also offers several other low-level convolution methods, such as <kbd>tf.nn.conv1d()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv1d">https://www.tensorflow.org/api_docs/python/tf/nn/conv1d</a>) and <kbd>tf.nn.conv3d()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv3d">https://www.tensorflow.org/api_docs/python/tf/nn/conv3d</a>),  for one-dimensional and three-dimensional data, respectively<span>, or</span> <kbd>tf.nn.depthwise_conv2d()</kbd> <span>(refer to the documentation at</span> <a href="https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d">https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d</a><span>) to convolve each channel of the images with different filters, and more.</span></div>
<p>So far, we have only presented convolutions with fixed filters. For CNNs, we have to make the filters trainable. Convolutional layers also apply a learned bias before passing the result to an activation function. This series of operations can, therefore, be implemented as follows:</p>
<div>
<pre># Initializing the trainable variables (for instance, the filters with values from a Glorot distribution, and the bias with zeros):<br/>kernels_shape = [k, k, D, N]<br/>glorot_uni_initializer = tf.initializers.GlorotUniform()<br/># ^ this object is defined to generate values following the Glorot distribution (note that other famous parameter more or less random initializers exist, also covered by TensorFlow)<br/>kernels = tf.Variable(glorot_uni_initializer(kernels_shape), <br/>                      trainable=True, name="filters")<br/>bias = tf.Variable(tf.zeros(shape=[N]), trainable=True, name="bias")<br/><br/># Defining our convolutional layer as a compiled function:<br/>@tf.function<br/>def conv_layer(x, kernels, bias, s):<br/>    z = tf.nn.conv2d(x, kernels, strides=[1,s,s,1], padding='VALID')<br/>    # Finally, applying the bias and activation function (for instance, ReLU):<br/>    return tf.nn.relu(z + bias)</pre></div>
<p>This feed-forward function can further be wrapped into a <kbd>Layer</kbd> object, similar to how the fully connected layer we implemented in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, was built around the matrix operations. Through the Keras API, TensorFlow 2 provides its own <kbd>tf.keras.layers.Layer</kbd> class, which we can extend (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer</a>). The following code block demonstrates how a simple convolution layer can be built on this:</p>
<pre>class SimpleConvolutionLayer(tf.keras.layers.Layer):<br/>    def __init__(self, num_kernels=32, kernel_size=(3, 3), stride=1):<br/>        """ Initialize the layer.<br/>        :param num_kernels: Number of kernels for the convolution<br/>        :param kernel_size: Kernel size (H x W)<br/>        :param stride: Vertical/horizontal stride<br/>        """<br/>        super().__init__() <br/>        self.num_kernels = num_kernels<br/>        self.kernel_size = kernel_size<br/>        self.stride = stride<br/><br/>    def build(self, input_shape):<br/>        """ Build the layer, initializing its parameters/variables.<br/>        This will be internally called the 1st time the layer is used.<br/>        :param input_shape: Input shape for the layer (for instance, BxHxWxC)<br/>        """<br/>        num_input_ch = input_shape[-1] # assuming shape format BHWC<br/>        # Now we know the shape of the kernel tensor we need:<br/>        kernels_shape = (*self.kernel_size, num_input_ch, self.num_kernels)<br/>        # We initialize the filter values fior instance, from a Glorot distribution:<br/>        glorot_init = tf.initializers.GlorotUniform()<br/>        self.kernels = self.add_weight( # method to add Variables to layer<br/>            name='kernels', shape=kernels_shape, initializer=glorot_init,<br/>            trainable=True) # and we make it trainable.<br/>        # Same for the bias variable (for instance, from a normal distribution):<br/>        self.bias = self.add_weight(<br/>            name='bias', shape=(self.num_kernels,), <br/>            initializer='random_normal', trainable=True)<br/><br/>    def call(self, inputs):<br/>        """ Call the layer, apply its operations to the input tensor."""<br/>        return conv_layer(inputs, self.kernels, self.bias, self.stride)</pre>
<p>Most of TensorFlow's mathematical operations (for example, in <kbd>tf.math</kbd> and <kbd>tf.nn</kbd>) already have their derivatives defined by the framework. Therefore, as long as a layer is composed of such operations, we do not have to manually define its backpropagation, saving quite some effort!</p>
<p>While this implementation has the advantage of being explicit, the Keras API also encapsulates the initialization of common layers (as presented in <a href="c7c49010-458f-47ef-a538-96118f9cd892.xhtml">Chapter 2</a>, <em>TensorFlow Basics and Training a Model</em>), thereby speeding up development. With the <kbd>tf.keras.layers</kbd> module, we can instantiate a similar convolutional layer in a single call, as follows:</p>
<pre>conv = tf.keras.layers.Conv2D(filters=N, kernel_size=(k, k), strides=s,<br/>                              padding='valid', activation='relu')</pre>
<p><kbd>tf.keras.layers.Conv2D()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D</a>) has a long list of additional parameters, encapsulating several concepts, such as weight regularization (presented later in this chapter). Therefore, it is recommended to use this method when building advanced CNNs, instead of spending time reimplementing such concepts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling layers</h1>
                </header>
            
            <article>
                
<p>Another commonly used category of layer introduced with CNNs is the <em>pooling</em> type.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Concept and hyperparameters</h1>
                </header>
            
            <article>
                
<p>These pooling layers are a bit <span><span>peculiar </span></span>because they do not have any trainable parameters. Each neuron simply takes the values in its <em>window</em> (the receptive field) and returns a single output, computed from a predefined function. The two most common pooling methods are max-pooling and average-pooling. <strong>Max-pooling</strong> layers return only the maximum value at each depth of the pooled area (refer to <em>Figure 3.5</em>), and <strong>average-pooling </strong>layers compute the average at each depth of the pooled area (refer to <em>Figure 3.6</em>).</p>
<p>Pooling layers are commonly used with a <em>stride</em> value equal to the size of their <em>window/kernel size</em>, in order to apply the pooling function over non-overlapping patches. Their purpose is to <em>reduce the spatial dimensionality of the data</em>, cutting down the total number of parameters needed in the network, as well as its computation time. For instance, a pooling layer with a <em>2 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">× </span></span>2</em> window size and stride of <em>2</em> (that is, <em>k</em> = 2 and <em>s</em> = 2) would take patches of four values at each depth and return a single number. It would thus divide <span>the height and the width of the features </span><span>by</span> <em>2;</em><span> that is, dividing </span><span>the number of computations for the following layers </span><span>by</span> <em>2 <span class="ui_qtext_rendered_qtext"><span class="texhtml"><span class="js-about-item-abstr">× </span></span></span>2 = 4</em><span>. Finally, note that, as with convolutional layers, you can pad the tensors before applying the operation (as shown in</span> <em>Figure 3.5</em><span>):</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0594d28a-0092-4675-ba9b-4d181ffa9ab2.png" style="width:46.42em;height:14.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.5: Illustration of a max-pooling operation with a window size of 3 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span> 3</span>, a padding of 1, and a stride of 2 on a single-channel input</div>
<p class="mce-root">Through the padding and stride parameters, it is thus possible to control the dimensions of the resulting tensors. <em>Figure 3.6</em> provides another example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aad0f6e0-c963-4531-be17-a1276f82aabc.png" style="width:47.58em;height:11.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.6: Illustration of an average-pooling operation with a window size of 2 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">× </span></span>2, a padding of 0, and a stride of 2 on a single-channel input</div>
<p class="CDPAlignLeft CDPAlign">With hyperparameters being similar to convolutional layers except for the absence of trainable kernels, pooling layers are, therefore, easy to use and lightweight solutions for controlling data dimensionality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow/Keras methods</h1>
                </header>
            
            <article>
                
<p>Also available from the <kbd>tf.nn</kbd> package, <kbd>tf.nn.max_pool()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool">https://www.tensorflow.org/api_docs/python/tf/nn/max_pool</a>) and <kbd>tf.nn.avg_pool()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool">https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool</a>) conveniently have a signature quite similar to <kbd>tf.nn.conv2d()</kbd>, as follows:</p>
<ul>
<li><kbd>value</kbd>: The batch of input images of shape (<em>B</em>, <em>H</em>, <em>W</em>, <em>D</em>), with <em>B </em>being the batch size</li>
<li><kbd>ksize</kbd>: A list of four integers representing the window size in each dimension; commonly, <em>[1, k, k, 1</em>] is used</li>
<li><kbd>strides</kbd>: A list of four integers representing the stride for each dimension of the batched input, similar to <kbd>tf.nn.conv2d()</kbd></li>
<li><kbd>padding</kbd>: A string defining which padding algorithm to use (<kbd>VALID</kbd> or <kbd>SAME</kbd>)</li>
<li><kbd>name</kbd>: The name to identify this operation (useful for creating clear, readable graphs)</li>
</ul>
<p><em>Figure 3.7</em> illustrates an average-pooling operation applied to an image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/898a506c-2d09-4aa2-bc4d-6f5b4939fd16.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.7: Example of average-pooling performed on an image with TensorFlow</div>
<p>In <em>Figure 3.8</em>, the max-pooling function is applied to the same image:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/39987d76-b942-4691-92fb-031dd4c60dc0.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.8: Example of another max-pooling operation, with an excessively large window size compared to the stride (purely for demonstration purposes)</div>
<p>Here, again, we can still use the higher-level API to make the instantiation slightly more succinct:</p>
<pre>avg_pool = tf.keras.layers.<strong>AvgPool2D</strong>(pool_size=k, strides=[s, s], padding='valid')<br/>max_pool = tf.keras.layers.<strong>MaxPool2D</strong>(pool_size=k, strides=[s, s], padding='valid')</pre>
<p><span>Since pooling layers do not have trainable weights, there is no real distinction between the pooling operation and the corresponding layer in TensorFlow</span>. This makes these operations not only lightweight, but easy to instantiate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully connected layers</h1>
                </header>
            
            <article>
                
<p>It is worth mentioning that FC layers are also used in CNNs, the same way they are in regular networks. We will present, in the following paragraphs, when they should be considered, and how to include them in CNNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Usage in CNNs</h1>
                </header>
            
            <article>
                
<p>While FC layers can be added to CNNs processing multidimensional data, this implies, however, that the input tensors passed to these layers must first be reshaped into a batched column vector—the way we did with the MNIST images for our simple network in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, and <a href="c7c49010-458f-47ef-a538-96118f9cd892.xhtml">Chapter 2</a>, <em>TensorFlow Basics and Training a Model</em> (that is, <em>flattening</em> the height, width, and depth dimensions into a single vector).</p>
<div class="packt_infobox">FC layers are also often called <strong>densely connected</strong>, or simply <strong>dense</strong> (as opposed to other CNN layers that have more limited connectivity).</div>
<p class="mce-root">While it can be advantageous in some cases for neurons to have access to the complete input map (for instance, to combine spatially distant features), fully connected layers have several shortcomings, as mentioned at the beginning of this chapter (for example, the loss of spatial information and the large number of parameters). Moreover, unlike other CNN layers, dense ones are defined by their input and output sizes. A specific dense layer will not work for inputs that have a shape different from the one it was configured for. Therefore, using FC layers in a neural network usually means losing the possibility to apply them to images of heterogeneous sizes.</p>
<p class="mce-root">Despite these shortcomings, these layers are still commonly used in CNNs. They are usually found among the final layers of a network, for instance, to convert the multidimensional features into a 1D classification vector.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow/Keras methods</h1>
                </header>
            
            <article>
                
<p>Although we already used TensorFlow's dense layers in the previous chapter, we did not stop to focus on their parameters and properties. Once again, the signature of <kbd>tf.keras.layers.Dense()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>) is comparable to that of previously introduced layers, with the difference that they do not accept any <kbd>strides</kbd> or <kbd>padding</kbd> for parameters, but instead use <kbd>units</kbd> representing the number of neurons/output size, as follows:</p>
<pre>fc = tf.keras.layers.Dense(units=output_size, activation='relu')</pre>
<p>Remember that you should, however, take care of <em>flattening</em> the multidimensional tensors before passing them to dense layers. <kbd>tf.keras.layers.Flatten()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten</a>) can be used as an intermediate layer for that purpose.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Effective receptive field</h1>
                </header>
            
            <article>
                
<p>As we will detail in this section, the <strong>effective receptive field</strong><span> (</span><strong>ERF</strong><span>)</span> of a neural network is an important notion in deep learning, as it may affect the ability of the network to cross-reference and combine distant elements in the input images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Definitions</h1>
                </header>
            
            <article>
                
<p>While the receptive field represents the local region of the previous layer that a neuron is connected to, the ERF defines <em>the region of the input image</em> (and not just of the previous layer), which affects the activation of a neuron for a given layer, as shown in <em>Figure 3.9</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0b54dcd5-94a2-42de-9d5a-6631a4214ad3.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.9: Illustration of the receptive field of a layer with a simple network of two convolutional layers</div>
<p>Note that it is common to find the term <strong>receptive field </strong>(<strong>RF</strong>) used in place of ERF, because RF can simply be referred to as the filter size or the window size of a layer. Some people also use RF or ERF to specifically define the input regions affecting each unit of the output layer (and not just any intermediary layer of a network).</p>
<p>Adding to the confusion, some researchers started calling ERF<em> </em>the subset of the input region that is actually affecting a neuron. This was introduced by Wenjie Luo et al. in their paper, <em>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, </em>published in <em>Advances in Neural Information Processing Systems (2016)</em>. Their idea was that not all pixels <em>seen </em>by a neuron contribute <em>equally</em> to its response. We can intuitively accept that, for instance, pixels at the center of the RF will have more weight than peripheral ones. The information held by these central pixels can be propagated along multiple paths in the intermediary layers of the network to reach a given neuron, while pixels in the periphery of the receptive field are connected to this neuron through a single path. Therefore, the ERF, as defined by Luo et al., follows a pseudo-Gaussian distribution, unlike the uniform distribution of a traditional ERF.</p>
<p>The authors make an interesting parallel between this representation of the receptive field and the human <strong>central fovea</strong>, the region of the eye responsible for our sharp central vision. This detailed part of the vision is at the basis of many human activities. Half the optical nerves are linked to the fovea (despite its relatively small size), in the same way that central pixels in effective receptive fields are connected to a higher number of artificial neurons.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Formula</h1>
                </header>
            
            <article>
                
<p>No matter what actual role its pixels are playing, the effective receptive field (named <em>R<sub>i</sub></em> here) of the <em>i</em><sup>th</sup> layer of a CNN can be recursively computed as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f651c9be-9751-49c1-875c-8f4e891fab2f.png" style="width:14.08em;height:3.92em;"/></p>
<p>In this equation, <em>k<sub>i</sub></em> is the filter size of the layer, and <em>s<sub>i</sub></em> is its stride (the last part of the equation thus represents the product of the strides for all the previous layers). As an example, we can apply this formula to the minimalist two-layer CNN presented in <em>Figure 3.9</em> to quantitatively evaluate the ERF of the second layer as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5395488d-6ad1-4058-9589-62aac19a931c.png" style="width:21.50em;height:3.92em;"/></p>
<p>This formula confirms that the ERF of a network is directly affected by the number of intermediary layers, their filter sizes, and the strides. Subsampling layers, such as pooling layers or layers with larger strides, greatly increase the ERF at the cost of lower feature resolution.</p>
<p>Because of the local connectivity of CNNs, you should keep in mind how layers and their hyperparameters will affect the flow of visual information across the networks when defining their architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNNs with TensorFlow</h1>
                </header>
            
            <article>
                
<p>Most state-of-the-art computer vision algorithms are based on CNNs built with the three different types of layers we just introduced (that is, convolutional, pooling, and FC), with some tweaks and tricks that we will present in this book. In this section, we will build our first CNN and apply it to our digit recognition task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing our first CNN</h1>
                </header>
            
            <article>
                
<p>For our first convolutional neural network, we will implement <em>LeNet-5</em>. First introduced by Yann Le Cun in 1995 (in <em>Learning algorithms for classification: A comparison on handwritten digit recognition</em>, <em>World Scientific Singapore</em>) and applied to the MNIST dataset, LeNet-5 may not be a recent network, but it is still commonly used to introduce people to CNNs. Indeed, with its seven layers, this network is straightforward to implement, while yielding interesting results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LeNet-5 architecture</h1>
                </header>
            
            <article>
                
<p class="mce-root">As shown in <em>Figure 3.10</em>, LeNet-5 is first composed of two blocks, each containing a convolutional layer (with the kernel size <em>k</em> = 5 and stride <span class="ui_qtext_rendered_qtext"><em>s</em></span> = 1) followed by a max-pooling layer (with <em>k</em> = 2 and <span class="ui_qtext_rendered_qtext"><em>s</em></span> = 2). In the first block, the input images are zero-padded by 2 on each side before convolution (that is, <em>p</em> = 2, hence an actual input size of <em>32 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> 32</em>), and the convolution layer has six different filters (<em>N</em> = 6). There is no padding before the second convolution (<em>p</em> = 0), and its number of filters is set to 16 (<em>N</em> = 16). After the two blocks, three fully connected layers merge the features together and lead to the final class estimation (the 10 digit classes). Before the first dense layer, the <em>5 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> 5 <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> 16</em> feature volume is flattened into a vector of 400 values. The complete architecture is represented in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/92f819f4-53b5-4b32-90e9-512d872806a1.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.10: <span>LeNet-5 architecture (rendered with the </span>NN-SVG tool by Alexander Lenail—http://alexlenail.me/NN-SVG)</div>
<p class="mce-root">In the original implementation, each convolution layer and dense layer except the last one uses <em>tanh</em> as an activation function. However, <em>ReLU</em> is nowadays preferred to <em>tanh</em>, replacing it in most LeNet-5 implementations. For the last layer, the <em>softmax</em> function is applied. This function takes a vector of <em>N</em> values and returns a same-size vector, <em>y,</em> with its values normalized into a probability distribution. In other words, <em>softmax</em> normalizes a vector so that its values are all between 0 and 1, and their sum is exactly equal to 1. Therefore, this function is commonly used at the end of neural networks applied to classification tasks in order to convert the network's predictions into per-class probability, as mentioned in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em> (that is, given an output tensor, <em>y</em> = [<em>y<sub>0</sub>, ..., y<sub>i</sub>, ..., y<sub>N</sub></em>], <em>y<sub>i</sub></em> represents how likely it is that the sample belongs to class <em>i</em> according to the network).</p>
<div class="packt_infobox">The network's raw predictions (that is, before normalization) are commonly named <strong>logits</strong>. These unbounded values are usually converted into probabilities with the <em>softmax</em> function. This normalization process makes the prediction more <em>readable</em> (each value represents the confidence of the network for the corresponding class; refer to the belief scores mentioned in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>) and simplifies the computation of the training loss (that is, the categorical cross-entropy for classification tasks).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow and Keras implementations</h1>
                </header>
            
            <article>
                
<p>We have all the tools in hand to implement this network. We suggest that you try them yourself, before checking the TensorFlow and Keras implementations provided. Reusing the notations and variables from <a href="c7c49010-458f-47ef-a538-96118f9cd892.xhtml">Chapter 2</a>, <em>TensorFlow Basics and Training a Model</em>, a LeNet-5 network using the Keras Sequential API would be as follows:</p>
<pre>from tensorflow.keras.model import Model, Sequential<br/>from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense<br/><br/>model = Sequential() # `Sequential` inherits from tf.keras.Model<br/># 1st block:<br/>model.add(Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', <br/> input_shape=(img_height, img_width, img_channels))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/># 2nd block:<br/>model.add(Conv2D(16, kernel_size=(5, 5), activation='relu')<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/># Dense layers:<br/>model.add(Flatten())<br/>model.add(Dense(120, activation='relu'))<br/>model.add(Dense(84, activation='relu'))<br/>model.add(Dense(num_classes, activation='softmax'))</pre>
<p>The model is created by instantiating and adding the layers one by one, <em>sequentially</em>. As mentioned in <a href="c7c49010-458f-47ef-a538-96118f9cd892.xhtml">Chapter 2</a>, <em>TensorFlow Basics and Training a Model</em>, Keras also provides the <strong>functional API</strong>. This API makes it possible to define models in a more object-oriented approach (as shown in the following code), though it is also possible to directly instantiate <kbd>tf.keras.Model</kbd> with the layer operations (as illustrated in some of our Jupyter notebooks):</p>
<pre>from tensorflow.keras import Model<br/>from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense<br/><br/><span class="k">class</span> <span class="nc">LeNet5</span><span class="p">(</span><span class="n">Model</span><span class="p">): # `Model` has the same API as `Layer` + extends it<br/></span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self, num_classes</span><span class="p">): # Create the model and its layers<br/></span>        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()<br/></span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> Conv2D(6, kernel_size=(5, 5), padding='same', <br/>                            activation='relu')<br/>        self.conv2 = Conv2D(16, kernel_size=(5, 5), activation='relu')<br/>        self.max_pool = MaxPooling2D(pool_size=(2, 2))<br/>        self.flatten = Flatten()<br/>        self.dense1 = Dense(120, activation='relu')<br/>        self.dense2 = Dense(84, activation='relu')<br/>        self.dense3 = Dense(num_classes, activation='softmax')<br/>    def call(self, x): # Apply the layers in order to process the inputs<br/>        x = self.max_pool(self.conv1(x)) # 1st block<br/>        x = self.max_pool(self.conv2(x)) # 2nd block<br/>        x = self.flatten(x)<br/>        x = self.dense3(self.dense2(self.dense1(x))) # dense layers<br/>        return x</pre>
<p>Keras layers can indeed behave like functions that can be applied to input data and chained until the desired output is obtained. The functional API allows you to build more complex neural networks; for example, when one specific layer is reused several times inside the networks, or when layers have multiple inputs or outputs.</p>
<div class="packt_tip">For those who have already experimented with PyTorch (<a href="https://pytorch.org">https://pytorch.org</a>), another machine learning framework, this object-oriented approach to building neural networks may seem familiar, as it is favored there.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application to MNIST</h1>
                </header>
            
            <article>
                
<p>We can now compile and train our model for digit classification. Pursuing this with the Keras API (and reusing the MNIST data variables prepared in the last chapter), we instantiate the optimizer (a simple <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) optimizer) and define the loss (the categorical cross-entropy) before launching the training, as follows:</p>
<pre>model.compile(<span>optimizer</span>=<span>'sgd'</span><span>, </span><span>loss</span>=<span>'sparse_categorical_crossentropy'</span><span>,<br/></span><span>              </span><span>metrics</span>=[<span>'accuracy'</span>])<br/># We also instantiate some Keras callbacks, that is, utility functions automatically called at some points during training to monitor it:<br/>callbacks = [<br/>    # To interrupt the training if `val_loss` stops improving for over 3 epochs:<br/>    tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),<br/>    # To log the graph/metrics into TensorBoard (saving files in `./logs`):<br/>    tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)]<br/># Finally, we launch the training:<br/>model.fit(x_train<span>, </span>y_train<span>, </span><span>batch_size=32, epochs</span>=8<span>0</span><span>, </span><span><br/>          </span><span>validation_data</span>=(x_test<span>, </span>y_test), callbacks=callbacks)</pre>
<div class="packt_tip">Note the use of <kbd>sparse_categorical_crossentropy</kbd>, instead of <kbd>categorical_crossentropy</kbd>, to avoid one-hot encoding the labels. This loss was described in <a href="c7c49010-458f-47ef-a538-96118f9cd892.xhtml">Chapter 2</a>, <em>TensorFlow Basics and Training a Model</em>.</div>
<p>After ~60 epochs, we observe that our network's accuracy on the validation data reaches above ~98.5%! Compared to our previous attempts with non-convolutional networks, the relative error has been divided by <em>2</em> (from a ~3.0% to ~1.5% relative error), which is a significant improvement (given the high accuracy already).</p>
<p>In the following chapters, we will fully appreciate the analytical power of CNNs, applying them to increasingly complex visual tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refining the training process</h1>
                </header>
            
            <article>
                
<p class="mce-root">Network architectures are not the only things to have improved over the years. The way that networks are trained has also evolved, improving how reliably and quickly they can converge. In this section, we will tackle some of the shortcomings of the gradient descent algorithm we covered in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, as well as some ways to avoid overfitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modern network optimizers</h1>
                </header>
            
            <article>
                
<p>Optimizing multidimensional functions, such as neural networks, is a complex task. The gradient descent solution we presented in the first chapter is an elegant solution, though it has some limitations that we will highlight in the following section. Thankfully, researchers have been developing new generations of optimization algorithms, which we will also discuss.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient descent challenges</h1>
                </header>
            
            <article>
                
<p>We previously presented how the parameters, <em>P</em>, of a neural network (that is, all the weight and bias parameters of its layers) can be iteratively updated during training to minimize the loss, <em>L</em>, backpropagating its gradient. If this gradient descent process could be summarized in a single equation, it would be the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d9fc0237-4e1b-4307-ac64-206590a62b1c.png" style="width:20.08em;height:3.25em;"/></p>
<p><img class="fm-editor-equation" src="assets/182e2239-4dff-46d3-a9db-a5e364c47e81.png" style="width:0.58em;height:0.83em;"/> is the learning rate hyperparameter, which accentuates or attenuates how the network's parameters are updated with regard to the gradient of the loss at every training iteration. While we mentioned that the learning rate value should be set with care, we did not explain how and why. The reasons for caution in this setup are threefold.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training velocity and trade-off</h1>
                </header>
            
            <article>
                
<p>We partially covered this point earlier. While setting a high learning rate may allow the trained network to converge faster (that is, in fewer iterations, as the parameters undergo larger updates each iteration), it also may prevent the network from finding a proper loss minimum. <em>Figure 3.11</em> is a famous illustration representing this trade-off between optimization over-cautiousness and haste:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6ddc34df-90e3-460b-a9af-8b410dc910d2.png" style="width:44.50em;height:12.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.11: Illustration of the learning rate trade-off</div>
<p>From <em>Figure 3.11</em>, we can observe that an excessively low learning rate will slow down convergence (diagram A on the left), while an excessively high learning rate may cause it to overshoot the local minima (diagram B on the right).</p>
<p>Intuitively, there should be a better solution than trial and error to find the proper learning rate. For instance, a popular solution is to dynamically adjust the learning rate during training, starting with a larger value (for faster exploration of the loss domain at first) and decreasing it after every epoch (for more careful updating when getting closer to the minimum). This process is named <strong>learning rate decay</strong>. Manual decaying can still be found in many implementations, though, nowadays, TensorFlow offers more advanced learning rate schedulers and optimizers with adaptive learning rates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Suboptimal local minima</h1>
                </header>
            
            <article>
                
<p>A common problem when optimizing complex (that is, <em>non-convex</em>) methods is getting stuck in <strong>suboptimal local minima</strong>. Indeed, gradient descent may lead us to a local minimum it cannot escape, even though a <em>better </em>minimum lies close by, as shown in <em>Figure 3.12</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/919aee27-8fb7-4891-bdf7-7494adf55da1.png" style="width:22.00em;height:13.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.12: Example of gradient descent ending up in a sub-optimal local minimum</div>
<p>Because of the random sampling of training samples (causing the gradients to often differ from one mini-batch to another), the SGD presented in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, is already able to <em>jump out </em>of shallow local minima.</p>
<p>Note that the gradient descent process cannot ensure the convergence to a <strong>global minimum</strong> (that is, the convergence to the best set of parameters among all possible combinations). This <span>woul</span>d imply sca<span>nning</span> <span>the complete loss domain, to make sure that a given minimum is indeed the</span> <em>best</em> (this would mean, for instance, computing the loss for all possible combinations of the parameters)<span>. Given the complexity of visual tasks and the large number of parameters needed to tackle them, data scientists are usually glad to just find a satisfying local minimum.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A single hyperparameter for heterogeneous parameters</h1>
                </header>
            
            <article>
                
<p>Finally, in traditional gradient descent, the same learning rate is used to update all the parameters of the network. However, not all these variables have the same sensitivity to changes, nor do they all impact the loss at every iteration.</p>
<p>It may seem beneficial to have different learning rates (for instance, per subset of parameters) to update crucial parameters more carefully, and to <span>more boldly </span>update parameters that are not <span>contributing </span>often enough to the network's predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced optimizers</h1>
                </header>
            
            <article>
                
<p>Some of the intuitions we presented in the previous paragraphs have been properly studied and formalized by researchers, leading to new optimization algorithms based on SGD. We will now list the most common of these optimizers, detailing their contributions and how to use them with TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Momentum algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">First suggested by Boris <span>Polyak (in</span> <em>Some methods of speeding up the convergence of iteration methods</em><span>,</span> <span>Elsevier, 1964), t</span>he momentum algorithm is based on SGD and inspired by the physics notion of <strong>momentum</strong>—as long as an object is moving downhill, its speed will increase with each step. Applied to gradient descent, the idea is to take previous parameter updates, <em>v<sub>i-1</sub></em>, <span>into account, </span><span>adding them to the new update terms,</span> <em>v<sub>i</sub></em><span>,</span><em><sub> </sub></em><span>as follows</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c055ee55-d604-4526-93ce-5678c5603373.png" style="width:11.25em;height:3.25em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/fceb4a4e-5c66-4e5b-93ec-931110036eb0.png" style="width:0.83em;height:1.08em;"/> (<em>mu</em>) is the momentum weighing (the value between 0 and 1), defining the fraction of the previous updates to apply. If the current and previous steps have the same direction, their magnitudes will add up, accelerating the SGD in this relevant direction. If they have different directions, the momentum will dampen these oscillations.</p>
<p>In <kbd>tf.optimizers</kbd> (also accessible as <kbd>tf.keras.optimizers</kbd>), momentum is defined as an optional parameter of SGD (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD</a>) as follows:</p>
<pre>optimizer = tf.optimizers.SGD(lr=0.01, <strong>momentum</strong>=0.9, # `momentum` = "mu"<br/>                              <strong>decay</strong>=0.0, <strong>nesterov</strong>=False)</pre>
<div class="packt_tip">This optimizer accepts a <kbd>decay</kbd> parameter, fixing the learning rate decay over each update (refer to the previous paragraphs).</div>
<p>This optimizer instance can then be directly passed as a parameter to <kbd>model.fit()</kbd> when launching the training through the Keras API. For more complex training scenarios (for instance, when training interdependent networks), the optimizer can also be called, providing it with the loss gradients and the model's trainable parameters. The following is an example of a simple training step implemented manually:</p>
<pre><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">batch_images, batch_gts</span><span class="p">): # typical training step<br/></span>    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">grad_tape</span><span class="p">: # Tell TF to tape the gradients<br/></span>        <span class="n">batch_preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_images</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">) # forward<br/></span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf.losses.MSE</span><span class="p">(</span><span class="n">batch_gts, batch_preds</span><span class="p">)     # compute loss<br/>    # Get the loss gradients w.r.t trainable parameters and back-propagate:<br/></span>    <span class="n">grads </span><span class="o">=</span> <span class="n">grad_tap</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="n">radient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)<br/></span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span></pre>
<p><kbd>tf.optimizers.SGD</kbd> has one interesting Boolean parameter—to switch from the common momentum method to Nesterov's algorithm. Indeed, a major problem of the former method is that by the time the network gets really close to its loss minimum, the accumulated momentum will usually be quite high, which may cause the method to miss or oscillate around the target minimum.</p>
<p>The <strong>Nesterov accelerated gradient</strong> (<strong>NAG</strong> or <strong>Nesterov momentum</strong>) offers a solution to this problem (a related course is <em>Introductory Lectures on Convex Programming Volume I: Basic course, </em>by Yurii Nesterov, <em>Springer Science and Business Media</em>). Back in the 1980s, Yurii Nesterov's idea was to give the optimizer the possibility to have a look at the slope ahead so that it <em>knows </em>it should slow down if the slope starts going up. More formally, Nesterov suggested directly reusing the past term <em>v<sub>i-1</sub></em> to estimate which values, <em>P<sub>i+1</sub></em>, the parameters would take if we keep following this direction. The gradient is then evaluated with respect to those approximate future parameters, and it is used to finally compute the actual update as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae9b495d-7747-4cdb-bbc6-8fcb195b9509.png" style="width:28.92em;height:3.17em;"/></p>
<p>This version of the momentum optimizer (where the loss is derived <span>w</span><span>ith respect </span>to the parameters' values updated according to the previous steps) is more adaptable to gradient changes, and can significantly speed up the gradient descent process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Ada family</h1>
                </header>
            
            <article>
                
<p><strong>Adagrad</strong>, <strong>Adadelta</strong>, and <strong>Adam</strong> are several iterations and variations around the idea of adapting the learning rate depending on the sensitivity and/or activation frequency of each neuron.</p>
<p>Developed first by John Duchi et al. (in <em>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</em>, Journal of Machine Learning Research, 2011), the <em>Adagrad</em> optimizer (for <em>adaptive gradients</em>) uses a neat formula (which we won't expand on here, though we invite you to search for it) to automatically decrease the learning rate more quickly for parameters linked to commonly found features, and more slowly for infrequent ones. In other words, as presented in the Keras documentation, <em>the more updates a parameter receives, the smaller the updates </em>(refer to the documentation at <a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a>). This optimization algorithm not only removes the need to manually adapt/decay the learning rate, but it also makes the SGD process more stable, especially for datasets with sparse representations.</p>
<p>Introducing <em>Adadelta</em> in 2013, Matthew D. Zeiler et al. (in <em>ADADELTA: An Adaptive Learning Rate Method</em>, <em>arXiv preprint</em>) offered a solution to one problem inherent to <em>Adagrad.</em> As it keeps decaying the learning rate every iteration, at some point, the learning rate becomes too small and the network just cannot learn anymore (except maybe for infrequent parameters). <em>Adadelta</em> avoids this problem by keeping in check the factors used to divide the learning rate for each parameter.</p>
<div class="packt_infobox"><strong>RMSprop</strong> by Geoffrey Hinton is another well-known optimizer (introduced in his Coursera course, <q>Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</q>). Associated with, and quite similar to <em>Adadelta</em>, <em>RMSprop</em> was also developed to correct <em>Adagrad</em>'s flaw.</div>
<p><strong>Adam</strong> (for <strong>adaptive moment estimation</strong>) is another iteration by Diederik P. Kingma et al. (in <em>Adam: A method for stochastic optimization</em>, ICLR, 2015). In addition to storing previous update terms, <em>v<sub>i</sub></em>, to adapt the learning rate for each parameter, <em>Adam</em> also keeps track of the past momentum values. It is, therefore, often identified as a mix between <em>Adadelta</em> and <em>momentum</em>. Similarly, <strong>Nadam</strong> is an optimizer inheriting from <em>Adadelta</em> and <em>NAG</em>.</p>
<p class="mce-root">All these various optimizers are available in the <kbd>tf.optimizers</kbd> package (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/train/">https://www.tensorflow.org/api_docs/python/tf/train/</a>). Note that there is no consensus regarding which of these optimizers may be the best. <em>Adam</em> is, however, preferred by many computer vision professionals for its effectiveness on scarce data. <em>RMSprop</em> is also often considered a good choice for recurrent neural networks (introduced in <a href="97884989-bb57-4611-8c66-ebe8ab387965.xhtml">Chapter 8</a>, <em>Video and Recurrent Neural Networks</em>).</p>
<div class="packt_infobox">A Jupyter notebook demonstrating how to use these various optimizers is provided in the Git repository. Each optimizer is also applied to the training of our <em>LeNet-5</em> for MNIST classification, in order to compare their convergence.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regularization methods</h1>
                </header>
            
            <article>
                
<p>Efficiently teaching neural networks so that they minimize the loss over training data is, however, not enough. We also want these networks to perform well once applied to new images. We do not want them to <em>overfit</em> the training set (as mentioned in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>). For our networks to generalize well, we mentioned that rich training sets (with enough variability to cover possible testing scenarios) and well-defined architectures (neither too shallow to avoid underfitting, nor too complex to prevent overfitting) are key. However, other methods have been developed over the years for <strong>regularization</strong>; for example, the process of refining the optimization phase to avoid overfitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Early stopping</h1>
                </header>
            
            <article>
                
<p>Neural networks start overfitting when they iterate too many times over the same small set of training samples. Therefore, a straightforward solution to prevent this problem is to figure out the number of training epochs a model needs. The number should be low enough to stop before the network starts overfitting, but still high enough for the network to learn all it can from this training set.</p>
<p><strong>Cross-validation</strong> is the key here to evaluate when training should be stopped. Providing a validation dataset to our optimizer, the latter can measure the performance of the model on images the network has not been directly optimized for. By <em>validating</em> the network, for instance, after each epoch, we can measure whether the training should continue (that is, when the validation accuracy appears to be still increasing) or be stopped (that is, when the validation accuracy stagnates or drops). The latter is called <strong>early stopping</strong>.</p>
<p>In practice, we usually monitor and plot the validation loss and metrics as a function of the training iterations, and we restore the saved weights at the optima (hence the importance of regularly saving the network during training). This monitoring, early stopping, and restoration of optimum weights can be automatically covered by one of the optional Keras callbacks (<kbd>tf.keras.callbacks.EarlyStopping</kbd>), as already showcased in our previous training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L1 and L2 regularization</h1>
                </header>
            
            <article>
                
<p>Another way to prevent overfitting is to modify the loss in order to include regularization as one of the training objectives. The L1 and L2 regularizers are prime examples of this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principles</h1>
                </header>
            
            <article>
                
<p>In machine learning, a <strong>regularization term</strong>, <em>R(P)</em>, computed over the parameters, <em>P</em>, of the method, <em>f</em>, to optimize (for instance, a neural network) can be added to the loss function, <em>L</em>, before training, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/70fe393a-72e8-46d1-8fba-1b25c99d17ca.png" style="width:22.17em;height:1.67em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/75a1c0fb-e5e8-42ef-8f01-90efa6fb04ad.png" style="width:0.75em;height:1.17em;"/> is a factor controlling the strength of the regularization (typically, to scale down the amplitude of the regularization term compared to the main loss), and <em>y = f(x, P) </em>is the output of the method, <em>f</em>, parametrized by <em>P</em> for the input data, <em>x</em>. By adding this term, <em>R(P)</em>, to the loss, we force the network not only to optimize its task, but to optimize it while <em>constraining</em> the values its parameters can take.</p>
<p>For L1 and L2 regularization, the respective terms are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d0e89b2e-ff40-46c1-904f-ae6fa01595d6.png" style="width:30.92em;height:2.75em;"/></p>
<p><strong>L2 regularization</strong> (also called <strong>ridge regularization</strong>) thus compels the network to minimize the sum of its squared parameter values. While this regularization leads to the decay of all parameter values over the optimization process, it more strongly punishes large parameters due to the squared term. Therefore, L2 regularization encourages the network <em>to keep its parameter values low and thus more homogeneously distributed</em>. It prevents the network from developing a small set of parameters with large values influencing its predictions (as it may prevent the network from generalizing).</p>
<p>On the other hand, the <strong>L1 regularizer</strong> (also called the <strong>LASSO</strong> (<strong>least absolute shrinkage and selection operator</strong>)<strong><em> </em>regularizer</strong>, first introduced in <em>Linear Inversion of Band-Limited Reflection Seismograms, </em>by <em>Fadil Santosa and William Symes, SIAM, 1986</em>) compels the network to minimize the sum of its absolute parameter values. The difference between this and L2 regularization may seem symbolic at first glance, but their properties are actually quite different. As larger weights are not penalized by squaring, L1 regularization instead makes the network shrink the parameters linked to less important features toward zero. Therefore, it prevents overfitting by forcing the network to ignore less meaningful features (for instance, tied to dataset noise). In other words, L1 regularization forces the network to adopt sparse parameters; that is, to rely on a smaller set of non-null parameters. This can be advantageous if the footprint of the network should be minimized (for mobile applications, for example).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow and Keras implementations</h1>
                </header>
            
            <article>
                
<p>To implement those techniques, we should define the regularization loss and attach this function to every target layer. At each training iteration, these additional losses should be computed over the layers' parameters, and summed with the main task-specific loss (for instance, the cross-entropy over the network's predictions) so that they can all be backpropagated together by the optimizer. Thankfully, TensorFlow 2 provides several tools to simplify this process.</p>
<p>Additional losses can be attached to <kbd>tf.keras.layers.Layer</kbd> and <kbd>tf.keras.Model</kbd> instances through their <kbd>.add_loss(losses, ...)</kbd> method, with the <kbd>losses</kbd> tensors or zero-argument callables returning the loss values. Once properly added to a layer (see the following code), these losses will be computed every time the layer/model is called. All the losses attached to a <kbd>Layer</kbd> or <kbd>Model</kbd> instance, as well as the losses attached to its sublayers, will be computed, and the list of loss values will be returned when calling the <kbd>.losses</kbd> property. To better understand this concept, we'll extend the simple convolution layer implemented <span>previously </span>to add optional regularization to its parameters:</p>
<pre>from functools import partial<br/><br/>def l2_reg(coef=1e-2): # reimplementation of tf.keras.regularizers.l2()<br/>    return lambda x: tf.reduce_sum(x ** 2) * coef<br/><br/>class ConvWithRegularizers(SimpleConvolutionLayer):<br/>    def __init__(self, num_kernels=32, kernel_size=(3, 3), stride=1,<br/>                 kernel_regularizer=l2_reg(), bias_regularizer=None):<br/>        super().__init__(num_kernels, kernel_size, stride)  <br/>        self.kernel_regularizer = kernel_regularizer<br/>        self.bias_regularizer = bias_regularizer<br/><br/>    def build(self, input_shape):<br/>        super().build(input_shape)<br/>        # Attaching the regularization losses to the variables.<br/>        if self.kernel_regularizer is not None:<br/>            # for instance, we tell TF to compute and save<br/>            # `tf.nn.l1_loss(self.kernels)` at each call (that is iteration):<br/>            self.add_loss(partial(self.kernel_regularizer, self.kernels))<br/>        if self.bias_regularizer is not None:<br/>            self.add_loss(partial(self.bias_regularizer, self.bias))</pre>
<p>Regularization losses should guide the models toward learning more robust features. They should not take precedence over the main training loss, which is preparing the model for its task. Therefore, we should be careful not to put too much weight on the regularization losses. Their values are usually dampened by a coefficient between 0 and 1 (refer to <kbd>coef</kbd> in our <kbd>l2_reg()</kbd> loss function). This weighing is especially important, for instance, when the main loss is averaged (for example, MSE and MAE). So that the regularization losses do not outweigh it, we should either make sure that they are also averaged over the parameters' dimensions, or we should decrease their coefficient <span>further</span>.</p>
<p>At each training iteration of a network composed of such layers, the regularization losses can be computed, listed, and added to the main loss as follows:</p>
<pre># We create a NN containing layers with regularization/additional losses:<br/>model = Sequential()<br/>model.add(ConvWithRegularizers(6, (5, 5), kernel_regularizer=l2_reg())<br/>model.add(...) # adding more layers<br/>model.add(Dense(num_classes, activation='softmax'))<br/><br/><span class="p"># We train it (c.f. function `training_step()` defined before):<br/>for epoch in range(epochs):<br/>    for (batch_images, batch_gts) in dataset:<br/>        with tf.GradientTape() as grad_tape:<br/>            loss = tf.losses.sparse_categorical_crossentropy(<br/>                batch_gts, </span><span class="p">model(batch_images)</span><span class="p">) # main loss<br/>            loss += sum(model.losses)           # list of addit. losses<br/>        # Get the gradients of combined losses and back-propagate:<br/>        grads = grad_tape.gradient(loss, model.trainable_variables)<br/>        optimizer.apply_gradients(zip(grads, model.trainable_variables))</span></pre>
<div class="packt_infobox">We introduced <kbd>.add_loss()</kbd>, as this method can greatly simplify the process of adding layer-specific losses to custom networks. However, when it comes to adding regularization losses, TensorFlow provides a more straightforward solution. We can simply pass the regularization loss function as a parameter of the <kbd>.add_weight()</kbd> method (also named <kbd>.add_variable()</kbd>) used to create and attach variables to a <kbd>Layer</kbd> instance. For example, the kernels' variable could be directly created with the regularization loss as follows: <kbd>self.kernels = self.add_weight(..., regularizer=self.kernel_regularizer)</kbd>. At each training iteration, the resulting regularization loss values can still be obtained through the layer or model's <kbd>.losses</kbd> property.</div>
<p>When using predefined Keras layers, we do not need to bother extending the classes to add regularization terms. These layers can receive regularizers for their variables as parameters. Keras even explicitly defines some regularizer callables in its <kbd>tf.keras.regularizers</kbd> module. Finally, when using Keras training operations (such as <kbd>model.fit(...)</kbd>), Keras automatically takes into account additional <kbd>model.losses</kbd> (that is, the regularization terms and other possible layer-specific losses), as follows:</p>
<div>
<pre># We instantiate a regularizer (L1 for example):<br/>l1_reg = <span>tf.keras.reg</span><span>ularizers.l1(0.01)<br/># We can then pass it as a parameter to the target model's layers:<br/>model = Sequential()<br/>model.add(Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', <br/>                 input_shape=input_shape, kernel_regularizer=l1_reg))<br/>model.add(...) # adding more layers<br/>model.fit(...) # training automatically taking into account the reg. terms.<br/></span></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far, the regularization methods we have covered are affecting the way networks are trained. Other solutions are affecting their architecture. <strong>Dropout</strong> is one such method and one of the most popular regularization tricks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Definition</h1>
                </header>
            
            <article>
                
<p>Introduced in <em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting </em>(<em>JMLR, 2014</em>) by Hinton and his team (who made numerous contributions to deep learning), <em>dropout</em> consists of randomly disconnecting (<em>dropping out</em>) some neurons of target layers at every training iteration. This method thus takes a hyperparameter ratio, <img class="fm-editor-equation" src="assets/9860fd1e-4b64-4956-b2f4-e190e386e1d9.png" style="width:0.75em;height:1.08em;"/>, which represents the probability that neurons are being turned off at each training step (usually set between 0.1 and 0.5). The concept is illustrated in <em>Figure 3.13</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3c40f227-304e-4293-a27c-15d56a959f64.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.13: Dropout represented on a simple neural network (note that dropped-out neurons of layers are randomly chosen in each iteration)</div>
<p>By artificially and randomly impairing the network, this method forces the learning of robust and concurrent features. For instance, as dropout may deactivate the neurons responsible for a key feature, the network has to figure out other significant features in order to reach the same prediction. This has the effect of developing redundant representations of data for prediction.</p>
<p>Dropout is also often explained as a cheap solution to simultaneously train a <em>multitude</em> of models (the randomly impaired versions of the original network). During the testing phase, dropout is not applied to the network, so the network's predictions can be seen as the combination of the results that the partial models would have provided. Therefore, this information averaging prevents the network from overfitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow and Keras methods</h1>
                </header>
            
            <article>
                
<p>Dropout can be called as a function through <kbd>tf.nn.dropout(x, rate, ...)</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">https://www.tensorflow.org/api_docs/python/tf/nn/dropout</a>) to directly obtain a tensor with values randomly dropped, or as a layer through <kbd>tf.keras.layers.Dropout()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/layers/dropout">https://www.tensorflow.org/api_docs/python/tf/layers/dropout</a>), which can be added to neural models. By default, <kbd>tf.keras.layers.Dropout()</kbd> is only applied during training (when the layer/model is called with the <kbd>training=True</kbd> parameter) and is deactivated otherwise (forwarding the values without any alteration).</p>
<p>Dropout layers should be added directly after layers we want to prevent from overfitting (as dropout layers will randomly drop values returned by their preceding layers, forcing them to adapt). For instance, you can apply dropout (for example, with a ratio, <img class="fm-editor-equation" src="assets/0d8d3c23-bc8e-4837-b207-6ab366cf4c6d.png" style="width:4.33em;height:1.42em;"/>) to a fully connected layer in Keras, as shown in the following code block:</p>
<pre>model = Sequential([ # ...<br/>    Dense(120, activation='relu'),<br/>    Dropout(0.2),    # ...<br/>])</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch normalization</h1>
                </header>
            
            <article>
                
<p>Though our list is not exhaustive, we will introduce a final common regularization method, which is also directly integrated into the networks' architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Definition</h1>
                </header>
            
            <article>
                
<p>Like dropout, <strong>batch normalization</strong> (proposed by Sergey Ioffe and Christian Szegedy in <em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>, <em>JMLR, 2015</em>) is an operation that can be inserted into neural networks and affects their training. This operation takes the batched results of the preceding layers and <em>normalizes</em> them; that is, it subtracts the batch mean and divides it by the batch standard deviation.</p>
<p>Since batches are randomly sampled in SGD (and thus are rarely the same <span>twice</span>), this means that the data will almost never be normalized the same way. Therefore, the network has to learn how to deal with these data fluctuations, making it more robust and generic. Furthermore, this normalization step concomitantly improves the way the gradients flow through the network, facilitating the SGD process.</p>
<div class="packt_infobox">The behavior of batch normalization layers is actually a bit more complex than what we have succinctly presented. These layers have a couple of trainable parameters that are used in denormalization operations, so that the next layer does not just try to learn how to undo the batch normalization.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow and Keras methods</h1>
                </header>
            
            <article>
                
<p>Similar to dropout, batch normalization is available in TensorFlow both as a function, <kbd>tf.nn.batch_normalization()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization">https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization</a>) and as a layer, <kbd>tf.keras.layers.BatchNormalization()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a>), making it straightforward to include this regularization tool inside networks.</p>
<p>All these various optimization techniques are precious tools for deep learning, especially when training CNNs on imbalanced or scarce datasets, which is often the case for custom applications (as elaborated on in <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets</em>).</p>
<div class="packt_infobox">Similar to the Jupyter notebook for the optimizers study, we provide another notebook demonstrating how these regularization methods can be applied, and how they affect the performance of our simple CNN.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">With the help of TensorFlow and Keras, we caught up with years of research in deep learning. As CNNs have become central to modern computer vision (and machine learning in general), it is essential to understand how they perform, and what kinds of layers they are composed of. As presented in this chapter, TensorFlow and Keras provide clear interfaces to efficiently build such networks. They are also implementing several advanced optimization and regularization techniques (such as various optimizers, L1/L2 regularization, dropout, and batch normalization) to improve the performance and robustness of trained models, which is important to keep in mind for any application.</p>
<p class="mce-root">We now have the tools to finally tackle more challenging computer vision tasks.</p>
<p class="mce-root">In the next chapter, we will therefore present several CNN architectures applied to the task of classifying large picture datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Why does the output of a convolutional layer have a smaller width and height than the input, unless it is padded?</li>
<li>What would be the output of a max-pooling layer with a receptive field of (2, 2) and stride of 2 on the input matrix in <em>Figure 3.6</em>?</li>
<li>How could LeNet-5 be implemented using the Keras functional API in a non-object-oriented manner?</li>
<li>How does L1/L2 regularization affect networks?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>On the importance of initialization and momentum in deep learning</em> (<a href="http://proceedings.mlr.press/v28/sutskever13.pdf">http://proceedings.mlr.press/v28/sutskever13.pdf</a>), by Ilya Sutskever et al. This often-referenced conference paper, published in 2013, presents and compares the momentum and NAG algorithms.</li>
<li><em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</em> (<a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a>), by Nitish Srivastava et al. This other conference paper, published in 2014, introduced dropout. It is a great read for those who want to know more about this method and see it applied to several famous computer vision datasets.</li>
</ul>


            </article>

            
        </section>
    </body></html>