["```\ndata_sets = []\n    for data_file in glob(file_pattern):\n        if label_data:\n            # read in contents as a DataFrame\n            subset_df = pd.read_csv(data_file, header=None)\n            # need to create a unit_id column explicitly\n            unit_id = range(1, subset_df.shape[0] + 1)\n            subset_df.insert(0, 'unit_id', unit_id)\n        else:\n            # read in contents as a DataFrame\n            subset_df = pd.read_csv(data_file, sep=' ', header=None, usecols=range(26))\n        # extract the id of the dataset from the name and add as a column\n        dataset_id = basename(data_file).split(\"_\")[1][:5]\n        subset_df.insert(0, 'dataset_id', dataset_id)\n        # add to list\n        data_sets.append(subset_df)\n    # combine dataframes\n    df = pd.concat(data_sets)\n    df.columns = columns\n    # return the result\n\n    return df\n```", "```\n$python3 make_dataset.py data/raw/ /data/processed/\n```", "```\n# load the processed data in CSV format\ntrain_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\nrul_df = pd.read_csv('RUL.csv')\n\n# for convenience, identify the sensor and operational setting columns\nsensor_columns = [col for col in train_df.columns if col.startswith(\"sensor\")]\nsetting_columns = [col for col in train_df.columns if col.startswith(\"setting\")]\n```", "```\nslice = train_df[(train_df.dataset_id == 'FD001') & (train_df.unit_id == 1)]\n```", "```\nfig, axes = plt.subplots(7, 3, figsize=(15, 10), sharex=True)\n\nfor index, ax in enumerate(axes.ravel()):\n    sensor_col = sensor_columns[index]\n    slice.plot(x='cycle', y=sensor_col, ax=ax, color='blue');\n    # label formatting\n    if index % 3 == 0:\n        ax.set_ylabel(\"Sensor reading\", size=10);\n    else:\n        ax.set_ylabel(\"\");\n    ax.set_xlabel(\"Time (cycle)\");\n    ax.set_title(sensor_col.title(), size=14);\n    ax.legend_.remove();\n\n# plot formatting\nfig.suptitle(\"Sensor reading : unit 1, dataset 1\", size=20, y=1.025)\nfig.tight_layout();\n```", "```\n# randomly select 10 units from dataset 1 to plot\nall_units = train_df[train_df['dataset_id'] == 'FD001']['unit_id'].unique()\nunits_to_plot = np.random.choice(all_units, size=10, replace=False)\n\n# get the data for these units\nplot_data = train_df[(train_df['dataset_id'] == 'FD001') &\n                     (train_df['unit_id'].isin(units_to_plot))].copy()\n\n# plot their sensor traces (overlaid)\nfig, axes = plt.subplots(7, 3, figsize=(15, 10), sharex=True)\n\nfor index, ax in enumerate(axes.ravel()):\n    sensor_col = sensor_columns[index]\n\n    for unit_id, group in plot_data.groupby('unit_id'):\n        # plot the raw sensor trace\n        (group.plot(x='cycle', y=sensor_col, alpha=0.45, ax=ax, color='gray', legend=False));\n        # overlay the 10-cycle rolling mean sensor trace for visual clarity\n        (group.rolling(window=10, on='cycle')\n             .mean()\n             .plot(x='cycle', y=sensor_col, alpha=.75, ax=ax, color='black', legend=False));\n\n    # label formatting\n    if index % 3 == 0:\n        ax.set_ylabel(\"Sensor Value\", size=10);\n    else:\n        ax.set_ylabel(\"\");\n    ax.set_title(sensor_col.title());\n    ax.set_xlabel(\"Time (Cycles)\");\n\n# plot formatting\nfig.suptitle(\"All Sensor Traces: Dataset 1 (Random Sample of 10 Units)\", size=20, y=1.025);\nfig.tight_layout();\n```", "```\n# generate the lifetimes series\nlifetimes = train_df.groupby(['dataset_id', 'unit_id'])['cycle'].max()\n\n# apply the above function to the data we're plotting\nplot_data['ctf'] = plot_data.apply(lambda r: cycles_until_failure(r, lifetimes), axis=1)\n\n# plot the sensor traces (overlaid)\nfig, axes = plt.subplots(7, 3, figsize=(15, 10), sharex=True)\nfor index, ax in enumerate(axes.ravel()):\n    sensor_col = sensor_columns[index]\n    # use the same subset of data as above\n    for unit_id, group in plot_data.groupby('unit_id'):\n        # plot the raw sensor trace, using ctf on the time axis\n        (group.plot(x='ctf', y=sensor_col, alpha=0.45, ax=ax, color='gray', legend=False));\n\n        # overlay the 10-cycle rolling mean sensor trace for visual clarity\n        (group.rolling(window=10, on='ctf')\n             .mean()\n             .plot(x='ctf', y=sensor_col, alpha=.75, ax=ax, color='black', legend=False));\n\n    # label formatting\n    if index % 3 == 0:\n        ax.set_ylabel(\"Sensor Value\", size=10);\n    else:\n        ax.set_ylabel(\"\");\n    ax.set_title(sensor_col.title());\n    ax.set_xlabel(\"Time Before Failure (Cycles)\");\n\n    # add a vertical red line to signal common time of failure\n    ax.axvline(x=0, color='r', linewidth=3);\n\n    # extend the x-axis to compensate\n    ax.set_xlim([None, 10]);\nfig.suptitle(\"All Sensor Traces: Dataset 1 (Random Sample of 10 Units)\", size=20, y=1.025);\nfig.tight_layout();\n```", "```\ndata_path = 'train_FD004.txt'\ndata = utils.load_data(data_path)\n```", "```\ndef make_cutoff_times(data):\n    gb = data.groupby(['unit_id'])\n    labels = []\n    for engine_no_df in gb:\n        instances = engine_no_df[1].shape[0]\n        label = [instances - i - 1 for i in range(instances)]\n        labels += label\n    return new_labels(data, labels)\n```", "```\ncutoff_times = utils.make_cutoff_times(data)\ncutoff_times.head()\n```", "```\ndef make_entityset(data):\n    es = ft.EntitySet('Dataset')\n    es.entity_from_dataframe(dataframe=data,\n                             entity_id='recordings',\n                             index='index',\n                             time_index='time')\n    es.normalize_entity(base_entity_id='recordings',\n                        new_entity_id='engines',\n                        index='engine_no')\n    es.normalize_entity(base_entity_id='recordings',\n                        new_entity_id='cycles',\n                        index='time_in_cycles')\n    return es\nes = make_entityset(data)\n```", "```\nEntityset: Dataset\n Entities:\n recordings [Rows: 20631, Columns: 28]\n engines [Rows: 100, Columns: 2]\n cycles [Rows: 362, Columns: 2]\n Relationships:\n recordings.engine_no -> engines.engine_no\n recordings.time_in_cycles -> cycles.time_in_cycles\n```", "```\nfm, features = ft.dfs(entityset=es,\n                      target_entity='engines',\n                      agg_primitives=['last', 'max', 'min'],\n                      trans_primitives=[],\n                      cutoff_time=cutoff_times,\n                      max_depth=3,\n                      verbose=True)\nfm.to_csv('FM.csv')\n```", "```\nfm = pd.read_csv('FM.csv', index_col='engine_no')\nX = fm.copy().fillna(0)\ny = X.pop('RUL')\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n```", "```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nyhat_median_predict = [np.median(y_train) for _ in y_test]\nprint('Baseline by median label: MAE = {:.2f}'.format(\n    mean_absolute_error(yhat_median_predict, y_test)))\n\n# Collect sensor readings from the sensor in training set\nrecordings_from_train = es['recordings'].df[es['recordings'].df['engine_no'].isin(y_train.index)]\nmedian_life = np.median(recordings_from_train.groupby(['engine_no']).apply(lambda df: df.shape[0]))\n\n# Collect sensor readings from the sensor in training set\nrecordings_from_test = es['recordings'].df[es['recordings'].df['engine_no'].isin(y_test.index)]\nlife_in_test = recordings_from_test.groupby(['engine_no']).apply(lambda df: df.shape[0])-y_test\n\n# Compute mean absolute error as the baseline by meadian of the RUL\nyhat_median_predict2 = (median_life - life_in_test).apply(lambda row: max(row, 0))\nprint('Baseline by median life: MAE = {:.2f}'.format(\n    mean_absolute_error(yhat_median_predict2, y_test)))\n```", "```\nBaseline by median label: MAE = 66.72\nBaseline by median life: MAE = 59.96\n```", "```\nrf = RandomForestRegressor() # first we instantiate RandomForestRegressor from scikit-learn\nrf.fit(X_train, y_train) # train the regressor model with traing set\n\npreds = rf.predict(X_test) # making predictin on unseen observation \nscores = mean_absolute_error(preds, y_test) # Computing MAE\n\nprint('Mean Abs Error: {:.2f}'.format(scores))\nhigh_imp_feats = utils.feature_importances(X, reg, feats=10) # Printing feature importance\n```", "```\nMean Abs Error: 31.04\n 1: LAST(recordings.cycles.LAST(recordings.sensor_measurement_4)) [0.395]\n 2: LAST(recordings.sensor_measurement_4) [0.192]\n 3: MAX(recordings.sensor_measurement_4) [0.064]\n 4: LAST(recordings.cycles.MIN(recordings.sensor_measurement_11)) [0.037]\n 5: LAST(recordings.cycles.MAX(recordings.sensor_measurement_12)) [0.029]\n 6: LAST(recordings.sensor_measurement_15) [0.020]\n 7: LAST(recordings.cycles.MAX(recordings.sensor_measurement_11)) [0.020]\n 8: LAST(recordings.cycles.LAST(recordings.sensor_measurement_15)) [0.018]\n 9: MAX(recordings.cycles.MAX(recordings.sensor_measurement_20)) [0.016]\n 10: LAST(recordings.time_in_cycles) [0.014]\n```", "```\ndata2 = utils.load_data('test_FD001.txt')\nes2 = make_entityset(data2)\nfm2 = ft.calculate_feature_matrix(entityset=es2, features=features, verbose=True)\nfm2.head()\n```", "```\nX = fm2.copy().fillna(0)\ny = pd.read_csv('RUL_FD004.txt', sep=' ', header=-1, names=['RUL'], index_col=False)\n\npreds2 = rf.predict(X)\nprint('Mean Abs Error: {:.2f}'.format(mean_absolute_error(preds2, y)))\n\nyhat_median_predict = [np.median(y_train) for _ in preds2]\nprint('Baseline by median label: MAE = {:.2f}'.format(\n    mean_absolute_error(yhat_median_predict, y)))\n\nyhat_median_predict2 = (median_life - es2['recordings'].df.groupby(['engine_no']).apply(lambda df: df.shape[0])).apply(lambda row: max(row, 0))\n\nprint('Baseline by median life: MAE = {:.2f}'.format(\n    mean_absolute_error(yhat_median_predict2 y)))\n```", "```\nMean Abs Error: 40.33\nBaseline by median label: Mean Abs Error = 52.08\nBaseline by median life: Mean Abs Error = 49.55\n```", "```\n#Prepare data for Keras based LSTM model\ndef prepareData(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    X_train = X_train.as_matrix(columns=None)\n    X_test = X_test.as_matrix(columns=None)\n    y_train = y_train.as_matrix(columns=None)\n    y_test = y_test.as_matrix(columns=None)\n    y_train = y_train.reshape((y_train.shape[0], 1))\n    y_test = y_test.reshape((y_test.shape[0], 1))\n    X_train = np.reshape(X_train,(X_train.shape[0], 1, X_train.shape[1]))\n    X_test = np.reshape(X_test,(X_test.shape[0], 1, X_test.shape[1]))    \n    return X_train, X_test, y_train, y_test\n```", "```\n#Create LSTM model\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import GaussianNoise\n\ndef createLSTMModel(X_train, hidden_neurons):\n    model = Sequential()\n    model.add(LSTM(hidden_neurons, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(Dense(hidden_neurons))\n    model.add(Dropout(0.7))\n    model.add(Dense(1))\n    model.add(Activation(\"linear\"))\n    model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n    return model\n```", "```\nX_train, X_test, y_train, y_test = prepareData(X, y)\nhidden_neurons = 128\nmodel = createLSTMModel(X_train, hidden_neurons)\nhistory = model.fit(X_train, y_train, batch_size=32, nb_epoch=5000, validation_split=0.20)\n```", "```\nTrain on 60 samples, validate on 15 samples\n Epoch 1/5000\n 60/60 [==============================] - ETA: 0s - loss: 7996.37 - 1s 11ms/step - loss: 7795.0232 - val_loss: 8052.6118\n Epoch 2/5000\n 60/60 [==============================] - ETA: 0s - loss: 6937.66 - 0s 301us/step - loss: 7466.3648 - val_loss: 7833.4321\n â€¦\n 60/60 [==============================] - ETA: 0s - loss: 1754.92 - 0s 259us/step - loss: 1822.5668 - val_loss: 1420.7977\n Epoch 4976/5000\n 60/60 [==============================] - ETA: 0s - loss: 1862.04\n```", "```\n# plot history\nplt.plot(history.history['loss'], label='Training')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.legend()\nplt.show()\n```", "```\npredicted = model.predict(X_test)\nrmse = np.sqrt(((predicted - y_test) ** 2).mean(axis=0))\nprint('Mean Abs Error: {:.2f}'.format(mean_absolute_error(predicted, y_test)))\n```", "```\ndef createLSTMModel(X_train, hidden_neurons):\n    model = Sequential()\n    model.add(LSTM(hidden_neurons, input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(GaussianNoise(0.2))\n    model.add(Dense(hidden_neurons))\n    model.add(Dropout(0.7))\n    model.add(Dense(1))\n    model.add(GaussianNoise(0.5))\n    model.add(Activation(\"linear\"))\n    model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n    return model    \n```", "```\nfrom tqdm import tqdm\nsplits = 10\ncutoff_time_list = []\nfor i in tqdm(range(splits)):\n    cutoff_time_list.append(utils.make_cutoff_times(data))\ncutoff_time_list[0].head()\n```", "```\nfrom sklearn.cluster import KMeans\nnclusters = 50\ndef make_entityset(data, nclusters, kmeans=None):\n    X = data[['operational_setting_1', 'operational_setting_2', 'operational_setting_3']]\n    if kmeans:\n        kmeans=kmeans\n    else:\n        kmeans = KMeans(n_clusters=nclusters).fit(X)\n    data['settings_clusters'] = kmeans.predict(X)\n        es = ft.EntitySet('Dataset')\n    es.entity_from_dataframe(dataframe=data,\n                             entity_id='recordings',\n                             index='index',\n                             time_index='time')\n    es.normalize_entity(base_entity_id='recordings', \n                        new_entity_id='engines',\n                        index='engine_no')\n    es.normalize_entity(base_entity_id='recordings', \n                        new_entity_id='settings_clusters',\n                        index='settings_clusters')\n    return es, kmeans\nes, kmeans = make_entityset(data, nclusters)\n```", "```\nEntityset: Dataset\n Entities:\n settings_clusters [Rows: 50, Columns: 2]\n recordings [Rows: 61249, Columns: 29]\n engines [Rows: 249, Columns: 2]\n Relationships:\n recordings.engine_no -> engines.engine_no\n recordings.settings_clusters -> settings_clusters.settings_clusters\n```", "```\nfrom featuretools.primitives import make_agg_primitive\nimport featuretools.variable_types as vtypes\nfrom tsfresh.feature_extraction.feature_calculators import (number_peaks, mean_abs_change, \n                                                            cid_ce, last_location_of_maximum, length)\nComplexity = make_agg_primitive(lambda x: cid_ce(x, False),\n                              input_types=[vtypes.Numeric],\n                              return_type=vtypes.Numeric,\n                              name=\"complexity\")\nfm, features = ft.dfs(entityset=es, \n                      target_entity='engines',\n                      agg_primitives=['last', 'max', Complexity],\n                      trans_primitives=[],\n                      chunk_size=.26,\n                      cutoff_time=cutoff_time_list[0],\n                      max_depth=3,\n                      verbose=True)\nfm.to_csv('Advanced_FM.csv')\nfm.head()\n```", "```\nfm_list = [fm]\nfor i in tqdm(range(1, splits)):\n    fm = ft.calculate_feature_matrix(entityset=make_entityset(data, nclusters, kmeans=kmeans)[0], \n         features=features, chunk_size=.26, cutoff_time=cutoff_time_list[i])\n    fm_list.append(fm)\n```", "```\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.feature_selection import RFE\n\ndef pipeline_for_test(fm_list, hyperparams={'n_estimators':100, 'max_feats':50, 'nfeats':50}, do_selection=False):\n    scores = []\n    regs = []\n    selectors = []\n    for fm in fm_list:\n        X = fm.copy().fillna(0)\n        y = X.pop('RUL')\n        reg = RandomForestRegressor(n_estimators=int(hyperparams['n_estimators']), \n              max_features=min(int(hyperparams['max_feats']), int(hyperparams['nfeats'])))\n        X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n        if do_selection:\n            reg2 = RandomForestRegressor(n_jobs=3)\n            selector=RFE(reg2,int(hyperparams['nfeats']),step=25)\n            selector.fit(X_train, y_train)\n            X_train = selector.transform(X_train)\n            X_test = selector.transform(X_test)\n            selectors.append(selector)\n        reg.fit(X_train, y_train)\n        regs.append(reg)\n        preds = reg.predict(X_test)\n        scores.append(mean_absolute_error(preds, y_test))\n    return scores, regs, selectors  \n\nscores, regs, selectors = pipeline_for_test(fm_list)\nprint([float('{:.1f}'.format(score)) for score in scores])\nprint('Average MAE: {:.1f}, Std: {:.2f}\\n'.format(np.mean(scores), np.std(scores)))\nmost_imp_feats = utils.feature_importances(fm_list[0], regs[0])\n```", "```\n[33.9, 34.5, 36.0, 32.1, 36.4, 30.1, 37.2, 34.7,38.6, 34.4]\n Average MAE: 33.1, Std: 4.63\n 1: MAX(recordings.settings_clusters.LAST(recordings.sensor_measurement_13)) [0.055]\n 2: MAX(recordings.sensor_measurement_13) [0.044]\n 3: MAX(recordings.sensor_measurement_4) [0.035]\n 4: MAX(recordings.settings_clusters.LAST(recordings.sensor_measurement_4)) [0.029]\n 5: MAX(recordings.sensor_measurement_11) [0.028]\n```", "```\nX = fm.copy().fillna(0)\ny = X.pop('RUL')\nX_train, X_test, y_train, y_test = prepareData(X, y)\n\nhidden_neurons = 128\nmodel = createLSTMModel(X_train, hidden_neurons)\n\nhistory = model.fit(X_train, y_train, batch_size=32, nb_epoch=5000, validation_split=0.20)\n# plot history\nplt.plot(history.history['loss'], label='Training')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.legend()\nplt.show()\n```", "```\npredicted = model.predict(X_test)\nprint('Mean Abs Error: {:.2f}'.format(mean_absolute_error(predicted, y_test)))\n```"]