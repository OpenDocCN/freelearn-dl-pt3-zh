<html><head></head><body>
  <div id="_idContainer2791">
    <h1 class="chapterNumber">15</h1>
    <h1 id="_idParaDest-403" class="chapterTitle">Imitation Learning and Inverse RL</h1>
    <p class="normal">Learning from demonstration is often called imitation learning. In the imitation learning setting, we have expert demonstrations and train our agent to mimic those expert demonstrations. Learning from demonstrations has many benefits, including helping an agent to learn more quickly. There are several approaches to perform imitation learning, and two of them are <strong class="keyword">supervised imitation learning</strong> and <strong class="keyword">Inverse Reinforcement Learning</strong> (<strong class="keyword">IRL</strong>). </p>
    <p class="normal">First, we will understand how we can perform imitation learning using supervised learning, and then we will learn about an algorithm called <strong class="keyword">Dataset Aggregation </strong>(<strong class="keyword">DAgger</strong>). Next, we will learn how to use demonstration data in a DQN using an algorithm called <strong class="keyword">Deep Q Learning from Demonstrations</strong> (<strong class="keyword">DQfD</strong>).</p>
    <p class="normal">Moving on, we will learn about IRL and how it differs from reinforcement learning. We will learn about one of the most popular IRL algorithms called <strong class="keyword">maximum entropy IRL</strong>. Toward the end of the chapter, we will understand how <strong class="keyword">Generative Adversarial Imitation Learning </strong>(<strong class="keyword">GAIL</strong>) works. </p>
    <p class="normal">In this chapter, we will learn about the following topics:</p>
    <ul>
      <li class="bullet">Supervised imitation learning</li>
      <li class="bullet">DAgger</li>
      <li class="bullet">Deep Q learning from demonstrations</li>
      <li class="bullet">Inverse reinforcement learning</li>
      <li class="bullet">Maximum entropy inverse reinforcement learning</li>
      <li class="bullet">Generative adversarial imitation learning</li>
    </ul>
    <p class="normal">Let's begin our chapter by understanding how supervised imitation learning works.</p>
    <h1 id="_idParaDest-404" class="title">Supervised imitation learning </h1>
    <p class="normal">In the imitation learning setting, our goal is to mimic the expert. Say, we want to train our agent <a id="_idIndexMarker1396"/>to drive a car. Instead of training the agent from scratch by having them interact with the environment, we can train them with expert demonstrations. Okay, what are expert demonstrations? An expert demonstrations are a set of trajectories consisting of state-action pairs where each action is performed by the expert.</p>
    <p class="normal">We can train an agent to mimic the actions performed by the expert in various respective states. Thus, we can view expert demonstrations as training data used to train our agent. The fundamental idea of imitation learning is to imitate (learn) the behavior of an expert.</p>
    <p class="normal">One of the simplest and most naive ways to perform imitation learning is to treat the imitation learning task as a supervised learning task. First, we collect a set of expert demonstrations, and then we train a classifier to perform the same action performed by the expert in the respective states. We can view this as a big multiclass classification problem and train our agent to perform the action performed by the expert in the respective states.</p>
    <p class="normal">Our goal is to minimize the loss <img src="../Images/B15558_15_001.png" alt="" style="height: 1.11em;"/> where <img src="../Images/B15558_04_122.png" alt="" style="height: 1.11em;"/> is the expert action and <img src="../Images/B15558_15_003.png" alt="" style="height: 1.11em;"/> denotes the action performed by our agent.</p>
    <p class="normal">Thus, in supervised imitation learning, we perform the following steps: </p>
    <ol>
      <li class="numbered">Collect the set of expert demonstrations</li>
      <li class="numbered">Initialize a policy <img src="../Images/B15558_10_044.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Learn the policy by minimizing the loss function <img src="../Images/B15558_15_001.png" alt="" style="height: 1.11em;"/></li>
    </ol>
    <p class="normal">However, there exist several challenges and drawbacks with this method. The knowledge of the agent is limited only to the expert demonstrations (training data), so if the agent comes across a new state that is not present in the expert demonstrations, then the agent will not know what action to perform in that state.</p>
    <p class="normal">Say, we train an agent to drive a car using supervised imitation learning and let the agent perform in the real world. If the training data has no state where the agent encounters a traffic signal, then our agent will have no clue about the traffic signal. </p>
    <p class="normal">Also, the accuracy of the agent is highly dependent on the knowledge of the expert. If the expert <a id="_idIndexMarker1397"/>demonstrations are poor or not optimal, then the agent cannot learn correct actions or the optimal policy.</p>
    <p class="normal">To overcome the challenges in supervised imitation learning, we introduce a new algorithm called DAgger. In the next section, we will learn how DAgger works and how it overcomes the limitations of supervised imitation learning.</p>
    <h1 id="_idParaDest-405" class="title">DAgger</h1>
    <p class="normal">DAgger<strong class="keyword"> </strong>is one <a id="_idIndexMarker1398"/>of the most-used imitation learning algorithms. Let's understand how DAgger works with an example. Let's revisit our example of training an agent to drive a car. First, we initialize an empty dataset <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal"><strong class="keyword">In the first iteration</strong>, we start off with some policy <img src="../Images/B15558_03_153.png" alt="" style="height: 0.84em;"/> to drive the car. Thus, we generate a trajectory <img src="../Images/B15558_14_143.png" alt="" style="height: 0.84em;"/> using the policy <img src="../Images/B15558_03_153.png" alt="" style="height: 0.84em;"/>. We know that the trajectory consists of a sequence of states and actions—that is, states visited by our policy <img src="../Images/B15558_15_010.png" alt="" style="height: 0.84em;"/> and actions made in those states using our policy <img src="../Images/B15558_15_010.png" alt="" style="height: 0.84em;"/>. Now, we create a new dataset <img src="../Images/B15558_15_012.png" alt="" style="height: 1.11em;"/> by taking only the states visited by our policy <img src="../Images/B15558_03_153.png" alt="" style="height: 0.84em;"/> and we use an expert to provide the actions for those states. That is, we take all the states from the trajectory and ask the expert to provide actions for those states.</p>
    <p class="normal">Now, we combine the new dataset <img src="../Images/B15558_15_014.png" alt="" style="height: 1.11em;"/> with our initialized empty dataset <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> and update <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_017.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Next, we train a classifier on this updated dataset <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> and learn a new policy <img src="../Images/B15558_03_159.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">In the second iteration</strong>, we use the new policy <img src="../Images/B15558_03_158.png" alt="" style="height: 0.84em;"/> to generate trajectories, create a new dataset <img src="../Images/B15558_15_021.png" alt="" style="height: 1.11em;"/> by taking <a id="_idIndexMarker1399"/>only the states visited by the new policy <img src="../Images/B15558_03_159.png" alt="" style="height: 0.84em;"/>, and ask the expert to provide the actions for those states.</p>
    <p class="normal">Now, we combine the dataset <img src="../Images/B15558_15_023.png" alt="" style="height: 1.11em;"/> with <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/> and update <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_026.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Next, we train a classifier on this updated dataset <img src="../Images/B15558_15_027.png" alt="" style="height: 1.11em;"/> and learn a new policy <img src="../Images/B15558_15_028.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">In the third iteration</strong>, we use the new policy <img src="../Images/B15558_04_094.png" alt="" style="height: 0.84em;"/> to generate trajectories and create a new dataset <img src="../Images/B15558_15_030.png" alt="" style="height: 1.11em;"/> by taking only the states visited by the new policy <img src="../Images/B15558_15_031.png" alt="" style="height: 0.84em;"/>, and then we ask the expert to provide the actions for those states.</p>
    <p class="normal">Now, we combine the dataset <img src="../Images/B15558_15_030.png" alt="" style="height: 1.11em;"/> with <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/> and update <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_035.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Next, we train a <a id="_idIndexMarker1400"/>classifier on this updated dataset <img src="../Images/B15558_15_036.png" alt="" style="height: 1.11em;"/> and learn a new policy <img src="../Images/B15558_15_037.png" alt="" style="height: 0.84em;"/>. In this way, DAgger works in a series of iterations until it finds the optimal policy.</p>
    <p class="normal">Now that we have a basic understanding of Dagger; let's go into more detail and learn how DAgger finds the optimal policy.</p>
    <h2 id="_idParaDest-406" class="title">Understanding DAgger</h2>
    <p class="normal">Let's suppose <a id="_idIndexMarker1401"/>we have a human expert, and let's denote the expert policy with <img src="../Images/B15558_15_038.png" alt="" style="height: 0.84em;"/>. We initialize an empty dataset <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/> and also a novice policy <img src="../Images/B15558_15_040.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal"><strong class="keyword">Iteration 1</strong>:</p>
    <p class="normal">In the first iteration, we create a new policy <img src="../Images/B15558_15_041.png" alt="" style="height: 0.84em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_042.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The preceding equation implies that we create a new policy <img src="../Images/B15558_04_086.png" alt="" style="height: 0.84em;"/> by taking some amount of expert policy <img src="../Images/B15558_15_044.png" alt="" style="height: 0.84em;"/> and some amount of novice policy <img src="../Images/B15558_15_045.png" alt="" style="height: 1.11em;"/>. How much of the expert policy and novice policy we take is decided by the parameter <img src="../Images/B15558_06_030.png" alt="" style="height: 1.11em;"/>. The value of <img src="../Images/B15558_15_047.png" alt="" style="height: 1.11em;"/> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_048.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">The value <a id="_idIndexMarker1402"/>of <em class="italic">p</em> is chosen between 0.1 and 0.9. Since we are in iteration 1, substituting <em class="italic">i</em> = 1, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_049.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Thus, substituting <img src="../Images/B15558_15_050.png" alt="" style="height: 1.11em;"/> in equation (1), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_051.png" alt="" style="height: 0.84em;"/></figure>
    <p class="normal">As we can observe, in the first iteration, the policy <img src="../Images/B15558_15_052.png" alt="" style="height: 0.84em;"/> is just an expert policy <img src="../Images/B15558_15_053.png" alt="" style="height: 0.84em;"/>. Now, we use this policy <img src="../Images/B15558_15_041.png" alt="" style="height: 0.84em;"/> and generate trajectories. Next, we create a new dataset <img src="../Images/B15558_15_055.png" alt="" style="height: 1.11em;"/> by collecting all the states visited by our policy <img src="../Images/B15558_03_153.png" alt="" style="height: 0.84em;"/> and ask the expert to provide actions of those states. So, our dataset will consist of <img src="../Images/B15558_15_057.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, we combine the dataset <img src="../Images/B15558_15_058.png" alt="" style="height: 1.11em;"/> with our initialized empty dataset <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/> and update <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_017.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Now that <a id="_idIndexMarker1403"/>we have an updated dataset <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/>, we train a classifier on this new dataset and extract a new policy. Let the new policy be <img src="../Images/B15558_15_063.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal"><strong class="keyword">Iteration 2</strong>:</p>
    <p class="normal">In the second iteration, we create a new policy <img src="../Images/B15558_15_064.png" alt="" style="height: 0.84em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_065.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The preceding equation implies that we create a new policy <img src="../Images/B15558_03_159.png" alt="" style="height: 0.84em;"/> by taking some amount of expert policy <img src="../Images/B15558_15_053.png" alt="" style="height: 0.84em;"/> and some amount of policy <img src="../Images/B15558_15_068.png" alt="" style="height: 1.11em;"/> that we obtained in the previous iteration. We know that the value of beta is chosen as: <img src="../Images/B15558_15_069.png" alt="" style="height: 1.29em;"/>. Thus, we have <img src="../Images/B15558_15_070.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Now, we use this policy <img src="../Images/B15558_15_071.png" alt="" style="height: 0.84em;"/> and generate trajectories. Next, we create a new dataset <img src="../Images/B15558_15_072.png" alt="" style="height: 1.11em;"/> by collecting all the states visited by our policy <img src="../Images/B15558_15_073.png" alt="" style="height: 0.84em;"/> and ask the expert to provide actions of those states. So, our dataset will consist of <img src="../Images/B15558_15_074.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, we <a id="_idIndexMarker1404"/>combine the dataset <img src="../Images/B15558_15_075.png" alt="" style="height: 1.11em;"/> with <img src="../Images/B15558_15_036.png" alt="" style="height: 1.11em;"/> and update <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_026.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Now that we have an updated dataset <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/>, we train a classifier on this new dataset and extract a new policy. Let that new policy be <img src="../Images/B15558_15_080.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">We repeat these steps for several iterations to obtain the optimal policy. As we can observe in each iteration, we aggregate our dataset <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/> and train a classifier to obtain the new policy. Notice that the value of <img src="../Images/B15558_09_151.png" alt="" style="height: 1.11em;"/> is decaying exponentially. This makes sense as over a series of iterations, our policy will become better and so we can reduce the importance of the expert policy.</p>
    <p class="normal">Now that we have understood how DAgger works, in the next section, we will look into the algorithm of DAgger for a better understanding.</p>
    <h2 id="_idParaDest-407" class="title">Algorithm – DAgger</h2>
    <p class="normal">The algorithm <a id="_idIndexMarker1405"/>of DAgger is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize an empty dataset <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize a policy <img src="../Images/B15558_15_045.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For iterations <em class="italic">i</em> = 1 to <em class="italic">N</em>:<ol>
          <li class="numbered-l2">Create a policy <img src="../Images/B15558_15_085.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Generate a trajectory using the policy <img src="../Images/B15558_15_086.png" alt="" style="height: 0.84em;"/>.</li>
          <li class="numbered-l2">Create a dataset <img src="../Images/B15558_15_087.png" alt="" style="height: 1.11em;"/> by collecting states visited by the policy <img src="../Images/B15558_15_086.png" alt="" style="height: 0.84em;"/> and actions of those states provided by the expert <img src="../Images/B15558_15_089.png" alt="" style="height: 0.84em;"/>. Thus, <img src="../Images/B15558_15_090.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Aggregate the dataset as <img src="../Images/B15558_15_091.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Train a classifier on the updated dataset <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> and extract a new policy <img src="../Images/B15558_15_093.png" alt="" style="height: 1.11em;"/>.</li>
        </ol>
      </li>
    </ol>
    <p class="normal">Now that we have learned the DAgger algorithm, in the next section, we will learn about DQfD.</p>
    <h1 id="_idParaDest-408" class="title">Deep Q learning from demonstrations</h1>
    <p class="normal">We learned <a id="_idIndexMarker1406"/>that in imitation learning, we try to learn from expert demonstrations. Can we make use of expert demonstrations in DQN and perform better? Yes! In this section, we will learn how to make use of expert demonstrations in DQN using an algorithm called<strong class="keyword"> </strong>DQfD.</p>
    <p class="normal">In the previous chapters, we have learned about several types of DQN. We started off with vanilla DQN, and then we explored various improvements to the DQN, such as double DQN, dueling DQN, prioritized experience replay, and more. In all these methods, the agent tries to learn from scratch by interacting with the environment. The agent interacts with the environment and stores their interaction experience in a buffer called a replay buffer and learns based on their experience.</p>
    <p class="normal">In order for the agent to perform better, it has to gather a lot of experience from the environment, add it to the replay buffer, and train itself. However, this method costs us a lot of training time. In all the previous methods we have learned so far, we have trained our agent in a simulator, so the agent gathers experience in the simulator environment to perform better. To learn the optimal policy, the agent has to perform a lot of interactions with the environment, and some of these interactions give the agent a very bad reward. This is tolerable in a simulator environment. But how can we train the agent in a real-world environment? We can't train the agent by directly interacting with the real-world environment and by making a lot of bad actions in the real-world environment. </p>
    <p class="normal">So, in those cases, we can train the agent in a simulator that corresponds to the particular real-world environment. But the problem is that it is hard to find an accurate simulator corresponding to the real-world environment for most use cases. However, we can easily obtain expert demonstrations.</p>
    <p class="normal">For instance, let's suppose we want to train an agent to play chess. Let's assume we don't find an accurate simulator to train the agent to play chess. But we can easily obtain good expert demonstrations of an expert playing chess. </p>
    <p class="normal">Now, can we make use of these expert demonstrations and train our agent? Yes! Instead of learning from scratch by interacting with the environment, if we add the expert demonstrations directly to the replay buffer and pre-train our agent based on these expert demonstrations, then the agent can learn better and faster.</p>
    <p class="normal">This is the fundamental idea behind DQfD. We fill the replay buffer with expert demonstrations and pre-train the agent. Note that these expert demonstrations are used only for pre-training the agent. Once the agent is pre-trained, the agent will interact with the environment and gather more experience and make use of it for learning. Thus DQfD consists of two phases, which are pre-training and training.</p>
    <p class="normal">First, we pre-train the agent based on the expert demonstrations, and then we train the agent by interacting with the environment. When the agent interacts with the environment, it collects some experience, and the agent's experience (self-generated data) also gets added to the replay buffer. The agent makes use of both the expert demonstrations and also the self-generated data for learning. We use a prioritized experience <a id="_idIndexMarker1407"/>replay buffer and give more priority to the expert demonstrations than the self-generated data. Now that we have a basic understanding of DQfD, let's go into detail and learn how exactly it works.</p>
    <h2 id="_idParaDest-409" class="title">Phases of DQfD</h2>
    <p class="normal">DQfD <a id="_idIndexMarker1408"/>consists of two phases:</p>
    <ul>
      <li class="bullet">Pre-training phase</li>
      <li class="bullet">Training phase</li>
    </ul>
    <h3 id="_idParaDest-410" class="title">Pre-training phase</h3>
    <p class="normal">In the <a id="_idIndexMarker1409"/>pre-training phase, the agent does not interact with the environment. We directly add the expert demonstrations to the replay buffer and the agent learns by sampling the expert demonstrations from the replay buffer. </p>
    <p class="normal">The agent learns from expert demonstrations by minimizing the loss <em class="italic">J</em>(<em class="italic">Q</em>) using gradient descent. However, pre-training with expert demonstrations alone is not sufficient for the agent to perform better because the expert demonstrations will not contain all possible transitions. But the pretraining with expert demonstrations acts as a good starting point to train our agent. Once the agent is pre-trained with demonstrations, then during the training phase, the agent will perform better actions in the environment from the initial iteration itself instead of performing random actions, and so the agent can learn quickly.</p>
    <h3 id="_idParaDest-411" class="title">Training phase</h3>
    <p class="normal">Once the <a id="_idIndexMarker1410"/>agent is pre-trained, we start the training phase, where the agent interacts with the environment and learns based on its experience. Since the agent has already learned some useful information from the expert demonstrations in the pre-training phase, it will not perform random actions in the environment.</p>
    <p class="normal">During the training phase, the agent interacts with the environment and stores its transition information (experience) in the replay buffer. We learned that our replay buffer will be pre-filled with the expert demonstrations data. So, now, our replay buffer will consist of a mixture of both expert demonstrations and the agent's experience (self-generated data). We sample a minibatch of experience from the replay buffer and train the agent. Note that here we use a prioritized replay buffer, so while sampling, we give more priority to the expert demonstrations than the agent-generated data. In this way, we train the agent by sampling experience from the replay buffer and minimize the loss using gradient descent.</p>
    <p class="normal">We learned that the agent interacts with the environment and stores the experience in the replay buffer. If the replay buffer is full, then we overwrite the buffer with new transition <a id="_idIndexMarker1411"/>information generated by the agent. However, we won't overwrite the expert demonstrations. So, the expert demonstrations will always remain in the replay buffer so that the agent can make use of expert demonstrations for learning.</p>
    <p class="normal">Thus, we have learned how to pre-train and train an agent with expert demonstrations. In the next section, we will learn about the loss function of DQfD.</p>
    <h2 id="_idParaDest-412" class="title">Loss function of DQfD</h2>
    <p class="normal">The loss <a id="_idIndexMarker1412"/>function of DQfD comprises the sum of four losses:</p>
    <ol>
      <li class="numbered" value="1">Double DQN loss</li>
      <li class="numbered">N-step double DQN loss</li>
      <li class="numbered">Supervised classification loss</li>
      <li class="numbered">L2 loss</li>
    </ol>
    <p class="normal">Now, we will look at each of these losses.</p>
    <p class="normal"><strong class="keyword">Double DQN loss</strong> – <img src="../Images/B15558_15_094.png" alt="" style="height: 1.11em;"/> represents <a id="_idIndexMarker1413"/>the 1-step double DQN loss.</p>
    <p class="normal"><strong class="keyword">N-step double DQN loss</strong> – <img src="../Images/B15558_15_095.png" alt="" style="height: 1.11em;"/> represents <a id="_idIndexMarker1414"/>the n-step double DQN loss.</p>
    <p class="normal"><strong class="keyword">Supervised classification loss</strong> – <img src="../Images/B15558_15_096.png" alt="" style="height: 1.11em;"/> represents <a id="_idIndexMarker1415"/>the supervised classification loss. It is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_097.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Where:</p>
    <ul>
      <li class="bullet"><em class="italic">a</em><sub class="" style="font-style: italic;">E</sub> is the action taken by the expert.</li>
      <li class="bullet"><em class="italic">l</em>(<em class="italic">a</em><sub class="" style="font-style: italic;">E</sub>, <em class="italic">a</em>) is known as the margin function or margin loss. It will be 0 when the action taken is equal to the expert action <em class="italic">a</em> = <em class="italic">a</em><sub class="" style="font-style: italic;">E</sub>; else, it is positive.</li>
    </ul>
    <p class="normal"><strong class="keyword">L2 regularization loss</strong> – <img src="../Images/B15558_15_098.png" alt="" style="height: 1.11em;"/> represents the L2 regularization loss. It prevents the agent from <a id="_idIndexMarker1416"/>overfitting to the demonstration data.</p>
    <p class="normal">Thus, the <a id="_idIndexMarker1417"/>final loss function will be the sum of all the preceding four losses:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_099.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where the value of <img src="../Images/B15558_15_100.png" alt="" style="height: 1.11em;"/> acts as a weighting factor and helps us to control the importance of the respective loss.</p>
    <p class="normal">Now that we have learned how DQfD works, we will look into the algorithm of DQfD in the next section. </p>
    <h2 id="_idParaDest-413" class="title">Algorithm – DQfD</h2>
    <p class="normal">The algorithm <a id="_idIndexMarker1418"/>of DQfD is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize the main network parameter <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_15_102.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> with the expert demonstrations</li>
      <li class="numbered">Set <em class="italic">d</em>: the number of time steps we want to delay updating the target network parameter</li>
      <li class="numbered"><strong class="keyword">Pre-training phase</strong>: For steps <img src="../Images/B15558_15_105.png" alt="" style="height: 1.11em;"/>:<ol>
          <li class="numbered-l2">Sample a minibatch of experience from the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> </li>
          <li class="numbered-l2">Compute the loss <em class="italic">J</em>(<em class="italic">Q</em>)</li>
          <li class="numbered-l2">Update the parameter of the network using gradient descent </li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <em class="italic">d</em> = 0:
        
        <p class="bullet-para">Update the target network parameter <img src="../Images/B15558_09_084.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/></p></li>
      </ol></li>
      <li class="numbered"><strong class="keyword">Training phase</strong>: For steps <em class="italic">t</em> = 1, 2, ..., <em class="italic">T</em>:<ol>
          <li class="numbered-l2">Select an action</li>
          <li class="numbered-l2">Perform the selected action and move to the next state, observe the reward, and store this transition information in the replay buffer <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Sample a minibatch of experience from the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> with prioritization</li>
          <li class="numbered-l2">Compute the loss <em class="italic">J</em>(<em class="italic">Q</em>)</li>
          <li class="numbered-l2">Update <a id="_idIndexMarker1419"/>the parameter of the network using gradient descent </li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <em class="italic">d</em> = 0:
        
        <p class="bullet-para">Update the target network parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/></p></li>
     </ol> </li>
    </ol>
    <p class="normal">That's it! In the next section, we will learn about a very interesting concept called IRL.</p>
    <h1 id="_idParaDest-414" class="title">Inverse reinforcement learning</h1>
    <p class="normal"><strong class="keyword">Inverse Reinforcement Learning (IRL)</strong> is one of the most exciting fields of reinforcement learning. In reinforcement learning, our goal is to learn the optimal policy. That is, our goal is to <a id="_idIndexMarker1420"/>find the optimal policy that gives the maximum return (sum of rewards of the trajectory). In order to find the optimal policy, first, we should know the reward function. A reward function tells us what reward we obtain by performing an action <em class="italic">a</em> in the state <em class="italic">s</em>. Once we have the reward function, we can train our agent to learn the optimal policy that gives the maximum reward. But the problem is that designing the reward function is not that easy for complex tasks. </p>
    <p class="normal">Consider designing the reward function for tasks such as an agent learning to walk, self-driving cars, and so on. In these cases, designing the reward function is not that handy and involves assigning rewards to a variety of agent behaviors. For instance, consider designing the reward function for an agent learning to drive a car. In this case, we need to assign a reward for every behavior of the agent. For example, we can assign a high reward if the agent follows the traffic signal, avoids pedestrians, doesn't hit any objects, and so on. But designing the reward function in this way is not optimal, and there is also a good chance that we might miss out on several behaviors of an agent. </p>
    <p class="normal">Okay, now the question is can we learn the reward function? Yes! If we have expert demonstrations, then we can learn the reward function from the expert demonstrations. But how can we do that exactly? Here is where IRL helps us. As the name suggests, IRL is the inverse of reinforcement learning.</p>
    <p class="normal">In RL, we try to find the optimal policy given the reward function, but in IRL, we try to learn the reward function given the expert demonstrations. Once we have derived the reward function from the expert demonstrations using IRL, we can use the reward function to train our agent to learn the optimal policy using any reinforcement learning algorithm. </p>
    <p class="normal">IRL consists of several interesting algorithms. In the next section, we will learn one of the most popular IRL algorithms, called maximum entropy IRL.</p>
    <h2 id="_idParaDest-415" class="title">Maximum entropy IRL</h2>
    <p class="normal">In <a id="_idIndexMarker1421"/>this section, we will learn <a id="_idIndexMarker1422"/>how to extract a reward function from the given set of expert demonstrations using an IRL algorithm called maximum entropy IRL. Before diving into maximum entropy IRL, let's learn some of the important terms that are required to understand how maximum entropy IRL works.</p>
    <h3 id="_idParaDest-416" class="title">Key terms</h3>
    <p class="normal"><strong class="keyword">Feature vector</strong> – We can <a id="_idIndexMarker1423"/>represent the state by a feature vector <em class="italic">f</em>. Let's say we have a state <em class="italic">s</em>; its feature vector can then be defined as <em class="italic">f</em><sub class="" style="font-style: italic;">s</sub>.</p>
    <p class="normal"><strong class="keyword">Feature count</strong> –<strong class="keyword"> </strong>Say we have a trajectory <img src="../Images/B15558_14_164.png" alt="" style="height: 0.84em;"/>; the feature count of the trajectory is then defined as the sum of the feature vectors of all the states in the trajectory:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_114.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_15_115.png" alt="" style="height: 1.11em;"/> denotes the feature count of the trajectory <img src="../Images/B15558_14_164.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">Reward function</strong> – The reward function can be defined as the linear combination of the features, that is, the sum of feature vectors multiplied by a weight <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_118.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/> denotes the weight and <em class="italic">f</em><sub class="" style="font-style: italic;">s</sub> denotes the feature vector. Note that this <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> is what we are trying to learn. When we obtain the optimal <img src="../Images/B15558_15_121.png" alt="" style="height: 1.11em;"/>, then we will have a correct reward function. We will learn how we can find the optimal <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> in the next section.</p>
    <p class="normal">We can represent the preceding equation in sigma notation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_123.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">We know that the feature count is the sum of feature vectors of all the states in the trajectory, so from (2), we can rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_124.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Thus, the <a id="_idIndexMarker1424"/>reward of the trajectory is just the weight multiplied by the feature count of the trajectory.</p>
    <h3 id="_idParaDest-417" class="title">Back to maximum entropy IRL</h3>
    <p class="normal">Now, let's <a id="_idIndexMarker1425"/>learn how maximum entropy IRL works. Consider that we have expert demonstrations <img src="../Images/B15558_15_125.png" alt="" style="height: 1.11em;"/>. Our goal is to learn the reward function from the given expert demonstrations. How can we do that?</p>
    <p class="normal">We have already learned that the reward function is given as <img src="../Images/B15558_15_126.png" alt="" style="height: 1.2em;"/>. Finding the optimal parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> helps us to learn the correct reward function. So, we will sample a trajectory <img src="../Images/B15558_14_140.png" alt="" style="height: 0.84em;"/> from the expert demonstrations <img src="../Images/B15558_15_036.png" alt="" style="height: 1.11em;"/> and try to find the reward function by finding the optimal parameter <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">The probability of a trajectory being sampled from the expert demonstrations is directly proportional to the exponential of the reward function. That is, trajectories that obtain more rewards are more likely to be sampled from our demonstrations than those trajectories that obtain less rewards:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_131.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The probability should be in the range of 0 to 1, right? But the value of <img src="../Images/B15558_15_132.png" alt="" style="height: 1.11em;"/> will not be in the range of 0 to 1. So, in order to normalize that, we introduce <em class="italic">z</em>, which acts as the normalization constant and is given as <img src="../Images/B15558_15_133.png" alt="" style="height: 2.69em;"/>. We can rewrite our preceding equation with <em class="italic">z</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_134.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Now, our <a id="_idIndexMarker1426"/>objective is to maximize <img src="../Images/B15558_15_135.png" alt="" style="height: 1.11em;"/>, that is, to maximize the log probability of selecting trajectories that give more rewards. So, we can define our objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_136.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Where <em class="italic">M</em><span class="mediaobject"> </span>denotes the number of demonstrations.</p>
    <p class="normal">Substituting (3) in (4), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_137.png" alt="" style="height: 2.78em;"/></figure>
    <p class="normal">Based on the log rule, <img src="../Images/B15558_15_138.png" alt="" style="height: 1.11em;"/>, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_139.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">The logarithmic and exponential terms cancel each other out, so the preceding equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_140.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">We know that <img src="../Images/B15558_15_133.png" alt="" style="height: 2.69em;"/>; substituting the value of <em class="italic">z</em>, we can rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_142.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">We know that <img src="../Images/B15558_15_126.png" alt="" style="height: 1.2em;"/>; substituting the value of <img src="../Images/B15558_15_144.png" alt="" style="height: 1.11em;"/>, our final simplified objective function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_145.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">To find <a id="_idIndexMarker1427"/>the optimal parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>, we compute the gradient of the preceding objective function <img src="../Images/B15558_15_147.png" alt="" style="height: 1.11em;"/> and update the value of <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_15_149.png" alt="" style="height: 1.11em;"/>. In the next section, we will learn how to compute the gradient <img src="../Images/B15558_09_043.png" alt="" style="height: 1.11em;"/>.</p>
    <h3 id="_idParaDest-418" class="title">Computing the gradient</h3>
    <p class="normal">We <a id="_idIndexMarker1428"/>learned that our objective function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_151.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">Now, we compute the gradient of the objective function with respect to <img src="../Images/B15558_15_152.png" alt="" style="height: 1.11em;"/>. After computation, our gradient is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_153.png" alt="" style="height: 3.24em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_15_154.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">The average <a id="_idIndexMarker1429"/>of the feature count is just the feature expectation <img src="../Images/B15558_15_155.png" alt="" style="height: 1.29em;"/>, so we can substitute <img src="../Images/B15558_15_156.png" alt="" style="height: 2.87em;"/> and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_157.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">We can rewrite the preceding equation by combining all the states of the trajectories as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_158.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Thus, using the preceding equation, we compute gradients and update the parameter <img src="../Images/B15558_13_233.png" alt="" style="height: 1.11em;"/>. If you look at the preceding equation, we can easily compute the first term, which is just the feature expectation <img src="../Images/B15558_15_160.png" alt="" style="height: 1.29em;"/>, but what about the <img src="../Images/B15558_15_161.png" alt="" style="height: 1.11em;"/> in the second term? <img src="../Images/B15558_15_162.png" alt="" style="height: 1.11em;"/> is called the state visitation frequency and it represents the probability of being in a given state. Okay, how can we compute <img src="../Images/B15558_15_163.png" alt="" style="height: 1.11em;"/>?</p>
    <p class="normal">If we have a policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/>, then we can use the policy to compute the state visitation frequency. But we don't have any policy yet. So, we can use a dynamic programming method, say, value iteration, to compute the policy. However, in order to compute the policy using the value iteration method, we require a reward function. So, we just feed our reward function <img src="../Images/B15558_15_165.png" alt="" style="height: 1.11em;"/> and extract the policy using the value iteration. Then, using the extracted policy, we compute the state visitation frequency.</p>
    <p class="normal">The steps <a id="_idIndexMarker1430"/>involved in computing the state visitation frequency using the policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/> are as follows:</p>
    <ol>
      <li class="numbered" value="1">Let the probability of visiting a state <em class="italic">s</em> at a time <em class="italic">t</em> be <img src="../Images/B15558_15_167.png" alt="" style="height: 1.11em;"/>. We can write the probability of visiting the initial state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> at a first time step <em class="italic">t</em> = 1 as: <img src="../Images/B15558_15_168.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Then for time steps <em class="italic">t</em> = 1 to <em class="italic">T</em>:<p class="bullet-para">Compute <img src="../Images/B15558_15_169.png" alt="" style="height: 2.69em;"/></p>
      </li>
      <li class="numbered">Compute the state visitation frequency as <img src="../Images/B15558_15_170.png" alt="" style="height: 2.69em;"/>.</li>
    </ol>
    <p class="normal">To get a clear understanding of how maximum entropy IRL works, let's look into the algorithm in the next section.</p>
    <h3 id="_idParaDest-419" class="title">Algorithm – maximum entropy IRL</h3>
    <p class="normal">The <a id="_idIndexMarker1431"/>algorithm of maximum entropy IRL is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize the parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> and gather the demonstrations <img src="../Images/B15558_15_125.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of iterations:<ol>
          <li class="numbered-l2">Compute the reward function <img src="../Images/B15558_15_126.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Compute the policy using value iteration with the reward function obtained in the previous step</li>
          <li class="numbered-l2">Compute state visitation frequency <img src="../Images/B15558_15_174.png" alt="" style="height: 1.11em;"/> using the policy obtained in the previous step</li>
          <li class="numbered-l2">Compute the gradient with respect to <img src="../Images/B15558_14_004.png" alt="" style="height: 1.11em;"/>, that is, <img src="../Images/B15558_15_176.png" alt="" style="height: 2.69em;"/></li>
          <li class="numbered-l2">Update the value of <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_13_286.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">Thus, over a series of iterations, we will find an optimal parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>. Once we have <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>, we can use it to define the correct reward function <img src="../Images/B15558_15_181.png" alt="" style="height: 1.11em;"/>. In the next section, we will learn about GAIL.</p>
    <h1 id="_idParaDest-420" class="title">Generative adversarial imitation learning</h1>
    <p class="normal"><strong class="keyword">Generative Adversarial Imitation Learning (GAIL)</strong> is another very popular IRL algorithm. As <a id="_idIndexMarker1432"/>the name suggests, it is based <a id="_idIndexMarker1433"/>on <strong class="keyword">Generative Adversarial Networks (GANs)</strong>, which we learned about in <em class="chapterRef">Chapter 7</em>, <em class="italic">Deep Learning Foundations</em>. To understand how GAIL works, we should first recap how GANs work.</p>
    <p class="normal">In a GAN, we have two networks: one is the generator and the other is the discriminator. The role of the generator is to generate new data points by learning the distribution of the input dataset. The role of the discriminator is to classify whether a given data point is generated by the generator (learned distribution) or whether it is from the real data distribution. </p>
    <p class="normal">Minimizing the loss function of a GAN implies minimizing the <strong class="keyword">Jensen Shannon (JS)</strong> divergence <a id="_idIndexMarker1434"/>between the real data distribution and the fake data distribution (learned distribution). The JS divergence is used to measure how two probability distributions differ from each other. Thus, when the JS divergence between the real and fake distributions is zero, it means that the real and fake data distributions are equal, that is, our generator network has successfully learned the real distribution.</p>
    <p class="normal">Now, let's <a id="_idIndexMarker1435"/>learn how to make use of GANs in an IRL setting. First, let's introduce a new term called <strong class="keyword">occupancy measure</strong>.<strong class="keyword"> </strong>It is defined as the distribution of the states and actions that our agent comes across while exploring the environment with some policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>. In simple terms, it is basically the distribution of state-action pairs following a policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>. The occupancy measure of a policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/> is denoted by <img src="../Images/B15558_15_185.png" alt="" style="height: 0.93em;"/>.</p>
    <p class="normal">In the imitation learning setting, we have an expert policy, and let's denote the expert policy by <img src="../Images/B15558_15_186.png" alt="" style="height: 0.84em;"/>. Similarly, let's denote the agent's policy by <img src="../Images/B15558_10_111.png" alt="" style="height: 0.84em;"/>. Now, our goal is to make our agent learn the expert policy. How can we do that? If we make the occupancy measure of the expert policy and the agent policy equal, then it implies that our agent has successfully learned the expert policy. That is, the occupancy measure is the distribution of the state-action pairs following a policy. If we can make the distribution of state-action pairs of the agent's policy equal to the distribution of state-action pairs of the expert's policy, then it means that our agent has learned the expert policy. Let's explore how we can do this using GANs.</p>
    <p class="normal">We can perceive the occupancy measure of the expert policy as the real data distribution and the occupancy measure of the agent policy as the fake data distribution. Thus, minimizing the JS divergence between the occupancy measure of the expert policy <img src="../Images/B15558_15_188.png" alt="" style="height: 0.93em;"/> and the occupancy measure of the agent policy <img src="../Images/B15558_15_189.png" alt="" style="height: 0.93em;"/> implies that the agent will learn the expert policy.</p>
    <p class="normal">With GANs, we <a id="_idIndexMarker1436"/>know that the role of the generator is to generate new data points by learning the distribution of a given dataset. Similarly, in GAIL, the role of the generator is to generate a new policy by learning the distribution (occupancy measure) of the expert policy. The role of the discriminator is to classify whether the given policy is the expert policy or the agent policy.</p>
    <p class="normal">With GANs, we know that, for a generator, the optimal discriminator is the one that is not able to distinguish between the real and fake data distributions; similarly, in GAIL, the optimal discriminator is the one that is unable to distinguish whether the generated state-action pair is from the agent policy or the expert policy.</p>
    <p class="normal">To make ourselves clear, let's understand the terms we use in GAIL by relating to GAN terminology: </p>
    <ul>
      <li class="bullet"><strong class="keyword">Real data distribution</strong> – Occupancy measure of the expert policy</li>
      <li class="bullet"><strong class="keyword">Fake data distribution</strong> – Occupancy measure of the agent policy</li>
      <li class="bullet"><strong class="keyword">Real data</strong> – State-action pair generated by the expert policy</li>
      <li class="bullet"><strong class="keyword">Fake data</strong> – State-action pair generated by the agent policy</li>
    </ul>
    <p class="normal">In a nutshell, we use the generator to generate the state-action pair in a way that the discriminator is not able to distinguish whether the state-action pair is generated using the expert policy or the agent policy. Both the generator and discriminator are neural networks. We train the generator to generate a policy similar to the expert policy using TRPO. The discriminator is a classifier, and it is optimized using Adam. Thus, we can define the objective function of GAIL as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_190.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> is the parameter of the generator and <img src="../Images/B15558_15_192.png" alt="" style="height: 0.93em;"/> is the parameter of the discriminator.</p>
    <p class="normal">Now that <a id="_idIndexMarker1437"/>we have an understanding of how GAIL works, let's get into more detail and learn how the preceding equation is derived.</p>
    <h2 id="_idParaDest-421" class="title">Formulation of GAIL </h2>
    <p class="normal">In this <a id="_idIndexMarker1438"/>section, we explore the math of GAIL and see how it works. You can skip this section if you are not interested in math. We know that in reinforcement learning, our objective is to find the optimal policy that gives the maximum reward. It can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_193.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">We can rewrite our objective function by adding the entropy of a policy as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_194.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">The preceding equation tells us that we can maximize the entropy of the policy while also maximizing the reward. Instead of defining the objective function in terms of the reward, we can also define the objective function in terms of cost. </p>
    <p class="normal">That is, we can define our RL objective function in terms of cost, as our objective is to find an optimal policy that minimizes the cost; this can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_195.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Where <em class="italic">c</em> is the cost. Thus, given the cost function, our goal is to find the optimal policy that minimizes the cost.</p>
    <p class="normal">Now, let's talk about IRL. We learned that in IRL, our objective is to find the reward function from the given set of expert demonstrations. We can also define the objective of IRL in terms of cost instead of reward. That is, we can define our IRL objective function in terms of cost, as our objective is to find the cost function under which the expert demonstration is optimal. The objective can be expressed using maximum causal entropy IRL as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_196.png" alt="" style="height: 1.84em;"/></figure>
    <p class="normal">What does the preceding equation imply? In the IRL setting, our goal is to learn the cost function given the expert demonstrations (expert policy). We know that the expert policy performs better than the other policy, so we try to learn the cost function <em class="italic">c</em>, which assigns low cost to the expert policy and high cost to other policies. Thus, the preceding <a id="_idIndexMarker1439"/>objective function implies that we try to find a cost function that assigns low cost to the expert policy and high cost to other policies.</p>
    <p class="normal">To reduce overfitting, we introduce the regularizer <img src="../Images/B15558_15_197.png" alt="" style="height: 1.11em;"/> and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_198.png" alt="" style="height: 1.84em;"/></figure>
    <p class="normal">From equation (6), we learned that in a reinforcement learning setting, given a cost, we obtain the optimal policy, and from (7), we learned that in an IRL setting, given an expert policy (expert demonstration), we obtain the cost. Thus, from (6) and (7), we can observe that the output of IRL can be sent as an input to the RL. That is, IRL results in the cost function and we can use this cost function as an input in RL to learn the optimal policy. Thus, we can write <img src="../Images/B15558_15_199.png" alt="" style="height: 1.11em;"/>, which implies that the result of IRL is fed as an input to the RL.</p>
    <p class="normal">We can express this in a functional composition form as <img src="../Images/B15558_15_200.png" alt="" style="height: 1.2em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_201.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">In equation (8), the following applies:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_15_202.png" alt="" style="height: 1.2em;"/> is the convex conjugate of the regularizer <img src="../Images/B15558_15_203.png" alt="" style="height: 1.11em;"/> </li>
      <li class="bullet"><img src="../Images/B15558_15_204.png" alt="" style="height: 0.93em;"/> is the occupancy measure of the agent policy</li>
      <li class="bullet"><img src="../Images/B15558_15_205.png" alt="" style="height: 1.02em;"/> is the occupancy measure of the expert policy </li>
    </ul>
    <p class="normal">To learn how exactly the equation (8) is derived, you can refer to the GAIL paper given in the <em class="italic">Further reading</em> section at the end of the chapter. The objective function (equation (8)) implies that we try to find the optimal policy whose occupancy measure is close to the <a id="_idIndexMarker1440"/>occupancy measure of the expert policy. The occupancy measure between the agent policy and the expert policy is measured by <img src="../Images/B15558_15_206.png" alt="" style="height: 1.2em;"/>. There are several choices for the regularizer <img src="../Images/B15558_15_207.png" alt="" style="height: 1.2em;"/>. We use a generative adversarial regularizer <img src="../Images/B15558_15_208.png" alt="" style="height: 1.2em;"/> and write our equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_209.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Thus, minimizing <img src="../Images/B15558_15_210.png" alt="" style="height: 1.4em;"/> basically implies that we minimize the JS divergence between the occupancy measure of the agent policy <img src="../Images/B15558_15_211.png" alt="" style="height: 0.93em;"/> and the expert policy <img src="../Images/B15558_15_212.png" alt="" style="height: 0.93em;"/>. Thus, we can rewrite the RHS of the equation (9) as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_213.png" alt="" style="height: 1.58em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_15_214.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_12_326.png" alt="" style="height: 1.11em;"/> is just the policy regularizer. We know that the JS divergence between the occupancy measure of the agent policy <img src="../Images/B15558_15_185.png" alt="" style="height: 0.93em;"/> and the expert policy <img src="../Images/B15558_15_217.png" alt="" style="height: 0.93em;"/> is minimized using the GAN, so we can just replace <img src="../Images/B15558_15_218.png" alt="" style="height: 1.96em;"/> in the preceding equation with the GAN objective function as:</p>
    <p class="normal"><img src="../Images/B15558_15_219.png" alt="" style="height: 1.58em;"/></p>
    <p class="normal">Where <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> is <a id="_idIndexMarker1441"/>the parameter of the generator and <img src="../Images/B15558_15_221.png" alt="" style="height: 0.93em;"/> is the parameter of the discriminator.</p>
    <p class="normal">Thus, our final objective function of GAIL becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_15_222.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">The objective equation implies that we can find the optimal policy by minimizing the occupancy measure of the expert policy and the agent policy, and we minimize that using GANs.</p>
    <p class="normal">The role of the generator is to generate a policy by learning the occupancy measure of the expert policy, and the role of the discriminator is to classify whether the generated policy is from the expert policy or the agent policy. So, we train the generator using TRPO and the discriminator is basically a neural network that tells us whether the policy generated by the generator is the expert policy or the agent policy.</p>
    <h1 id="_idParaDest-422" class="title">Summary</h1>
    <p class="normal">We started the chapter by understanding what imitation learning is and how supervised imitation learning works. Next, we learned about the DAgger algorithm, where we aggregate the dataset obtained over a series of iterations and learn the optimal policy. </p>
    <p class="normal">After looking at DAgger, we learned about DQfD, where we prefill the replay buffer with expert demonstrations and pre-train the agent with expert demonstrations before the training phase.</p>
    <p class="normal">Moving on, we learned about IRL. We understood that in reinforcement learning, we try to find the optimal policy given the reward function, but in IRL, we try to learn the reward function given the expert demonstrations. When we have derived the reward function from the expert demonstrations using IRL, we can use the reward function to train our agent to learn the optimal policy using any reinforcement learning algorithm. We then explored how to learn the reward function using the maximum entropy IRL algorithm.</p>
    <p class="normal">At the end of the chapter, we learned about GAIL, where we used GANs to learn the optimal policy. In the next chapter, we will explore a reinforcement learning library called Stable Baselines.</p>
    <h1 id="_idParaDest-423" class="title">Questions</h1>
    <p class="normal">Let's assess our understanding of imitation learning and IRL. Try answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">How does supervised imitation learning work?</li>
      <li class="numbered">How does DAgger differ from supervised imitation learning?</li>
      <li class="numbered">Explain the different phases of DQfD.</li>
      <li class="numbered">Why do we need IRL?</li>
      <li class="numbered">What is a feature vector?</li>
      <li class="numbered">How does GAIL work? </li>
    </ol>
    <h1 id="_idParaDest-424" class="title">Further reading</h1>
    <p class="normal">For more information, refer to the following papers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning </strong>by <em class="italic">Stephane Ross</em>, <em class="italic">Geoffrey J. Gordon</em>, <em class="italic">J. Andrew Bagnell</em>, <a href="https://arxiv.org/pdf/1011.0686.pdf"><span class="url">https://arxiv.org/pdf/1011.0686.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Deep Q-learning from Demonstrations </strong>by <em class="italic">Todd Hester</em>, <em class="italic">et al</em>., <a href="https://arxiv.org/pdf/1704.03732.pdf"><span class="url">https://arxiv.org/pdf/1704.03732.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Maximum Entropy Inverse Reinforcement Learning</strong> by <em class="italic">Brian D. Ziebart</em>,<em class="italic"> Andrew Maas</em>, <em class="italic">J.Andrew Bagnell</em>, <em class="italic">Anind K. Dey</em>, <a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf"><span class="url">https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Generative Adversarial Imitation Learning </strong>by <em class="italic">Jonathan Ho</em>, <em class="italic">Stefano Ermon</em>, <a href="https://arxiv.org/pdf/1606.03476.pdf"><span class="url">https://arxiv.org/pdf/1606.03476.pdf</span></a></li>
    </ul>
  </div>
</body></html>