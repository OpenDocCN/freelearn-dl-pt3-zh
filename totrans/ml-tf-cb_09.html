<html><head></head><body>
  <div id="_idContainer121">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-266" class="chapterTitle">Recurrent Neural Networks</h1>
    <p class="normal"><strong class="keyword">Recurrent Neural Networks</strong> (<strong class="keyword">RNNs</strong>) are the primary modern approach for modeling data that is <a id="_idIndexMarker492"/>sequential in nature. The word "recurrent" in the name of the architecture class refers to the fact that the output of the current step becomes the input to the next one (and potentially further ones as well). At each element in the sequence, the model considers both the current input and what it "remembers" about the preceding elements. </p>
    <p class="normal"><strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>) tasks are one of the primary areas of application for RNNs: if you<a id="_idIndexMarker493"/> are reading through this very sentence, you are picking up the context of each word from the words that came before it. NLP models based on RNNs can build on this approach to achieve generative tasks, such as novel text creation, as well as predictive ones such as sentiment classification or machine translation.</p>
    <p class="normal">In this chapter, we'll cover the following topics: </p>
    <ul>
      <li class="bullet">Text generation</li>
      <li class="bullet">Sentiment classification</li>
      <li class="bullet">Time series – stock price prediction</li>
      <li class="bullet">Open-domain question answering</li>
    </ul>
    <p class="normal">The first topic we'll tackle is text generation: it demonstrates quite easily how we can use an RNN to generate novel content, and can therefore serve as a gentle introduction to RNNs.</p>
    <h1 id="_idParaDest-267" class="title">Text generation</h1>
    <p class="normal">One of <a id="_idIndexMarker494"/>the best-known applications used to <a id="_idIndexMarker495"/>demonstrate the strength of RNNs is generating novel text (we will return to this application later, in the chapter on Transformer architectures). </p>
    <p class="normal">In this recipe, we will use a <strong class="keyword">Long Short-Term Memory</strong> (<strong class="keyword">LSTM</strong>) architecture—a popular variant of RNNs—to build a<a id="_idIndexMarker496"/> text generation model. The name LSTM <a id="_idIndexMarker497"/>comes from the motivation for their development: "vanilla" RNNs struggled with long dependencies (known<a id="_idIndexMarker498"/> as the vanishing gradient problem) and the architectural solution of LSTM solved that. LSTM models achieve that by maintaining a cell state, as well as a "carry" to ensure that the signal (in the form of a gradient) is not lost as the sequence is processed. At each time step, the LSTM model considers the current word, the carry, and the cell state jointly.</p>
    <p class="normal">The topic itself is not that trivial, but for practical purposes, full comprehension of the structural design is not essential. It suffices to keep in mind that an LSTM cell allows past information to be reinjected at a later point in time.</p>
    <p class="normal">We will train our model on the NYT comment and headlines dataset (<a href="https://www.kaggle.com/aashita/nyt-comments"><span class="url">https://www.kaggle.com/aashita/nyt-comments</span></a>) and will use it to generate new headlines. We chose this dataset for its moderate size (the recipe should be reproducible without access to a powerful workstation) and availability (Kaggle is freely accessible, unlike some data sources accessible only via paywall).</p>
    <h2 id="_idParaDest-268" class="title">How to do it...</h2>
    <p class="normal">As usual, first we import the necessary packages.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras 
<span class="hljs-comment"># keras module for building LSTM </span>
<span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Embedding, LSTM, Dense
<span class="hljs-keyword">from</span> keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> EarlyStopping
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">import</span> keras.utils <span class="hljs-keyword">as</span> ku
</code></pre>
    <p class="normal">We want to make sure our results are reproducible – due to the nature of the interdependencies within the Python deep learning universe, we need to initialize multiple random mechanisms.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> string, os 
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
warnings.simplefilter(action=<span class="hljs-string">'ignore'</span>, category=FutureWarning)
</code></pre>
    <p class="normal">The next step involves importing the necessary functionality from Keras itself:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Embedding, LSTM, Dense
<span class="hljs-keyword">from</span> keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> EarlyStopping
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">import</span> keras.utils <span class="hljs-keyword">as</span> ku 
</code></pre>
    <p class="normal">Finally, it is typically convenient—if not always in line with what purists deem best practice—to customize the level of warnings displayed in the execution of our code. It is mainly to deal with ubiquitous warnings around assigning value to a subset of a DataFrame: clean demonstration is more important in the current context than sticking to the coding standards expected in a production environment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)
warnings.simplefilter(action=<span class="hljs-string">'ignore'</span>, category=FutureWarning)
</code></pre>
    <p class="normal">We shall <a id="_idIndexMarker499"/>define some functions that will streamline the <a id="_idIndexMarker500"/>code later on. First, let's clean the text:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">clean_text</span><span class="hljs-function">(</span><span class="hljs-params">txt</span><span class="hljs-function">):</span>
    txt = <span class="hljs-string">""</span>.join(v <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> txt <span class="hljs-keyword">if</span> v <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation).lower()
    txt = txt.encode(<span class="hljs-string">"utf8"</span>).decode(<span class="hljs-string">"ascii"</span>,<span class="hljs-string">'ignore'</span>)
    <span class="hljs-keyword">return</span> txt 
</code></pre>
    <p class="normal">Let's use a wrapper around the built-in TensorFlow tokenizer as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_sequence_of_tokens</span><span class="hljs-function">(</span><span class="hljs-params">corpus</span><span class="hljs-function">):</span>
    <span class="hljs-comment">## tokenization</span>
    tokenizer.fit_on_texts(corpus)
    total_words = len(tokenizer.word_index) + <span class="hljs-number">1</span>
    
    <span class="hljs-comment">## convert data to sequence of tokens </span>
    input_sequences = []
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> corpus:
        token_list = tokenizer.texts_to_sequences([line])[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, len(token_list)):
            n_gram_sequence = token_list[:i+<span class="hljs-number">1</span>]
            input_sequences.append(n_gram_sequence)
    <span class="hljs-keyword">return</span> input_sequences, total_words
</code></pre>
    <p class="normal">A frequently useful step is to wrap up a model-building step inside a function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_model</span><span class="hljs-function">(</span><span class="hljs-params">max_sequence_len, total_words</span><span class="hljs-function">):</span>
    input_len = max_sequence_len - <span class="hljs-number">1</span>
    model = Sequential()
    
    model.add(Embedding(total_words, <span class="hljs-number">10</span>, input_length=input_len))
    model.add(LSTM(<span class="hljs-number">100</span>))
    model.add(Dense(total_words, activation=<span class="hljs-string">'softmax'</span>))
    model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
    
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">The <a id="_idIndexMarker501"/>following is some boilerplate for padding the <a id="_idIndexMarker502"/>sequences (the utility of this will become clearer in the course of the recipe):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">generate_padded_sequences</span><span class="hljs-function">(</span><span class="hljs-params">input_sequences</span><span class="hljs-function">):</span>
    max_sequence_len = max([len(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> input_sequences])
    input_sequences = np.array(pad_sequences(input_sequences,                       maxlen=max_sequence_len, padding=<span class="hljs-string">'pre'</span>))
    
    predictors, label = input_sequences[:,:<span class="hljs-number">-1</span>],input_sequences[:,<span class="hljs-number">-1</span>]
    label = ku.to_categorical(label, num_classes=total_words)
    <span class="hljs-keyword">return</span> predictors, label, max_sequence_len
</code></pre>
    <p class="normal">Finally, we create a function that will be used to generate text from our fitted model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">generate_text</span><span class="hljs-function">(</span><span class="hljs-params">seed_text, next_words, model, max_sequence_len</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[<span class="hljs-number">0</span>]
        token_list = pad_sequences([token_list],                      maxlen=max_sequence_len<span class="hljs-number">-1</span>, padding=<span class="hljs-string">'pre'</span>)
        predicted = model.predict_classes(token_list, verbose=<span class="hljs-number">0</span>)
        
        output_word = <span class="hljs-string">""</span>
        <span class="hljs-keyword">for</span> word,index <span class="hljs-keyword">in</span> tokenizer.word_index.items():
            <span class="hljs-keyword">if</span> index == predicted:
                output_word = word
                <span class="hljs-keyword">break</span>
        seed_text += <span class="hljs-string">" "</span>+output_word
    <span class="hljs-keyword">return</span> seed_text.title()
</code></pre>
    <p class="normal">The next step is to load our dataset (the <code class="Code-In-Text--PACKT-">break</code> clause serves as a fast way to only pick up articles and not comment datasets):</p>
    <pre class="programlisting code"><code class="hljs-code">curr_dir = <span class="hljs-string">'../input/'</span>
all_headlines = []
<span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> os.listdir(curr_dir):
    <span class="hljs-keyword">if</span> <span class="hljs-string">'Articles'</span> <span class="hljs-keyword">in</span> filename:
        article_df = pd.read_csv(curr_dir + filename)
        all_headlines.extend(list(article_df.headline.values))
        <span class="hljs-keyword">break</span>
all_headlines[:<span class="hljs-number">10</span>]
</code></pre>
    <p class="normal">We <a id="_idIndexMarker503"/>can inspect the first few elements as <a id="_idIndexMarker504"/>follows:</p>
    <pre class="programlisting code"><code class="hljs-code">['The Opioid Crisis Foretold',
 'The Business Deals That Could Imperil Trump',
 'Adapting to American Decline',
 'The Republicans' Big Senate Mess',
 'States Are Doing What Scott Pruitt Won't',
 'Fake Pearls, Real Heart',
 'Fear Beyond Starbucks',
 'Variety: Puns and Anagrams',
 'E.P.A. Chief's Ethics Woes Have Echoes in His Past',
 'Where Facebook Rumors Fuel Thirst for Revenge']
</code></pre>
    <p class="normal">As is usually the case with real-life text data, we need to clean the input text. For simplicity, we perform only the basic preprocessing: punctuation removal and conversion of all words to lowercase:</p>
    <pre class="programlisting code"><code class="hljs-code">corpus = [clean_text(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_headlines]
</code></pre>
    <p class="normal">This is what the top 10 rows look like after the cleaning operation:</p>
    <pre class="programlisting code"><code class="hljs-code">corpus[:<span class="hljs-number">10</span>]
['the opioid crisis foretold',
 'the business deals that could imperil trump',
 'adapting to american decline',
 'the republicans big senate mess',
 'states are doing what scott pruitt wont',
 'fake pearls real heart',
 'fear beyond starbucks',
 'variety puns and anagrams',
 'epa chiefs ethics woes have echoes in his past',
 'where facebook rumors fuel thirst for revenge']
</code></pre>
    <p class="normal">The next step is tokenization. Language models require input data in the form of sequences—given a sequence of words (tokens), the generation task boils down to predicting the next most likely token in the context. We can utilize the built-in tokenizer from the <code class="Code-In-Text--PACKT-">preprocessing</code> module of Keras.</p>
    <p class="normal">After <a id="_idIndexMarker505"/>cleaning up, we tokenize the input text: this is a <a id="_idIndexMarker506"/>process of extracting individual tokens (words or terms) from a corpus. We utilize the built-in tokenizer to retrieve the tokens and their respective indices. Each document is converted into a series of tokens:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenizer = Tokenizer()
inp_sequences, total_words = get_sequence_of_tokens(corpus)
inp_sequences[:<span class="hljs-number">10</span>]
[[1, 708],
 [1, 708, 251],
 [1, 708, 251, 369],
 [1, 370],
 [1, 370, 709],
 [1, 370, 709, 29],
 [1, 370, 709, 29, 136],
 [1, 370, 709, 29, 136, 710],
 [1, 370, 709, 29, 136, 710, 10],
 [711, 5]]
</code></pre>
    <p class="normal">The vectors like <code class="Code-In-Text--PACKT-">[1,708]</code>, <code class="Code-In-Text--PACKT-">[1,708, 251]</code> represent the n-grams generated from the input data, where an integer is an index of the token in the overall vocabulary generated from the corpus.</p>
    <p class="normal">We have transformed our dataset into a format of sequences of tokens—possibly of different lengths. There are two choices: go with RaggedTensors (which are a slightly more advanced topic in terms of usage) or equalize the lengths to adhere to the standard requirement of most RNN models. For the sake of simplicity of presentation, we proceed with the latter solution: padding sequences shorter than the threshold using the <code class="Code-In-Text--PACKT-">pad_sequence</code> function. This step is easily combined with formatting the data into predictors and labels: </p>
    <pre class="programlisting code"><code class="hljs-code">predictors, label, max_sequence_len =                              generate_padded_sequences(inp_sequences)
</code></pre>
    <p class="normal">We utilize a simple LSTM architecture using the Sequential API:</p>
    <ol>
      <li class="numbered">Input layer: takes the tokenized sequence</li>
      <li class="numbered">LSTM layer: generates <a id="_idIndexMarker507"/>the output using <a id="_idIndexMarker508"/>LSTM units – we take 100 as a default value for the sake of demonstration, but the parameter (along with several others) is customizable</li>
      <li class="numbered">Dropout layer: we regularize the LSTM output to reduce the risk of overfitting</li>
      <li class="numbered">Output layer: generates the most likely output token:
        <pre class="programlisting code"><code class="hljs-code">model = create_model(max_sequence_len, total_words)
model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 23, 10)            31340     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               44400     
_________________________________________________________________
dense_1 (Dense)              (None, 3134)              316534    
=================================================================
Total params: 392,274
Trainable params: 392,274
Non-trainable params: 0
_________________________________________________________________
</code></pre>
      </li>
    </ol>
    <p class="normal">We can now train our model using the standard Keras syntax:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(predictors, label, <span class="hljs-attribute">epochs</span>=100, <span class="hljs-attribute">verbose</span>=2)
</code></pre>
    <p class="normal">Now that we have a fitted model, we can <a id="_idIndexMarker509"/>examine its performance: how good are the headlines <a id="_idIndexMarker510"/>generated by our LSTM based on a seed text? We achieve this by tokenizing the seed text, padding the sequence, and passing it into the model to obtain our predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">print</span> (generate_text(<span class="hljs-string">"united states"</span>, <span class="hljs-number">5</span>, model, max_sequence_len))
United States Shouldnt Sit Still An Atlantic
<span class="hljs-keyword">print</span> (generate_text(<span class="hljs-string">"president trump"</span>, <span class="hljs-number">5</span>, model, max_sequence_len))
President Trump Vs Congress Bird Moving One
<span class="hljs-keyword">print</span> (generate_text(<span class="hljs-string">"joe biden"</span>, <span class="hljs-number">8</span>, model, max_sequence_len))
Joe Biden Infuses The Constitution Invaded Canada Unique Memorial Award
<span class="hljs-keyword">print</span> (generate_text(<span class="hljs-string">"india and china"</span>, <span class="hljs-number">8</span>, model, max_sequence_len))
India And China Deal And The Young Think Again To It
<span class="hljs-keyword">print</span> (generate_text(<span class="hljs-string">"european union"</span>, <span class="hljs-number">4</span>, model, max_sequence_len))
European Union Infuses The Constitution Invaded
</code></pre>
    <p class="normal">As you can see, even with a relatively simple setup (a moderately sized dataset and a vanilla model), we can generate text that looks somewhat realistic. Further fine-tuning would of course allow for more sophisticated content, which is a topic we will cover in <em class="chapterRef">Chapter 10</em>, <em class="italic">Transformers</em>.</p>
    <h2 id="_idParaDest-269" class="title">See also</h2>
    <p class="normal">There are multiple excellent resources <a id="_idIndexMarker511"/>online for learning about RNNs:</p>
    <ul>
      <li class="bullet">For an excellent introduction – with great examples – see the post by Andrej Karpathy: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><span class="url">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</span></a></li>
      <li class="bullet">A curated list of resources (tutorials, repositories) can be found at <a href="https://github.com/kjw0612/awesome-rnn"><span class="url">https://github.com/kjw0612/awesome-rnn</span></a></li>
      <li class="bullet">Another great introduction can be found at <a href="https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf"><span class="url">https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf</span></a></li>
    </ul>
    <h1 id="_idParaDest-270" class="title">Sentiment classification</h1>
    <p class="normal">A popular task in NLP is<a id="_idIndexMarker512"/> sentiment classification: based on the content of a text snippet, identify the sentiment expressed therein. Practical applications include analysis of reviews, survey responses, social media comments, or healthcare materials. </p>
    <p class="normal">We will train our network on the Sentiment140 dataset introduced in <a href="https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf"><span class="url">https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf</span></a>, which contains 1.6 million tweets annotated with three classes: negative, neutral, and positive. In order to avoid issues with locale, we standardize the encoding (this part is best done from the console level and not inside the notebook). The logic is the following: the original dataset contains raw text that—by its very nature—can contain non-standard characters (such as emojis, which are obviously common in social media communication). We want to convert the text to UTF8—the de facto standard for NLP in English. The fastest way to do it is by using a Linux command-line functionality:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">Iconv</code> is a standard tool for conversion between encodings</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">-f</code> and <code class="Code-In-Text--PACKT-">-t</code> flags denote the input encoding and the target one, respectively</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">-o</code> specifies the output file:</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code">iconv -f LATIN1 -t UTF8 training<span class="hljs-number">.1600000</span>.processed.noemoticon.csv -o training_cleaned.csv
</code></pre>
    <h2 id="_idParaDest-271" class="title">How to do it...</h2>
    <p class="normal">We begin by importing the necessary packages as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences
<span class="hljs-keyword">from</span> tensorflow.keras.utils <span class="hljs-keyword">import</span> to_categorical
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> regularizers
</code></pre>
    <p class="normal">Next, we define the hyperparameters of our model:</p>
    <ul>
      <li class="bullet">The embedding dimension is the size of word embedding we will use. In this recipe, we will use GloVe: an unsupervised learning algorithm trained on aggregated word co-occurrence statistics from a combined corpus of Wikipedia and Gigaword. The resulting vectors for (English) words give us an efficient way of representing text and are <a id="_idIndexMarker513"/>commonly referred to as embeddings.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">max_length</code> and <code class="Code-In-Text--PACKT-">padding_type</code> are parameters specifying how we pad the sequences (see previous recipe).</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">training_size</code> specifies<a id="_idIndexMarker514"/> the size of the target corpus.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">test_portion</code> defines the proportion of the data we will use as a holdout.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">dropout_val</code> and <code class="Code-In-Text--PACKT-">nof_units</code> are hyperparameters for the model:</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code">embedding_dim = <span class="hljs-number">100</span>
max_length = <span class="hljs-number">16</span>
trunc_type=<span class="hljs-string">'post'</span>
padding_type=<span class="hljs-string">'post'</span>
oov_tok = <span class="hljs-string">"&lt;OOV&gt;"</span>
training_size=<span class="hljs-number">160000</span>
test_portion=<span class="hljs-number">.1</span>
num_epochs = <span class="hljs-number">50</span>
dropout_val = <span class="hljs-number">0.2</span>
nof_units = <span class="hljs-number">64</span>
</code></pre>
    <p class="normal">Let's encapsulate the model creation step into a function. We define a fairly simple one for our classification task—an embedding layer, followed by regularization and convolution, pooling, and then the RNN layer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_model</span><span class="hljs-function">(</span><span class="hljs-params">dropout_val, nof_units</span><span class="hljs-function">):</span>
    
    model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+<span class="hljs-number">1</span>, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=<span class="hljs-literal">False</span>),
    tf.keras.layers.Dropout(dropout_val),
    tf.keras.layers.Conv1D(<span class="hljs-number">64</span>, <span class="hljs-number">5</span>, activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.MaxPooling1D(pool_size=<span class="hljs-number">4</span>),
    tf.keras.layers.LSTM(nof_units),
    tf.keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)
    ])
    model.compile(loss=<span class="hljs-string">'binary_crossentropy'</span>,optimizer=<span class="hljs-string">'adam'</span>,                  metrics=[<span class="hljs-string">'accuracy'</span>])
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">Collect the <a id="_idIndexMarker515"/>content of the corpus we will train on:</p>
    <pre class="programlisting code"><code class="hljs-code">num_sentences = <span class="hljs-number">0</span>
<span class="hljs-keyword">with</span> open(<span class="hljs-string">"../input/twitter-sentiment-clean-dataset/training_cleaned.csv"</span>) <span class="hljs-keyword">as</span> csvfile:
    reader = csv.reader(csvfile, delimiter=<span class="hljs-string">','</span>)
    <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> reader:
        list_item=[]
        list_item.append(row[<span class="hljs-number">5</span>])
        this_label=row[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">if</span> this_label==<span class="hljs-string">'0'</span>:
            list_item.append(<span class="hljs-number">0</span>)
        <span class="hljs-keyword">else</span>:
            list_item.append(<span class="hljs-number">1</span>)
        num_sentences = num_sentences + <span class="hljs-number">1</span>
        corpus.append(list_item)
</code></pre>
    <p class="normal">Convert to sentence format:</p>
    <pre class="programlisting code"><code class="hljs-code">sentences=[]
labels=[]
random.shuffle(corpus)
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(training_size):
    sentences.append(corpus[x][<span class="hljs-number">0</span>])
    labels.append(corpus[x][<span class="hljs-number">1</span>])
    Tokenize the sentences:
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
vocab_size = len(word_index)
sequences = tokenizer.texts_to_sequences(sentences)
</code></pre>
    <p class="normal">Normalize the sentence lengths with padding (see previous section):</p>
    <pre class="programlisting code"><code class="hljs-code">padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
</code></pre>
    <p class="normal">Divide the dataset into training and holdout sets:</p>
    <pre class="programlisting code"><code class="hljs-code">split = int(test_portion * training_size)
test_sequences = padded[<span class="hljs-number">0</span>:split]
training_sequences = padded[split:training_size]
test_labels = labels[<span class="hljs-number">0</span>:split]
training_labels = labels[split:training_size]
</code></pre>
    <p class="normal">A crucial step in<a id="_idIndexMarker516"/> using RNN-based models for NLP applications is the <code class="Code-In-Text--PACKT-">embeddings</code> matrix:</p>
    <pre class="programlisting code"><code class="hljs-code">embeddings_index = {};
<span class="hljs-keyword">with</span> open(<span class="hljs-string">'../input/glove6b/glove.6B.100d.txt'</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:
        values = line.split();
        word = values[<span class="hljs-number">0</span>];
        coefs = np.asarray(values[<span class="hljs-number">1</span>:], dtype=<span class="hljs-string">'float32'</span>);
        embeddings_index[word] = coefs;
embeddings_matrix = np.zeros((vocab_size+<span class="hljs-number">1</span>, embedding_dim));
<span class="hljs-keyword">for</span> word, i <span class="hljs-keyword">in</span> word_index.items():
    embedding_vector = embeddings_index.get(word);
    <span class="hljs-keyword">if</span> embedding_vector <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        embeddings_matrix[i] = embedding_vector;
</code></pre>
    <p class="normal">With all the preparations completed, we can set up the model:</p>
    <pre class="programlisting code"><code class="hljs-code">model = create_model(dropout_val, nof_units)
model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 16, 100)           13877100  
_________________________________________________________________
dropout (Dropout)            (None, 16, 100)           0         
_________________________________________________________________
conv1d (Conv1D)              (None, 12, 64)            32064     
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 3, 64)             0         
_________________________________________________________________
lstm (LSTM)                  (None, 64)                33024     
_________________________________________________________________
dense (Dense)                (None, 1)                 65        
=================================================================
Total params: 13,942,253
Trainable params: 65,153
Non-trainable params: 13,877,100
_________________________________________________________________
</code></pre>
    <p class="normal">Training is<a id="_idIndexMarker517"/> performed in the usual way:</p>
    <pre class="programlisting code"><code class="hljs-code">num_epochs = <span class="hljs-number">50</span>
history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=<span class="hljs-number">2</span>)
Train on 144000 samples, validate on 16000 samples
Epoch 1/50
144000/144000 - 47s - loss: 0.5685 - acc: 0.6981 - val_loss: 0.5454 - val_acc: 0.7142
Epoch 2/50
144000/144000 - 44s - loss: 0.5296 - acc: 0.7289 - val_loss: 0.5101 - val_acc: 0.7419
Epoch 3/50
144000/144000 - 42s - loss: 0.5130 - acc: 0.7419 - val_loss: 0.5044 - val_acc: 0.7481
Epoch 4/50
144000/144000 - 42s - loss: 0.5017 - acc: 0.7503 - val_loss: 0.5134 - val_acc: 0.7421
Epoch 5/50
144000/144000 - 42s - loss: 0.4921 - acc: 0.7563 - val_loss: 0.5025 - val_acc: 0.7518
Epoch 6/50
144000/144000 - 42s - loss: 0.4856 - acc: 0.7603 - val_loss: 0.5003 - val_acc: 0.7509
</code></pre>
    <p class="normal">We can also <a id="_idIndexMarker518"/>assess the quality of our model visually:</p>
    <pre class="programlisting code"><code class="hljs-code">acc = history.history[<span class="hljs-string">'acc'</span>]
val_acc = history.history[<span class="hljs-string">'val_acc'</span>]
loss = history.history[<span class="hljs-string">'loss'</span>]
val_loss = history.history[<span class="hljs-string">'val_loss'</span>]
epochs = range(len(acc))
plt.plot(epochs, acc, <span class="hljs-string">'r'</span>, label=<span class="hljs-string">'Training accuracy'</span>)
plt.plot(epochs, val_acc, <span class="hljs-string">'b'</span>, label=<span class="hljs-string">'Validation accuracy'</span>)
plt.title(<span class="hljs-string">'Training and validation accuracy'</span>)
plt.legend()
plt.figure()
plt.plot(epochs, loss, <span class="hljs-string">'r'</span>, label=<span class="hljs-string">'Training Loss'</span>)
plt.plot(epochs, val_loss, <span class="hljs-string">'b'</span>, label=<span class="hljs-string">'Validation Loss'</span>)
plt.title(<span class="hljs-string">'Training and validation loss'</span>)
plt.legend()
plt.show()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_09_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.1: Training versus validation accuracy over epochs</p>
    <figure class="mediaobject"><img src="../Images/B16254_09_02.png" alt="Obraz zawierający zrzut ekranu  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 9.2: Training versus validation loss over epochs</p>
    <p class="normal">As we can see from<a id="_idIndexMarker519"/> both graphs, the model already achieves good performance after a limited number of epochs and it stabilizes after that, with only minor fluctuations. Potential improvements would involve early stopping, and extending the size of the dataset.</p>
    <h2 id="_idParaDest-272" class="title">See also</h2>
    <p class="normal">Readers interested in the applications of RNNs to sentiment classification can investigate the <a id="_idIndexMarker520"/>following resources:</p>
    <ul>
      <li class="bullet">TensorFlow documentation tutorial: <a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn"><span class="url">https://www.tensorflow.org/tutorials/text/text_classification_rnn</span></a></li>
      <li class="bullet"><a href="https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49"><span class="url">https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49</span></a> is one of many articles demonstrating the application of RNNs to sentiment detection and it contains an extensive list of references</li>
      <li class="bullet">GloVe documentation can be found at <a href="https://nlp.stanford.edu/projects/glove/"><span class="url">https://nlp.stanford.edu/projects/glove/</span></a> </li>
    </ul>
    <h1 id="_idParaDest-273" class="title">Stock price prediction</h1>
    <p class="normal">Sequential models such as RNNs are naturally well suited to <a id="_idIndexMarker521"/>time series prediction—and one of the <a id="_idIndexMarker522"/>most advertised applications is the prediction of financial quantities, especially prices of different financial instruments. In this recipe, we demonstrate how to apply LSTM to the problem of time series prediction. We will focus on the price of Bitcoin—the most popular cryptocurrency.</p>
    <p class="normal">A disclaimer is in order: this is a demonstration example on a popular dataset. It is not intended as investment advice of any kind; building a reliable time series prediction model applicable in finance is a challenging endeavor, outside the scope of this book.</p>
    <h2 id="_idParaDest-274" class="title">How to do it...</h2>
    <p class="normal">We begin by importing the necessary packages:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np 
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd 
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> LSTM
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler
</code></pre>
    <p class="normal">The general parameters for our task are the future horizon of our prediction and the hyperparameter for the network:</p>
    <pre class="programlisting code"><code class="hljs-code">prediction_days = <span class="hljs-number">30</span>
nof_units =<span class="hljs-number">4</span>
</code></pre>
    <p class="normal">As before, we will encapsulate our model creation step in a function. It accepts a single parameter, <code class="Code-In-Text--PACKT-">units</code>, which is the dimension of the inner cells in LSTM:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_model</span><span class="hljs-function">(</span><span class="hljs-params">nunits</span><span class="hljs-function">):</span>
    <span class="hljs-comment"># Initialising the RNN</span>
    regressor = Sequential()
    <span class="hljs-comment"># Adding the input layer and the LSTM layer</span>
    regressor.add(LSTM(units = nunits, activation = <span class="hljs-string">'sigmoid'</span>, input_shape = (<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>)))
    <span class="hljs-comment"># Adding the output layer</span>
    regressor.add(Dense(units = <span class="hljs-number">1</span>))
    <span class="hljs-comment"># Compiling the RNN</span>
    regressor.compile(optimizer = <span class="hljs-string">'adam'</span>, loss = <span class="hljs-string">'mean_squared_error'</span>)
    
    <span class="hljs-keyword">return</span> regressor
</code></pre>
    <p class="normal">We <a id="_idIndexMarker523"/>can now proceed to load the data, with the <a id="_idIndexMarker524"/>usual formatting of the timestamp. For the sake of our demonstration, we will predict the average daily price—hence the grouping operation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Import the dataset and encode the date</span>
df = pd.read_csv(<span class="hljs-string">"../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv"</span>)
df[<span class="hljs-string">'date'</span>] = pd.to_datetime(df[<span class="hljs-string">'Timestamp'</span>],unit=<span class="hljs-string">'s'</span>).dt.date
group = df.groupby(<span class="hljs-string">'date'</span>)
Real_Price = group[<span class="hljs-string">'Weighted_Price'</span>].mean()
</code></pre>
    <p class="normal">The next step is to split the data into training and test periods:</p>
    <pre class="programlisting code"><code class="hljs-code">df_train= Real_Price[:len(Real_Price)-prediction_days]
df_test= Real_Price[len(Real_Price)-prediction_days:]
</code></pre>
    <p class="normal">Preprocessing could theoretically be avoided, but it tends to help convergence in practice:</p>
    <pre class="programlisting code"><code class="hljs-code">training_set = df_train.values
training_set = np.reshape(training_set, (len(training_set), <span class="hljs-number">1</span>))
sc = MinMaxScaler()
training_set = sc.fit_transform(training_set)
X_train = training_set[<span class="hljs-number">0</span>:len(training_set)<span class="hljs-number">-1</span>]
y_train = training_set[<span class="hljs-number">1</span>:len(training_set)]
X_train = np.reshape(X_train, (len(X_train), <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">Fitting the model is straightforward:</p>
    <pre class="programlisting code"><code class="hljs-code">regressor = create_model(nunits = nof_unit)
regressor.fit(X_train, y_train, batch_size = <span class="hljs-number">5</span>, epochs = <span class="hljs-number">100</span>)
Epoch 1/100
3147/3147 [==============================] - 6s 2ms/step - loss: 0.0319
Epoch 2/100
3147/3147 [==============================] - 3s 928us/step - loss: 0.0198
Epoch 3/100
3147/3147 [==============================] - 3s 985us/step - loss: 0.0089
Epoch 4/100
3147/3147 [==============================] - 3s 1ms/step - loss: 0.0023
Epoch 5/100
3147/3147 [==============================] - 3s 886us/step - loss: 3.3583e-04
Epoch 6/100
3147/3147 [==============================] - 3s 957us/step - loss: 1.0990e-04
Epoch 7/100
3147/3147 [==============================] - 3s 830us/step - loss: 1.0374e-04
Epoch 8/100
</code></pre>
    <p class="normal">With <a id="_idIndexMarker525"/>a fitted model we can generate a <a id="_idIndexMarker526"/>prediction over the forecast horizon, keeping in mind the need to invert our normalization so that the values are back on the original scale:</p>
    <pre class="programlisting code"><code class="hljs-code">test_set = df_test.values
inputs = np.reshape(test_set, (len(test_set), <span class="hljs-number">1</span>))
inputs = sc.transform(inputs)
inputs = np.reshape(inputs, (len(inputs), <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
predicted_BTC_price = regressor.predict(inputs)
predicted_BTC_price = sc.inverse_transform(predicted_BTC_price)
</code></pre>
    <p class="normal">This is <a id="_idIndexMarker527"/>what our forecasted results <a id="_idIndexMarker528"/>look like:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.figure(figsize=(<span class="hljs-number">25</span>,<span class="hljs-number">15</span>), dpi=<span class="hljs-number">80</span>, facecolor=<span class="hljs-string">'w'</span>, edgecolor=<span class="hljs-string">'k'</span>)
ax = plt.gca()  
plt.plot(test_set, color = <span class="hljs-string">'red'</span>, label = <span class="hljs-string">'Real BTC Price'</span>)
plt.plot(predicted_BTC_price, color = <span class="hljs-string">'blue'</span>, label = <span class="hljs-string">'Predicted BTC Price'</span>)
plt.title(<span class="hljs-string">'BTC Price Prediction'</span>, fontsize=<span class="hljs-number">40</span>)
df_test = df_test.reset_index()
x=df_test.index
labels = df_test[<span class="hljs-string">'date'</span>]
plt.xticks(x, labels, rotation = <span class="hljs-string">'vertical'</span>)
<span class="hljs-keyword">for</span> tick <span class="hljs-keyword">in</span> ax.xaxis.get_major_ticks():
    tick.label1.set_fontsize(<span class="hljs-number">18</span>)
<span class="hljs-keyword">for</span> tick <span class="hljs-keyword">in</span> ax.yaxis.get_major_ticks():
    tick.label1.set_fontsize(<span class="hljs-number">18</span>)
plt.xlabel(<span class="hljs-string">'Time'</span>, fontsize=<span class="hljs-number">40</span>)
plt.ylabel(<span class="hljs-string">'BTC Price(USD)'</span>, fontsize=<span class="hljs-number">40</span>)
plt.legend(loc=<span class="hljs-number">2</span>, prop={<span class="hljs-string">'size'</span>: <span class="hljs-number">25</span>})
plt.show()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_09_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.3: Actual price and predicted price over time</p>
    <p class="normal">Overall, it is clear that even a simple <a id="_idIndexMarker529"/>model can generate a reasonable <a id="_idIndexMarker530"/>prediction—with an important caveat: this approach only works as long as the environment is stationary, that is, the nature of the relationship between past and present values remains stable over time. Regime changes and sudden interventions might have a dramatic impact on the price, if for example a major jurisdiction were to restrict the usage of cryptocurrencies (as has been the case over the last decade). Such occurrences can be modeled, but they require more elaborate approaches to feature engineering and are outside the scope of this chapter.</p>
    <h1 id="_idParaDest-275" class="title">Open-domain question answering</h1>
    <p class="normal"><strong class="keyword">Question-answering</strong> (<strong class="keyword">QA</strong>) systems <a id="_idIndexMarker531"/>aim to emulate the human process of searching for information online, with machine learning methods employed to improve the accuracy of the provided answers. In this recipe, we will demonstrate how to <a id="_idIndexMarker532"/>use RNNs to predict long and short responses to <a id="_idIndexMarker533"/>questions about Wikipedia articles. We will use the Google Natural Questions dataset, along with which an excellent visualization<a id="_idIndexMarker534"/> helpful for understanding the idea behind QA can be found at <a href="https://ai.google.com/research/NaturalQuestions/visualization"><span class="url">https://ai.google.com/research/NaturalQuestions/visualization</span></a>.</p>
    <p class="normal">The basic idea can be summarized as follows: for each article-question pair, you must predict/select long- and short-form answers to the question drawn <em class="italic">directly from the article</em>:</p>
    <ul>
      <li class="bullet">A long answer would be a longer section of text that answers the question—several sentences or a paragraph.</li>
      <li class="bullet">A short answer might be a sentence or phrase, or even in some cases a simple YES/NO. The short answers are always contained within, or a subset of, one of the plausible long answers.</li>
      <li class="bullet">A given article can (and very often will) allow for both long <em class="italic">and</em> short answers, depending on the question.</li>
    </ul>
    <p class="normal">The recipe presented in this chapter is adapted from code made public by Xing Han Lu: <a href="https://www.kaggle.com/xhlulu"><span class="url">https://www.kaggle.com/xhlulu</span></a>.</p>
    <h2 id="_idParaDest-276" class="title">How to do it...</h2>
    <p class="normal">As usual, we start by loading the necessary packages. This time we are using the fasttext embeddings for<a id="_idIndexMarker535"/> our representation (available from <a href="https://fasttext.cc/"><span class="url">https://fasttext.cc/</span></a>). Other popular choices include GloVe (used in the sentiment detection section) and<a id="_idIndexMarker536"/> ELMo (<a href="https://allennlp.org/elmo"><span class="url">https://allennlp.org/elmo</span></a>). There is no clearly superior one in terms of performance on NLP tasks, so we'll switch our choices as we go to demonstrate the different possibilities:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> gc
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm_notebook <span class="hljs-keyword">as</span> tqdm
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Model
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> LSTM, Bidirectional, GlobalMaxPooling1D, Dropout
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing <span class="hljs-keyword">import</span> text, sequence
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm_notebook <span class="hljs-keyword">as</span> tqdm
<span class="hljs-keyword">import</span> fasttext
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> load_model
</code></pre>
    <p class="normal">The general<a id="_idIndexMarker537"/> settings are as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">embedding_path = <span class="hljs-string">'/kaggle/input/fasttext-crawl-300d-2m-with-subword/crawl-300d-2m-subword/crawl-300d-2M-subword.bin'</span>
</code></pre>
    <p class="normal">Our next step is to add some boilerplate code to streamline the code flow later. Since the task at hand is a <a id="_idIndexMarker538"/>little more involved than in the <a id="_idIndexMarker539"/>previous instances (or less intuitive), we wrap up more of the preparation work inside the dataset building functions. Due to the size of the dataset, we only load a subset of the training data and sample the negative-labeled data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_train</span><span class="hljs-function">(</span><span class="hljs-params">train_path, n_rows=</span><span class="hljs-number">200000</span><span class="hljs-params">, sampling_rate=</span><span class="hljs-number">15</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">with</span> open(train_path) <span class="hljs-keyword">as</span> f:
        processed_rows = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(n_rows)):
            line = f.readline()
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> line:
                <span class="hljs-keyword">break</span>
            line = json.loads(line)
            text = line[<span class="hljs-string">'document_text'</span>].split(<span class="hljs-string">' '</span>)
            question = line[<span class="hljs-string">'question_text'</span>]
            annotations = line[<span class="hljs-string">'annotations'</span>][<span class="hljs-number">0</span>]
            <span class="hljs-keyword">for</span> i, candidate <span class="hljs-keyword">in</span> enumerate(line[<span class="hljs-string">'long_answer_candidates'</span>]):
                label = i == annotations[<span class="hljs-string">'long_answer'</span>][<span class="hljs-string">'candidate_index'</span>]
                start = candidate[<span class="hljs-string">'start_token'</span>]
                end = candidate[<span class="hljs-string">'end_token'</span>]
                <span class="hljs-keyword">if</span> label <span class="hljs-keyword">or</span> (i % sampling_rate == <span class="hljs-number">0</span>):
                    processed_rows.append({
                        <span class="hljs-string">'text'</span>: <span class="hljs-string">" "</span>.join(text[start:end]),
                        <span class="hljs-string">'is_long_answer'</span>: label,
                        <span class="hljs-string">'question'</span>: question,
                        <span class="hljs-string">'annotation_id'</span>: annotations[<span class="hljs-string">'annotation_id'</span>]
                    })
        train = pd.DataFrame(processed_rows)
        
        <span class="hljs-keyword">return</span> train
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_test</span><span class="hljs-function">(</span><span class="hljs-params">test_path</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">with</span> open(test_path) <span class="hljs-keyword">as</span> f:
        processed_rows = []
        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(f):
            line = json.loads(line)
            text = line[<span class="hljs-string">'document_text'</span>].split(<span class="hljs-string">' '</span>)
            question = line[<span class="hljs-string">'question_text'</span>]
            example_id = line[<span class="hljs-string">'example_id'</span>]
            <span class="hljs-keyword">for</span> candidate <span class="hljs-keyword">in</span> line[<span class="hljs-string">'long_answer_candidates'</span>]:
                start = candidate[<span class="hljs-string">'start_token'</span>]
                end = candidate[<span class="hljs-string">'end_token'</span>]
                processed_rows.append({
                    <span class="hljs-string">'text'</span>: <span class="hljs-string">" "</span>.join(text[start:end]),
                    <span class="hljs-string">'question'</span>: question,
                    <span class="hljs-string">'example_id'</span>: example_id,
                    <span class="hljs-string">'sequence'</span>: <span class="hljs-string">f'</span><span class="hljs-subst">{start}</span><span class="hljs-string">:</span><span class="hljs-subst">{end}</span><span class="hljs-string">'</span>
                })
        test = pd.DataFrame(processed_rows)
    
    <span class="hljs-keyword">return</span> test
</code></pre>
    <p class="normal">With the next function, we train a Keras tokenizer to encode the text and questions into a list of integers (tokenization), then <a id="_idIndexMarker540"/>pad them to a fixed length to form a single NumPy array for text and <a id="_idIndexMarker541"/>another for questions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">compute_text_and_questions</span><span class="hljs-function">(</span><span class="hljs-params">train, test, tokenizer</span><span class="hljs-function">):</span>
    train_text = tokenizer.texts_to_sequences(train.text.values)
    train_questions = tokenizer.texts_to_sequences(train.question.values)
    test_text = tokenizer.texts_to_sequences(test.text.values)
    test_questions = tokenizer.texts_to_sequences(test.question.values)
    
    train_text = sequence.pad_sequences(train_text, maxlen=<span class="hljs-number">300</span>)
    train_questions = sequence.pad_sequences(train_questions)
    test_text = sequence.pad_sequences(test_text, maxlen=<span class="hljs-number">300</span>)
    test_questions = sequence.pad_sequences(test_questions)
    
    <span class="hljs-keyword">return</span> train_text, train_questions, test_text, test_questions
</code></pre>
    <p class="normal">As usual with RNN-based models for NLP, we need an embeddings matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_embedding_matrix</span><span class="hljs-function">(</span><span class="hljs-params">tokenizer, path</span><span class="hljs-function">):</span>
    embedding_matrix = np.zeros((tokenizer.num_words + <span class="hljs-number">1</span>, <span class="hljs-number">300</span>))
    ft_model = fasttext.load_model(path)
    <span class="hljs-keyword">for</span> word, i <span class="hljs-keyword">in</span> tokenizer.word_index.items():
        <span class="hljs-keyword">if</span> i &gt;= tokenizer.num_words - <span class="hljs-number">1</span>:
            <span class="hljs-keyword">break</span>
        embedding_matrix[i] = ft_model.get_word_vector(word)
    
    <span class="hljs-keyword">return</span> embedding_matrix
</code></pre>
    <p class="normal">Next is our model construction step, wrapped up in a function:</p>
    <ol>
      <li class="numbered" value="1">We build two 2-layer bidirectional LSTMs; one to read the questions, and one to read the text</li>
      <li class="numbered">We <a id="_idIndexMarker542"/>concatenate the output and pass it to a fully <a id="_idIndexMarker543"/>connected layer</li>
      <li class="numbered">We use sigmoid on the output:</li>
    </ol>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_model</span><span class="hljs-function">(</span><span class="hljs-params">embedding_matrix</span><span class="hljs-function">):</span>
    embedding = Embedding(
        *embedding_matrix.shape, 
        weights=[embedding_matrix], 
        trainable=<span class="hljs-literal">False</span>, 
        mask_zero=<span class="hljs-literal">True</span>
    )
    
    q_in = Input(shape=(<span class="hljs-literal">None</span>,))
    q = embedding(q_in)
    q = SpatialDropout1D(<span class="hljs-number">0.2</span>)(q)
    q = Bidirectional(LSTM(<span class="hljs-number">100</span>, return_sequences=<span class="hljs-literal">True</span>))(q)
    q = GlobalMaxPooling1D()(q)
    
    
    t_in = Input(shape=(<span class="hljs-literal">None</span>,))
    t = embedding(t_in)
    t = SpatialDropout1D(<span class="hljs-number">0.2</span>)(t)
    t = Bidirectional(LSTM(<span class="hljs-number">150</span>, return_sequences=<span class="hljs-literal">True</span>))(t)
    t = GlobalMaxPooling1D()(t)
    
    hidden = concatenate([q, t])
    hidden = Dense(<span class="hljs-number">300</span>, activation=<span class="hljs-string">'relu'</span>)(hidden)
    hidden = Dropout(<span class="hljs-number">0.5</span>)(hidden)
    hidden = Dense(<span class="hljs-number">300</span>, activation=<span class="hljs-string">'relu'</span>)(hidden)
    hidden = Dropout(<span class="hljs-number">0.5</span>)(hidden)
    
    out1 = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)(hidden)
    
    model = Model(inputs=[t_in, q_in], outputs=out1)
    model.compile(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">With the toolkit that we've defined, we can construct the datasets as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">directory = <span class="hljs-string">'../input/tensorflow2-question-answering/'</span>
train_path = directory + <span class="hljs-string">'simplified-nq-train.jsonl'</span>
test_path = directory + <span class="hljs-string">'simplified-nq-test.jsonl'</span>
train = build_train(train_path)
test = build_test(test_path)
</code></pre>
    <p class="normal">This <a id="_idIndexMarker544"/>is what the dataset looks <a id="_idIndexMarker545"/>like:</p>
    <pre class="programlisting code"><code class="hljs-code">train.head()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_09_04.png" alt="Obraz zawierający stół  Opis wygenerowany automatycznie"/></figure>
    <pre class="programlisting code"><code class="hljs-code">tokenizer = text.Tokenizer(lower=<span class="hljs-literal">False</span>, num_words=<span class="hljs-number">80000</span>)
<span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> tqdm([train.text, test.text, train.question, test.question]):
    tokenizer.fit_on_texts(text.values)
train_target = train.is_long_answer.astype(int).values
train_text, train_questions, test_text, test_questions = compute_text_and_questions(train, test, tokenizer)
<span class="hljs-keyword">del</span> train
</code></pre>
    <p class="normal">We can now construct the model itself:</p>
    <pre class="programlisting code"><code class="hljs-code">embedding_matrix = build_embedding_matrix(tokenizer, embedding_path)
model = build_model(embedding_matrix)
model.summary()
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None)]       0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, None)]       0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, None, 300)    24000300    input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
spatial_dropout1d (SpatialDropo (None, None, 300)    0           embedding[0][0]                  
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (None, None, 300)    0           embedding[1][0]                  
__________________________________________________________________________________________________
bidirectional (Bidirectional)   (None, None, 200)    320800      spatial_dropout1d[0][0]          
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, None, 300)    541200      spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 200)          0           bidirectional[0][0]              
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 300)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 500)          0           global_max_pooling1d[0][0]       
                                                                 global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
dense (Dense)                   (None, 300)          150300      concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 300)          0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 300)          90300       dropout[0][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 300)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            301         dropout_1[0][0]                  
==================================================================================================
Total params: 25,103,201
Trainable params: 1,102,901
Non-trainable params: 24,000,300
__________________________________________________________________________________________________
</code></pre>
    <p class="normal">The <a id="_idIndexMarker546"/>fitting is next, and that proceeds in <a id="_idIndexMarker547"/>the usual manner:</p>
    <pre class="programlisting code"><code class="hljs-code">train_history = model.fit(
    [train_text, train_questions], 
    train_target,
    epochs=<span class="hljs-number">2</span>,
    validation_split=<span class="hljs-number">0.2</span>,
    batch_size=<span class="hljs-number">1024</span>
)
</code></pre>
    <p class="normal">Now, we <a id="_idIndexMarker548"/>can build a test set to have a look at our generated <a id="_idIndexMarker549"/>answers:</p>
    <pre class="programlisting code"><code class="hljs-code">directory = <span class="hljs-string">'/kaggle/input/tensorflow2-question-answering/'</span>
test_path = directory + <span class="hljs-string">'simplified-nq-test.jsonl'</span>
test = build_test(test_path)
submission = pd.read_csv(<span class="hljs-string">"../input/tensorflow2-question-answering/sample_submission.csv"</span>)
test_text, test_questions = compute_text_and_questions(test, tokenizer)
</code></pre>
    <p class="normal">We generate the actual predictions:</p>
    <pre class="programlisting code"><code class="hljs-code">test_target = model.predict([test_text, test_questions], batch_size=<span class="hljs-number">512</span>)
test[<span class="hljs-string">'target'</span>] = test_target
result = (
    test.query(<span class="hljs-string">'target &gt; 0.3'</span>)
    .groupby(<span class="hljs-string">'example_id'</span>)
    .max()
    .reset_index()
    .loc[:, [<span class="hljs-string">'example_id'</span>, <span class="hljs-string">'PredictionString'</span>]]
)
result.head()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_09_05.png" alt="Obraz zawierający stół  Opis wygenerowany automatycznie"/></figure>
    <p class="normal">As you <a id="_idIndexMarker550"/>can see, LSTM allows us to handle fairly abstract tasks such as answering different types of questions. The bulk of the work in this recipe <a id="_idIndexMarker551"/>was around formatting the data into a suitable input format, and then postprocessing the results—the actual modeling occurs in a very similar fashion to that in the preceding chapters. </p>
    <h1 id="_idParaDest-277" class="title">Summary</h1>
    <p class="normal">In this chapter, we have demonstrated the different capabilities of RNNs. They can handle diverse tasks with a sequential component (text generation and classification, time series prediction, and QA) within a unified framework. In the next chapter, we shall introduce transformers: an important architecture class that made it possible to reach new state-of-the-art results with NLP problems. </p>
  </div>
</body></html>