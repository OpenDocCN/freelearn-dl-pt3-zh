["```\nInitialize  for every state-action pair\n\nfor  episodes:\n\n    while  is not a final state:\n         # env() take a step in the environment\n\n```", "```\ndef SARSA(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n    nA = env.action_space.n\n    nS = env.observation_space.n\n    test_rewards = []\n    Q = np.zeros((nS, nA))\n    games_reward = []\n```", "```\n    for ep in range(num_episodes):\n        state = env.reset()\n        done = False\n        tot_rew = 0\n\n        if eps > 0.01:\n            eps -= eps_decay\n\n        action = eps_greedy(Q, state, eps)\n```", "```\n        while not done:\n            next_state, rew, done, _ = env.step(action) # Take one step in the environment\n\n            next_action = eps_greedy(Q, next_state, eps)\n            Q[state][action] = Q[state][action] + lr*(rew + gamma*Q[next_state][next_action] - Q[state][action]) # (4.5)\n            state = next_state\n            action = next_action\n           tot_rew += rew\n            if done:\n                games_reward.append(tot_rew)\n```", "```\n        if (ep % 300) == 0:\n            test_rew = run_episodes(env, Q, 1000)\n            print(\"Episode:{:5d} Eps:{:2.4f} Rew:{:2.4f}\".format(ep, eps, test_rew))\n            test_rewards.append(test_rew)\n    return Q\n```", "```\ndef eps_greedy(Q, s, eps=0.1):\n    if np.random.uniform(0,1) < eps:\n        # Choose a random action\n        return np.random.randint(Q.shape[1])\n    else:\n    # Choose the greedy action\n    return greedy(Q, s)\n```", "```\ndef greedy(Q, s):    \n    return np.argmax(Q[s])\n\n```", "```\ndef run_episodes(env, Q, num_episodes=100, to_print=False):\n    tot_rew = []\n    state = env.reset()\n    for _ in range(num_episodes):\n        done = False\n        game_rew = 0\n        while not done:\n            next_state, rew, done, _ = env.step(greedy(Q, state))\n            state = next_state\n            game_rew += rew \n            if done:\n                state = env.reset()\n                tot_rew.append(game_rew)\n    if to_print:\n        print('Mean score: %.3f of %i games!'%(np.mean(tot_rew), num_episodes))\n    else:\n        return np.mean(tot_rew)\n```", "```\nif __name__ == '__main__':\n    env = gym.make('Taxi-v2')\n    env.reset()\n    Q = SARSA(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)\n```", "```\nInitialize  for every state-action pair\n\nfor  episodes:\n\n    while  is not a final state:\n\n         # env() take a step in the environment\n\n```", "```\ndef Q_learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n    nA = env.action_space.n\n    nS = env.observation_space.n\n\n    # Q(s,a) -> each row is a different state and each columns represent a different action\n    Q = np.zeros((nS, nA))\n\n    games_reward = []\n    test_rewards = []\n```", "```\n    for ep in range(num_episodes):\n        state = env.reset()\n        done = False\n        tot_rew = 0\n        if eps > 0.01:\n            eps -= eps_decay\n```", "```\n        while not done:\n            action = eps_greedy(Q, state, eps)\n            next_state, rew, done, _ = env.step(action) # Take one step in the environment\n\n            # get the max Q value for the next state\n            Q[state][action] = Q[state][action] + lr*(rew + gamma*np.max(Q[next_state]) - Q[state][action]) # (4.6)\n            state = next_state\n            tot_rew += rew\n\n            if done:\n                games_reward.append(tot_rew)\n```", "```\n        if (ep % 300) == 0:\n            test_rew = run_episodes(env, Q, 1000)\n            print(\"Episode:{:5d} Eps:{:2.4f} Rew:{:2.4f}\".format(ep, eps, test_rew))\n            test_rewards.append(test_rew)\n    return Q\n```", "```\nif __name__ == '__main__':\n    env = gym.make('Taxi-v2')\n    Q = Q_learning(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)\n```"]