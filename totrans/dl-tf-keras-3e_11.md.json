["```\n if np.random.rand() <= epsilon:\n        a = random.randrange(action_size)\n  else:\n        a = np.argmax(model.predict(s)) \n```", "```\npip install gym \n```", "```\npip install gym[all] \n```", "```\npip install box2d-py \n```", "```\nfrom gym import envs\n\nenvall = envs.registry.all()\nlen(envall) \n```", "```\nfrom tqdm import tqdm\nList = []\nfor e in tqdm(envall):\n    try:\n        env = e.make()\n        List.append([e.id, env.observation_space, env.action_space, env.reward_range])\n        env.close() \n    except:\n        continue \n```", "```\nenv = gym.make('MountainCar-v0')\nprint(f\"The Observation space is        {env.observation_space}\" )\nprint(f\"Upper Bound for Env Observation {env.observation_space.high}\")\nprint(f\"Lower Bound for Env Observation {env.observation_space.low}\")\nprint(f\"Action Space                    {env.action_space}\")\nenv.seed(0)\nobs = env.reset()\nprint(f\"The initial observation is      {obs}\")\n# Take a random actionget the new observation space\nnew_obs, reward, done, info = env.step(env.action_space.sample())\nprint(f\"The new observation is          {new_obs}\")\nenv.close() \n```", "```\ne = 'LunarLander-v2'\nenv = gym.make(e)\nobs = env.reset() \nimg = env.render(mode='rgb_array')\nenv.close()\nplt.imshow(img) \n```", "```\ne = 'CartPole-v0'\nenv = gym.make(e)\nenv.reset()\nimg = env.render(mode='rgb_array')\nenv.close()\nplt.imshow(img) \n```", "```\ne = 'SpaceInvaders-v0'\nenv = gym.make(e)\nenv.reset()\nimg = env.render(mode='rgb_array')\nenv.close()\nplt.imshow(img) \n```", "```\nimport gym\nenv_name = 'Breakout-v0'\nenv = gym.make(env_name)\nobs = env.reset()\nenv.render() \n```", "```\nenv.close() \n```", "```\nimport gym\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation \n```", "```\nenv_name = 'Breakout-v0'\nenv = gym.make(env_name) \n```", "```\nframes = [] # array to store state space at each step\nenv.reset()\ndone = False\nfor _ in range(300): \n    #print(done)\n    frames.append(env.render(mode='rgb_array'))\n    obs,reward,done, _ = env.step(env.action_space.sample())\n    if done:\n        break \n```", "```\npatch = plt.imshow(frames[0])\nplt.axis('off')\ndef animate(i):\n    patch.set_data(frames[i])\n    anim = animation.FuncAnimation(plt.gcf(), animate, \\\n        frames=len(frames), interval=10)\n    anim.save('random_agent.gif', writer='imagemagick') \n```", "```\nfrom collections import deque\nfrom gym import spaces\nimport numpy as np\n#Class to concat observations\nclass ConcatObservations(gym.Wrapper):\n    def __init__(self, env, n):\n        gym.Wrapper.__init__(self, env)\n        shape = env.observation_space.shape\n        self.n = n\n        self.frames = deque([], maxlen=n)\n        self.observation_space = \\\n            spaces.Box(low=0, high=255, shape=((n,) + shape), dtype=env.observation_space.dtype)\n    def reset(self):  #reset function\n        obs = self.env.reset()\n        for _ in range(self.n):\n            self.frames.append(obs)\n        return self._get_obs()\n    def step(self, action): #step function\n        obs, reward, done, info = self.env.step(action)\n        self.frames.append(obs)\n        return self._get_obs(), reward, done, info\n    def _get_obs(self):\n        return np.array(self.frames) \n```", "```\nenv = gym.make(\"BreakoutNoFrameskip-v4\")\nprint(f\"The original observation space is  {env.observation_space}\") \n```", "```\n### OUTPUT: \n```", "```\n>>>The original observation space is  Box(0, 255, (210, 160, 3), uint8) \n```", "```\nenv = ConcatObservations(env, 4)\nprint(f\"The new observation space is  {env.observation_space}\") \n```", "```\n### OUTPUT: \n```", "```\nThe new observation space is  Box(0, 255, (4, 210, 160, 3), uint8) \n```", "```\nclass ClippedRewards(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n        self.reward_range = (-10,10)\n    def reward(self, reward):\n        \"\"\"Clip to {+10, 0, -10} by its sign.\"\"\"\n        return reward if reward >= -10 and reward <= 10 else 10 * np.sign(reward) \n```", "```\nenv = ClippedRewards(gym.make(\"CartPole-v0\"))\nprint(f'Clipped reward range: {env.reward_range}')\nenv.close() \n```", "```\n### OUTPUT: \n```", "```\nClipped reward range: (-10, 10) \n```", "```\nimport gym\nenv = gym.make(\"Breakout-v0\")\nenv = gym.wrappers.Monitor(env, 'recording', force=True)\nobservation = env.reset()\nfor _ in range(1000):\n    #env.render()\n    action = env.action_space.sample()\n    # your agent here (this takes random actions)\n    observation, reward, done, info = env.step(action)\n    if done:\n        observation = env.reset()\nenv.close() \n```", "```\n!pip install pyglet\n!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n!pip install gym pyvirtualdisplay > /dev/null 2>&1 \n```", "```\nfrom pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(600, 400))\ndisplay.start() \n```", "```\n!wget http://www.atarimania.com/roms/Roms.rar\n!mkdir /content/ROM/\n!unrar e /content/Roms.rar /content/ROM/\n!python -m atari_py.import_roms /content/ROM/ \n```", "```\nimport random\nimport gym\nimport math\nimport numpy as np\nfrom collections import deque\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam \n```", "```\nEPOCHS = 1000\nTHRESHOLD = 45\nMONITOR = True \n```", "```\nclass DQN():\n    def __init__(self, env_string, batch_size=64):\n        self.memory = deque(maxlen=100000)\n        self.env = gym.make(env_string)\n        input_size = self.env.observation_space.shape[0]\n        action_size = self.env.action_space.n\n        self.batch_size = batch_size\n        self.gamma = 1.0\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n\n        alpha=0.01\n        alpha_decay=0.01\n        if MONITOR: self.env = gym.wrappers.Monitor(self.env,\n        'data/'+env_string, force=True)\n\n        # Init model\n        self.model = Sequential()\n        self.model.add(Dense(24, input_dim=input_size,\n        activation='tanh'))\n        self.model.add(Dense(48, activation='tanh'))\n        self.model.add(Dense(action_size, activation='linear'))\n        self.model.compile(loss='mse', optimizer=Adam(lr=alpha,\n        decay=alpha_decay)) \n```", "```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 24)                120       \n\n dense_1 (Dense)             (None, 48)                1200      \n\n dense_2 (Dense)             (None, 2)                 98        \n\n=================================================================\nTotal params: 1,418\nTrainable params: 1,418\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\ndef remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\ndef replay(self, batch_size):\n        x_batch, y_batch = [], []\n        minibatch = random.sample(self.memory, min(len(self.memory),\n        batch_size))\n        for state, action, reward, next_state, done in minibatch:\n             y_target = self.model.predict(state)\n             y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n             x_batch.append(state[0])\n             y_batch.append(y_target[0])\n\n        self.model.fit(np.array(x_batch), np.array(y_batch),\n        batch_size=len(x_batch), verbose=0) \n```", "```\ndef choose_action(self, state, epsilon):\n        if np.random.random() <= epsilon:\n            return self.env.action_space.sample()\n        else:\n            return np.argmax(self.model.predict(state)) \n```", "```\ndef train(self):\n    scores = deque(maxlen=100)\n    avg_scores = []\n    for e in range(EPOCHS):\n        state = self.env.reset()\n        state = self.preprocess_state(state)\n        done = False\n        i = 0\n        while not done:\n            action = self.choose_action(state,self.epsilon)\n            next_state, reward, done, _ = self.env.step(action)\n            next_state = self.preprocess_state(next_state)\n            self.remember(state, action, reward, next_state, done)\n            state = next_state\n            self.epsilon = max(self.epsilon_min,\n            self.epsilon_decay*self.epsilon) # decrease epsilon\n            i += 1\n        scores.append(i)\n        mean_score = np.mean(scores)\n        avg_scores.append(mean_score)\n        if mean_score >= THRESHOLD and e >= 100:\n            print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n            return avg_scores\n        if e % 100 == 0:\n            print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n    self.replay(self.batch_size)\n    print('Did not solve after {} episodes :('.format(e))\n    return avg_scores \n```", "```\ndef preprocess_state(self, state):\n    return np.reshape(state, [1, self.input_size]) \n```", "```\nenv_string = 'CartPole-v0'\nagent = DQN(env_string)\nscores = agent.train() \n```", "```\n[Episode 0] - Mean survival time over last 100 episodes was 28.0 ticks.\n[Episode 100] - Mean survival time over last 100 episodes was 15.71 ticks.\n[Episode 200] - Mean survival time over last 100 episodes was 27.81 ticks.\nRan 259 episodes. Solved after 159 trials ✔ \n```", "```\nimport matplotlib.pyplot as plt\nplt.plot(scores)\nplt.show() \n```", "```\nagent.env.close() \n```", "```\ndef preprocess_state(self, img):\n    img_temp = img[31:195]  # Choose the important area of the image\n    img_temp = tf.image.rgb_to_grayscale(img_temp)\n    img_temp = tf.image.resize(img_temp, [self.IM_SIZE, self.IM_SIZE],\n    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    img_temp = tf.cast(img_temp, tf.float32)\n    return img_temp[:,:,0] \n```", "```\ndef combine_images(self, img1, img2):\n    if len(img1.shape) == 3 and img1.shape[0] == self.m:\n        im = np.append(img1[1:,:, :],np.expand_dims(img2,0), axis=2)\n        return tf.expand_dims(im, 0)\n    else:\n        im = np.stack([img1]*self.m, axis = 2)\n        return tf.expand_dims(im, 0) \n```", "```\ndef __init__(self, env_string,batch_size=64, IM_SIZE = 84, m = 4):\n    self.memory = deque(maxlen=5000)\n    self.env = gym.make(env_string)\n    input_size = self.env.observation_space.shape[0]\n    action_size = self.env.action_space.n\n    self.batch_size = batch_size\n    self.gamma = 1.0\n    self.epsilon = 1.0\n    self.epsilon_min = 0.01\n    self.epsilon_decay = 0.995\n    self.IM_SIZE = IM_SIZE\n    self.m = m\n\n    alpha=0.01\n    alpha_decay=0.01\n    if MONITOR: self.env = gym.wrappers.Monitor(self.env, '../data/'+env_string, force=True)\n\n    # Init model\n    self.model = Sequential()\n    self.model.add( Conv2D(32, 8, (4,4), activation='relu',padding='valid', input_shape=(IM_SIZE, IM_SIZE, m)))\n    self.model.add( Conv2D(64, 4, (2,2), activation='relu',padding='valid'))\n    self.model.add( Conv2D(64, 3, (1,1), activation='relu',padding='valid'))\n    self.model.add(Flatten())\n    self.model.add(Dense(512, activation='elu'))\n    self.model.add(Dense(action_size, activation='linear'))\n    self.model.compile(loss='mse', optimizer=Adam(lr=alpha, decay=alpha_decay)) \n```", "```\ndef train(self):\n    scores = deque(maxlen=100)\n    avg_scores = []\n\n    for e in range(EPOCHS):\n        state = self.env.reset()\n        state = self.preprocess_state(state)\n        state = self.combine_images(state, state)\n        done = False\n        i = 0\n        while not done:\n            action = self.choose_action(state,self.epsilon)\n            next_state, reward, done, _ = self.env.step(action)\n            next_state = self.preprocess_state(next_state)\n            next_state = self.combine_images(next_state, state)\n            #print(next_state.shape)\n            self.remember(state, action, reward, next_state, done)\n            state = next_state\n            self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n            i += reward\n        scores.append(i)\n        mean_score = np.mean(scores)\n        avg_scores.append(mean_score)\n        if mean_score >= THRESHOLD and e >= 100:\n            print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n            return avg_scores\n        if e % 100 == 0:\n            print('[Episode {}] - Score over last 100 episodes was {}.'.format(e, mean_score))\n        self.replay(self.batch_size)\n\n    print('Did not solve after {} episodes :('.format(e))\n    return avg_scores \n```"]