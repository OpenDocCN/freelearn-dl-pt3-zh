["```\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    import numpy as np\n    data = tfds.load(\"iris\", split=\"train\") \n    ```", "```\n    for batch in data.batch(batch_size, drop_remainder=True):\n        labels = tf.one_hot(batch['label'], 3)\n        X = batch['features']\n        X = (X - np.mean(X)) / np.std(X) \n    ```", "```\n    epochs = 1000 \n    batch_size = 32\n    input_size = 4\n    output_size = 3\n    learning_rate = 0.001 \n    ```", "```\n    weights = tf.Variable(tf.random.normal(shape=(input_size, \n                                                  output_size), \n                                            dtype=tf.float32))\n    biases  = tf.Variable(tf.random.normal(shape=(output_size,), \n                                           dtype=tf.float32)) \n    ```", "```\n    logits = tf.add(tf.matmul(X, weights), biases) \n    ```", "```\n    loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(labels, logits)) \n    ```", "```\n    optimizer = tf.optimizers.SGD(learning_rate)\n    with tf.GradientTape() as tape:\n       logits = tf.add(tf.matmul(X, weights), biases)\n       loss = tf.reduce_mean(\n          tf.nn.softmax_cross_entropy_with_logits(labels, logits))\n    gradients = tape.gradient(loss, [weights, biases])\n    optimizer.apply_gradients(zip(gradients, [weights, biases])) \n    ```", "```\n    print(f\"final loss is: {loss.numpy():.3f}\")\n    preds = tf.math.argmax(tf.add(tf.matmul(X, weights), biases), axis=1)\n    ground_truth = tf.math.argmax(labels, axis=1)\n    for y_true, y_pred in zip(ground_truth.numpy(), preds.numpy()):\n        print(f\"real label: {y_true} fitted: {y_pred}\") \n    ```", "```\n    row_dim, col_dim = 3, 3\n    zero_tsr = tf.zeros(shape=[row_dim, col_dim], dtype=tf.float32) \n    ```", "```\n    ones_tsr = tf.ones([row_dim, col_dim]) \n    ```", "```\n    filled_tsr = tf.fill([row_dim, col_dim], 42) \n    ```", "```\n    constant_tsr = tf.constant([1,2,3]) \n    ```", "```\n    zeros_similar = tf.zeros_like(constant_tsr) \n    ones_similar = tf.ones_like(constant_tsr) \n    ```", "```\n    linear_tsr = tf.linspace(start=0.0, stop=1.0, num=3) \n    ```", "```\n    integer_seq_tsr = tf.range(start=6, limit=15, delta=3) \n    ```", "```\n    randunif_tsr = tf.random.uniform([row_dim, col_dim], \n                                     minval=0, maxval=1) \n    ```", "```\nrandnorm_tsr = tf.random.normal([row_dim, col_dim], \n                                 mean=0.0, stddev=1.0) \n```", "```\nruncnorm_tsr = tf.random.truncated_normal([row_dim, col_dim], \n                                          mean=0.0, stddev=1.0) \n```", "```\nshuffled_output = tf.random.shuffle(input_tensor) \ncropped_output = tf.image.random_crop(input_tensor, crop_size) \n```", "```\nheight, width = (64, 64)\nmy_image = tf.random.uniform([height, width, 3], minval=0,\n         maxval=255, dtype=tf.int32)\ncropped_image = tf.image.random_crop(my_image, \n       [height//2, width//2, 3]) \n```", "```\nmy_var = tf.Variable(tf.zeros([row_dim, col_dim])) \n```", "```\ntf.executing_eagerly()\nTrue \n```", "```\nx = [[2.]]\nm = tf.matmul(x, x)\nprint(\"the result is {}\".format(m))\nthe result is [[4.]] \n```", "```\n    identity_matrix = tf.linalg.diag([1.0, 1.0, 1.0]) \n    A = tf.random.truncated_normal([2, 3]) \n    B = tf.fill([2,3], 5.0) \n    C = tf.random.uniform([3,2]) \n    D = tf.convert_to_tensor(np.array([[1., 2., 3.],\n                                       [-3., -7., -1.],\n                                       [0., 5., -2.]]), \n                             dtype=tf.float32) \n    print(identity_matrix)\n\n    [[ 1\\.  0\\.  0.] \n     [ 0\\.  1\\.  0.] \n     [ 0\\.  0\\.  1.]] \n    print(A) \n    [[ 0.96751703  0.11397751 -0.3438891 ] \n     [-0.10132604 -0.8432678   0.29810596]] \n    print(B) \n    [[ 5\\.  5\\.  5.] \n     [ 5\\.  5\\.  5.]] \n    print(C)\n    [[ 0.33184157  0.08907614] \n     [ 0.53189191  0.67605299] \n     [ 0.95889051 0.67061249]] \n    ```", "```\n    print(D) \n    [[ 1\\.  2\\.  3.] \n     [-3\\. -7\\. -1.] \n     [ 0\\.  5\\. -2.]] \n    ```", "```\n    print(A+B) \n    [[ 4.61596632  5.39771316  4.4325695 ] \n     [ 3.26702736  5.14477345  4.98265553]] \n    print(B-B)\n    [[ 0\\.  0\\.  0.] \n     [ 0\\.  0\\.  0.]] \n    print(tf.matmul(B, identity_matrix)) \n    [[ 5\\.  5\\.  5.] \n     [ 5\\.  5\\.  5.]] \n    ```", "```\n    print(tf.multiply(D, identity_matrix))\n    [[ 1\\.  0\\.  0.] \n     [-0\\. -7\\. -0.] \n     [ 0\\.  0\\. -2.]] \n    ```", "```\n    print(tf.transpose(C)) \n    [[0.33184157 0.53189191 0.95889051]\n     [0.08907614 0.67605299 0.67061249]] \n    ```", "```\n    print(tf.linalg.det(D))\n    -38.0 \n    ```", "```\n    print(tf.linalg.inv(D))\n    [[-0.5        -0.5        -0.5       ] \n     [ 0.15789474  0.05263158  0.21052632] \n     [ 0.39473684  0.13157895  0.02631579]] \n    ```", "```\n    print(tf.linalg.cholesky(identity_matrix))\n    [[ 1\\.  0\\.  1.] \n     [ 0\\.  1\\.  0.] \n     [ 0\\.  0\\.  1.]] \n    ```", "```\n    print(tf.linalg.eigh(D))\n    [[-10.65907521  -0.22750691   2.88658212] \n     [  0.21749542   0.63250104  -0.74339638] \n     [  0.84526515   0.2587998    0.46749277] \n     [ -0.4880805    0.73004459   0.47834331]] \n    ```", "```\nimport tensorflow as tf \n```", "```\n    print(tf.math.divide(3, 4))\n    0.75 \n    print(tf.math.truediv(3, 4)) \n    tf.Tensor(0.75, shape=(), dtype=float64) \n    ```", "```\n    print(tf.math.floordiv(3.0,4.0)) \n    tf.Tensor(0.0, shape=(), dtype=float32) \n    ```", "```\n    print(tf.math.mod(22.0, 5.0))\n    tf.Tensor(2.0, shape=(), dtype=float32) \n    ```", "```\n    print(tf.linalg.cross([1., 0., 0.], [0., 1., 0.]))\n    tf.Tensor([0\\. 0\\. 1.], shape=(3,), dtype=float32) \n    ```", "```\n# Tangent function (tan(pi/4)=1) \ndef pi_tan(x):\n    return tf.tan(3.1416/x)\nprint(pi_tan(4))\ntf.Tensor(1.0000036, shape=(), dtype=float32) \n```", "```\ndef custom_polynomial(value): \n    return tf.math.subtract(3 * tf.math.square(value), value) + 10\nprint(custom_polynomial(11))\ntf.Tensor(362, shape=(), dtype=int32) \n```", "```\n    print(tf.nn.relu([-3., 3., 10.]))\n    tf.Tensor([ 0\\.  3\\. 10.], shape=(3,), dtype=float32) \n    ```", "```\n    print(tf.nn.relu6([-3., 3., 10.]))\n    tf.Tensor([ 0\\.  3\\. 6.], shape=(3,), dtype=float32) \n    ```", "```\n    print(tf.nn.sigmoid([-1., 0., 1.]))\n    tf.Tensor([0.26894143 0.5 0.7310586 ], shape=(3,), dtype=float32) \n    ```", "```\n    ((exp(x) â€“ exp(-x))/(exp(x) + exp(-x)) \n    ```", "```\n    print(tf.nn.tanh([-1., 0., 1.]))\n    tf.Tensor([-0.7615942  0\\. 0.7615942], shape=(3,), dtype=float32) \n    ```", "```\n    print(tf.nn.softsign([-1., 0., -1.]))\n    tf.Tensor([-0.5  0\\.  -0.5], shape=(3,), dtype=float32) \n    ```", "```\n    print(tf.nn.softplus([-1., 0., -1.]))\n    tf.Tensor([0.31326166 0.6931472  0.31326166], shape=(3,), dtype=float32) \n    ```", "```\n    print(tf.nn.elu([-1., 0., -1.])) \n    tf.Tensor([-0.63212055  0\\. -0.63212055], shape=(3,), dtype=float32) \n    ```", "```\ndef swish(x):\n    return x * tf.nn.sigmoid(x)\nprint(swish([-1., 0., 1.]))\ntf.Tensor([-0.26894143  0\\.  0.7310586 ], shape=(3,), dtype=float32) \n```", "```\npip install tensorflow-datasets \n```", "```\n    import tensorflow_datasets as tfds\n    iris = tfds.load('iris', split='train') \n    ```", "```\n    import tensorflow_datasets as tfds\n    birthdata_url = 'https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/birthweight.dat' \n    path = tf.keras.utils.get_file(birthdata_url.split(\"/\")[-1], birthdata_url)\n    def map_line(x):\n        return tf.strings.to_number(tf.strings.split(x))\n    birth_file = (tf.data\n                  .TextLineDataset(path)\n                  .skip(1)     # Skip first header line\n                  .map(map_line)\n                 ) \n    ```", "```\n    import tensorflow_datasets as tfds\n    housing_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n    path = tf.keras.utils.get_file(housing_url.split(\"/\")[-1], housing_url)\n    def map_line(x):\n        return tf.strings.to_number(tf.strings.split(x))\n    housing = (tf.data\n               .TextLineDataset(path)\n               .map(map_line)\n              ) \n    ```", "```\n    import tensorflow_datasets as tfds\n    mnist = tfds.load('mnist', split=None)\n    mnist_train = mnist['train']\n    mnist_test = mnist['test'] \n    ```", "```\n    import tensorflow_datasets as tfds\n    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n    path = tf.keras.utils.get_file(zip_url.split(\"/\")[-1], zip_url, extract=True)\n    path = path.replace(\"smsspamcollection.zip\", \"SMSSpamCollection\")\n    def split_text(x):\n        return tf.strings.split(x, sep='\\t')\n    text_data = (tf.data\n                 .TextLineDataset(path)\n                 .map(split_text)\n                ) \n    ```", "```\n    import tensorflow_datasets as tfds\n    movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n    path = tf.keras.utils.get_file(movie_data_url.split(\"/\")[-1], movie_data_url, extract=True)\n    path = path.replace('.tar.gz', '')\n    with open(path+filename, 'r', encoding='utf-8', errors='ignore') as movie_file:\n        for response, filename in enumerate(['\\\\rt-polarity.neg', '\\\\rt-polarity.pos']):\n            with open(path+filename, 'r') as movie_file:\n                for line in movie_file:\n                    review_file.write(str(response) + '\\t' + line.encode('utf-8').decode())\n\n    def split_text(x):\n        return tf.strings.split(x, sep='\\t')\n    movies = (tf.data\n              .TextLineDataset('movie_reviews.txt')\n              .map(split_text)\n             ) \n    ```", "```\n    import tensorflow_datasets as tfds\n    ds, info = tfds.load('cifar10', shuffle_files=True, with_info=True)\n    print(info)\n    cifar_train = ds['train']\n    cifar_test = ds['test'] \n    ```", "```\n    import tensorflow_datasets as tfds\n    shakespeare_url = 'https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/shakespeare.txt'\n    path = tf.keras.utils.get_file(shakespeare_url.split(\"/\")[-1], shakespeare_url)\n    def split_text(x):\n        return tf.strings.split(x, sep='\\n')\n    shakespeare_text = (tf.data\n                        .TextLineDataset(path)\n                        .map(split_text)\n                       ) \n    ```", "```\n    import os\n    import pandas as pd\n    from zipfile import ZipFile\n    from urllib.request import urlopen, Request\n    import tensorflow_datasets as tfds\n    sentence_url = 'https://www.manythings.org/anki/deu-eng.zip'\n    r = Request(sentence_url, headers={'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11'})\n    b2 = [z for z in sentence_url.split('/') if '.zip' in z][0] #gets just the '.zip' part of the url\n    with open(b2, \"wb\") as target:\n        target.write(urlopen(r).read()) #saves to file to disk\n    with ZipFile(b2) as z:\n        deu = [line.split('\\t')[:2] for line in z.open('deu.txt').read().decode().split('\\n')]\n    os.remove(b2) #removes the zip file\n    # saving to disk prepared en-de sentence file\n    with open(\"deu.txt\", \"wb\") as deu_file:\n        for line in deu:\n            data = \",\".join(line)+'\\n'\n            deu_file.write(data.encode('utf-8'))\n\n    def split_text(x):\n        return tf.strings.split(x, sep=',')\n    text_data = (tf.data\n                 .TextLineDataset(\"deu.txt\")\n                 .map(split_text)\n                ) \n    ```", "```\nimport tensorflow_datasets as tfds\ndataset_name = \"...\"\ndata = tfds.load(dataset_name, split=None)\ntrain = data['train']\ntest = data['test'] \n```"]