["```\n    import numpy as np\n    ```", "```\n    import matplotlib.pyplot as plt\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow import keras\n    ```", "```\n    #CSV sales data\n    ```", "```\n    url = 'https://raw.githubusercontent.com/oluwole-packt/datasets/main/sales_data.csv'\n    ```", "```\n    # Load the CSV data into a pandas DataFrame\n    ```", "```\n    df = pd.read_csv(url)\n    ```", "```\n    df['Date'] = pd.to_datetime(df['Date'])\n    ```", "```\n    df.set_index('Date', inplace=True)\n    ```", "```\n    data = df['Sales'].values\n    ```", "```\n    window_size = 20\n    ```", "```\n    X, y = [], []\n    ```", "```\n    for i in range(window_size, len(data)):\n    ```", "```\n        X.append(data[i-window_size:i])\n    ```", "```\n        y.append(data[i])\n    ```", "```\n    X = np.array(X)\n    ```", "```\n    y = np.array(y)\n    ```", "```\n    train_size = int(len(X) * 0.8)\n    ```", "```\n    X_train, X_val = X[:train_size], X[train_size:]\n    ```", "```\n    y_train, y_val = y[:train_size], y[train_size:]\n    ```", "```\n    batch_size = 128\n    ```", "```\n    buffer_size = 10000\n    ```", "```\n    train_data = tf.data.Dataset.from_tensor_slices(\n    ```", "```\n        (X_train, y_train))\n    ```", "```\n    train_data = train_data.cache().shuffle(\n    ```", "```\n        buffer_size).batch(batch_size).prefetch(\n    ```", "```\n        tf.data.experimental.AUTOTUNE)\n    ```", "```\n    # Model\n    ```", "```\n    model = Sequential()\n    ```", "```\n    model.add(Dense(10, activation='relu',\n    ```", "```\n        input_shape=(window_size,)))\n    ```", "```\n    model.add(Dense(10, activation='relu'))\n    ```", "```\n    model.add(Dense(1))\n    ```", "```\n    # ExponentialDecay\n    ```", "```\n    lr_exp = tf.keras.optimizers.schedules.ExponentialDecay(\n    ```", "```\n        initial_learning_rate=0.1,\n    ```", "```\n        decay_steps=100, decay_rate=0.96)\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(\n    ```", "```\n        learning_rate=lr_exp)\n    ```", "```\n    model.compile(optimizer=optimizer, loss='mse')\n    ```", "```\n    history_exp = model.fit(X_train, y_train, epochs=100)\n    ```", "```\n    # Evaluation\n    ```", "```\n    forecast_exp = model.predict(X_val)\n    ```", "```\n    mae_exp = mean_absolute_error(y_val, forecast_exp)\n    ```", "```\n    mse_exp = mean_squared_error(y_val, forecast_exp)\n    ```", "```\n    # Plot\n    ```", "```\n    plt.plot(forecast_exp,\n    ```", "```\n        label='Exponential Decay Predicted')\n    ```", "```\n    plt.plot(y_val, label='Actual')\n    ```", "```\n    plt.title('Exponential Decay LR')\n    ```", "```\n    plt.legend()\n    ```", "```\n    plt.show()\n    ```", "```\n    # PiecewiseConstantDecay\n    ```", "```\n    lr_piecewise = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n    ```", "```\n        [30, 60], [0.1, 0.01, 0.001])\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(\n    ```", "```\n        learning_rate=lr_piecewise)\n    ```", "```\n    model.compile(optimizer=optimizer, loss='mse')\n    ```", "```\n    history_piecewise = model.fit(X_train, y_train,\n    ```", "```\n        epochs=100)\n    ```", "```\n    # PolynomialDecay\n    ```", "```\n    lr_poly = tf.keras.optimizers.schedules.PolynomialDecay(\n    ```", "```\n    initial_learning_rate=0.1,\n    ```", "```\n        decay_steps=100,\n    ```", "```\n        end_learning_rate=0.01,\n    ```", "```\n        power=1.0)\n    ```", "```\n    # Define learning rate schedule\n    ```", "```\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    ```", "```\n        lambda epoch: 1e-7 * 10**(epoch / 10))\n    ```", "```\n    # Define optimizer with initial learning rate\n    ```", "```\n    optimizer = tf.keras.optimizers.SGD(\n    ```", "```\n        learning_rate=1e-7, momentum=0.9)\n    ```", "```\n    model.compile(optimizer=optimizer, loss='mse')\n    ```", "```\n    history = model.fit(train_data, epochs=200,\n    ```", "```\n        callbacks=[lr_schedule], verbose=0)\n    ```", "```\n    lrs = 1e-7 * (10 ** (np.arange(200) / 10))\n    ```", "```\n    plt.semilogx(lrs, history.history[\"loss\"])\n    ```", "```\n    plt.axis([1e-7, 1e-3, 0, 300])\n    ```", "```\n    plt.xlabel('Learning Rate')\n    ```", "```\n    plt.ylabel('Loss')\n    ```", "```\n    plt.title('Learning Rate vs Loss')\n    ```", "```\n    plt.show()\n    ```", "```\n    # Create sequences\n    ```", "```\n    window_size = 20\n    ```", "```\n    X = []\n    ```", "```\n    y = []\n    ```", "```\n    for i in range(window_size, len(data)):\n    ```", "```\n        X.append(data[i-window_size:i])\n    ```", "```\n        y.append(data[i])\n    ```", "```\n    X = np.array(X)\n    ```", "```\n    y = np.array(y)\n    ```", "```\n    # Train/val split\n    ```", "```\n    split = int(0.8 * len(X))\n    ```", "```\n    X_train, X_val = X[:split], X[split:]\n    ```", "```\n    y_train, y_val = y[:split], y[split:]\n    ```", "```\n    # Reshape data\n    ```", "```\n    X_train = X_train.reshape(-1, window_size, 1)\n    ```", "```\n    X_val = X_val.reshape(-1, window_size, 1)\n    ```", "```\n    # Set batch size and shuffle buffer\n    ```", "```\n    batch_size = 128\n    ```", "```\n    shuffle_buffer = 1000\n    ```", "```\n    train_data = tf.data.Dataset.from_tensor_slices(\n    ```", "```\n        (X_train, y_train))\n    ```", "```\n    train_data = train_data.shuffle(\n    ```", "```\n        shuffle_buffer).batch(batch_size)\n    ```", "```\n    # Build model\n    ```", "```\n    model = Sequential()\n    ```", "```\n    model.add(Conv1D(filters=64, kernel_size=3,\n    ```", "```\n        strides=1,\n    ```", "```\n        padding='causal',\n    ```", "```\n        activation='relu',\n    ```", "```\n        input_shape=(window_size, 1)))\n    ```", "```\n    model.add(MaxPooling1D(pool_size=2))\n    ```", "```\n    model.add(Conv1D(filters=32, kernel_size=3,\n    ```", "```\n        strides=1,\n    ```", "```\n        padding='causal',\n    ```", "```\n        activation='relu'))\n    ```", "```\n    model.add(MaxPooling1D(pool_size=2))\n    ```", "```\n    model.add(Flatten())\n    ```", "```\n    model.add(Dense(16, activation='relu'))\n    ```", "```\n    model.add(Dense(1))\n    ```", "```\n    model.compile(loss='mse', optimizer='adam')\n    ```", "```\n    # Train model\n    ```", "```\n    model.fit(train_data, epochs=100)\n    ```", "```\n    # Make predictions\n    ```", "```\n    preds = model.predict(X_val)\n    ```", "```\n    # Calculate metrics\n    ```", "```\n    mae = mean_absolute_error(y_val, preds)\n    ```", "```\n    mse = mean_squared_error(y_val, preds)\n    ```", "```\n    # Print metrics\n    ```", "```\n    print('MAE: ', mae)\n    ```", "```\n    print('MSE: ', mse)\n    ```", "```\n    # Create sequences\n    ```", "```\n    seq_len = 20\n    ```", "```\n    X = []\n    ```", "```\n    y = []\n    ```", "```\n    for i in range(seq_len, len(data)):\n    ```", "```\n        X.append(data[i-seq_len:i])\n    ```", "```\n        y.append(data[i])\n    ```", "```\n    X = np.array(X)\n    ```", "```\n    y = np.array(y)\n    ```", "```\n    # Train/val split\n    ```", "```\n    split = int(0.8*len(X))\n    ```", "```\n    X_train, X_val = X[:split], X[split:]\n    ```", "```\n    y_train, y_val = y[:split], y[split:]\n    ```", "```\n    # Create dataset\n    ```", "```\n    batch_size = 128\n    ```", "```\n    dataset = tf.data.Dataset.from_tensor_slices(\n    ```", "```\n        (X_train, y_train))\n    ```", "```\n    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n    ```", "```\n    model = tf.keras.models.Sequential([\n    ```", "```\n        tf.keras.layers.Lambda(lambda x: tf.expand_dims(\n    ```", "```\n            x, axis=-1),\n    ```", "```\n            input_shape=[None]),\n    ```", "```\n        tf.keras.layers.SimpleRNN(40,\n    ```", "```\n            return_sequences=True),\n    ```", "```\n        tf.keras.layers.SimpleRNN(40),\n    ```", "```\n        tf.keras.layers.Dense(1),\n    ```", "```\n        tf.keras.layers.Lambda(lambda x: x * 100.0)\n    ```", "```\n    ])\n    ```", "```\n    model.compile(optimizer=tf.keras.optimizers.Adam(\n    ```", "```\n        learning_rate=8e-4), loss='mse')\n    ```", "```\n    # Train model\n    ```", "```\n    model.fit(dataset, epochs=100)momentum=0.9))\n    ```", "```\n    # Create sequences\n    ```", "```\n    seq_len = 20\n    ```", "```\n    X = []\n    ```", "```\n    y = []\n    ```", "```\n    for i in range(seq_len, len(data)):\n    ```", "```\n        X.append(data[i-seq_len:i])\n    ```", "```\n        y.append(data[i])\n    ```", "```\n    X = np.array(X)\n    ```", "```\n    X = X.reshape(X.shape[0], X.shape[1], 1)\n    ```", "```\n    y = np.array(y)\n    ```", "```\n    # Train/val split\n    ```", "```\n    split = int(0.8*len(X))\n    ```", "```\n    X_train, X_val = X[:split], X[split:]\n    ```", "```\n    y_train, y_val = y[:split], y[split:]\n    ```", "```\n    # Set batch size and buffer size\n    ```", "```\n    batch_size = 64\n    ```", "```\n    buffer_size = 1000\n    ```", "```\n    # Create dataset\n    ```", "```\n    dataset = tf.data.Dataset.from_tensor_slices(\n    ```", "```\n        (X_train, y_train))\n    ```", "```\n    dataset = dataset.shuffle(\n    ```", "```\n        buffer_size).batch(batch_size)\n    ```", "```\n    model_lstm = tf.keras.models.Sequential([\n    ```", "```\n        tf.keras.layers.LSTM(50, return_sequences=True,\n    ```", "```\n            input_shape=[None, 1]),\n    ```", "```\n        tf.keras.layers.LSTM(50),\n    ```", "```\n        tf.keras.layers.Dense(1)\n    ```", "```\n    ])\n    ```", "```\n       # Build the Model\n    ```", "```\n    model = tf.keras.models.Sequential([\n    ```", "```\n        tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n    ```", "```\n        strides=1,\n    ```", "```\n        activation=\"relu\",\n    ```", "```\n        padding='causal',\n    ```", "```\n        input_shape=[window_size, 1]),\n    ```", "```\n        tf.keras.layers.LSTM(64, return_sequences=True),\n    ```", "```\n        tf.keras.layers.LSTM(64),\n    ```", "```\n        tf.keras.layers.Dense(30, activation=\"relu\"),\n    ```", "```\n        tf.keras.layers.Dense(10, activation=\"relu\"),\n    ```", "```\n        tf.keras.layers.Dense(1),\n    ```", "```\n    import numpy as np\n    ```", "```\n    import matplotlib.pyplot as plt\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow import keras\n    ```", "```\n    import yfinance as yf\n    ```", "```\n    df_apple = yf.Ticker(tickerSymbol)\n    ```", "```\n    df_apple = df_apple.history(period='1d',\n    ```", "```\n        start='2013-01-01', end='2023-01-01')\n    ```", "```\n    plt.figure(figsize=(14,7))\n    ```", "```\n    plt.plot(df_apple.index, df_apple['Close'],\n    ```", "```\n        label='Close price')\n    ```", "```\n    plt.title('Historical prices for AAPL')\n    ```", "```\n    plt.xlabel('Date')\n    ```", "```\n    plt.ylabel('Price')\n    ```", "```\n    plt.grid(True)\n    ```", "```\n    plt.legend()\n    ```", "```\n    plt.show()\n    ```", "```\n    Series = df_apple['Close'].values\n    ```", "```\n    # Sliding window\n    ```", "```\n    window_size = 20\n    ```", "```\n    X, y = [], []\n    ```", "```\n    for i in range(window_size, len(data)):\n    ```", "```\n        X.append(data[i-window_size:i])\n    ```", "```\n        y.append(data[i])\n    ```", "```\n    X = np.array(X)\n    ```", "```\n    y = np.array(y)\n    ```", "```\n    # Train/val split\n    ```", "```\n    train_size = int(len(X) * 0.8)\n    ```", "```\n    X_train, X_val = X[:train_size], X[train_size:]\n    ```", "```\n    y_train, y_val = y[:train_size], y[train_size:]\n    ```", "```\n    # Dataset using tf.data\n    ```", "```\n    batch_size = 128\n    ```", "```\n    buffer_size = 10000\n    ```", "```\n    train_data = tf.data.Dataset.from_tensor_slices(\n    ```", "```\n        (X_train, y_train))\n    ```", "```\n    train_data = train_data.cache().shuffle(\n    ```", "```\n        buffer_size).batch(batch_size).prefetch(\n    ```", "```\n        tf.data.experimental.AUTOTUNE)\n    ```"]