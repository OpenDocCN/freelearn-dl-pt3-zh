<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introduction to AI for Cybersecurity Professionals</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we'll distinguish between the various branches of </span><strong><span class="koboSpan" id="kobo.3.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">AI</span></strong><span class="koboSpan" id="kobo.6.1">), focusing on the pros and cons of the different approaches of automated learning in the field of </span><strong><span class="koboSpan" id="kobo.7.1">c</span></strong><span><strong><span class="koboSpan" id="kobo.8.1">ybersecurity</span></strong><span class="koboSpan" id="kobo.9.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">We will introduce different strategies for learning and optimizing of the various algorithms, and we'll also look at the main concepts of AI in action using Jupyter Notebooks and the </span><kbd><span class="koboSpan" id="kobo.11.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.12.1"> Python library.</span></p>
<p><span class="koboSpan" id="kobo.13.1">This chapter will cover the following topics:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">Applying AI in cybersecurity</span></li>
<li><span class="koboSpan" id="kobo.15.1">The evolution from expert systems to data mining and AI</span></li>
<li><span class="koboSpan" id="kobo.16.1">The different forms of automated learning</span></li>
<li><span class="koboSpan" id="kobo.17.1">The characteristics of algorithm training and optimization</span></li>
<li><span class="koboSpan" id="kobo.18.1">Beginning with AI via Jupyter Notebooks</span></li>
<li><span class="koboSpan" id="kobo.19.1">Introducing AI in the context of cybersecurity</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Applying AI in cybersecurity</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The application of AI to cybersecurity is an experimental research area that's not without problems, which we will try to explain during this chapter. </span><span class="koboSpan" id="kobo.2.2">However, it is undeniable that the results achieved so far are promising, and that in the near future the methods of analysis will become common practice, with clear and positive consequences in the cybersecurity professional field, both in terms of new job opportunities and new challenges.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.3.1">When dealing with the topic of applying AI to cybersecurity, the reactions from insiders are often ambivalent. </span><span class="koboSpan" id="kobo.3.2">In fact, reactions of skepticism alternate with conservative attitudes, partly caused by the fear that machines will supplant human operators, despite the high technical and professional skills of humans, acquired from years of hard work.</span></p>
<p><span class="koboSpan" id="kobo.4.1">However, in the near future, companies and organizations will increasingly need to invest in automated analysis tools that enable a rapid and adequate response to current and future cybersecurity challenges. </span><span class="koboSpan" id="kobo.4.2">Therefore, the scenario that is looming is actually a combination of skills, rather than a clash between human operators and machines. </span><span class="koboSpan" id="kobo.4.3">It is therefore likely that the AI within the field of cybersecurity will take charge of the dirty work, that is, the selection of potential suspect cases, leaving the most advanced tasks to the security analysts, letting them investigate in more depth the threats that deserve the most attention.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Evolution in AI: from expert systems to data mining</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To understand the advantages associated with the adoption of AI in the field of cybersecurity, it is necessary to introduce the underlying logic to the different methodological approaches that characterize AI.</span></p>
<p><span class="koboSpan" id="kobo.3.1">We will start with a brief historical analysis of the evolution of AI in order to fully evaluate the potential benefits of applying it in the field of cybersecurity.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">A brief introduction to expert systems</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the first attempts at automated learning consisted of defining the </span><strong><span class="koboSpan" id="kobo.3.1">rule-based</span></strong><span class="koboSpan" id="kobo.4.1"> decision system applied to a given application domain, covering all the possible ramifications and concrete cases that could be found in the real world. </span><span class="koboSpan" id="kobo.4.2">In this way, all the possible options were hardcoded within the automated learning solutions, and were verified by experts in the field.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The fundamental limitation of such </span><strong><span class="koboSpan" id="kobo.6.1">expert systems</span></strong><span class="koboSpan" id="kobo.7.1"> consisted of the fact that they reduced the decisions to Boolean values (which reduce everything down to a binary choice), thus limiting the ability to adapt the solutions to the different nuances of real-world use cases.</span></p>
<p><span class="koboSpan" id="kobo.8.1">In fact, expert systems do not learn anything new compared to hardcoded solutions, but limit themselves to looking for the right answer within a (potentially very large) knowledge base that is not able to adapt to new problems that were not addressed previously.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Reflecting the indeterministic nature of reality</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Since the concrete cases that we come across in the real world cannot simply be represented using just true/false classification models (although experts in the sector strive to list all possible cases, there is always something in reality that escapes classification), it is therefore necessary to make the best use of the data at our disposal in order to let latent tendencies and anomalous cases (such as </span><strong><span class="koboSpan" id="kobo.3.1">outliers</span></strong><span class="koboSpan" id="kobo.4.1">) emerge, making use of statistical and probabilistic models that can more appropriately reflect the </span><strong><span class="koboSpan" id="kobo.5.1">indeterministic</span></strong><span class="koboSpan" id="kobo.6.1"> nature of reality.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Going beyond statistics toward machine learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Although the introduction of statistical models broke through the limitations of expert systems, the underlying rigidity of the approach remained, because statistical models, such as rule-based decisions, were in fact established in advance and could not be modified to adapt to new data. For example, one of the most commonly used statistical models is the Gaussian distribution. </span><span class="koboSpan" id="kobo.2.2">The statistician could then decide that the data comes from a Gaussian distribution, and try to estimate the parameters that characterize the hypothetical distribution that best describes the data being analyzed, without taking into consideration alternative models.</span></p>
<p><span class="koboSpan" id="kobo.3.1">To overcome these limits, it was therefore necessary to adopt an </span><strong><span class="koboSpan" id="kobo.4.1">iterative</span></strong><span class="koboSpan" id="kobo.5.1"> approach, which allowed the introduction of </span><strong><span class="koboSpan" id="kobo.6.1">machine learning</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong><span class="koboSpan" id="kobo.8.1">ML</span></strong><span class="koboSpan" id="kobo.9.1">) algorithms capable of generalizing the descriptive models starting from the available data, thus autonomously generating its own features, without limiting itself to predefined target functions, but adapting them to the continuous evolution of the algorithm training process.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Mining data for models</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The difference in approach compared to the predefined </span><strong><span class="koboSpan" id="kobo.3.1">static</span></strong><span class="koboSpan" id="kobo.4.1"> models is also reflected in the research field known as </span><strong><span class="koboSpan" id="kobo.5.1">data mining</span></strong><span class="koboSpan" id="kobo.6.1">.</span></p>
<p><span class="koboSpan" id="kobo.7.1">An adequate definition of the data mining process consists of the discovery of adequate representative models, starting with the data. </span><span class="koboSpan" id="kobo.7.2">Also, in this case, instead of adopting pre-established statistical models, we can use ML algorithms based on the training data to identify the most suitable predictive model (this is more true when we are not able to understand the nature of the data at our disposal).</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.8.1">However, the </span><strong><span class="koboSpan" id="kobo.9.1">algorithmic approach</span></strong><span class="koboSpan" id="kobo.10.1"> is not always adequate. </span><span class="koboSpan" id="kobo.10.2">When the nature of the data is clear and conforms to known models, there is no advantage in using ML algorithms instead of pre-defined models. </span><span class="koboSpan" id="kobo.10.3">The next step, which absorbs and extends the advantages of the previous approaches, adding the ability to manage cases not covered in the training data, leads us to AI.</span></p>
<p><span class="koboSpan" id="kobo.11.1">AI is a wider field of research than ML, which can manage data of a more generic and abstract nature than ML, thus enabling the transfer of common solutions to different types of data without the need for complete retraining. </span><span class="koboSpan" id="kobo.11.2">In this way, it is possible, for example, to recognize objects from color images, starting with objects originally obtained from black and white samples.</span></p>
<p><span class="koboSpan" id="kobo.12.1">Therefore, AI</span><strong><span class="koboSpan" id="kobo.13.1"> </span></strong><span class="koboSpan" id="kobo.14.1">is considered as a broad field of research that includes ML. In turn, ML includes </span><strong><span class="koboSpan" id="kobo.15.1">deep learning</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong><span class="koboSpan" id="kobo.17.1">DL</span></strong><span class="koboSpan" id="kobo.18.1">) which is ML method based on artificial neural networks, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.19.1"><img src="assets/717ca094-e6e1-442f-af8f-e127250f9d94.png" style="width:30.58em;height:22.75em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Types of machine learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The process of mechanical learning from data can take different forms, with different characteristics and predictive abilities.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.3.1">In the case of ML (which, as we have seen, is a branch of research belonging to AI), it is common to distinguish between the following types of ML:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.4.1">Supervised learning</span></li>
<li><span class="koboSpan" id="kobo.5.1">Unsupervised learning</span></li>
<li><span class="koboSpan" id="kobo.6.1">Reinforcement learning</span></li>
</ul>
<p><span class="koboSpan" id="kobo.7.1">The differences between these learning modalities are attributable to the type of result (output) that we intend to achieve, based on the nature of the input required to produce it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Supervised learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the case of </span><strong><span class="koboSpan" id="kobo.3.1">supervised learning</span></strong><span class="koboSpan" id="kobo.4.1">, algorithm training is conducted using an input dataset, from which the type of output that we have to obtain is already known.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In practice, the algorithms must be trained to identify the relationships between the variables being trained, trying to optimize the learning parameters on the basis of the target variables</span><span><span class="koboSpan" id="kobo.6.1"> (also called </span></span><strong><span class="koboSpan" id="kobo.7.1">labels</span></strong><span><span class="koboSpan" id="kobo.8.1">)</span></span><span class="koboSpan" id="kobo.9.1"> that, as mentioned, are already known.</span></p>
<p><span class="koboSpan" id="kobo.10.1">An example of a supervised learning algorithm is classification algorithms, which are particularly used in the field of cybersecurity for </span><strong><span class="koboSpan" id="kobo.11.1">spam classification</span></strong><span class="koboSpan" id="kobo.12.1">.</span></p>
<p><span class="koboSpan" id="kobo.13.1">A </span><strong><span class="koboSpan" id="kobo.14.1">spam filter</span></strong><span class="koboSpan" id="kobo.15.1"> is in fact trained by submitting </span><span><span class="koboSpan" id="kobo.16.1">an input dataset </span></span><span class="koboSpan" id="kobo.17.1">to the algorithm containing many examples of emails that have already been previously classified as spam (the emails were malicious or unwanted) or ham (the emails were genuine and harmless).</span></p>
<p><span class="koboSpan" id="kobo.18.1">The classification algorithm of the spam filter must therefore learn to classify the new emails it will receive in the future, referring to the spam or ham classes based on the training previously performed on the input dataset of the already classified emails.</span></p>
<p><span class="koboSpan" id="kobo.19.1">Another example of supervised algorithms is regression algorithms. </span><span class="koboSpan" id="kobo.19.2">Ultimately, there are the following main supervised algorithms:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Regression (linear and logistic)</span></li>
<li><strong><span class="koboSpan" id="kobo.21.1">k-Nearest Neighbors</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong><span class="koboSpan" id="kobo.23.1">k-NNs</span></strong><span class="koboSpan" id="kobo.24.1">)</span></li>
<li><strong><span class="koboSpan" id="kobo.25.1">Support vector machines</span></strong><span class="koboSpan" id="kobo.26.1"> (</span><strong><span class="koboSpan" id="kobo.27.1">SVMs</span></strong><span class="koboSpan" id="kobo.28.1">)</span></li>
<li><span class="koboSpan" id="kobo.29.1">Decision trees and random forests</span></li>
<li><strong><span class="koboSpan" id="kobo.30.1">Neural networks</span></strong><span class="koboSpan" id="kobo.31.1"> (</span><strong><span class="koboSpan" id="kobo.32.1">NNs</span></strong><span class="koboSpan" id="kobo.33.1">)</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Unsupervised learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the case of </span><strong><span class="koboSpan" id="kobo.3.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.4.1">, the algorithms must try </span><strong><span class="koboSpan" id="kobo.5.1">to classify the data independently</span></strong><span class="koboSpan" id="kobo.6.1">, without the aid of a previous classification provided by the analyst. </span><span class="koboSpan" id="kobo.6.2">In the context of cybersecurity, unsupervised learning algorithms are important for identifying new (not previously detected) forms of </span><strong><span class="koboSpan" id="kobo.7.1">malware</span></strong><span class="koboSpan" id="kobo.8.1"> attacks, </span><strong><span class="koboSpan" id="kobo.9.1">frauds</span></strong><span class="koboSpan" id="kobo.10.1">, and </span><strong><span class="koboSpan" id="kobo.11.1">email spamming</span></strong><span class="koboSpan" id="kobo.12.1"> campaigns.</span></p>
<p><span class="koboSpan" id="kobo.13.1">Here are some examples of unsupervised algorithms:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">Dimensionality reduction:
</span><ul>
<li><strong><span class="koboSpan" id="kobo.15.1">Principal component analysis</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong><span class="koboSpan" id="kobo.17.1">PCA</span></strong><span class="koboSpan" id="kobo.18.1">)</span></li>
<li><span class="koboSpan" id="kobo.19.1">PCA Kernel</span></li>
</ul>
</li>
<li><span class="koboSpan" id="kobo.20.1">Clustering:
</span><ul>
<li><span class="koboSpan" id="kobo.21.1">k-means</span></li>
<li><strong><span class="koboSpan" id="kobo.22.1">Hierarchical cluster analysis</span></strong><span class="koboSpan" id="kobo.23.1"> (</span><strong><span class="koboSpan" id="kobo.24.1">HCA</span></strong><span class="koboSpan" id="kobo.25.1">)</span></li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Reinforcement learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the case of </span><strong><span class="koboSpan" id="kobo.3.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">RL</span></strong><span class="koboSpan" id="kobo.6.1">), a different learning strategy is followed, which emulates the trial and error approach. </span><span class="koboSpan" id="kobo.6.2">Thus, drawing information from the feedback obtained during the learning path, with the aim of maximizing the reward finally obtained based on the number of correct decisions that the algorithm has selected.</span></p>
<p><span class="koboSpan" id="kobo.7.1">In practice, the learning process takes place in an unsupervised manner, with the particularity that a </span><strong><span class="koboSpan" id="kobo.8.1">positive reward</span></strong><span class="koboSpan" id="kobo.9.1"> is assigned to each correct decision (and a </span><strong><span class="koboSpan" id="kobo.10.1">negative reward</span></strong><span class="koboSpan" id="kobo.11.1"> for incorrect decisions) taken at each step of the learning path. </span><span class="koboSpan" id="kobo.11.2">At the end of the learning process, the decisions of the algorithm are reassessed based on the final reward achieved.</span></p>
<p><span class="koboSpan" id="kobo.12.1">Given its dynamic nature, it is no coincidence that RL is more similar to the general approach adopted by AI than to the common algorithms developed in ML.</span></p>
<p><span class="koboSpan" id="kobo.13.1">The following are some examples of RL algorithms:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">Markov process</span></li>
<li><span class="koboSpan" id="kobo.15.1">Q-learning</span></li>
<li><strong><span class="koboSpan" id="kobo.16.1">Temporal difference</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong><span class="koboSpan" id="kobo.18.1">TD</span></strong><span class="koboSpan" id="kobo.19.1">) methods</span></li>
<li><span class="koboSpan" id="kobo.20.1">Monte Carlo methods</span></li>
</ul>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.21.1">In particular, </span><strong><span class="koboSpan" id="kobo.22.1">Hidden Markov Models</span></strong><span class="koboSpan" id="kobo.23.1"> (</span><strong><span class="koboSpan" id="kobo.24.1">HMM</span></strong><span class="koboSpan" id="kobo.25.1">) (which make use of the Markov process) are extremely important in the detection of polymorphic malware threats.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Algorithm training and optimization</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">When preparing automated learning procedures, we will often face a series of challenges. </span><span class="koboSpan" id="kobo.2.2">We need to overcome these challenges in order to recognize and avoid compromising the reliability of the procedures themselves, thus preventing the possibility of drawing erroneous or hasty conclusions that, in the context of cybersecurity, can have devastating consequences.</span></p>
<p><span class="koboSpan" id="kobo.3.1">One of the main problems that we often face, especially in the case of the configuration of threat detection procedures, is the management of </span><strong><span class="koboSpan" id="kobo.4.1">false positives</span></strong><span class="koboSpan" id="kobo.5.1">; that is, cases detected by the algorithm and classified as potential threats, which in reality are not. </span><span class="koboSpan" id="kobo.5.2">We will discuss false positives and ML evaluation metrics in more depth in </span><a href="98ce7db1-f53d-47ca-b6ca-ec0e5f882566.xhtml"><span class="koboSpan" id="kobo.6.1">Chapter 7</span></a><span class="koboSpan" id="kobo.7.1">, </span><em><span class="koboSpan" id="kobo.8.1">Fraud Prevention with Cloud AI Solutions</span></em><span class="koboSpan" id="kobo.9.1">, and </span><a href="55892989-888d-4407-ac91-7f939c0802bd.xhtml"><span class="koboSpan" id="kobo.10.1">Chapter 9</span></a><span class="koboSpan" id="kobo.11.1">, </span><em><span class="koboSpan" id="kobo.12.1">Evaluating Algorithms</span></em><span class="koboSpan" id="kobo.13.1">.</span></p>
<p><span class="koboSpan" id="kobo.14.1">The management of false positives is particularly burdensome in the case of </span><strong><span class="koboSpan" id="kobo.15.1">detection systems</span></strong><span class="koboSpan" id="kobo.16.1"> aimed at contrasting </span><strong><span class="koboSpan" id="kobo.17.1">networking threats</span></strong><span class="koboSpan" id="kobo.18.1">, given that the number of events detected are often so high that they absorb and saturate all the human resources dedicated to threat detection activities.</span></p>
<p><span class="koboSpan" id="kobo.19.1">On the other hand, even correct (true positive) reports, if in excessive numbers, contribute to functionally overloading the analysts, distracting them from priority tasks. </span><span class="koboSpan" id="kobo.19.2">The need to optimize the learning procedures therefore emerges in order to reduce the number of cases that need to be analyzed in depth by the analysts.</span></p>
<p><span class="koboSpan" id="kobo.20.1">This optimization activity often starts with the selection and cleaning</span><em><span class="koboSpan" id="kobo.21.1"> </span></em><span class="koboSpan" id="kobo.22.1">of the data submitted to the algorithms.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to find useful sources of data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the case of </span><strong><span class="koboSpan" id="kobo.3.1">anomaly detection</span></strong><span class="koboSpan" id="kobo.4.1">, for example, particular attention must be paid to the data being analyzed. </span><span class="koboSpan" id="kobo.4.2">An effective anomaly detection activity presupposes that the training data does not contain the anomalies sought, but that on the contrary, they reflect the normal situation of reference.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.5.1">If, on the other hand, the training data was biased with the anomalies being investigated, the anomaly detection activity would lose much of its reliability and utility in accordance with the principle commonly known as </span><strong><span class="koboSpan" id="kobo.6.1">GIGO</span></strong><span class="koboSpan" id="kobo.7.1">, which stands for </span><strong><span class="koboSpan" id="kobo.8.1">garbage in, garbage out</span></strong><span class="koboSpan" id="kobo.9.1">.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Given the increasing availability of raw data in real time, often the preliminary cleaning of data is considered a challenge in itself. </span><span class="koboSpan" id="kobo.10.2">In fact, it's often necessary to conduct a preliminary skim of the data, </span><strong><span class="koboSpan" id="kobo.11.1">eliminating irrelevant or redundant</span></strong><span class="koboSpan" id="kobo.12.1"> information. We can then </span><strong><span class="koboSpan" id="kobo.13.1">present the data</span></strong><span class="koboSpan" id="kobo.14.1"> to the algorithms in a correct form, which can improve their ability to learn, adapting to the form of data on the basis of the type of algorithm used.</span></p>
<p><span class="koboSpan" id="kobo.15.1">For example, a </span><strong><span class="koboSpan" id="kobo.16.1">classification algorithm</span></strong><span class="koboSpan" id="kobo.17.1"> will be able to identify a more representative and more effective model in cases in which the input data will be presented in a </span><strong><span class="koboSpan" id="kobo.18.1">grouped form</span></strong><span class="koboSpan" id="kobo.19.1">, or is capable of being </span><strong><span class="koboSpan" id="kobo.20.1">linearly separable</span></strong><span class="koboSpan" id="kobo.21.1">. </span><span class="koboSpan" id="kobo.21.2">In the same way, the presence of </span><strong><span class="koboSpan" id="kobo.22.1">variables</span></strong><span class="koboSpan" id="kobo.23.1"> (also known as </span><strong><span class="koboSpan" id="kobo.24.1">dimensions</span></strong><span class="koboSpan" id="kobo.25.1">) containing </span><strong><span class="koboSpan" id="kobo.26.1">empty fields</span></strong><span class="koboSpan" id="kobo.27.1"> weighs down the computational effort of the algorithm and produces less reliable predictive models due to the phenomenon known as the </span><strong><span class="koboSpan" id="kobo.28.1">curse of dimensionality.</span></strong></p>
<p><span class="koboSpan" id="kobo.29.1">This</span><strong><span class="koboSpan" id="kobo.30.1"> </span></strong><span class="koboSpan" id="kobo.31.1">occurs when the number of features, that is, dimensions</span><em><span class="koboSpan" id="kobo.32.1">,</span></em><span class="koboSpan" id="kobo.33.1"> increases without improving the relevant information, simply resulting in data being dispersed in the increased space of research:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.34.1"><img src="assets/d7829c5c-8d6e-4cc2-8568-955a70786472.png" style="width:34.08em;height:18.75em;"/></span></p>
<p><span class="koboSpan" id="kobo.35.1">Also, the </span><strong><span class="koboSpan" id="kobo.36.1">sources</span></strong><span class="koboSpan" id="kobo.37.1"> from which we draw our test cases (samples) are important. </span><span class="koboSpan" id="kobo.37.2">Think, for example, of a case in which we have to predict the mischievous behavior of an </span><strong><span class="koboSpan" id="kobo.38.1">unknown executable</span></strong><span class="koboSpan" id="kobo.39.1">. </span><span class="koboSpan" id="kobo.39.2">The problem in question is reduced to the definition of </span><strong><span class="koboSpan" id="kobo.40.1">a model of classification</span></strong><span class="koboSpan" id="kobo.41.1"> of the executable, which must be traced back to one of two categories: </span><strong><span class="koboSpan" id="kobo.42.1">genuine </span></strong><span class="koboSpan" id="kobo.43.1">and </span><strong><span class="koboSpan" id="kobo.44.1">malicious</span></strong><span class="koboSpan" id="kobo.45.1">.</span></p>
<p class="mce-root"><span><span class="koboSpan" id="kobo.46.1">To achieve such a result, we need to train our classification</span></span><span class="koboSpan" id="kobo.47.1"> algorithm </span><span><span class="koboSpan" id="kobo.48.1">by providing it with a number of examples of executables that are considered malicious as an input dataset.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Quantity versus quality</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">When it all boils down to quantity versus quality, we are immediately faced with the following two problems:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.3.1">What types of malware can we consider most representative of the most probable risks and threats to our company?</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.4.1">How many example cases (samples) should we collect and administer to the algorithms in order to obtain a reliable result in terms of both effectiveness and predictive efficiency of future threats?</span></li>
</ul>
<p><span class="koboSpan" id="kobo.5.1">The answers to the two questions are closely related to the knowledge that the analyst has of the </span><strong><span class="koboSpan" id="kobo.6.1">specific organizational realm</span></strong><span class="koboSpan" id="kobo.7.1"> in which they must operate. </span></p>
<p><span class="koboSpan" id="kobo.8.1">All this could lead the analyst to believe that the creation of a </span><em><span class="koboSpan" id="kobo.9.1">honey-pot</span></em><span class="koboSpan" id="kobo.10.1">, which is useful for gathering malicious samples in the wild that will be fed to the algorithms as training samples, would be </span><strong><span class="koboSpan" id="kobo.11.1">more representative</span></strong><span class="koboSpan" id="kobo.12.1"> of the level of risk to which the organization is exposed than the use of datasets as examples of generic threats. At the same time, the number of test examples to be submitted to the algorithm is determined by the </span><strong><span class="koboSpan" id="kobo.13.1">characteristics of the data</span></strong><span class="koboSpan" id="kobo.14.1"> themselves. </span><span class="koboSpan" id="kobo.14.2">These can, in fact, present a prevalence of cases (skewness) of a certain type, to the detriment of other types, leading to a </span><strong><span class="koboSpan" id="kobo.15.1">distortion</span></strong><span class="koboSpan" id="kobo.16.1"> in the predictions of the algorithm toward the classes that are most numerous, when in reality, the most relevant information for our investigation is represented by a class with a smaller number of cases.</span></p>
<p><span class="koboSpan" id="kobo.17.1">In conclusion, it will not be a matter of being able to simply choose the best algorithm for our goals (which often does not exist), but mainly to select </span><strong><span class="koboSpan" id="kobo.18.1">the most representative cases</span></strong><span class="koboSpan" id="kobo.19.1"> (samples) to be submitted to a set of algorithms, which we will try to optimize based on the results obtained.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting to know Python's libraries</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following sections, we will explore the concepts presented so far, presenting some sample code that make use of a series of Python libraries that are among the most well known and widespread in the </span><span><span class="koboSpan" id="kobo.3.1">field of </span></span><span class="koboSpan" id="kobo.4.1">ML:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">NumPy (version 1.13.3)</span></li>
<li><span class="koboSpan" id="kobo.6.1">pandas (version 0.20.3)</span></li>
<li><span class="koboSpan" id="kobo.7.1">Matplotlib (version 2.0.2)</span></li>
<li><span class="koboSpan" id="kobo.8.1">scikit-learn (version 0.20.0)</span></li>
<li><span class="koboSpan" id="kobo.9.1">Seaborn (version 0.8.0)</span></li>
</ul>
<p><span class="koboSpan" id="kobo.10.1">The sample code will be shown here in the form of snippets, along with screenshots representing their output. </span><span class="koboSpan" id="kobo.10.2">Do not worry if not all of the implementation details are clear to you at first glance; we will have the opportunity to understand the implementation aspects of every single algorithm throughout the book.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Supervised learning example – linear regression</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As our first example, we'll look at one of the most commonly used algorithms in the field of supervised learning, namely linear regression. </span><span class="koboSpan" id="kobo.2.2">Taking advantage of the </span><kbd><span class="koboSpan" id="kobo.3.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.4.1"> Python library, we instantiate a linear regression object, by importing the </span><kbd><span class="koboSpan" id="kobo.5.1">LinearRegression</span></kbd><span class="koboSpan" id="kobo.6.1"> class included in the </span><kbd><span class="koboSpan" id="kobo.7.1">linear_model</span></kbd><span class="koboSpan" id="kobo.8.1"> package of the </span><kbd><span class="koboSpan" id="kobo.9.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.10.1"> library.</span></p>
<p><span class="koboSpan" id="kobo.11.1">The model will be trained with a training dataset obtained by invoking the </span><kbd><span class="koboSpan" id="kobo.12.1">rand()</span></kbd><span class="koboSpan" id="kobo.13.1"> method of the </span><kbd><span class="koboSpan" id="kobo.14.1">RandomState</span></kbd><span class="koboSpan" id="kobo.15.1"> class, which belongs to the </span><kbd><span class="koboSpan" id="kobo.16.1">random</span></kbd><em><span class="koboSpan" id="kobo.17.1"> </span></em><span class="koboSpan" id="kobo.18.1">package of the Python </span><kbd><span class="koboSpan" id="kobo.19.1">numpy</span></kbd><span class="koboSpan" id="kobo.20.1"> library. </span><span class="koboSpan" id="kobo.20.2">The training data is distributed following the linear model of, </span><span class="koboSpan" id="kobo.21.1"><img class="fm-editor-equation" src="assets/8ae7778d-7243-44b3-8dc7-98c5da07b544.png" style="width:7.17em;height:1.58em;"/></span><span class="koboSpan" id="kobo.22.1">. </span><span class="koboSpan" id="kobo.22.2">The training of the model is carried out by invoking the </span><kbd><span class="koboSpan" id="kobo.23.1">fit()</span></kbd><span class="koboSpan" id="kobo.24.1"> method on the </span><kbd><span class="koboSpan" id="kobo.25.1">lreg</span></kbd><span class="koboSpan" id="kobo.26.1"> object of the </span><kbd><span class="koboSpan" id="kobo.27.1">LinearRegression</span></kbd><span class="koboSpan" id="kobo.28.1"> class.</span></p>
<p><span class="koboSpan" id="kobo.29.1">At this point, we will try to predict data that is not included in the training dataset by invoking the </span><kbd><span class="koboSpan" id="kobo.30.1">predict()</span></kbd><span class="koboSpan" id="kobo.31.1"> method on the </span><kbd><span class="koboSpan" id="kobo.32.1">lreg</span></kbd><span class="koboSpan" id="kobo.33.1"> object.</span></p>
<p><span class="koboSpan" id="kobo.34.1">The training dataset, together with the values interpolated by the model, are finally printed on screen using the </span><kbd><span class="koboSpan" id="kobo.35.1">scatter()</span></kbd><span class="koboSpan" id="kobo.36.1"> and </span><kbd><span class="koboSpan" id="kobo.37.1">plot()</span></kbd><span class="koboSpan" id="kobo.38.1"> methods of the </span><kbd><span class="koboSpan" id="kobo.39.1">matplotlib</span></kbd><span class="koboSpan" id="kobo.40.1"> library:</span></p>
<pre><span class="koboSpan" id="kobo.41.1">%matplotlib inline</span><br/><span class="koboSpan" id="kobo.42.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.43.1">import numpy as np
</span><br/><span class="koboSpan" id="kobo.44.1">from sklearn.linear_model import LinearRegression
</span><br/><span class="koboSpan" id="kobo.45.1">pool = np.random.RandomState(10)</span><br/><span class="koboSpan" id="kobo.46.1">x = 5 * pool.rand(30)</span><br/><span class="koboSpan" id="kobo.47.1">y = 3 * x - 2 + pool.randn(30)</span><br/><span class="koboSpan" id="kobo.48.1"># y = 3x - 2;
</span><br/><span class="koboSpan" id="kobo.49.1">lregr = LinearRegression(fit_intercept=False)
X = x[:, np.newaxis]</span><br/><span class="koboSpan" id="kobo.50.1">lregr.fit(X, y)</span><br/><span class="koboSpan" id="kobo.51.1">lspace = np.linspace(0, 5)</span><br/><span class="koboSpan" id="kobo.52.1">X_regr = lspace[:, np.newaxis]</span><br/><span class="koboSpan" id="kobo.53.1">y_regr = lregr.predict(X_regr)</span><br/><span class="koboSpan" id="kobo.54.1">plt.scatter(x, y);</span><br/><span class="koboSpan" id="kobo.55.1">plt.plot(X_regr, y_regr);</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.56.1">The preceding code generates the following output, which shows how well the data samples are approximated by the straight line returned by the LinearRegression model:</span><br/></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.57.1"><img src="assets/31508237-8c78-4f32-8309-67496b91b06a.png" style="width:26.67em;height:18.08em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Unsupervised learning example – clustering</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As an example of unsupervised learning, we use the </span><kbd><span class="koboSpan" id="kobo.3.1">GaussianMixture</span></kbd><span class="koboSpan" id="kobo.4.1"> clustering model. </span><span class="koboSpan" id="kobo.4.2">Through this type of model, we will try to bring the data back to a collection of </span><strong><span class="koboSpan" id="kobo.5.1">Gaussian blobs</span></strong><span class="koboSpan" id="kobo.6.1">.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The training data is loaded from a file in </span><kbd><span class="koboSpan" id="kobo.8.1">.csv</span></kbd><span class="koboSpan" id="kobo.9.1"> format (comma-separated values) and stored in a </span><kbd><span class="koboSpan" id="kobo.10.1">DataFrame</span></kbd><span class="koboSpan" id="kobo.11.1"> object of the </span><kbd><span class="koboSpan" id="kobo.12.1">pandas</span></kbd><span class="koboSpan" id="kobo.13.1"> Python library. </span><span class="koboSpan" id="kobo.13.2">Once the data is loaded, we proceed to </span><strong><span class="koboSpan" id="kobo.14.1">reduce its dimensionality</span></strong><span class="koboSpan" id="kobo.15.1"> in order to identify a representation that reduces the original dimensions (features) from four to two, trying to maintain the features that are </span><strong><span class="koboSpan" id="kobo.16.1">most representative</span></strong><span class="koboSpan" id="kobo.17.1"> of the samples.</span></p>
<p><span class="koboSpan" id="kobo.18.1">The reduction of dimensionality prevents the disadvantages connected to the phenomenon of the curse of dimensionality, improves the computational efficiency, and simplifies the visualization of the data.</span></p>
<p><span class="koboSpan" id="kobo.19.1">The technique we will use for dimensionality reduction is known as </span><strong><span class="koboSpan" id="kobo.20.1">principal component analysis</span></strong><span class="koboSpan" id="kobo.21.1"> (</span><strong><span class="koboSpan" id="kobo.22.1">PCA</span></strong><span class="koboSpan" id="kobo.23.1">), and is available in the </span><kbd><span class="koboSpan" id="kobo.24.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.25.1"> library.</span></p>
<p><span class="koboSpan" id="kobo.26.1">Once the data dimensions are reduced from four to two, we will try to classify the data using the </span><kbd><span class="koboSpan" id="kobo.27.1">GaussianMixture</span></kbd><span class="koboSpan" id="kobo.28.1"> model as follows:</span></p>
<pre><span class="koboSpan" id="kobo.29.1">import pandas as pd
import seaborn as sns
</span><br/><span class="koboSpan" id="kobo.30.1">data_df = pd.read_csv("../datasets/clustering.csv")
data_df.describe()
X_data = data_df.drop('class_1', axis=1)
y_data = data_df['class_1']

from sklearn.decomposition import PCA   
</span><br/><span class="koboSpan" id="kobo.31.1">pca = PCA(n_components=2)               
pca.fit(X_data)                         
X_2D = pca.transform(X_data)            
data_df['PCA1'] = X_2D[:, 0]
data_df['PCA2'] = X_2D[:, 1]

from sklearn.mixture import GaussianMixture         
</span><br/><span class="koboSpan" id="kobo.32.1">gm = GaussianMixture(n_components=3, covariance_type='full')     
gm.fit(X_data)                         </span><br/><span class="koboSpan" id="kobo.33.1">y_gm = gm.predict(X_data)              
data_df['cluster'] = y_gm
sns.lmplot("PCA1", "PCA2", data=data_df, col='cluster', fit_reg=False)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.34.1">As can be seen in the following screenshot, the clustering algorithm has succeeded in classifying the data automatically in an appropriate manner, without having previously received information on the current labels associated with the various samples:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1"><img src="assets/e14d8b9d-9ae0-4758-9144-9e9194d19e88.png"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Simple NN example – perceptron</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this section, we will show a simple NN model, known as a </span><strong><span class="koboSpan" id="kobo.3.1">perceptron.</span></strong></p>
<p><strong><span class="koboSpan" id="kobo.4.1">NNs</span></strong><span class="koboSpan" id="kobo.5.1"> and </span><strong><span class="koboSpan" id="kobo.6.1">DL</span></strong><span class="koboSpan" id="kobo.7.1"> are subfields of ML aimed at emulating the human brain's learning capabilities. </span><span class="koboSpan" id="kobo.7.2">NN and DL will be addressed in more depth in </span><a href="aaf59353-00b3-4625-8732-63aad02cc8e5.xhtml"><span class="koboSpan" id="kobo.8.1">Chapter 3</span></a><span class="koboSpan" id="kobo.9.1">, </span><em><span class="koboSpan" id="kobo.10.1">Ham or Spam? </span><span class="koboSpan" id="kobo.10.2">Detecting Email Cybersecurity Threats with AI</span></em><span class="koboSpan" id="kobo.11.1">, and </span><a href="18f56dc2-fd40-4669-bef1-0b594d9e1572.xhtml"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></a><span class="koboSpan" id="kobo.13.1">, </span><em><span class="koboSpan" id="kobo.14.1">GANs – Attacks and Defenses</span></em><span class="koboSpan" id="kobo.15.1">.</span></p>
<p><span class="koboSpan" id="kobo.16.1">However rudimentary it is, a perceptron is nonetheless able to adequately classify samples that tend to group together (in technical terms, those that are </span><strong><span class="koboSpan" id="kobo.17.1">linearly separable</span></strong><span class="koboSpan" id="kobo.18.1">).</span></p>
<p><span class="koboSpan" id="kobo.19.1">One of the most common uses of a perceptron in the field of cybersecurity, as we will see, is in the area of </span><strong><span class="koboSpan" id="kobo.20.1">spam filtering</span></strong><span class="koboSpan" id="kobo.21.1">.</span></p>
<p><span class="koboSpan" id="kobo.22.1">In the following example, we will use the </span><kbd><span class="koboSpan" id="kobo.23.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.24.1"> implementation of the perceptron algorithm:</span></p>
<pre><span class="koboSpan" id="kobo.25.1">from matplotlib.colors import ListedColormap </span><br/><span class="koboSpan" id="kobo.26.1"># Thanks to Sebastian Raschka for 'plot_decision_regions' function </span><br/><span class="koboSpan" id="kobo.27.1">def plot_decision_regions(X, y, classifier, resolution=0.02): </span><br/><span class="koboSpan" id="kobo.28.1"> # setup marker generator and color map </span><br/><span class="koboSpan" id="kobo.29.1"> markers = ('s', 'x', 'o', '^', 'v')</span><br/><span class="koboSpan" id="kobo.30.1"> colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')</span><br/><span class="koboSpan" id="kobo.31.1"> cmap = ListedColormap(colors[:len(np.unique(y))]) </span><br/><span class="koboSpan" id="kobo.32.1"> # plot the decision surface </span><br/><span class="koboSpan" id="kobo.33.1"> x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br/><span class="koboSpan" id="kobo.34.1"> x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br/><span class="koboSpan" id="kobo.35.1"> xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), </span><br/><span class="koboSpan" id="kobo.36.1"> np.arange(x2_min, x2_max, resolution))</span><br/><span class="koboSpan" id="kobo.37.1"> Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br/><span class="koboSpan" id="kobo.38.1"> Z = Z.reshape(xx1.shape)</span><br/><span class="koboSpan" id="kobo.39.1"> plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br/><span class="koboSpan" id="kobo.40.1"> plt.xlim(xx1.min(), xx1.max())</span><br/><span class="koboSpan" id="kobo.41.1"> plt.ylim(xx2.min(), xx2.max()) </span><br/><span class="koboSpan" id="kobo.42.1"> # plot class samples </span><br/><span class="koboSpan" id="kobo.43.1"> for idx, cl in enumerate(np.unique(y)):</span><br/><span class="koboSpan" id="kobo.44.1"> plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], </span><br/><span class="koboSpan" id="kobo.45.1"> alpha=0.8, c=cmap(idx), </span><br/><span class="koboSpan" id="kobo.46.1"> marker=markers[idx], label=cl) </span><br/><span class="koboSpan" id="kobo.47.1">from sklearn.linear_model import perceptron</span><br/><span class="koboSpan" id="kobo.48.1">from sklearn.datasets import make_classification </span><br/><span class="koboSpan" id="kobo.49.1">X, y = make_classification(30, 2, 2, 0, weights=[.3, .3], random_state=300) </span><br/><span class="koboSpan" id="kobo.50.1">plt.scatter(X[:,0], X[:,1], s=50)</span><br/><span class="koboSpan" id="kobo.51.1">pct = perceptron.Perceptron(max_iter=100, verbose=0, random_state=300, </span><br/><span class="koboSpan" id="kobo.52.1">fit_intercept=True, eta0=0.002)</span><br/><span class="koboSpan" id="kobo.53.1">pct.fit(X, y)</span><br/><span class="koboSpan" id="kobo.54.1">plot_decision_regions(X, y, classifier=pct)</span><br/><span class="koboSpan" id="kobo.55.1">plt.title('Perceptron')</span><br/><span class="koboSpan" id="kobo.56.1">plt.xlabel('X')</span><br/><span class="koboSpan" id="kobo.57.1">plt.ylabel('Y')</span><br/><span class="koboSpan" id="kobo.58.1">plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.59.1">The preceding code generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.60.1"><img src="assets/1ea26dda-0ec6-46c2-9037-586293bae8ea.png" style="width:27.00em;height:19.25em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">AI in the context of cybersecurity</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">With the exponential increase in the spread of threats associated with the daily diffusion of new malware, it is practically impossible to think of dealing effectively with these threats using only analysis conducted by human operators. </span><span class="koboSpan" id="kobo.2.2">It is necessary to introduce algorithms that allow us to automate that introductory phase of analysis known as </span><strong><span class="koboSpan" id="kobo.3.1">triage</span></strong><span class="koboSpan" id="kobo.4.1">, that is to say, to conduct a </span><strong><span class="koboSpan" id="kobo.5.1">preliminary screening</span></strong><span class="koboSpan" id="kobo.6.1"> of the threats to be submitted to the attention of the cybersecurity professionals, allowing us to respond in a timely and effective manner to ongoing attacks.</span></p>
<p><span class="koboSpan" id="kobo.7.1">We need to be able to respond in a dynamic fashion, adapting to the changes in the context related to the presence of </span><strong><span class="koboSpan" id="kobo.8.1">unprecedented</span></strong><span class="koboSpan" id="kobo.9.1"> threats. </span><span class="koboSpan" id="kobo.9.2">This implies not only that the analysts manage the tools and methods of cybersecurity, but that they can also correctly interpret and evaluate the results offered by AI and ML </span><span><span class="koboSpan" id="kobo.10.1">algorithms</span></span><span class="koboSpan" id="kobo.11.1">.</span></p>
<p><span class="koboSpan" id="kobo.12.1">Cybersecurity professionals are therefore called to understand the </span><strong><span class="koboSpan" id="kobo.13.1">logic of the algorithms</span></strong><span class="koboSpan" id="kobo.14.1">, thus proceeding to the </span><strong><span class="koboSpan" id="kobo.15.1">fine tuning</span></strong><span class="koboSpan" id="kobo.16.1"> of their learning phases, based on the results and objectives to be achieved.</span></p>
<p><span class="koboSpan" id="kobo.17.1">Some of the tasks related to the use of AI are as follows:</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.18.1">Classification</span></strong><span><span class="koboSpan" id="kobo.19.1">: This is one of the main tasks in the framework of cybersecurity. </span><span class="koboSpan" id="kobo.19.2">It's used to properly identify types of similar attacks, such as different pieces of </span></span><strong><span class="koboSpan" id="kobo.20.1">malware</span></strong><span class="koboSpan" id="kobo.21.1"> </span><span><span class="koboSpan" id="kobo.22.1">belonging to the same family, that is, having common characteristics and behavior, even if their signatures are distinct (just think of</span></span> <strong><span class="koboSpan" id="kobo.23.1">polymorphic malware</span></strong><span><span class="koboSpan" id="kobo.24.1">). </span><span class="koboSpan" id="kobo.24.2">In the same way, it is important to be able to adequately classify emails, distinguishing</span></span> <strong><span class="koboSpan" id="kobo.25.1">spam</span></strong> <span><span class="koboSpan" id="kobo.26.1">from legitimate emails.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.27.1">Clustering</span></strong><span><span class="koboSpan" id="kobo.28.1">: Clustering is distinguished from classification by the ability to automatically identify the classes to which the samples belong when information about classes is not available in advance (this is a typical goal, as we have seen, of unsupervised learning). </span><span class="koboSpan" id="kobo.28.2">This task is of fundamental importance in</span></span> <strong><span class="koboSpan" id="kobo.29.1">malware analysis</span></strong> <span><span class="koboSpan" id="kobo.30.1">and</span></span> <strong><span class="koboSpan" id="kobo.31.1">forensic analysis</span></strong><span><span class="koboSpan" id="kobo.32.1">.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.33.1">Predictive analysis</span></strong><span><span class="koboSpan" id="kobo.34.1">: By exploiting NNs and DL</span></span><span class="koboSpan" id="kobo.35.1">, </span><span><span class="koboSpan" id="kobo.36.1">it is possible to identify threats as they occur. </span><span class="koboSpan" id="kobo.36.2">To this end, a</span></span> <strong><span class="koboSpan" id="kobo.37.1">highly dynamic</span></strong> <span><span class="koboSpan" id="kobo.38.1">approach must be adopted, which allows algorithms to optimize their learning capabilities automatically.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.39.1">Possible uses of AI in cybersecurity are as follows:</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.40.1">Network protection</span></strong><span><span class="koboSpan" id="kobo.41.1">: The use of ML allows the implementation of highly sophisticated</span></span> <strong><span class="koboSpan" id="kobo.42.1">intrusion detection systems</span></strong> <span><span class="koboSpan" id="kobo.43.1">(</span></span><strong><span class="koboSpan" id="kobo.44.1">IDS</span></strong><span><span><span class="koboSpan" id="kobo.45.1">), which are to be used in the network perimeter protection area.</span></span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.46.1">Endpoint protection</span></strong><span><span class="koboSpan" id="kobo.47.1">: Threats such as</span></span> <strong><span class="koboSpan" id="kobo.48.1">ransomware</span></strong><span class="koboSpan" id="kobo.49.1"> </span><span><span class="koboSpan" id="kobo.50.1">can be adequately detected by adopting algorithms that learn the behaviors that are typical of these types of malware, thus overcoming the </span></span><strong><span class="koboSpan" id="kobo.51.1">limitations of traditional antivirus</span></strong> <span><span class="koboSpan" id="kobo.52.1">software.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.53.1">Application security</span></strong><span><span class="koboSpan" id="kobo.54.1">: Some of the most insidious types of attacks on web applications include</span></span> <strong><span class="koboSpan" id="kobo.55.1">Server Side Request Forgery</span></strong> <span><span class="koboSpan" id="kobo.56.1">(</span><strong><span class="koboSpan" id="kobo.57.1">SSRF</span></strong><span class="koboSpan" id="kobo.58.1">) attacks,</span></span> <strong><span class="koboSpan" id="kobo.59.1">SQL injection</span></strong><span><span class="koboSpan" id="kobo.60.1">, </span><strong><span class="koboSpan" id="kobo.61.1">Cross-Site Scripting</span></strong></span><span><span class="koboSpan" id="kobo.62.1"> (</span><strong><span class="koboSpan" id="kobo.63.1">XSS</span></strong><span class="koboSpan" id="kobo.64.1">), and</span></span><span class="koboSpan" id="kobo.65.1"> </span><strong><span class="koboSpan" id="kobo.66.1">Distributed Denial of Service</span></strong><span><span class="koboSpan" id="kobo.67.1"> (</span><strong><span class="koboSpan" id="kobo.68.1">DDoS</span></strong><span class="koboSpan" id="kobo.69.1">) attacks. </span><span class="koboSpan" id="kobo.69.2">These are all types of threats that can be adequately countered by using AI and ML tools and algorithms.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.70.1">Suspect user behavior</span></strong><span><span class="koboSpan" id="kobo.71.1">: Identifying attempts at</span></span> <strong><span class="koboSpan" id="kobo.72.1">fraud</span></strong> <span><span class="koboSpan" id="kobo.73.1">or compromising applications by malicious users at the very moment they occur is one of the emerging areas of application of DL.</span></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we have introduced the fundamental concepts of AI and ML in relation to the context of cybersecurity. </span><span class="koboSpan" id="kobo.2.2">We have presented some of the strategies adopted in the management of automated learning process, and the possible problems that data analysts face. </span><span class="koboSpan" id="kobo.2.3">The concepts and tools that we have learned in this chapter will be used and adapted in the following chapters, addressing the specific problems of cybersecurity.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In the next chapter, we will learn how to manage Jupyter interactive notebooks in more depth, which allows the reader to interactively execute the instructions given and display the results of the execution in real time.</span></p>
<p><span class="koboSpan" id="kobo.4.1">During the course of the book, the concepts of AI and ML will be presented from time to time in the topics covered in the individual chapters, trying to provide a practical interpretation of the algorithms examined. </span><span class="koboSpan" id="kobo.4.2">For those interested in examining the implementation details of the various algorithms used, we suggest that you consult </span><em><span class="koboSpan" id="kobo.5.1">Python Machine Learning - Second Edition</span></em><span class="koboSpan" id="kobo.6.1"> by Sebastian Raschka and Vahid Mirjalili, published by Packt Publishing.</span></p>


            </article>

            
        </section>
    </body></html>