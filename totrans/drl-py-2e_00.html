<html><head></head><body>
  <div id="_idContainer006">
    <h1 id="_idParaDest-5" class="chapterTitle">Preface</h1>
    <p class="normal">With significant enhancement in the quality and quantity of algorithms in recent years, this second edition of <em class="italic">Hands-On Reinforcement Learning with Python</em> has been revamped into an example-rich guide to learning state-of-the-art <strong class="keyword">reinforcement learning</strong> (<strong class="keyword">RL</strong>) and deep RL algorithms with TensorFlow 2 and the OpenAI Gym toolkit.</p>
    <p class="normal">In addition to exploring RL basics and foundational concepts such as the Bellman equation, Markov decision processes, and dynamic programming, this second edition dives deep into the full spectrum of value-based, policy-based, and actor-critic RL methods. It explores state-of-the-art algorithms such as DQN, TRPO, PPO and ACKTR, DDPG, TD3, and SAC in depth, demystifying the underlying math and demonstrating implementations through simple code examples.</p>
    <p class="normal">The book has several new chapters dedicated to new RL techniques including distributional RL, imitation learning, inverse RL, and meta RL. You will learn to leverage Stable Baselines, an improvement of OpenAI's baseline library, to implement popular RL algorithms effortlessly. The book concludes with an overview of promising approaches such as meta-learning and imagination augmented agents in research.</p>
    <h1 id="_idParaDest-6" class="title">Who this book is for</h1>
    <p class="normal">If you're a machine learning developer with little or no experience with neural networks interested in artificial intelligence and want to learn about reinforcement learning from scratch, this book is for you. Basic familiarity with linear algebra, calculus, and Python is required. Some experience with TensorFlow would be a plus.</p>
    <h1 id="_idParaDest-7" class="title">What this book covers</h1>
    <p class="normal"><em class="chapterRef">Chapter 1</em>, <em class="italic">Fundamentals of Reinforcement Learning</em>, helps you build a strong foundation on RL concepts. We will learn about the key elements of RL, the Markov decision process, and several important fundamental concepts such as action spaces, policies, episodes, the value function, and the Q function. At the end of the chapter, we will learn about some of the interesting applications of RL and we will also look into the key terms and terminologies frequently used in RL.</p>
    <p class="normal"><em class="chapterRef">Chapter 2</em>, <em class="italic">A Guide to the Gym Toolkit</em>, provides a complete guide to OpenAI's Gym toolkit. We will understand several interesting environments provided by Gym in detail by implementing them. We will begin our hands-on RL journey from this chapter by implementing several fundamental RL concepts using Gym.</p>
    <p class="normal"><em class="chapterRef">Chapter 3</em>, <em class="italic">The Bellman Equation and Dynamic Programming</em>, will help us understand the Bellman equation in detail with extensive math. Next, we will learn two interesting classic RL algorithms called the value and policy iteration methods, which we can use to find the optimal policy. We will also see how to implement value and policy iteration methods for solving the Frozen Lake problem. </p>
    <p class="normal"><em class="chapterRef">Chapter 4</em>, <em class="italic">Monte Carlo Methods</em>, explains the model-free method, Monte Carlo. We will learn what prediction and control tasks are, and then we will look into Monte Carlo prediction and Monte Carlo control methods in detail. Next, we will implement the Monte Carlo method to solve the blackjack game using the Gym toolkit. </p>
    <p class="normal"><em class="chapterRef">Chapter 5</em>, <em class="italic">Understanding Temporal Difference Learning</em>, deals with one of the most popular and widely used model-free methods called <strong class="keyword">Temporal Difference </strong>(<strong class="keyword">TD</strong>) learning. First, we will learn how the TD prediction method works in detail, and then we will explore the on-policy TD control method called SARSA and the off-policy TD control method called Q learning in detail. We will also implement TD control methods to solve the Frozen Lake problem using Gym.</p>
    <p class="normal"><em class="chapterRef">Chapter 6</em>, <em class="italic">Case Study – The MAB Problem</em>, explains one of the classic problems in RL called the <strong class="keyword">multi-armed bandit</strong> (<strong class="keyword">MAB</strong>) problem. We will start the chapter by understanding what the MAB problem is and then we will learn about several exploration strategies such as epsilon-greedy, softmax exploration, upper confidence bound, and Thompson sampling methods for solving the MAB problem in detail.</p>
    <p class="normal"><em class="chapterRef">Chapter 7</em>, <em class="italic">Deep Learning Foundations</em>, helps us to build a strong foundation on deep learning. We will start the chapter by understanding how artificial neural networks work. Then we will learn several interesting deep learning algorithms, such as recurrent neural networks, LSTM networks, convolutional neural networks, and generative adversarial networks.</p>
    <p class="normal"><em class="chapterRef">Chapter 8</em>, <em class="italic">A Primer on TensorFlow</em>, deals with one of the most popular deep learning libraries called TensorFlow. We will understand how to use TensorFlow by implementing a neural network to recognize handwritten digits. Next, we will learn to perform several math operations using TensorFlow. Later, we will learn about TensorFlow 2.0 and see how it differs from the previous TensorFlow versions.</p>
    <p class="normal"><em class="chapterRef">Chapter 9</em>, <em class="italic">Deep Q Network and Its Variants</em>, enables us to kick-start our deep RL journey. We will learn about one of the most popular deep RL algorithms called the <strong class="keyword">Deep Q Network</strong> (<strong class="keyword">DQN</strong>). We will understand how DQN works step by step along with the extensive math. We will also implement a DQN to play Atari games. Next, we will explore several interesting variants of DQN, called Double DQN, Dueling DQN, DQN with prioritized experience replay, and DRQN. </p>
    <p class="normal"><em class="chapterRef">Chapter 10</em>, <em class="italic">Policy Gradient Method</em>, covers policy gradient methods. We will understand how the policy gradient method works along with the detailed derivation. Next, we will learn several variance reduction methods such as policy gradient with reward-to-go and policy gradient with baseline. We will also understand how to train an agent for the Cart Pole balancing task using policy gradient.</p>
    <p class="normal"><em class="chapterRef">Chapter 11</em>, <em class="italic">Actor-Critic Methods – A2C and A3C</em>, deals with several interesting actor-critic methods such as advantage actor-critic and asynchronous advantage actor-critic. We will learn how these actor-critic methods work in detail, and then we will implement them for a mountain car climbing task using OpenAI Gym.</p>
    <p class="normal"><em class="chapterRef">Chapter 12</em>, <em class="italic">Learning DDPG, TD3, and SAC</em>, covers state-of-the-art deep RL algorithms such as deep deterministic policy gradient, twin delayed DDPG, and soft actor, along with step by step derivation. We will also learn how to implement the DDPG algorithm for performing the inverted pendulum swing-up task using Gym.</p>
    <p class="normal"><em class="chapterRef">Chapter 13</em>, <em class="italic">TRPO, PPO, and ACKTR Methods</em>, deals with several popular policy gradient methods such as TRPO and PPO. We will dive into the math behind TRPO and PPO step by step and understand how TRPO and PPO helps an agent find the optimal policy. Next, we will learn to implement PPO for performing the inverted pendulum swing-up task. At the end, we will learn about the actor-critic method called actor-critic using Kronecker-Factored trust region in detail.</p>
    <p class="normal"><em class="chapterRef">Chapter 14</em>, <em class="italic">Distributional Reinforcement Learning</em>, covers distributional RL algorithms. We will begin the chapter by understanding what distributional RL is. Then we will explore several interesting distributional RL algorithms such as categorical DQN, quantile regression DQN, and distributed distributional DDPG.</p>
    <p class="normal"><em class="chapterRef">Chapter 15</em>, <em class="italic">Imitation Learning and Inverse RL</em>, explains imitation and inverse RL algorithms. First, we will understand how supervised imitation learning, DAgger, and deep Q learning from demonstrations work in detail. Next, we will learn about maximum entropy inverse RL. At the end of the chapter, we will learn about generative adversarial imitation learning. </p>
    <p class="normal"><em class="chapterRef">Chapter 16</em>, <em class="italic">Deep Reinforcement Learning with Stable Baselines</em>, helps us to understand how to implement deep RL algorithms using a library called Stable Baselines. We will learn what Stable Baselines is and how to use it in detail by implementing several interesting Deep RL algorithms such as DQN, A2C, DDPG TRPO, and PPO. </p>
    <p class="normal"><em class="chapterRef">Chapter 17</em>, <em class="italic">Reinforcement Learning Frontiers</em>, covers several interesting avenues in RL, such as meta RL, hierarchical RL, and imagination augmented agents in detail. </p>
    <h1 id="_idParaDest-8" class="title">To get the most out of this book</h1>
    <p class="normal">You need the following software for this book:</p>
    <ul>
      <li class="bullet">Anaconda </li>
      <li class="bullet">Python </li>
      <li class="bullet">Any web browser</li>
    </ul>
    <h2 id="_idParaDest-9" class="title">Download the example code files</h2>
    <p class="normal">You can download the example code files for this book from your account at <a href="http://www.packtpub.com"><span class="url">http://www.packtpub.com</span></a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support"><span class="url">http://www.packtpub.com/support</span></a> and register to have the files emailed directly to you.</p>
    <p class="normal">You can download the code files by following these steps:</p>
    <ol>
      <li class="numbered">Log in or register at <a href="http://www.packtpub.com"><span class="url">http://www.packtpub.com</span></a>.</li>
      <li class="numbered">Select the <strong class="screenText">SUPPORT</strong> tab.</li>
      <li class="numbered">Click on <strong class="screenText">Code Downloads &amp; Errata</strong>.</li>
      <li class="numbered">Enter the name of the book in the <strong class="screenText">Search</strong> box and follow the on-screen instructions.</li>
    </ol>
    <p class="normal">Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
    <ul>
      <li class="bullet">WinRAR / 7-Zip for Windows</li>
      <li class="bullet">Zipeg / iZip / UnRarX for Mac</li>
      <li class="bullet">7-Zip / PeaZip for Linux</li>
    </ul>
    <p class="normal">The code bundle for the book is also hosted on GitHub at <a href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python"><span class="url">https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python</span></a>. We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/"><span class="url">https://github.com/PacktPublishing/</span></a>. Check them out!</p>
    <h2 id="_idParaDest-10" class="title">Download the color images</h2>
    <p class="normal">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf"><span class="url">https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf</span></a>.</p>
    <h2 id="_idParaDest-11" class="title">Conventions used</h2>
    <p class="normal">There are a number of text conventions used throughout this book.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">CodeInText</code>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. For example: "<code class="Code-In-Text--PACKT-">epsilon_greedy</code> computes the optimal policy."</p>
    <p class="normal">A block of code is set as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy</span><span class="hljs-function">(</span><span class="hljs-params">epsilon</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">if</span> np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &lt; epsilon:
        <span class="hljs-keyword">return</span> env.action_space.sample()
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> np.argmax(Q)
</code></pre>
    <p class="normal">When we wish to draw your attention to a particular part of a code block, the relevant lines or items are highlighted:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy</span><span class="hljs-function">(</span><span class="hljs-params">epsilon</span><span class="hljs-function">):</span>
<span class="code-highlight"><strong class="hljs-slc">    </strong><strong class="hljs-keyword-slc">if</strong><strong class="hljs-slc"> np.random.uniform(</strong><strong class="hljs-number-slc">0</strong><strong class="hljs-slc">,</strong><strong class="hljs-number-slc">1</strong><strong class="hljs-slc">) &lt; epsilon:</strong></span>
        <span class="hljs-keyword">return</span> env.action_space.sample()
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> np.argmax(Q)
</code></pre>
    <p class="normal">Any command-line input or output is written as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">source activate universe
</code></pre>
    <p class="normal"><strong class="keyword">Bold</strong>: Indicates a new term, an important word, or words that you see on the screen, for example, in menus or dialog boxes, also appear in the text like this. For example: "The<strong class="keyword"> Markov Reward Process</strong> (<strong class="keyword">MRP</strong>) is an extension of the Markov chain with the reward function."</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Warnings or important notes appear like this.</p>
    </div>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Tips and tricks appear like this.</p>
    </div>
    <h1 id="_idParaDest-12" class="title">Get in touch</h1>
    <p class="normal">Feedback from our readers is always welcome.</p>
    <p class="normal"><strong class="keyword">General feedback</strong>: Email <code class="Code-In-Text--PACKT-">feedback@packtpub.com</code>, and mention the book's title in the subject of your message. If you have questions about any aspect of this book, please email us at <code class="Code-In-Text--PACKT-">questions@packtpub.com</code>.</p>
    <p class="normal"><strong class="keyword">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book we would be grateful if you would report this to us. Please visit, <a href="http://www.packtpub.com/submit-errata"><span class="url">http://www.packtpub.com/submit-errata</span></a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
    <p class="normal"><strong class="keyword">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <code class="Code-In-Text--PACKT-">copyright@packtpub.com</code> with a link to the material.</p>
    <p class="normal"><strong class="keyword">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com"><span class="url">http://authors.packtpub.com</span></a>.</p>
    <h2 id="_idParaDest-13" class="title">Reviews</h2>
    <p class="normal">Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
    <p class="normal">For more information about Packt, please visit <a href="http://packtpub.com"><span class="url">packtpub.com</span></a>.</p>
  </div>
</body></html>