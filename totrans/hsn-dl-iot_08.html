<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Physiological and Psychological State Detection in IoT</h1>
                </header>
            
            <article>
                
<p>Human physiological and psychological states can provide very useful information about a person's activity and emotions. This information can be used in many application domains, including smart homes, smart cars, entertainment, education, rehabilitation and health support, sports, and industrial manufacturing, to improve existing services and/or offer new services. Many IoT applications incorporate sensors and processors for human pose estimation or activity and emotion recognition. However, the detection of activities or emotions based on the sensor data is a challenging task. In recent years, DL-based approaches have become a popular and effective way to address this challenge.</p>
<p>This chapter presents DL-based human physiological and psychological state detection techniques for IoT applications in general. The first part of this chapter will briefly describe different IoT applications and their physiological and psychological state detection-based decision making. In addition, it will briefly discuss two IoT applications and their physiological and psychological state detection-based implementations in a real-world scenario. In the second part of the chapter, we will present the DL-based implementations of the two IoT applications. In this chapter, we will cover the following topics:</p>
<ul>
<li>IoT-based human physiological and psychological state detection</li>
<li>Use case one: IoT-based remote progress monitoring of physiotherapy</li>
<li>Implementation of IoT-based remote progress monitoring of physiotherapy</li>
<li>Use case two: the smart classroom</li>
<li>Implementation of the s<span>mart classroom</span></li>
<li>Deep learning for human activity and emotion detection in IoT</li>
<li>LSTM and CNNs and transfer learning for HAR/FER in IoT applications</li>
<li>Data collection</li>
<li>Data preprocessing</li>
<li>Model training</li>
<li>Evaluation of the models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IoT-based human physiological and psychological state detection </h1>
                </header>
            
            <article>
                
<p style="text-align: justify">In recent years, human physiological and psychological state detection have been used in many application domains to improve existing services and/or offer new services. IoT, combined with DL techniques, can be used in applications to detect human physiological and psychological states. The following diagram highlights a few key applications of these detection approaches:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1079 image-border" src="assets/78f2e175-fada-4f76-a8be-67400c6d4590.png" style="width:79.50em;height:37.08em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we will learn in detail about the two state detection variants:</p>
<ul style="text-align: justify">
<li style="text-align: justify"><strong>Physiological state detection</strong>: Physiological state or activity detection are useful tools in many applications, including assisted living for vulnerable people, such as the elderly, and in remote physical therapy/rehabilitation systems. In assisted living for elderly people, falls among older people are detrimental to the health of the victim because of the associated risk of physical injury. A fall can also have financial consequences, due to the medical costs and need for hospitalization. Moreover, falls can also reduce the person's life expectancy, especially in the case of a <strong>long-lie</strong>. It is also worth noting that the medical expenses linked with falls are extremely high. For example, the yearly cost of falls in the US alone is expected to reach US $67 billion by 2020. In this context, automated and remote fall detection using DL-supported IoT applications can address the challenge, thus improving the quality of life for elderly people and minimizing the associated costs. Another key area of human activity detection applications is the remote physical therapy monitoring system. This is the first use case for this chapter, and we will present an overview of it in the next section.</li>
<li>
<p style="text-align: justify"><strong>Psychological state detection: </strong>Facial expressions are good reflections of human psychological states, and they are important factors in human communication that help us to understand the intentions of <span>humans</span>. Generally, we can infer the emotional states of other people, such as joy, sadness, and anger, by analyzing their facial expressions and vocal tone. Forms of non-verbal communication make up two-thirds of all human interactions. Facial expressions, in their imparted emotional meaning, are one of the main non-verbal interpersonal communication channels. Hence, facial expression-based emotion detection can be useful in understanding people's behavior. It can, therefore, help to improve existing services and/or new services, including personalized customer services. IoT applications, such as smart healthcare, smart education, and security and safety, can improve their services through DL-based emotion detection or sentiment analysis. For example, in a smart classroom, a teacher can analyze the students’ sentiment in real time or quasi real time to offer personalized and/or group-orientated teaching. This will improve their learning experience.</p>
</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case one – remote progress monitoring of physiotherapy </h1>
                </header>
            
            <article>
                
<p style="text-align: justify">Physical therapy is a big part of healthcare. There is a huge gap between the demand for physical therapy and our ability to deliver that therapy. Most countries in the world are still greatly dependent on the one-to-one patient-therapist interaction (which is the gold standard), but it is not a scalable solution and not cost-effective for either patients or healthcare providers. In addition, most existing therapies and their updates rely on average data instead of an individual’s unique data, and sometimes this data is qualitative (for example, <em>Yes, I did what you told me to do</em>) rather than quantitative. This is a challenge regarding effective therapy. Finally, many people—especially elderly people—are living with <strong>multiple chronic conditions </strong>(<strong>MCC</strong>), and these conditions are generally treated separately. This can result in suboptimal care or even cases where these conditions can conflict with each other. For example, in the case of a patient with diabetes and back pain: a diabetic carer may recommend walking, whereas a back pain carer may forbid it. In this context, IoT is already changing healthcare. It <span>can address most of these challenges</span> with the support of machine learning/deep learning and data analysis tools, and offer effective physiotherapy by providing real-time or quasi real-time information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of use case one</h1>
                </header>
            
            <article>
                
<p>Progress monitoring is a key challenge in traditional therapy. An IoT-based therapy can solve the progress monitoring issue. The following diagram briefly presents how the IoT-based remote physiotherapy monitoring system will work:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-910 image-border" src="assets/bfb73806-d56e-4a24-9626-2f6359fab343.png" style="width:53.58em;height:35.42em;"/></p>
<p style="text-align: justify">One of the key components of this application is the activity monitoring of the subject (patient) that will help the therapist remotely observe how the <span><span>patient</span></span> is complying with the suggested therapy, and whether they are making progress. As shown in the preceding diagram, the IoT-based remote physiotherapy monitoring system consists of four main elements:</p>
<ul>
<li style="text-align: justify"><strong>Sensors and patient-side computing platform</strong>: For this use case, we are considering two sensors: an accelerometer and a gyroscope. Both of them measure three-dimensional readings linked with the subject's activities. For these sensors, we can use dedicated sensors or a smartphone's sensors (these sensors are embedded within most smartphones). For the client-side computing platform, we can consider Raspberry Pi for the dedicated sensors and the smartphone (if we are using the smartphone sensors). The sensors need to be properly placed in order to measure signals correctly. The sensors can be used for continuous or event-wise (such as during exercise) monitoring of the subject’s activities.</li>
<li style="text-align: justify"><strong>Care providers and therapists</strong>: Care providers, such as hospitals with doctors and medical/healthcare databases, are connected through a cloud platform/HealthCloud. The main care provider for the therapy use case is a therapist, and hospitals/doctors will offer support to the therapist when required.</li>
<li style="text-align: justify"><strong>DL-based human activity detection</strong>: In this phase, the edge-computing device will be installed with an app. The installed app on the smartphone or Raspberry Pi will be loaded with a pretrained human activity detection and classification model. Once the accelerometer and gyroscope detect any signal, they send it to the smartphone or Raspberry Pi for processing and detection using the DL model, and finally inform the therapist for their feedback or intervention if necessary.</li>
<li style="text-align: justify"><strong>HealthCloud for model learning</strong>: The HealthCloud is a cloud computing platform mainly designed for healthcare-related services. This will train the selected DL model in human activity detection and classification using reference datasets. This learned model will be preinstalled in the smartphone or Raspberry Pi.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case two — IoT-based smart classroom </h1>
                </header>
            
            <article>
                
<p>The higher education dropout rate is increasing worldwide. For example, dropout rates among UK university students have increased for the third consecutive year. Three of the top eight reasons for these dropouts are:</p>
<ul>
<li>Lack of quality time with teachers and counselors</li>
<li>Demotivating school environment</li>
<li>Lack of student support</li>
</ul>
<p>One of the key challenges in addressing these issues is knowing the students (such as knowing whether a student is following a topic or not) and delivering lectures/tutorials and other support accordingly. One potential approach is to know the emotions of the students, which is challenging in a large classroom, computer lab, or in e-learning environments. The use of technologies (including IoT with the support of DL models) can help to recognize emotion using facial expression and/or speech. The second use case of this chapter aims at increasing student performance in the classroom by detecting emotions and managing the lecture/lab accordingly.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of use case two</h1>
                </header>
            
            <article>
                
<p>The following diagram shows a simplified implementation of an IoT-based smart classroom application:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4e9c55a8-bd4c-4f57-a31c-fe7911ee824b.png"/></p>
<p>The facial-expression-based emotion analysis implementation consists of three main elements:</p>
<ul>
<li><strong>Sensors and computing platform: </strong>For this use case, we need at least one CCTV camera that can cover the classroom and be connected to the computing platform wirelessly or via a concealed cable in the walls. The lecturer's computer in the classroom can work as the computing platform. The computer will continuously process the video signals and convert them into images for the image-based facial expression analysis.</li>
<li><strong>Facial expression-based emotion detection</strong>: The lecturer's computer will be installed with an app. The installed app will be loaded with a pretrained facial expression-based detection and classification model. Once the DL model receives the facial images of the students, it identifies their emotions (such as happy/unhappy/confused) regarding a lecture and notifies the lecturer to take the necessary action.</li>
<li><strong>Desktop or server for model learning:</strong> The lecturer's computer will be connected to a university server or cloud computing platform, and this will train/retrain the model for facial expression-based emotion recognition and classification using reference datasets. This learned model will be preinstalled in the lecturer's PC in the classroom.</li>
</ul>
<p>All of the following sections will describe the implementation of the DL-based human activity and emotion recognition needed for the aforementioned use cases. All of the necessary codes are available in the chapter's code folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning for human activity and emotion detection in IoT</h1>
                </header>
            
            <article>
                
<p>It is important to understand the working principle of an accelerometer- and gyroscope-based human activity detection system, and of a facial expression-based emotion detection system, before discussing the useful deep learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatic human activity recognition system</h1>
                </header>
            
            <article>
                
<p>Automatic <strong>human activity recognition</strong> (<strong>HAR</strong>) systems detect human activity, based on raw accelerometer and gyroscope signals. The following diagram shows a schematic of a DL-based HAR that consists of three different phases. They are as follows:</p>
<ul>
<li>IoT deployment or instrumentation of the subject or person</li>
<li>Feature extraction and model development</li>
<li>Activity classification/identification</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e3327277-f93f-46f8-9279-7cebe6cdef39.png"/></p>
<p>Generally, classical HAR approaches mainly rely on heuristic hand-crafted feature extraction methods, which is a complex process and not well suited for resource-constrained IoT devices. More recent DL-based HAR approaches perform the feature extraction automatically, and they can work well on resource-constrained IoT devices. Most HAR approaches consider six different activities, including walking, running, sitting, standing, climbing upstairs, and coming downstairs. These activities exhibit differences in accelerometer and gyroscope signals, and a classifier exploits the differences to identify the current activity—which could form a part of physiotherapy (such as running).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automated human emotion detection system</h1>
                </header>
            
            <article>
                
<p>Automated <strong>human emotion recognition</strong> (<strong>HER</strong>) can be done by using either one of the following signals/inputs from the subject (human) or a combination thereof:</p>
<ul>
<li>Facial expression</li>
<li>Speech/audio</li>
<li>Text</li>
</ul>
<p class="mce-root"/>
<p style="text-align: justify">This chapter considers the <strong>facial expression recognition</strong> (<strong>FER</strong>)-based HER. A DL-based automated FER consists of three main steps: preprocessing, deep feature learning, and classification. The following diagram highlights these main steps in an FER-based HER. </p>
<p>Image processing for facial expression analysis requires preprocessing, since the different types of emotions (such as anger, disgust, fear, happiness, sadness, surprise, and neutral) have subtle differences. Variations in input images that are irrelevant to FER, including different backgrounds, illuminations, and head poses, can be removed through preprocessing to improve the model prediction/classification accuracy. Face alignment, data augmentation, and image normalization are a few of the key preprocessing techniques. Most open source datasets for FER are not sufficient to generalize the FER approach. Data augmentation is essential in order to improve an existing dataset in terms of FER. Face alignment and image normalization are useful for improving individual images. The final stage of the FER DL pipeline is for the DL algorithm to learn and classify the features, hence emotions. Most image recognition DL algorithms, including CNN and RNN, are suitable for the final stage:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2569a659-4081-4324-a81a-9958f6c121e1.png"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning models for HAR and emotion detection</h1>
                </header>
            
            <article>
                
<p style="text-align: justify">Generally, human activity recognition systems use accelerometer and gyroscope signals, which are time series data. Sometimes, the recognition process uses a combination of time series and spatial data. In this context, <strong>Recurrent Neural Network</strong> (<strong>RNN</strong>) and LSTM are potential candidates for the former type of signals because of their capability to incorporate temporal features of input during evolution. On the other hand, CNNs are good for spatial aspects of accelerometer and gyroscope signals. Hence, a combination or hybrid of CNNs and LSTMs/RNNs is ideal for the former type of signals. We will use an LSTM model for the HAR use case as it can address the temporal aspects of human activities.</p>
<p style="text-align: justify">Unlike HAR systems, FER-based human emotion detection systems generally rely on facial expressions images, which rely on the local or spatial correlations between the pixel values of the images. Any DL model that works well for image recognition is fine for an FER task and, equally, for emotion detection. A number of deep learning algorithms or models have been used for image recognition, and the <strong>deep belief network</strong> (<strong>DBN</strong>) and CNNs are the top two candidates. In this chapter, we are considering CNNs because of their performance in image recognition.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM, CNNs, and transfer learning for HAR/FER in IoT applications</h1>
                </header>
            
            <article>
                
<p>LSTM is the widely used DL model for HAR—including in IoT-based HAR—because its memory capacity can deal better with time series data (such as HAR data) than other models, including CNN. The LSTM implementation of HAR can support transfer learning and is suitable for resource-constrained IoT devices. Generally, FER relies on image processing, and the CNN is the best model for image processing. Therefore, we implement use case two (FER) using a CNN model. In <a href="b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml"/><a href="b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml">Chapter 3</a>, <em>Image Recognition in IoT</em>, we presented an overview of two popular implementations of the CNN (such as incentive V3 and Mobilenets) and their corresponding transfer learning. In the following paragraphs, we briefly present an overview of the baseline LSTM.</p>
<p style="text-align: justify">LSTM is an extension of RNNs. Many variants of LSTM are proposed, and they follow the baseline LSTM. The following is a schematic diagram of the baseline LSTM:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fee37e7b-6974-4ddf-bc1c-d04d9a769793.png" style="width:39.17em;height:26.92em;"/></p>
<p>As shown in the preceding diagram, LSTM mainly consists of two components. They have a memory cell or neuron, and each cell or neuron has a multiplicative forget gate, read gate, and write gate. These gates control the access to memory cells/neurons and prevent them from being disturbed by irrelevant inputs. These gates are controlled through 0/1 or off/on. For example, if the forget gate is on/1, the neuron/cell writes its data to itself, and if the gate is off/0, the neuron forgets its last content. Other gates are controlled in a similar fashion.</p>
<p style="text-align: justify">Unlike RNNs, LSTMs use forget gates to actively control the cell/neuron states and ensure they do not degrade. Importantly, LSTM models perform better than RNN models in the case of data that has a long dependency in time. Many IoT applications, such as human activity recognition and disaster prediction based on environmental monitoring, exhibit this long-time dependency.</p>
<p style="text-align: justify">As the FER considered for use case two is based on image processing, CNNs are the best choice. CNN has different implementations, including a simple CNN, two versions of Mobilenets, and Incentive3. Use case two will explore a simple CNN and Mobilenet V1 for the FER part of the implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data collection</h1>
                </header>
            
            <article>
                
<p>Data collection for HAR and/FER is a challenging task for many reasons, including privacy. As a result, open source quality datasets are limited in number. For the HAR implementation in use case one, we are using a very popular and open source <strong>Wireless Sensor Data Mining</strong> (<strong>WISDM</strong>) lab dataset . The dataset consists of 54,901 samples collected from 36 different subjects. For privacy reasons, usernames are masked with ID numbers from 1-36. The data was collected for six different activities undertaken by the subjects: standing, sitting, jogging, walking, going downstairs, and climbing upstairs. The dataset contains three-axis accelerometer data with more than 200 time steps for each sample. The following screenshot is a <span><span>sample </span></span>of the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/96d959f8-1fb5-4f89-9718-7e82fb7f64bb.png" style="width:39.58em;height:18.25em;"/></p>
<p>For the FER-based emotion detection in use case two, we used two different datasets. The first one is the popular and open source FER2013 dataset. This dataset contains 48 x 48 pixel grayscale images of human faces. These images are preprocessed and ready to be used directly for training and validation. The images can be classified into seven categories (<em>0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise,</em> and <em>6=Neutral</em>). The dataset in CSV format contains information about pixel values of the face images rather than the images. The following screenshot shows a few values of the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9bcdb674-6626-49b7-ae3d-0f0898470dd7.png"/></p>
<p>The split ratio between the training and the validation dataset is <em>80:20</em>.</p>
<p>We also prepared a dataset through Google search, particularly for Mobilenet V1. The dataset is not a big one, as it consists of five classes of emotions, and each of those consists of more than 100 images. These images are not preprocessed. The following diagram shows a folder view of the prepared dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/54928366-063a-44fa-ac2b-1d5051415c06.png"/></p>
<p class="mce-root"/>
<p>For data collection (each class of the dataset), we can follow a four step process:</p>
<ol>
<li><strong>Search:</strong> Use any browser (we used Chrome), go to Google, and search the appropriate word combination for the class/emotion (such as <em>angry human</em>) in Google images.</li>
<li><strong>Image URL gatherings</strong>: This step utilizes a few lines of JavaScript code to gather the image URLs. The gathered URLs can be used in Python to download the images. To do that, select the JavaScript console (assuming you will use the Chrome web browser, but you can use Firefox as well) by clicking the <span class="packt_screen">View</span> | <span class="packt_screen">Developer</span> | <span class="packt_screen">JavaScript</span> console (in macOS) and customize and control<strong> </strong><strong>Google Chrome</strong> | <strong>More tools</strong> | <strong>Developer tools</strong> (Windows OS). Once you have selected the JavaScript console, this will enable you to execute JavaScript in a REPL-like manner. Now, do the following in order:
<ol>
<li><span>Scroll down the page until you have found all images relevant to your query. From there, you need to grab the URLs for the images.</span></li>
</ol>
</li>
</ol>
<p style="padding-left: 150px">Switch back to the JavaScript console and then copy and paste the following JavaScript snippet into the console:</p>
<div>
<pre style="padding-left: 150px">// Get the jquery into the JavaScript console<br/>var script = document.createElement('script');<br/>script.src = "https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js";<br/>document.getElementsByTagName('head')[0].appendChild(script)</pre></div>
<ol start="2">
<li style="list-style-type: none">
<ol start="2">
<li>The preceding code snippet will pull down the jQuery JavaScript library, and now you can use a CSS selector to grab a list of URLs using the following snippet:</li>
</ol>
</li>
</ol>
<div>
<pre style="padding-left: 120px">// Grab the chosen URLs<br/>var urls = $('.rg_di .rg_meta').map(function() { return JSON.parse($(this).text()).ou; });<br/><br/></pre></div>
<ol start="3">
<li style="list-style-type: none">
<ol start="3">
<li>Finally, write the URLs to a file (one per line) using the following snippet:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">// write the URls to file (one per line)<br/>var textToSave = urls.toArray().join('\n');<br/>var hiddenElement = document.createElement('a');<br/>hiddenElement.href = 'data:attachment/text,' + encodeURI(textToSave);<br/>hiddenElement.target = '_blank';<br/>hiddenElement.download = 'emotion_images_urls.txt';<br/>hiddenElement.click();</pre>
<p style="padding-left: 120px">Once you execute the preceding code snippet, you will have a file named <kbd>emotion_images_urls.txt</kbd> in your default download directory.</p>
<ol start="3">
<li><strong>Downloading the images</strong>: Now, you are ready to download the images running <kbd>download_images.py</kbd> <span>(available in the code folder of the chapter) on the previously downloaded <kbd>emotion_images_urls.txt</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px">python download_images.py emotion_images_urls.txt</pre>
<ol start="4">
<li><strong>Exploration</strong>:<strong> </strong>Once we have downloaded the images, we need to explore the images in order to delete the irrelevant ones. We can do this through a bit of manual inspection. After that, we need to resize and crop match our requirements.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data exploration</h1>
                </header>
            
            <article>
                
<p>In this section, we will examine in more detail the datasets that we will be using:</p>
<ul>
<li><strong>HAR dataset</strong>:<strong> </strong>The dataset is a text file that consists of the different subjects accelerations for each of the six activities. We can do a data distribution check for the dataset as it is not easy to perceive the data distribution by looking at the text file only. The following graph summarizes the breakdown for the training set:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><sup><img class="aligncenter size-full wp-image-911 image-border" src="assets/6bccc879-56a3-4d13-8030-66e1b6500f92.png" style="width:20.42em;height:16.75em;"/></sup></p>
<p style="padding-left: 60px">As we can see from the preceding graph, the training dataset consists of more walking and jogging data than the other four activities. This is good for the DL model, since walking and jogging are moving activities, where the range of acceleration data could be wide. To visualize this, we have explored activity-wise acceleration measurements/data for 200 time steps for each activity. <span>The following </span><span>screenshot</span><span> represents 200 time step acceleration measurements for sitting:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-912 image-border" src="assets/fb95debf-8537-4d23-a147-f39438dd5467.png" style="width:53.50em;height:37.33em;"/></p>
<p><span>The following </span><span>screenshot</span><span> represents 200 time step acceleration measurements for standing:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-913 image-border" src="assets/de028247-dee1-43f9-9457-481c235ff5ed.png" style="width:56.25em;height:39.25em;"/></p>
<p><span>The following </span><span>screenshot</span><span> represents 200 time step acceleration measurements for walking:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-914 image-border" src="assets/5c2dedc6-73ea-4372-9891-cf093c734c0e.png" style="width:68.25em;height:47.25em;"/></p>
<p><span>The following </span><span>screenshot</span><span> represents 200 time step acceleration measurements for </span><span>jogging</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-915 image-border" src="assets/464b6e30-9e87-493c-b45e-e71b4af961b6.png" style="width:69.42em;height:47.58em;"/></p>
<p style="padding-left: 60px">It is clear from the preceding diagrams that <span>walking and jogging activities are busier than the other activities as they reflect the user's movements.</span></p>
<ul>
<li><strong>FER dataset</strong>:<strong> </strong>We need to convert the FER2013 dataset pixel values of the face images into actual images to explore them. We can use the following code to convert the pixel values to images:</li>
</ul>
<pre style="padding-left: 60px">import os<br/>import csv<br/>import argparse<br/>import numpy as np<br/>import scipy.misc<br/>parser = argparse.ArgumentParser()<br/>parser.add_argument('-f', '--file', <em>required</em>=True, <em>help</em>="path of the csv file")<br/>parser.add_argument('-o', '--output', <em>required</em>=True, <em>help</em>="path of the output directory")<br/>args = parser.parse_args()<br/>w, h = 48, 48<br/>image = np.zeros((h, w), dtype=np.uint8)<br/>id = 1<br/>with open(args.file) as csvfile:<br/>    datareader = csv.reader(csvfile, delimiter =',')<br/>    next(datareader,None)<br/>    for row in datareader:       <br/>        emotion = row[0]<br/>        pixels = row[1].split()<br/>        usage = row[2]<br/>        pixels_array = np.asarray(pixels, dtype=np.int)<br/>        image = pixels_array.reshape(w, h)<br/>        stacked_image = np.dstack((image,) * 3)<br/>        image_folder = os.path.join(args.output, usage)<br/>        if not os.path.exists(image_folder):<br/>            os.makedirs(image_folder)<br/>        image_file =  os.path.join(image_folder , emotion +'_'+ str(id) +'.jpg')<br/>        scipy.misc.imsave(image_file, stacked_image)<br/>        id+=1<br/>        if id % 100 == 0:<br/>            print('Processed {} images'.format(id))<br/>print("Finished conversion to {} images".format(id))</pre>
<p style="padding-left: 60px">We can execute the previous code using the following code:</p>
<pre style="padding-left: 60px"><strong>python imager_converter.py</strong></pre>
<p style="padding-left: 60px">Once we have the diagram, we can run the following code for image exploration: </p>
<pre style="padding-left: 60px"><strong>python image_explorer.py</strong></pre>
<p style="padding-left: 60px"><span>This will produce a figure similar to the following image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bff0196b-d563-496a-ad52-b2a3ad85bdb3.png"/></p>
<p>As we can see in the preceding image, the FER dataset is preprocessed well. On the other hand, the second dataset (we named it FER2019) is not preprocessed, including the image sizes, as can be seen in the following image:</p>
<p> </p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/738621cb-f6c4-4c56-941f-f477391992e9.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preprocessing </h1>
                </header>
            
            <article>
                
<p>Data preprocessing is an essential step for a deep learning pipeline. The HAR and FER2013 datasets are preprocessed well. However, the downloaded image files for the second dataset of use case two are not preprocessed. As shown in the preceding image, the images are not uniform in size or pixels and the dataset is not large in size; hence, they require data augmentation. Popular augmentation techniques are flip, rotation, scale, crop, translation, and Gaussian noise. Many tools are available for each of these activities. You can use the tools or write their own script to do the data augmentation. A useful tool is <strong>Augmentor</strong>, a Python library for machine learning. We can install the tool in our Python and use it for augmentation. The following code (<kbd>data_augmentation.py</kbd>) is a simple data augmentation process that executes flipping, rotation, cropping, and resizing of the input images:</p>
<pre># Import the module<br/>import Augmentor<br/>da = Augmentor.Pipeline("data_augmentation_test")<br/># Define the augmentation<br/>da.rotate90(probability=0.5)<br/>da.rotate270(probability=0.5)<br/>da.flip_left_right(probability=0.8)<br/>da.flip_top_bottom(probability=0.3)<br/>da.crop_random(probability=1, percentage_area=0.5)<br/>da.resize(probability=1.0, width=120, height=120)<br/># Do the augmentation operation: sampling<br/>da.sample(25)</pre>
<p>The following image presents two original images and their augmented samples (3 out of 25 samples):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/16b0faa7-a06b-4825-b765-cda89f838ecd.png"/></p>
<p>As shown in the preceding image, the augmented images are uniform in size, flipped, and rotated.</p>
<p>The following are two key issues to be noted during the training image set preparation:</p>
<ul>
<li><strong>Data size</strong>: We need to collect at least 100 images for each class to train a model that works well. The more we can gather, the better the likely accuracy of the trained model. However, one-shot learning (an object categorization technique) can work using fewer than 100 training samples. We also made sure that the images are a good representation of what our application will actually face in real implementation.</li>
<li><strong>Data heterogeneity</strong><span><em>:</em></span> Data collected for training should be heterogeneous. For example, images for FER should be from a diverse range of skin tones or different views of the same expressions.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training </h1>
                </header>
            
            <article>
                
<p>As we mentioned earlier, we are using LSTM for use case one and two implementations of CNN (simple CNN and Mobilenet V1) for use case two. All of these DL implementations support transfer learning for both use cases that do not require training from scratch. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case one</h1>
                </header>
            
            <article>
                
<p>We consider a stacked LSTM, which is a popular DL model for sequence prediction, including time series problems. A stacked LSTM architecture consists of two or more LSTM layers. We implemented the HAR for use case one, using a two-layered stacked LSTM architecture. The following diagram presents a two-layered LSTM, where the first layer provides a sequence of outputs instead of a single value output to the second LSTM layer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/06b140d6-c70c-442a-93af-5194e6d1018f.png"/></p>
<p class="mce-root"/>
<p>We can train and test the model by running the <kbd>LSTM -HAR.py</kbd> code, available in the <kbd>use-case-1</kbd> folder (after making the necessary changes to your setup, such as the <kbd>data</kbd> directory):</p>
<pre><strong>python LSTM-HAR.py</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case two</h1>
                </header>
            
            <article>
                
<p>We used two different architectures of CNN for the FER-based emotion detection in the smart classroom. The first one is a simple CNN architecture. To train the model on the FER2013 dataset, we need to run <kbd>CNN-FER2013.py</kbd>, which is available in the chapter's <kbd>use-case-2</kbd> code folder, or use the notebook. To run in all default settings of <kbd>CNN-FER2013.py</kbd> <em>(</em>after making any necessary changes to your setup, such as the <kbd>data</kbd> directory)<em>,</em> we need to run the following in the Command Prompt:</p>
<pre><strong>python CNN-FER2013.py</strong></pre>
<p>The training and testing of the model on the FER2103 dataset could take a few hours. The following diagram, generated from the TensorBoard log files, presents the network used for use case two:</p>
<p class="CDPAlignCenter CDPAlign"> <img src="assets/2e49dc9c-ed05-4ac9-9e21-002a00a9b6e4.png"/></p>
<p>We can retrain Mobilenet V1 on FER2019 by running the following code:</p>
<pre><strong>python retrain.py \</strong><br/><br/><strong>--output_graph=trained_model_mobilenetv1/retrained_graph.pb \</strong><br/><strong>--output_labels=trained_model_mobilenetv1/retrained_labels.txt   \</strong><br/><strong>--architecture =mobilenet_1.0_224 \</strong><br/><strong>--image_dir= your dataset directory</strong></pre>
<p>Once we run the preceding commands, they will generate the retrain models (<kbd>retrained_graph.pb</kbd>) and label text (<kbd>retrained_labels.txt</kbd> ) in the given directory. This will also store the model's summary information in a directory. The summary information (the <kbd>--summaries_dir</kbd> argument with <kbd>retrain_logs</kbd> as the default value) can be used by the TensorBoard to visualize different aspects of the models, including the networks and their performance graphs. If we type the following command into the Terminal or command window, it will run the TensorBoard:</p>
<div>
<pre>tensorboard --logdir retrain_logs</pre></div>
<p>Once the TensorBoard is running, navigate your web browser to <kbd>localhost:6006</kbd> to view the TensorBoard and the network of the corresponding model. The following diagram presents a network for the Mobilenet V1 architecture used in the implementation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d39947f7-a751-4270-9fbf-2630e6d6610b.png" style="width:41.42em;height:50.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model evaluation</h1>
                </header>
            
            <article>
                
<p style="text-align: justify">We can evaluate the models in three different aspects:</p>
<ul>
<li style="text-align: justify">Learning/(re)training time</li>
<li style="text-align: justify">Storage requirement</li>
<li style="text-align: justify">Performance (accuracy) </li>
</ul>
<p style="text-align: justify">In terms of training time, in a desktop (Intel Xenon CPU E5-1650 v3@3.5 GHz and 32 GB RAM) with GPU support, LSTM on the HAR dataset, CNN on FER2013, and Mobilenet V1 on the FER2019 dataset, it took less than an hour to train/retrain the model. </p>
<p style="text-align: justify">The storage requirement of a model is an essential consideration in resource-constrained IoT devices. The following diagram presents the storage requirements for the three models we tested for the two use cases. As shown in the diagram, the simple CNN takes up only 2.6 MB, smaller than one sixth of the Mobilenet V1 (17.1 MB). Also, the LSTM for the HAR took up 1.6 MB (not in the diagram) of storage. In terms of storage requirements, all the models are fine to be deployed in many resource-constrained IoT devices, including Raspberry Pi or smartphones:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d70ca87d-aa69-4dec-8ae4-9e6c18547674.png" style="width:66.33em;height:25.50em;"/></p>
<p style="text-align: justify">Finally, we have evaluated the performance of the models. Two levels of performance evaluation can be executed for the use cases:</p>
<ul>
<li style="text-align: justify">Dataset-wide evaluation or testing has been done during the retraining phase in the desktop PC platform/server side.</li>
<li style="text-align: justify">Individual activity signals for human activity and facial images for emotion detection were tested or evaluated in the Raspberry Pi 3 environment.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance (use case one)</h1>
                </header>
            
            <article>
                
<p><span>The following graph presents the progressive training and test accuracy of the LSTM model against the HAR dataset. As we can see from the graph, training accuracy is close to 1.0, or 100%, and test accuracy is above .90, or 90%. With this</span> test accuracy, we believe that the LSTM model can detect human activities in most cases, including whether the subject is doing the assigned physiotherapy activities:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-916 image-border" src="assets/12ffe603-6af5-4dc5-890b-bfa2f92e8b97.png" style="width:43.25em;height:29.92em;"/></p>
<p class="mce-root"/>
<p>The following diagram is a confusion matrix of the model against the HAR test dataset. As seen in the diagram, the model gets confused between <strong>Downstairs</strong> and <strong>Upstairs</strong>, and <strong>Sitting</strong> and <strong>Standing</strong> activities, as they have very limited or zero mobility, which means there is no significant acceleration to differentiate them:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-917 image-border" src="assets/0ad58033-786b-41ef-8e67-55988260b906.png" style="width:75.33em;height:70.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance (use case two) </h1>
                </header>
            
            <article>
                
<p style="text-align: justify">The following screenshot shows the training and validation performance of the simple CNN model on the FER2013 dataset. The accuracy of this dataset is not great (training–.83, and validation–.63), but the test or validation accuracy should be able to detect the distinctive and necessary emotions (such as happy, sad, and confused) for the smart classroom:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cee139c1-5676-4c9b-9bb3-34509c430bd6.png" style="width:51.25em;height:24.33em;"/></p>
<p class="mce-root"/>
<p style="text-align: justify"><span>The following diagram is a confusion matrix of the model against the FER2013 test dataset. As expected, the model is showing confusion for all the expressions (such as 156 angry expressions being detected as sad expressions). This is one of the applications of deep learning where further research is needed to improve performance: </span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-918 image-border" src="assets/35f1d314-5857-4ec4-a487-ac1da3c460f6.png" style="width:56.92em;height:36.58em;"/></p>
<p>For use case two, we have tested Mobilenet V1. The following diagrams shows the overall performance of model Mobilenet V1 on the FER2019 dataset. As we can see from the figure, this is showing better training accuracy, but no improvement in validation and test accuracy. One potential reason for this could be the size and quality of the data, since, after data augmentation, every sample may not contain a facial expression image. Further preprocessing that includes manual inspection may improve data quality and the model's performance:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d2bf23a-5c49-470b-9ab9-a9120a05c56c.png" style="width:46.83em;height:27.58em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/53844e93-cfa2-40ec-bd76-5c109ec4c24d.png" style="width:59.83em;height:33.42em;"/></p>
<p class="mce-root"/>
<p>In order to test the model on an individual image, and transfer the learning of the model, we need to do the following:</p>
<ul>
<li>Export the trained model (such as <kbd>fer2013_trained.hdf5</kbd>) and the <kbd>label_image.py</kbd> file (image classifier) into a Raspberry Pi (installed with TensorFlow)/smartphone.</li>
<li>Run the image classifier (do not forget to update the <kbd>test_image</kbd> path)<span> </span>using the following command:</li>
</ul>
<pre style="padding-left: 60px"><strong>python label_image.py</strong></pre>
<p>This will produce the test result for your test image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p style="text-align: justify">Automatic human physiological and psychological state detection is becoming a popular means by which people can learn a person's physical and mental state to interact and react accordingly. There are many applications within smart education, healthcare, and entertainment where these state detection techniques can be useful. Machine learning and DL algorithms are essential for these detection techniques. In the first part of this chapter, we briefly described different IoT applications using human physiological and psychological state detection. We also briefly discussed two potential use cases of IoT where DL algorithms can be useful in human physiological and psychological state detection. The first use case considers an IoT-based remote physiotherapy progress monitoring system. The second use case is an IoT-based smart classroom application that <span><span>uses</span></span> facial expressions of the students to know their feedback. In the second part of the chapter, we briefly discussed the data collection process for the use cases, and we discussed the rationale behind selecting LSTM for the HAR and CNNs for the FER. The remainder of the chapter described all of the necessary components of the DL pipeline for these models and their results.</p>
<p style="text-align: justify">One of the key challenges in IoT applications is security. Many IoT applications, such as driverless cars, connected healthcare, and smart grid, are mission-critical applications. Security is an essential element for these and many other IoT applications. In the next chapter, we will discuss security in IoT applications, and show how deep learning can be used for IoT security solutions. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>K. Rapp, C. Becker, I.D. Cameron, H.H. König, and G. Büchele, <em>Epidemiology of falls in residential aged care: analysis of more than 70,000 falls from residents of Bavarian nursing homes</em>, J. Am. Med. Dir. Assoc. 13 (2) (2012) 187.e1–187.e6.</li>
<li>Centers for disease control and prevention. <em>Cost of Falls Among Older Adults</em>, 2014. <a href="http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html">http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html</a> (accessed 14.04.19).</li>
<li>M. S. Hossain and G. Muhammad, <em>Emotion-Aware Connected Healthcare Big Data Towards 5G</em>, in IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2399-2406, Aug. 2018.</li>
<li>M. A. Razzaque, Muta Tah Hira, and Mukta Dira. 2017. Q<em>oS in Body Area Networks: A Survey. ACM Trans</em>. Sen. Netw. 13, 3, Article 25 (August 2017), 46 pages. </li>
<li>Nigel Bosch, Sidney K. D'Mello, Ryan S. Baker, Jaclyn Ocumpaugh, Valerie Shute, Matthew Ventura, Lubin Wang, and Weinan Zhao. 2016. Detecting student emotions in computer-enabled classrooms. In <em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</em>(IJCAI'16), Gerhard Brewka (Ed.). AAAI Press 4125-4129.</li>
<li><span>Isabel Sagenmüller, </span><em>Student retention: 8 reasons people drop out of higher education</em>, <a href="https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education">https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education</a>.</li>
<li> Nikki Bardsley, <em>Drop-out rates among university students increases for third consecutive year</em>, <a href="https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year">https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year</a>.<a href="https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year"/></li>
<li>S. Hochreiter and J. Schmidhuber, <em>Long Short-Term Memory</em>, neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.</li>
<li><a href="http://www.cis.fordham.edu/wisdm/dataset.php">http://www.cis.fordham.edu/wisdm/dataset.php</a>.</li>
<li>I. Goodfellow, D. Erhan, PL Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, DH Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis, J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra, J. Xie, L. Romaszko, B. Xu, Z. Chuang, and Y. Bengio., <em>Challenges in Representation Learning: A report on three machine learning contests</em>. arXiv 2013.</li>
</ul>


            </article>

            
        </section>
    </body></html>