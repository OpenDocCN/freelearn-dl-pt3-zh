<html><head></head><body>
		<div id="_idContainer099">
			<h1 id="_idParaDest-140"><em class="italic"><a id="_idTextAnchor167"/>Chapter 6</em>: Reinforcement Learning in the Real World – Building Intelligent Agents to Complete Your To-Dos</h1>
			<p>An RL Agent needs to interact with the environment to learn and train. Training RL Agents for real-world applications usually comes with physical limitations and challenges. This is because the Agent could potentially cause damage to the real-world system it is dealing with while learning. Fortunately, there are a lot of tasks in the real world that do not necessarily have such challenges, and yet can be very useful for completing the day-to-day real-world tasks that are available in our To-Do lists! </p>
			<p>The recipes in this chapter will help you build RL Agents that can complete tasks on the internet, ranging from responding to annoying popups, booking flights on the web, managing emails and social media accounts, and more. We can do all of this without using a bunch of APIs that change over time or utilizing hardcoded scripts that stop working when a web page is updated. You will be training the Agents to complete such To-Do tasks by using the mouse and keyboard, just like how a human would! This chapter will also help you build the <strong class="bold">WebGym</strong> API, which is an OpenAI Gym-compatible generic RL learning environment interface that you can use to convert more than 50+ web tasks into training environments for RL and train your own RL Agents.</p>
			<p>Specifically, the following recipes will be covered in this chapter:</p>
			<ul>
				<li>Building learning environments for real-world RL</li>
				<li>Building an RL Agent to complete tasks on the web – Call to Action</li>
				<li>Building a visual auto-login bot</li>
				<li>Training an RL Agent to automate flight booking for your travel</li>
				<li>Training an RL Agent to manage your emails</li>
				<li>Training an RL Agent to automate your social media account management</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor168"/>Technical requirements</h1>
			<p>The code in this book has been extensively tested on Ubuntu 18.04 and Ubuntu 20.04, which means it should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6+ installed, along with the necessary Python packages listed in the Getting ready sections of each recipe, the code should run fine on Windows and Mac OSX too. It is advised that you create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Installing Miniconda or Anaconda for Python virtual environment management is recommended. You will also need to install the Chromium chrome driver on your system. On Ubuntu 18.04+, you can install it by using the <strong class="source-inline">sudo apt-get install chromium-chromedriver</strong> command.</p>
			<p>The complete code for each recipe in each chapter will be available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor169"/>Building learning environments for real-world RL</h1>
			<p>This recipe will teach you how to set up and build WebGym, a <strong class="bold">World of Bits</strong> (<strong class="bold">WoB</strong>)-based OpenAI Gym <a id="_idIndexMarker559"/>compatible learning platform for training RL Agents for <a id="_idIndexMarker560"/>world wide web-based real-world tasks. WoB is an open domain platform for web-based Agents. For more information <a id="_idIndexMarker561"/>about WoB, check out <a id="_idIndexMarker562"/>the following link: <a href="http://proceedings.mlr.press/v70/shi17a/shi17a.pdf">http://proceedings.mlr.press/v70/shi17a/shi17a.pdf</a>. </p>
			<p>WebGym provides learning environments for Agents to perceive the world wide web how we (humans) perceive it – using the pixels rendered on our display screen. The Agent interacts with the environment using keyboard and mouse events as actions. This allows the Agent to experience the world wide web how we do, which means we don't need to make any additional modifications for the Agents to train. This allows us to train RL Agents that can directly work with web-based pages and applications to complete real-world tasks.</p>
			<p>The following image <a id="_idIndexMarker563"/>shows a sample <strong class="bold">Click-To-Action</strong> (<strong class="bold">CTA</strong>) environment, where <a id="_idIndexMarker564"/>the task is to click on a specific link to get to the next page or step in the process:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B15074_06_01.jpg" alt="Figure 6.1 – Sample CTA task requiring a specific link to be clicked "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Sample CTA task requiring a specific link to be clicked</p>
			<p>Another <a id="_idIndexMarker565"/>example of a CTA task is depicted in the following image:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B15074_06_02.jpg" alt="Figure 6.2 – Sample CTA task requiring a specific option to be selected and submitted "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Sample CTA task requiring a specific option to be selected and submitted</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor170"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. WebGym is built on top of the miniwob-plusplus benchmark, which has also been made available as part of this book's code repository for ease of use. </p>
			<p>Now, let's begin!</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor171"/>How to do it…</h2>
			<p>We will build WebGym by defining the custom <strong class="source-inline">reset</strong> and <strong class="source-inline">step</strong> methods. Then, we will define the <a id="_idIndexMarker566"/>state and action spaces for the <a id="_idIndexMarker567"/>training environments. First, we'll look at the implementation of the <strong class="source-inline">miniwob_env</strong> module. Let's get started:</p>
			<ol>
				<li>Let's begin by importing the necessary Python modules:<p class="source-code">import os</p><p class="source-code">import gym</p><p class="source-code">from PIL import Image</p><p class="source-code">from miniwob.action import MiniWoBCoordClick</p><p class="source-code">from miniwob.environment import MiniWoBEnvironment</p></li>
				<li>Let's specify the directory where we will import the local <strong class="source-inline">miniwob</strong> environment:<p class="source-code">cur_path_dir = \</p><p class="source-code">    os.path.dirname(os.path.realpath(__file__))</p><p class="source-code">miniwob_dir = os.path.join(cur_path_dir, "miniwob",</p><p class="source-code">                           "html", "miniwob")</p></li>
				<li>Now, we can start to subclass <strong class="source-inline">MiniWoBEnvironment</strong>. We can then call the super class's initialization function to initialize the environment and set the values for <strong class="source-inline">base_url</strong> before we configure the <strong class="source-inline">miniwob</strong> environment:<p class="source-code">class MiniWoBEnv(MiniWoBEnvironment, gym.Env):</p><p class="source-code">    def __init__(</p><p class="source-code">        self,</p><p class="source-code">        env_name: str,</p><p class="source-code">        obs_im_shape,</p><p class="source-code">        num_instances: int = 1,</p><p class="source-code">        miniwob_dir: str = miniwob_dir,</p><p class="source-code">        seeds: list = [1],</p><p class="source-code">    ):</p><p class="source-code">        super().__init__(env_name)</p><p class="source-code">        self.base_url = f"file://{miniwob_dir}"</p><p class="source-code">        self.configure(num_instances=num_instances,</p><p class="source-code">                     seeds=seeds, base_url=self.base_url)</p><p class="source-code">        # self.set_record_screenshots(True)</p><p class="source-code">        self.obs_im_shape = obs_im_shape</p></li>
				<li>It's time <a id="_idIndexMarker568"/>to customize the <strong class="source-inline">reset(…)</strong> method. To allow environments to be randomized, we will use a <strong class="source-inline">seeds</strong> argument <a id="_idIndexMarker569"/>to take a random seed. This can be used to generate random start states and tasks so that the Agent we train does not overfit to a fixed/static web page:<p class="source-code">def reset(self, seeds=[1], mode=None, </p><p class="source-code">record_screenshots=False):</p><p class="source-code">        """Forces stop and start all instances.</p><p class="source-code">        Args:</p><p class="source-code">            seeds (list[object]): Random seeds to set for </p><p class="source-code">            each instance;</p><p class="source-code">                If specified, len(seeds) must be equal to</p><p class="source-code">                the number of instances.</p><p class="source-code">                A None entry in the list = do not set a </p><p class="source-code">                new seed.</p><p class="source-code">            mode (str): If specified, set the data mode </p><p class="source-code">                to this value before starting new</p><p class="source-code">                episodes.</p><p class="source-code">            record_screenshots (bool): Whether to record </p><p class="source-code">                screenshots of the states.</p><p class="source-code">        Returns:</p><p class="source-code">            states (list[MiniWoBState])</p><p class="source-code">        """</p><p class="source-code">        miniwob_state = super().reset(seeds, mode, </p><p class="source-code">                  record_screenshots=True)</p><p class="source-code">        return [</p><p class="source-code">            state.screenshot.resize(self.obs_im_shape, </p><p class="source-code">                                    Image.ANTIALIAS)</p><p class="source-code">            for state in miniwob_state</p><p class="source-code">        ]</p></li>
				<li>Next, we will <a id="_idIndexMarker570"/>redefine the <strong class="source-inline">step(…)</strong> method. Let's complete the implementation in two steps. First, we will <a id="_idIndexMarker571"/>define the method with docstrings that explain the arguments:<p class="source-code">    def step(self, actions):</p><p class="source-code">        """Applies an action on each instance and returns </p><p class="source-code">        the results.</p><p class="source-code">        Args:</p><p class="source-code">            actions (list[MiniWoBAction or None])</p><p class="source-code">        Returns:</p><p class="source-code">            tuple (states, rewards, dones, info)</p><p class="source-code">                states (list[PIL.Image.Image])</p><p class="source-code">                rewards (list[float])</p><p class="source-code">                dones (list[bool])</p><p class="source-code">                info (dict): additional debug </p><p class="source-code">                information.</p><p class="source-code">                    Global debug information is directly </p><p class="source-code">                    in the root level</p><p class="source-code">                    Local information for instance i is </p><p class="source-code">                    in info['n'][i]</p><p class="source-code">        """</p></li>
				<li>In this <a id="_idIndexMarker572"/>step, we will complete our <a id="_idIndexMarker573"/>implementation of the <strong class="source-inline">step(…)</strong> method:<p class="source-code">        states, rewards, dones, info = \</p><p class="source-code">                                super().step(actions)</p><p class="source-code">        # Obtain screenshot &amp; Resize image obs to match </p><p class="source-code">        # config</p><p class="source-code">        img_states = [</p><p class="source-code">            state.screenshot.resize(self.obs_im_shape) \</p><p class="source-code">            if not dones[i] else None</p><p class="source-code">            for i, state in enumerate(states)</p><p class="source-code">        ]</p><p class="source-code">        return img_states, rewards, dones, info</p></li>
				<li>That completes our <strong class="source-inline">MiniWoBEnv</strong> class implementation! To test our class implementation and to understand how to use the class, we will write a quick <strong class="source-inline">main()</strong> function:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = MiniWoBVisualEnv("click-pie")</p><p class="source-code">    for _ in range(10):</p><p class="source-code">        obs = env.reset()</p><p class="source-code">        done = False</p><p class="source-code">        while not done:</p><p class="source-code">            action = [MiniWoBCoordClick(90, 150)]</p><p class="source-code">            obs, reward, done, info = env.step(action)</p><p class="source-code">            [ob.show() for ob in obs if ob is not None]</p><p class="source-code">    env.close()</p></li>
				<li>You can save the preceding script as <strong class="source-inline">miniwob_env.py</strong> and execute it to see the sample <a id="_idIndexMarker574"/>environment being acted <a id="_idIndexMarker575"/>on by a random Agent. In the next few steps, we will extend <strong class="source-inline">MiniWoBEnv</strong> in order to create an OpenAI Gym-compatible learning environment interface. Let's begin by creating a new file named <strong class="source-inline">envs.py</strong> and include with the following imports:<p class="source-code">import gym.spaces</p><p class="source-code">import numpy as np</p><p class="source-code">import string</p><p class="source-code">from miniwob_env import MiniWoBEnv</p><p class="source-code">from miniwob.action import MiniWoBCoordClick, MiniWoBType</p></li>
				<li>For the <a id="_idIndexMarker576"/>first environment, we will <a id="_idIndexMarker577"/>implement the <strong class="source-inline">MiniWoBVisualClickEnv</strong> class:<p class="source-code">class MiniWoBVisualClickEnv(MiniWoBEnv):</p><p class="source-code">    def __init__(self, name, num_instances=1):</p><p class="source-code">        """RL environment with visual observations and </p><p class="source-code">           touch/mouse-click action space</p><p class="source-code">            Two dimensional, continuous-valued action </p><p class="source-code">            space allows Agents to specify (x, y)</p><p class="source-code">            coordinates on the visual rendering to click/</p><p class="source-code">            touch to interact with the world-of bits</p><p class="source-code">        Args:</p><p class="source-code">            name (str): Name of the supported \</p><p class="source-code">            MiniWoB-PlusPlus environment</p><p class="source-code">            num_instances (int, optional): Number of \</p><p class="source-code">            parallel env instances. Defaults to 1.</p><p class="source-code">        """</p><p class="source-code">        self.miniwob_env_name = name</p><p class="source-code">        self.task_width = 160</p><p class="source-code">        self.task_height = 210</p><p class="source-code">        self.obs_im_width = 64</p><p class="source-code">        self.obs_im_height = 64</p><p class="source-code">        self.num_channels = 3  # RGB</p><p class="source-code">        self.obs_im_size = (self.obs_im_width, \</p><p class="source-code">                            self.obs_im_height)</p><p class="source-code">        super().__init__(self.miniwob_env_name, </p><p class="source-code">                         self.obs_im_size, </p><p class="source-code">                         num_instances)</p></li>
				<li>Let's also <a id="_idIndexMarker578"/>define the observation <a id="_idIndexMarker579"/>and action space for this environment in the<strong class="source-inline"> __init__</strong> method:<p class="source-code">        self.observation_space = gym.spaces.Box(</p><p class="source-code">            0,</p><p class="source-code">            255,</p><p class="source-code">            (self.obs_im_width, self.obs_im_height, </p><p class="source-code">             self.num_channels),</p><p class="source-code">            dtype=int,</p><p class="source-code">        )</p><p class="source-code">        self.action_space = gym.spaces.Box(</p><p class="source-code">            low=np.array([0, 0]),</p><p class="source-code">            high=np.array([self.task_width, </p><p class="source-code">                           self.task_height]),</p><p class="source-code">            shape=(2,),</p><p class="source-code">            dtype=int,</p><p class="source-code">        )</p></li>
				<li>Next, we will further extend the <strong class="source-inline">reset(…)</strong> method to provide an OpenAI Gym-compatible interface method:<p class="source-code">    def reset(self, seeds=[1]):</p><p class="source-code">        """Forces stop and start all instances.</p><p class="source-code">        Args:</p><p class="source-code">            seeds (list[object]): Random seeds to set for </p><p class="source-code">            each instance;</p><p class="source-code">                If specified, len(seeds) must be equal to </p><p class="source-code">                the number of instances.</p><p class="source-code">                A None entry in the list = do not set a </p><p class="source-code">                new seed.</p><p class="source-code">        Returns:</p><p class="source-code">            states (list[PIL.Image])</p><p class="source-code">        """</p><p class="source-code">        obs = super().reset(seeds)</p><p class="source-code">        # Click somewhere to Start!</p><p class="source-code">        # miniwob_state, _, _, _ = super().step(</p><p class="source-code">        # self.num_instances * [MiniWoBCoordClick(10,10)]</p><p class="source-code">        # )</p><p class="source-code">        return obs</p></li>
				<li>The next <a id="_idIndexMarker580"/>important piece is the <strong class="source-inline">step</strong> method. We <a id="_idIndexMarker581"/>will implement it in the following two steps:<p class="source-code">    def step(self, actions):</p><p class="source-code">        """Applies an action on each instance and returns </p><p class="source-code">           the results.</p><p class="source-code">        Args:</p><p class="source-code">            actions (list[(x, y) or None]);</p><p class="source-code">              - x is the number of pixels from the left</p><p class="source-code">                of browser window</p><p class="source-code">              - y is the number of pixels from the top of </p><p class="source-code">                browser window</p><p class="source-code">        Returns:</p><p class="source-code">            tuple (states, rewards, dones, info)</p><p class="source-code">                states (list[PIL.Image.Image])</p><p class="source-code">                rewards (list[float])</p><p class="source-code">                dones (list[bool])</p><p class="source-code">                info (dict): additional debug </p><p class="source-code">                information.</p><p class="source-code">                    Global debug information is directly </p><p class="source-code">                    in the root level</p><p class="source-code">                    Local information for instance i is </p><p class="source-code">                    in info['n'][i]</p><p class="source-code">        """</p></li>
				<li>To complete the <strong class="source-inline">step</strong> method's implementation, let's check if the dimensions of the <a id="_idIndexMarker582"/>actions are as expected and <a id="_idIndexMarker583"/>then bind the actions if necessary. Finally, we must execute a step in the environment:<p class="source-code">        assert (</p><p class="source-code">            len(actions) == self.num_instances</p><p class="source-code">        ), f"Expected len(actions)={self.num_instances}.\</p><p class="source-code">             Got {len(actions)}."</p><p class="source-code">        def clamp(action, low=self.action_space.low,\</p><p class="source-code">                   high=self.action_space.high):</p><p class="source-code">            low_x, low_y = low</p><p class="source-code">            high_x, high_y = high</p><p class="source-code">            return (</p><p class="source-code">                max(low_x, min(action[0], high_x)),</p><p class="source-code">                max(low_y, min(action[1], high_y)),</p><p class="source-code">            )</p><p class="source-code">        miniwob_actions = \</p><p class="source-code">            [MiniWoBCoordClick(*clamp(action)) if action\</p><p class="source-code">            is not None else None for action in actions]</p><p class="source-code">        return super().step(miniwob_actions)</p></li>
				<li>We can <a id="_idIndexMarker584"/>use a descriptive name for <a id="_idIndexMarker585"/>the class to register the environment with the Gym registry:<p class="source-code">class MiniWoBClickButtonVisualEnv(MiniWoBVisualClickEnv):</p><p class="source-code">    def __init__(self, num_instances=1):</p><p class="source-code">        super().__init__("click-button", num_instances)</p></li>
				<li>Finally, to register the environment with OpenAI Gym's registry locally, we must add the <a id="_idIndexMarker586"/>environment registration <a id="_idIndexMarker587"/>information to the <strong class="source-inline">__init__.py</strong> file:<p class="source-code">import sys</p><p class="source-code">import os</p><p class="source-code">from gym.envs.registration import register</p><p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p><p class="source-code">_AVAILABLE_ENVS = {</p><p class="source-code">    "MiniWoBClickButtonVisualEnv-v0": {</p><p class="source-code">        "entry_point": \</p><p class="source-code">             "webgym.envs:MiniWoBClickButtonVisualEnv",</p><p class="source-code">        "discription": "Click the button on a web page",</p><p class="source-code">    }</p><p class="source-code">}</p><p class="source-code">for env_id, val in _AVAILABLE_ENVS.items():</p><p class="source-code">    register(id=env_id, </p><p class="source-code">             entry_point=val.get("entry_point"))</p><p>With that, we have completed this recipe!</p></li>
			</ol>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor172"/>How it works…</h2>
			<p>We have extended the implementation of <strong class="source-inline">MiniWoB-plusplus</strong> in <strong class="source-inline">MiniWoBEnv</strong> so that we can use file-based web pages to represent tasks. We extended the <strong class="source-inline">MiniWoBEnv</strong> class even further to provide an OpenAI Gym-compatible interface in <strong class="source-inline">MiniWoBVisualClickEnv</strong>.</p>
			<p>To get a clear picture of how an RL Agent will be learning to complete the task in this environment, consider the following screenshot. Here, the Agent tries to understand the objective of <a id="_idIndexMarker588"/>the task by trying out different actions, which in this environment translates to clicking on different areas of the web <a id="_idIndexMarker589"/>page (represented on the right-hand side by blue dots). Eventually, the RL Agent clicks on the correct button and starts to understand what the task description means, as well as what the buttons are intended for, since it was rewarded for clicking on the correct spot:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B15074_06_03.jpg" alt="Figure 6.3 – Visualizing the Agent's actions while it's learning to complete the CTA task "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Visualizing the Agent's actions while it's learning to complete the CTA task</p>
			<p>Now, it's time to move on to the next recipe!</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor173"/>Building an RL Agent to complete tasks on the web – Call to Action</h1>
			<p>This recipe will teach you how to implement an RL training script so that you can train an RL Agent <a id="_idIndexMarker590"/>to handle <strong class="bold">Call-To-Action</strong> (<strong class="bold">CTA</strong>) type tasks <a id="_idIndexMarker591"/>for you. CTA buttons are the actionable <a id="_idIndexMarker592"/>buttons that you typically find on web pages that you need to click in order to proceed to the next step. While there are several CTA button examples available, some common examples include the <strong class="source-inline">OK</strong>/<strong class="source-inline">Cancel</strong> dialog boxes, where you need you to click to acknowledge/dismiss the pop-up notification, and the <strong class="source-inline">Click to learn more</strong> button. In this recipe, you will instantiate a RL training environment that provides visual rendering for the web pages containing <a id="_idIndexMarker593"/>a CTA task. You will be training a <strong class="bold">proximal policy optimization</strong> (<strong class="bold">PPO</strong>)-based deep RL Agent that's been implemented using TensorFlow 2.x to learn how to complete the task at hand. </p>
			<p>The following image illustrates a set of observations from a randomized CTA environment (with different seeds) so that you understand the task that the Agent will be solving:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B15074_06_04.jpg" alt="Figure 6.4 – Screenshot of the Agent's observations from a randomized CTA environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Screenshot of the Agent's observations from a randomized CTA environment</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor174"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, then you are ready to get started:</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import (Conv2D,Dense,Dropout,Flatten,Input,Lambda,MaxPool2D,)</p>
			<p class="source-code">import webgym  # Used to register webgym environments</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor175"/>How to do it…</h2>
			<p>In this recipe, we will be implementing a complete training script, including command-line argument <a id="_idIndexMarker594"/>parsing for training hyperparameter <a id="_idIndexMarker595"/>configuration. As you may have noticed from the <strong class="source-inline">import</strong> statements, we will be using Keras's functional API <a id="_idIndexMarker596"/>for TensorFlow 2.x to implement the <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>) we will be using as part of the Agent's algorithm implementation.</p>
			<p>The following steps will guide you through the implementation:</p>
			<ol>
				<li value="1">Let's begin by defining the command-line arguments for the CTA Agent training script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch5-Click-To-Action-Agent")</p><p class="source-code">parser.add_argument("--env", default="MiniWoBClickButtonVisualEnv-v0")</p><p class="source-code">parser.add_argument("--update-freq", type=int, default=16)</p><p class="source-code">parser.add_argument("--epochs", type=int, default=3)</p><p class="source-code">parser.add_argument("--actor-lr", type=float, default=1e-4)</p><p class="source-code">parser.add_argument("--Critic-lr", type=float, default=1e-4)</p><p class="source-code">parser.add_argument("--clip-ratio", type=float, default=0.1)</p><p class="source-code">parser.add_argument("--gae-lambda", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p></li>
				<li>Next, we <a id="_idIndexMarker597"/>will create a TensorBoard <a id="_idIndexMarker598"/>logger so that we can log and visualize the live training progress of the CTA Agent:<p class="source-code">args = parser.parse_args()</p><p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>In the following steps, we will implement the <strong class="source-inline">Actor</strong> class. However, we will begin by implementing the <strong class="source-inline">__init__</strong> method:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound, std_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = np.array(action_bound)</p><p class="source-code">        self.std_bound = std_bound</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.eps = 1e-5</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.model.summary()  # Print a summary of the </p><p class="source-code">        @ Actor model</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Nadam(args.actor_lr)</p></li>
				<li>Next, we will <a id="_idIndexMarker599"/>define the DNN that <a id="_idIndexMarker600"/>will represent the Actor's model. We will split the implementation of the DNN into multiple steps as it's going to be a bit long due to several neural network layers being stacked together. As the first and main processing step, we will implement a block by stacking convolution-pooling-convolution-pooling layers:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(</p><p class="source-code">            filters=64,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="same",</p><p class="source-code">            input_shape=self.state_dim,</p><p class="source-code">            data_format="channels_last",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                          (conv1)</p><p class="source-code">        conv2 = Conv2D(</p><p class="source-code">            filters=32,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                         (conv2)</p></li>
				<li>Now, we <a id="_idIndexMarker601"/>will flatten the output <a id="_idIndexMarker602"/>from the pooling layer so that we can start using fully connected or dense layers with dropout to generate the output we expect from the Actor network:<p class="source-code">        flat = Flatten()(pool2)</p><p class="source-code">        dense1 = Dense(</p><p class="source-code">            16, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(</p><p class="source-code">            8, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p><p class="source-code">        # action_dim[0] = 2</p><p class="source-code">        output_val = Dense(</p><p class="source-code">            self.action_dim[0],</p><p class="source-code">            activation="relu",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p></li>
				<li>We need <a id="_idIndexMarker603"/>to scale and clip the <a id="_idIndexMarker604"/>predicted value so that the values are bounded and lie within the range we expect the actions to be in. Let's <a id="_idIndexMarker605"/>use the <strong class="bold">Lambda layer</strong> to implement custom clipping and scaling, as shown in the following code snippet:<p class="source-code">        mu_output = Lambda(</p><p class="source-code">            lambda x: tf.clip_by_value(x * \</p><p class="source-code">              self.action_bound, 1e-9, self.action_bound)</p><p class="source-code">        )(output_val)</p><p class="source-code">        std_output_1 = Dense(</p><p class="source-code">            self.action_dim[0],</p><p class="source-code">            activation="softplus",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p><p class="source-code">        std_output = Lambda(</p><p class="source-code">            lambda x: tf.clip_by_value(</p><p class="source-code">                x * self.action_bound, 1e-9, \</p><p class="source-code">                self.action_bound / 2</p><p class="source-code">            )</p><p class="source-code">        )(std_output_1)</p><p class="source-code">        return tf.keras.models.Model(</p><p class="source-code">            inputs=obs_input, outputs=[mu_output, std_output], name="Actor"</p><p class="source-code">        )</p></li>
				<li>That <a id="_idIndexMarker606"/>completes our <strong class="source-inline">nn_model</strong> implementation. Now, let's <a id="_idIndexMarker607"/>define a convenience function to get an action, given a state:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        # Convert [Image] to np.array(np.adarray)</p><p class="source-code">        state_np = np.array([np.array(s) for s in state])</p><p class="source-code">        if len(state_np.shape) == 3:</p><p class="source-code">            # Convert (w, h, c) to (1, w, h, c)</p><p class="source-code">            state_np = np.expand_dims(state_np, 0)</p><p class="source-code">        mu, std = self.model.predict(state_np)</p><p class="source-code">        action = np.random.normal(mu, std + self.eps, \</p><p class="source-code">                            size=self.action_dim).astype(</p><p class="source-code">            "int"</p><p class="source-code">        )</p><p class="source-code">        # Clip action to be between 0 and max obs screen </p><p class="source-code">        # size</p><p class="source-code">        action = np.clip(action, 0, self.action_bound)</p><p class="source-code">        # 1 Action per instance of env; Env expects: </p><p class="source-code">        # (num_instances, actions)</p><p class="source-code">        action = (action,)</p><p class="source-code">        log_policy = self.log_pdf(mu, std, action)</p><p class="source-code">        return log_policy, action</p></li>
				<li>Now, it's <a id="_idIndexMarker608"/>time to implement the <a id="_idIndexMarker609"/>main train method. This will update the parameters of the Actor network:<p class="source-code">    def train(self, log_old_policy, states, actions, </p><p class="source-code">    gaes):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            mu, std = self.model(states, training=True)</p><p class="source-code">            log_new_policy = self.log_pdf(mu, std, </p><p class="source-code">                                          actions)</p><p class="source-code">            loss = self.compute_loss(log_old_policy, </p><p class="source-code">                           log_new_policy, actions, gaes)</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>Although we are using <strong class="source-inline">compute_loss</strong> and <strong class="source-inline">log_pdf</strong> in the preceding <strong class="source-inline">train</strong> method, we <a id="_idIndexMarker610"/>haven't really defined <a id="_idIndexMarker611"/>them yet! Let's implement them one after the other, starting with the <strong class="source-inline">compute_loss</strong> method:<p class="source-code">    def compute_loss(self, log_old_policy, </p><p class="source-code">    log_new_policy, actions, gaes):</p><p class="source-code">        # Avoid INF in exp by setting 80 as the upper </p><p class="source-code">        # bound since,</p><p class="source-code">        # tf.exp(x) for x&gt;88 yeilds NaN (float32)</p><p class="source-code">        ratio = tf.exp(</p><p class="source-code">            tf.minimum(log_new_policy - \</p><p class="source-code">                tf.stop_gradient(log_old_policy), 80)</p><p class="source-code">        )</p><p class="source-code">        gaes = tf.stop_gradient(gaes)</p><p class="source-code">        clipped_ratio = tf.clip_by_value(</p><p class="source-code">            ratio, 1.0 - args.clip_ratio, 1.0 + \</p><p class="source-code">            args.clip_ratio</p><p class="source-code">        )</p><p class="source-code">        surrogate = -tf.minimum(ratio * gaes, \</p><p class="source-code">                                clipped_ratio * gaes)</p><p class="source-code">        return tf.reduce_mean(surrogate)</p></li>
				<li>In this step, we will implement the <strong class="source-inline">log_pdf</strong> method:<p class="source-code">    def log_pdf(self, mu, std, action):</p><p class="source-code">        std = tf.clip_by_value(std, self.std_bound[0],</p><p class="source-code">                               self.std_bound[1])</p><p class="source-code">        var = std ** 2</p><p class="source-code">        log_policy_pdf = -0.5 * (action - mu) ** 2 / var\</p><p class="source-code">                         - 0.5 * tf.math.log(</p><p class="source-code">            var * 2 * np.pi</p><p class="source-code">        )</p><p class="source-code">        return tf.reduce_sum(log_policy_pdf, 1,</p><p class="source-code">                             keepdims=True)</p></li>
				<li>The previous <a id="_idIndexMarker612"/>step concludes out <a id="_idIndexMarker613"/>Actor implementation. Now, it's time to start implementing the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.model.summary()  # Print a summary of the </p><p class="source-code">        # Critic model</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Nadam(args.Critic_lr)</p></li>
				<li>Next up is the <strong class="source-inline">Critic</strong> class's neural network model. Like the Actor's neural network model, this is going to be a DNN. We will split the implementation into a few steps. First, let's implement a convolution-pooling-convolution-pooling block:<p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(</p><p class="source-code">            filters=64,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="same",</p><p class="source-code">            input_shape=self.state_dim,</p><p class="source-code">            data_format="channels_last",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\</p><p class="source-code">                          (conv1)</p><p class="source-code">        conv2 = Conv2D(</p><p class="source-code">            filters=32,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=2)\</p><p class="source-code">                          (conv2)</p></li>
				<li>While we <a id="_idIndexMarker614"/>could stack more <a id="_idIndexMarker615"/>blocks or layers to deepen the neural network, for our current task, we already have a sufficient number of parameters in the DNN to learn how to perform well at the CTA task. Let's add the fully connected layers so that we can eventually produce the state-conditioned action value:<p class="source-code">        flat = Flatten()(pool2)</p><p class="source-code">        dense1 = Dense(</p><p class="source-code">            16, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(</p><p class="source-code">            8, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p><p class="source-code">        value = Dense(</p><p class="source-code">            1, activation="linear", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout2)</p></li>
				<li>Let's implement <a id="_idIndexMarker616"/>a method <a id="_idIndexMarker617"/>that will compute the Critic's learning loss, which is essentially the mean-squared error between the temporal difference learning targets and the values predicted by the Critic:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p></li>
				<li>Let's finalize our <strong class="source-inline">Critic</strong> class by implementing the <strong class="source-inline">train</strong> method to update the Critic's parameters:<p class="source-code">    def train(self, states, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model(states, training=True)</p><p class="source-code">            # assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, \</p><p class="source-code">                            tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, \</p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, \</p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>Now, we <a id="_idIndexMarker618"/>can utilize the Actor <a id="_idIndexMarker619"/>and the Critic implementation to build our PPO Agent so that it can work with high-dimensional (image) observations. Let's begin by defining the <strong class="source-inline">PPOAgent</strong> class's <strong class="source-inline">__init__</strong> method:<p class="source-code">class PPOAgent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = self.env.observation_space.shape</p><p class="source-code">        self.action_dim = self.env.action_space.shape</p><p class="source-code">        # Set action_bounds to be within the actual </p><p class="source-code">        # task-window/browser-view of the Agent</p><p class="source-code">        self.action_bound = [self.env.task_width, </p><p class="source-code">                             self.env.task_height]</p><p class="source-code">        self.std_bound = [1e-2, 1.0]</p><p class="source-code">        self.actor = Actor(</p><p class="source-code">            self.state_dim, self.action_dim, </p><p class="source-code">            self.action_bound, self.std_bound</p><p class="source-code">        )</p><p class="source-code">        self.Critic = Critic(self.state_dim)</p></li>
				<li>We will <a id="_idIndexMarker620"/>be using <strong class="bold">Generalized Advantage Estimates</strong> (<strong class="bold">GAE</strong>) to <a id="_idIndexMarker621"/>update our policy. So, let's implement a <a id="_idIndexMarker622"/>method that will calculate the GAE target values:<p class="source-code">    def gae_target(self, rewards, v_values, next_v_value, </p><p class="source-code">    done):</p><p class="source-code">        n_step_targets = np.zeros_like(rewards)</p><p class="source-code">        gae = np.zeros_like(rewards)</p><p class="source-code">        gae_cumulative = 0</p><p class="source-code">        forward_val = 0</p><p class="source-code">        if not done:</p><p class="source-code">            forward_val = next_v_value</p><p class="source-code">        for k in reversed(range(0, len(rewards))):</p><p class="source-code">            delta = rewards[k] + args.gamma * \</p><p class="source-code">                    forward_val - v_values[k]</p><p class="source-code">            gae_cumulative = args.gamma * \</p><p class="source-code">                             args.gae_lambda * \</p><p class="source-code">                             gae_cumulative + delta</p><p class="source-code">            gae[k] = gae_cumulative</p><p class="source-code">            forward_val = v_values[k]</p><p class="source-code">            n_step_targets[k] = gae[k] + v_values[k]</p><p class="source-code">        return gae, n_step_targets</p></li>
				<li>We are at the core of this script! Let's define the training routine for the deep PPO Agent. We <a id="_idIndexMarker623"/>will split the <a id="_idIndexMarker624"/>implementation into multiple steps to make it easy to follow. We will begin with the outermost loop, which must be running for a configurable maximum number of episodes:<p class="source-code">    def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                state_batch = []</p><p class="source-code">                action_batch = []</p><p class="source-code">                reward_batch = []</p><p class="source-code">                old_policy_batch = []</p><p class="source-code">                episode_reward, done = 0, False</p><p class="source-code">                state = self.env.reset()</p><p class="source-code">                prev_state = state</p><p class="source-code">                step_num = 0</p></li>
				<li>Next, we will implement the logic for stepping through the environment and handling the end of an episode by checking the <strong class="source-inline">done</strong> values from the environments:<p class="source-code">                    while not done:</p><p class="source-code">                        log_old_policy, action = \</p><p class="source-code">                             self.actor.get_action(state)</p><p class="source-code">                    next_state, reward, dones, _ = \</p><p class="source-code">                         self.env.step(action)</p><p class="source-code">                    step_num += 1</p><p class="source-code">                    print(</p><p class="source-code">                        f"ep#:{ep} step#:{step_num} \</p><p class="source-code">                        step_rew:{reward} \</p><p class="source-code">                        action:{action} dones:{dones}"</p><p class="source-code">                    )</p><p class="source-code">                    done = np.all(dones)</p><p class="source-code">                    if done:</p><p class="source-code">                        next_state = prev_state</p><p class="source-code">                    else:</p><p class="source-code">                        prev_state = next_state</p><p class="source-code">                    state = np.array([np.array(s) for s\</p><p class="source-code">                                      in state])</p><p class="source-code">                    next_state = np.array([np.array(s) \</p><p class="source-code">                                    for s in next_state])</p><p class="source-code">                    reward = np.reshape(reward, [1, 1])</p><p class="source-code">                    log_old_policy = np.reshape(</p><p class="source-code">                                  log_old_policy, [1, 1])</p><p class="source-code">                    state_batch.append(state)</p><p class="source-code">                    action_batch.append(action)</p><p class="source-code">                    reward_batch.append((reward + 8) / 8)</p><p class="source-code">                    old_policy_batch.append(</p><p class="source-code">                                          log_old_policy)</p></li>
				<li>Next, we <a id="_idIndexMarker625"/>will implement the logic <a id="_idIndexMarker626"/>that will check for the end of an episode or if it is time to update and perform an update step:<p class="source-code">                    if len(state_batch) &gt;= \</p><p class="source-code">                    args.update_freq or done:</p><p class="source-code">                        states = \</p><p class="source-code">                            np.array([state.squeeze() \</p><p class="source-code">                            for state in state_batch])</p><p class="source-code">                        # Convert ([x, y],) to [x, y]</p><p class="source-code">                        actions = np.array([action[0] \</p><p class="source-code">                            for action in action_batch])</p><p class="source-code">                        rewards = np.array(</p><p class="source-code">                            [reward.squeeze() for reward\</p><p class="source-code">                             in reward_batch]</p><p class="source-code">                        )</p><p class="source-code">                        old_policies = np.array(</p><p class="source-code">                            [old_pi.squeeze() for old_pi\</p><p class="source-code">                             in old_policy_batch]</p><p class="source-code">                        )</p><p class="source-code">                        v_values = self.Critic.model.\</p><p class="source-code">                            predict(states)</p><p class="source-code">                        next_v_value = self.Critic.\</p><p class="source-code">                            model.predict(next_state)</p><p class="source-code">                        gaes, td_targets = \</p><p class="source-code">                            self.gae_target(</p><p class="source-code">                                rewards, v_values, \</p><p class="source-code">                                next_v_value, done</p><p class="source-code">                        )</p><p class="source-code">                        actor_losses, Critic_losses=[],[]</p></li>
				<li>Now that <a id="_idIndexMarker627"/>we have the updated <a id="_idIndexMarker628"/>GAE targets, we can train the Actor and Critic networks and log the losses and other training metrics for tracking purposes:<p class="source-code">                        for epoch in range(args.epochs):</p><p class="source-code">                            actor_loss = \</p><p class="source-code">                                self.actor.train(</p><p class="source-code">                                    old_policies, states,</p><p class="source-code">                                    actions, gaes</p><p class="source-code">                                )</p><p class="source-code">                            actor_losses.append(</p><p class="source-code">                                 actor_loss)</p><p class="source-code">                            Critic_loss = \</p><p class="source-code">                                self.Critic.train(states,</p><p class="source-code">                                              td_targets)</p><p class="source-code">                            Critic_losses.append(</p><p class="source-code">                                             Critic_loss)</p><p class="source-code">                        # Plot mean actor &amp; Critic losses </p><p class="source-code">                        # on every update</p><p class="source-code">                        tf.summary.scalar("actor_loss", </p><p class="source-code">                             np.mean(actor_losses), </p><p class="source-code">                             step=ep)</p><p class="source-code">                        tf.summary.scalar(</p><p class="source-code">                            "Critic_loss", </p><p class="source-code">                             np.mean(Critic_losses),</p><p class="source-code">                             step=ep</p><p class="source-code">                        )</p><p class="source-code">                        state_batch = []</p><p class="source-code">                        action_batch = []</p><p class="source-code">                        reward_batch = []</p><p class="source-code">                        old_policy_batch = []</p><p class="source-code">                    episode_reward += reward[0][0]</p><p class="source-code">                    state = next_state[0]</p></li>
				<li>Finally, let's implement the <strong class="source-inline">__main__</strong> function to train the CTA Agent:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "MiniWoBClickButtonVisualEnv-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    cta_Agent = PPOAgent(env)</p><p class="source-code">    cta_Agent.train()</p></li>
			</ol>
			<p>That completes this recipe! Let's briefly recap on how it works.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor176"/>How it works…</h2>
			<p>In this recipe, we <a id="_idIndexMarker629"/>implemented a PPO-based <a id="_idIndexMarker630"/>deep RL Agent and provided a training mechanism to develop a CTA Agent. Note that for simplicity, we used one instance of the environment, though the code can scale for a greater number of environment instances to speed up training.</p>
			<p>To understand how the Agent training progresses, consider the following sequence of images. During the initial stages of training, when the Agent is trying to understand the task and the objective of the task, the Agent may just be executing random actions (exploration) or even clicking outside the screen, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B15074_06_05.jpg" alt="Figure 6.5 – Agent clicking outside the screen (no visible blue dot) during initial exploration"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Agent clicking outside the screen (no visible blue dot) during initial exploration</p>
			<p>As the Agent <a id="_idIndexMarker631"/>learns by stumbling upon the <a id="_idIndexMarker632"/>correct button to click, it starts to make progress. The following screenshot shows the Agent making some progress:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B15074_06_06.jpg" alt="Figure 6.6 – Deep PPO Agent making progress in the CTA task "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Deep PPO Agent making progress in the CTA task</p>
			<p>Finally, when the episode is complete or ends (due to a time limit), the Agent receives an observation <a id="_idIndexMarker633"/>similar to the one shown in the following screenshot (left):</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B15074_06_07.jpg" alt="Figure 6.7 – End of episode observation (left) and summary of performance (right) "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – End of episode observation (left) and summary of performance (right)</p>
			<p>Now, it's time <a id="_idIndexMarker634"/>to move on to the next recipe!</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor177"/>Building a visual auto-login bot</h1>
			<p>Imagine that you have an Agent or a bot that watches what you are doing and automatically logs you <a id="_idIndexMarker635"/>into websites whenever you click on a login screen. While browser plugins exist that can automatically log you in, they do so using hardcoded scripts that only work on the pre-programmed website's login URLs. But what if you had an Agent that only relied on the rendered web page – just like you do to perform a task – and worked even when the URL changes and when you are on a new website with no prior saved data? How cool would that be?! This recipe will help you develop a script that will train an Agent to log in on a web page! You will learn how to randomize, customize, and increase the generality of the Agent to get it to work on any login screen.</p>
			<p>An example of randomizing and customizing the usernames and passwords for a task can be seen in the following image: </p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B15074_06_08.jpg" alt="Figure 6.8 – Sample observations from a randomized user login task "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Sample observations from a randomized user login task</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor178"/>Getting ready</h2>
			<p>To complete <a id="_idIndexMarker636"/>this recipe, make sure you have the latest version. First, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, then you are ready to get started:</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import (Conv2D,Dense,Dropout,Flatten,Input,Lambda,MaxPool2D,)</p>
			<p class="source-code">import webgym  # Used to register webgym environments</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor179"/>How to do it…</h2>
			<p>In this recipe, we will <a id="_idIndexMarker637"/>implement the deep RL-based Login Agent using the PPO algorithm.</p>
			<p>Let's get started:</p>
			<ol>
				<li value="1">First, let's set up the training script's command-line arguments and logging:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch5-Login-Agent")</p><p class="source-code">parser.add_argument("--env", default="MiniWoBLoginUserVisualEnv-v0")</p><p class="source-code">parser.add_argument("--update-freq", type=int, default=16)</p><p class="source-code">parser.add_argument("--epochs", type=int, default=3)</p><p class="source-code">parser.add_argument("--actor-lr", type=float, default=1e-4)</p><p class="source-code">parser.add_argument("--Critic-lr", type=float, default=1e-4)</p><p class="source-code">parser.add_argument("--clip-ratio", type=float, default=0.1)</p><p class="source-code">parser.add_argument("--gae-lambda", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p><p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>We can <a id="_idIndexMarker638"/>now directly jump into the <strong class="source-inline">Critic</strong> class's definition:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.model.summary()  # Print a summary of the </p><p class="source-code">        # Critic model</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Nadam(args.Critic_lr)</p></li>
				<li>Now, let's define the DNN for the Critic model. We'll begin by implementing a perception block composed of convolution-pooling-convolution-pooling. In the subsequent steps, we'll add more depth to the network by stacking another perception block:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(</p><p class="source-code">            filters=64,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="same",</p><p class="source-code">            input_shape=self.state_dim,</p><p class="source-code">            data_format="channels_last",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\</p><p class="source-code">                         (conv1)</p><p class="source-code">        conv2 = Conv2D(</p><p class="source-code">            filters=32,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=2)</p><p class="source-code">                         (conv2)</p></li>
				<li>Next, we will <a id="_idIndexMarker639"/>add another perception block so that we can extract more features:<p class="source-code">        conv3 = Conv2D(</p><p class="source-code">            filters=16,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool2)</p><p class="source-code">        pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                          (conv3)</p><p class="source-code">        conv4 = Conv2D(</p><p class="source-code">            filters=8,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool3)</p><p class="source-code">        pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">(conv4)</p></li>
				<li>Next, we will <a id="_idIndexMarker640"/>add a flattening layer, followed by fully connected (dense) layers, to bring down the shape of the network's output to a single action value:<p class="source-code">        flat = Flatten()(pool4)</p><p class="source-code">        dense1 = Dense(</p><p class="source-code">            16, activation="relu", </p><p class="source-code">            kernel_initializer=self.weight_initializer</p><p class="source-code">        )(flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(</p><p class="source-code">            8, activation="relu", </p><p class="source-code">            kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p><p class="source-code">        value = Dense(</p><p class="source-code">            1, activation="linear", </p><p class="source-code">            kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout2)</p><p class="source-code">        return tf.keras.models.Model(inputs=obs_input, </p><p class="source-code">                         outputs=value, name="Critic")</p></li>
				<li>To finalize <a id="_idIndexMarker641"/>our Critic implementation, let's define the <strong class="source-inline">compute_loss</strong> method and the <strong class="source-inline">update</strong> method in order to train the parameters:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p><p class="source-code">    def train(self, states, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model(states, training=True)</p><p class="source-code">            # assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, </p><p class="source-code">                            tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>We can now work on implementing the <strong class="source-inline">Actor</strong> class. We'll initialize the <strong class="source-inline">Actor</strong> class in this step and continue our implementation in the subsequent steps:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound, std_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = np.array(action_bound)</p><p class="source-code">        self.std_bound = std_bound</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.eps = 1e-5</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.model.summary()  # Print a summary of the </p><p class="source-code">        # Actor model</p><p class="source-code">        self.opt = tf.keras.optimizers.Nadam(</p><p class="source-code">                                          args.actor_lr)</p></li>
				<li>We will use <a id="_idIndexMarker642"/>a similar DNN architecture for our Actor as we did in our Critic implementation. So, the <strong class="source-inline">nn_model</strong> method's implementation will remain the same except for the last few layers, where the Actor and Critic's implementation will vary. The Actor network model produces the mean and the standard deviation as output. This depends on the action space dimensions. On the other hand, the Critic network produces a state-conditioned action value, irrespective of the dimensions of the action space. The layers that differ from the Critic's DNN implementation are listed here:<p class="source-code">        # action_dim[0] = 2</p><p class="source-code">        output_val = Dense(</p><p class="source-code">            self.action_dim[0],</p><p class="source-code">            activation="relu",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p><p class="source-code">        # Scale &amp; clip x[i] to be in range [0, </p><p class="source-code">        # action_bound[i]]</p><p class="source-code">        mu_output = Lambda(</p><p class="source-code">            lambda x: tf.clip_by_value(x * \</p><p class="source-code">              self.action_bound, 1e-9, self.action_bound)</p><p class="source-code">        )(output_val)</p><p class="source-code">        std_output_1 = Dense(</p><p class="source-code">            self.action_dim[0],</p><p class="source-code">            activation="softplus",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p><p class="source-code">        std_output = Lambda(</p><p class="source-code">            lambda x: tf.clip_by_value(</p><p class="source-code">                x * self.action_bound, 1e-9, </p><p class="source-code">                self.action_bound / 2</p><p class="source-code">            )</p><p class="source-code">        )(std_output_1)</p><p class="source-code">        return tf.keras.models.Model(</p><p class="source-code">            inputs=obs_input, outputs=[mu_output, </p><p class="source-code">              std_output], name="Actor"</p><p class="source-code">        )</p></li>
				<li>Let's implement <a id="_idIndexMarker643"/>some methods that will compute the Actor's loss and <strong class="source-inline">log_pdf</strong>:<p class="source-code">    def log_pdf(self, mu, std, action):</p><p class="source-code">        std = tf.clip_by_value(std, self.std_bound[0], </p><p class="source-code">                               self.std_bound[1])</p><p class="source-code">        var = std ** 2</p><p class="source-code">        log_policy_pdf = -0.5 * (action - mu) ** 2 / var\</p><p class="source-code">                         - 0.5 * tf.math.log(</p><p class="source-code">            var * 2 * np.pi</p><p class="source-code">        )</p><p class="source-code">        return tf.reduce_sum(log_policy_pdf, 1, </p><p class="source-code">                             keepdims=True)</p><p class="source-code">    def compute_loss(self, log_old_policy, </p><p class="source-code">                     log_new_policy, actions, gaes):</p><p class="source-code">        # Avoid INF in exp by setting 80 as the upper </p><p class="source-code">        # bound since,</p><p class="source-code">        # tf.exp(x) for x&gt;88 yeilds NaN (float32)</p><p class="source-code">        ratio = tf.exp(</p><p class="source-code">            tf.minimum(log_new_policy - \</p><p class="source-code">                    tf.stop_gradient(log_old_policy), 80)</p><p class="source-code">        )</p><p class="source-code">        gaes = tf.stop_gradient(gaes)</p><p class="source-code">        clipped_ratio = tf.clip_by_value(</p><p class="source-code">            ratio, 1.0 - args.clip_ratio, 1.0 + \</p><p class="source-code">            args.clip_ratio</p><p class="source-code">        )</p><p class="source-code">        surrogate = -tf.minimum(ratio * gaes, </p><p class="source-code">                                clipped_ratio * gaes)</p><p class="source-code">        return tf.reduce_mean(surrogate)</p></li>
				<li>With the <a id="_idIndexMarker644"/>help of these helper methods, our training method implementation becomes simpler:<p class="source-code">    def train(self, log_old_policy, states, actions, </p><p class="source-code">    gaes):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            mu, std = self.model(states, training=True)</p><p class="source-code">            log_new_policy = self.log_pdf(mu, std, </p><p class="source-code">                                          actions)</p><p class="source-code">            loss = self.compute_loss(log_old_policy, </p><p class="source-code">                                     log_new_policy,</p><p class="source-code">                                     actions, gaes)</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>Finally, let's implement a method that will get an action from the Actor when it's given a state as input:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        # Convert [Image] to np.array(np.adarray)</p><p class="source-code">        state_np = np.array([np.array(s) for s in state])</p><p class="source-code">        if len(state_np.shape) == 3:</p><p class="source-code">            # Convert (w, h, c) to (1, w, h, c)</p><p class="source-code">            state_np = np.expand_dims(state_np, 0)</p><p class="source-code">        mu, std = self.model.predict(state_np)</p><p class="source-code">        action = np.random.normal(mu, std + self.eps, </p><p class="source-code">                            size=self.action_dim).astype(</p><p class="source-code">            "int"</p><p class="source-code">        )</p><p class="source-code">        # Clip action to be between 0 and max obs </p><p class="source-code">        # screen size</p><p class="source-code">        action = np.clip(action, 0, self.action_bound)</p><p class="source-code">        # 1 Action per instance of env; Env expects: </p><p class="source-code">        # (num_instances, actions)</p><p class="source-code">        action = (action,)</p><p class="source-code">        log_policy = self.log_pdf(mu, std, action)</p><p class="source-code">        return log_policy, action</p></li>
				<li>That completes <a id="_idIndexMarker645"/>our Actor implementation. We can now tie both the Actor and the Critic together using the <strong class="source-inline">PPOAgent</strong> class implementation. Since the GAE target calculations were discussed in the previous recipe, we will skip this and focus on the training method's implementation:<p class="source-code">               while not done:</p><p class="source-code">                    # self.env.render()</p><p class="source-code">                    log_old_policy, action = \</p><p class="source-code">                        self.actor.get_action(state)</p><p class="source-code">                    next_state, reward, dones, _ = \</p><p class="source-code">                        self.env.step(action)</p><p class="source-code">                    step_num += 1</p><p class="source-code">                    # Convert action[2] from int idx to </p><p class="source-code">                    # char for verbose printing</p><p class="source-code">                    action_print = []</p><p class="source-code">                    for a in action:  # Map apply</p><p class="source-code">                        action_verbose = (a[:2], \</p><p class="source-code">                        self.get_typed_char(a[2]))</p><p class="source-code">                        action_print.append(</p><p class="source-code">                                      action_verbose)</p><p class="source-code">                    print(</p><p class="source-code">                        f"ep#:{ep} step#:{step_num} </p><p class="source-code">                        step_rew:{reward} \</p><p class="source-code">                        action:{action_print} \</p><p class="source-code">                        dones:{dones}"</p><p class="source-code">                    )</p><p class="source-code">                    done = np.all(dones)</p><p class="source-code">                    if done:</p><p class="source-code">                        next_state = prev_state</p><p class="source-code">                    else:</p><p class="source-code">                        prev_state = next_state</p><p class="source-code">                    state = np.array([np.array(s) for \</p><p class="source-code">                        s in state])</p><p class="source-code">                    next_state = np.array([np.array(s) \</p><p class="source-code">                        for s in next_state])</p><p class="source-code">                    reward = np.reshape(reward, [1, 1])</p><p class="source-code">                    log_old_policy = np.reshape(</p><p class="source-code">                                  log_old_policy, [1, 1])</p><p class="source-code">                    state_batch.append(state)</p><p class="source-code">                    action_batch.append(action)</p><p class="source-code">                    reward_batch.append((reward + 8) / 8)</p><p class="source-code">                    old_policy_batch.append(\</p><p class="source-code">                                          log_old_policy)</p></li>
				<li>The Agent's <a id="_idIndexMarker646"/>update is performed at a preset frequency in terms of the number of samples collected or at the end of every episode – whichever occurs first:<p class="source-code">                    if len(state_batch) &gt;= \</p><p class="source-code">                        args.update_freq or done:</p><p class="source-code">                        states = np.array([state.\</p><p class="source-code">                                     squeeze() for state\</p><p class="source-code">                                     in state_batch])</p><p class="source-code">                        actions = np.array([action[0]\</p><p class="source-code">                             for action in action_batch])</p><p class="source-code">                        rewards = np.array(</p><p class="source-code">                            [reward.squeeze() for reward\</p><p class="source-code">                             in reward_batch])</p><p class="source-code">                        old_policies = np.array(</p><p class="source-code">                            [old_pi.squeeze() for old_pi\</p><p class="source-code">                             in old_policy_batch])</p><p class="source-code">                        v_values = self.Critic.model.\</p><p class="source-code">                                       predict(states)</p><p class="source-code">                        next_v_value = self.Critic.\</p><p class="source-code">                                model.predict(next_state)</p><p class="source-code">                        gaes, td_targets = \</p><p class="source-code">                                    self.gae_target(</p><p class="source-code">                            rewards, v_values, \</p><p class="source-code">                            next_v_value, done)</p><p class="source-code">                        actor_losses, Critic_losses=[],[]</p><p class="source-code">                        for epoch in range(args.epochs):</p><p class="source-code">                            actor_loss = \</p><p class="source-code">                                self.actor.train(</p><p class="source-code">                                    old_policies, states,</p><p class="source-code">                                  actions, gaes)</p><p class="source-code">                            actor_losses.append(</p><p class="source-code">                                              actor_loss)</p><p class="source-code">                            Critic_loss = self.Critic.\</p><p class="source-code">                                train(states, td_targets)</p><p class="source-code">                            Critic_losses.append(</p><p class="source-code">                                             Critic_loss)</p></li>
				<li>Finally, we <a id="_idIndexMarker647"/>can run <strong class="source-inline">MiniWoBLoginUserVisualEnv-v0</strong> and train the Agent using the following snippet of code:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "MiniWoBLoginUserVisualEnv-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    cta_Agent = PPOAgent(env)</p><p class="source-code">    cta_Agent.train()</p></li>
			</ol>
			<p>That completes our script for the auto-login Agent. It's time for you to run the script to see the Agent's training process in action!</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor180"/>How it works…</h2>
			<p>The login task <a id="_idIndexMarker648"/>involves clicking on the correct form field and typing in the correct username and/or password. For an Agent to be able to do this, it needs to master how to use a mouse and keyboard, in addition to processing the visual web page to understand the task and the web login form. With enough samples, the deep RL Agent will learn a policy to complete this task. Let's take a look at the state of the Agent's progress, snapshotted at different stages.</p>
			<p>The following <a id="_idIndexMarker649"/>image shows the Agent successfully entering the username and correctly clicking on the password field to enter the password, but not being able to complete the task yet:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B15074_06_09.jpg" alt="Figure 6.9 – Screenshot of a trained Agent successfully entering the username but not a password "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – Screenshot of a trained Agent successfully entering the username but not a password</p>
			<p>In the following image, you can see that the Agent has learned to enter both the username and password, but they are not quite right for the task to be classed as complete:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B15074_06_10.jpg" alt="Figure 6.10 – Agent entering both the username and password but incorrectly "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Agent entering both the username and password but incorrectly</p>
			<p>The same Agent <a id="_idIndexMarker650"/>with a different checkpoint, after several thousand more episodes of learning, is close to completing the task, as shown in the following image</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B15074_06_11.jpg" alt="Figure 6.11 – A well-trained Agent model about to complete the login task successfully "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – A well-trained Agent model about to complete the login task successfully</p>
			<p>Now that you understand how the Agent works and behaves, you can customize it to your liking and <a id="_idIndexMarker651"/>use use cases to train the Agent to automatically log into any custom website you want!</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor181"/>Training an RL Agent to automate flight booking for your travel</h1>
			<p>In this recipe, you <a id="_idIndexMarker652"/>will learn how to implement <a id="_idIndexMarker653"/>a deep RL Agent based on the <strong class="bold">Deep Deterministic Policy Gradient</strong> (<strong class="bold">DDPG</strong>) algorithm using TensorFlow 2.x and train the Agent to visually operate flight booking websites using a keyboard and mouse to book flights! This task is quite useful but complicated due to the varying amount of task parameters we need to implement, such as source city, destination, date, and more. The following image shows a sample of the start states from a randomized <strong class="source-inline">MiniWoBBookFlightVisualEnv</strong> flight booking environment:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B15074_06_12.jpg" alt="Figure 6.12 – Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv environment</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor182"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, then you are ready to get started:</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import (Conv2D,Dense,Dropout, Flatten, Input,Lambda,MaxPool2D)</p>
			<p class="source-code">import webgym  # Used to register webgym environments</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor183"/>How to do it…</h2>
			<p>In this recipe, we <a id="_idIndexMarker654"/>will be implementing a complete training script that you will be able to customize and train to book flights!</p>
			<p>Let's get started:</p>
			<ol>
				<li value="1">First, let's expose the hyperparameters as configurable arguments to the training script:<p class="source-code">parser = argparse.ArgumentParser(</p><p class="source-code">    prog="TFRL-Cookbook-Ch5-SocialMedia-Mute-User-DDPGAgent"</p><p class="source-code">)</p><p class="source-code">parser.add_argument("--env", default="Pendulum-v0")</p><p class="source-code">parser.add_argument("--actor_lr", type=float, default=0.0005)</p><p class="source-code">parser.add_argument("--Critic_lr", type=float, default=0.001)</p><p class="source-code">parser.add_argument("--batch_size", type=int, default=64)</p><p class="source-code">parser.add_argument("--tau", type=float, default=0.05)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--train_start", type=int, default=2000)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Next, we'll <a id="_idIndexMarker655"/>set up TensorBoard logging for live visualization of the training progress:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)	</p></li>
				<li>We'll be using a Replay Buffer to implement Experience Reply. Let's implement a simple <strong class="source-inline">ReplayBuffer</strong> class:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state, </p><p class="source-code">    done):</p><p class="source-code">        self.buffer.append([state, action, reward, </p><p class="source-code">                            next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, </p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">                            map(np.asarray, zip(*sample))</p><p class="source-code">        states = \</p><p class="source-code">            np.array(states).reshape(args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).\</p><p class="source-code">                      reshape(args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states,\</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>Let's start <a id="_idIndexMarker656"/>by implementing the <strong class="source-inline">Actor</strong> class:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = action_bound</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.eps = 1e-5</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = tf.keras.optimizers.Adam(</p><p class="source-code">                                           args.actor_lr)</p></li>
				<li>The DNN model for the Actor will be composed of two perception blocks, each containing <a id="_idIndexMarker657"/>convolution-pooling-convolution-pooling layers, as in our previous recipe. We'll skip this here and look at the implementation of the <strong class="source-inline">train</strong> method instead. The full source code, as always, will be available in this cookbook's code repository. Let's continue with our <strong class="source-inline">train</strong> and <strong class="source-inline">predict</strong> method implementations:<p class="source-code">    def train(self, states, q_grads):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            grads = tape.gradient(</p><p class="source-code">                self.model(states), </p><p class="source-code">                self.model.trainable_variables, </p><p class="source-code">                -q_grads</p><p class="source-code">            )</p><p class="source-code">        self.opt.apply_gradients(zip(grads, \</p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">    def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p></li>
				<li>The last piece of our <strong class="source-inline">Actor</strong> class is to implement a function to get the action:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        # Convert [Image] to np.array(np.adarray)</p><p class="source-code">        state_np = np.array([np.array(s) for s in state])</p><p class="source-code">        if len(state_np.shape) == 3:</p><p class="source-code">            # Convert (w, h, c) to (1, w, h, c)</p><p class="source-code">            state_np = np.expand_dims(state_np, 0)</p><p class="source-code">        action = self.model.predict(state_np)</p><p class="source-code">        # Clip action to be between 0 and max obs </p><p class="source-code">        # screen size</p><p class="source-code">        action = np.clip(action, 0, self.action_bound)</p><p class="source-code">        # 1 Action per instance of env; Env expects: </p><p class="source-code">        # (num_instances, actions)</p><p class="source-code">        return action</p></li>
				<li>With that, our <strong class="source-inline">Actor</strong> class <a id="_idIndexMarker658"/>is ready. Now, we can move on and implement the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim, action_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.Critic_lr)</p></li>
				<li>Similar to the <strong class="source-inline">Actor</strong> class's DNN model, we will be reusing a similar architecture for our <strong class="source-inline">Critic</strong> class from the previous recipe, with two perception blocks. You can refer to the full source code of this recipe or the DNN implementation in the previous recipe for completeness. Let's jump into the implementation of the <strong class="source-inline">predict</strong> and <strong class="source-inline">g_gradients</strong> computations for the Q function:<p class="source-code">    def predict(self, inputs):</p><p class="source-code">        return self.model.predict(inputs)</p><p class="source-code">    def q_gradients(self, states, actions):</p><p class="source-code">        actions = tf.convert_to_tensor(actions)</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            tape.watch(actions)</p><p class="source-code">            q_values = self.model([states, actions])</p><p class="source-code">            q_values = tf.squeeze(q_values)</p><p class="source-code">        return tape.gradient(q_values, actions)</p></li>
				<li>In order to update our Critic model, we need a loss to drive the parameter updates and <a id="_idIndexMarker659"/>an actual training step to perform the update. In this step, we will implement these two core methods:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p><p class="source-code">    def train(self, states, actions, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model([states, actions], </p><p class="source-code">                                 training=True)</p><p class="source-code">            assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, </p><p class="source-code">                           tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>It's time <a id="_idIndexMarker660"/>to bring the Actor and Critic together to implement the DDPGAgent! Let's dive into it:<p class="source-code">class DDPGAgent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = self.env.observation_space.shape</p><p class="source-code">        self.action_dim = self.env.action_space.shape</p><p class="source-code">        self.action_bound = self.env.action_space.high</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">        self.actor = Actor(self.state_dim, </p><p class="source-code">                      self.action_dim, self.action_bound)</p><p class="source-code">        self.Critic = Critic(self.state_dim, </p><p class="source-code">                             self.action_dim)</p><p class="source-code">        self.target_actor = Actor(self.state_dim, </p><p class="source-code">                      self.action_dim, self.action_bound)</p><p class="source-code">        self.target_Critic = Critic(self.state_dim, </p><p class="source-code">                                    self.action_dim)</p><p class="source-code">        actor_weights = self.actor.model.get_weights()</p><p class="source-code">        Critic_weights = self.Critic.model.get_weights()</p><p class="source-code">        self.target_actor.model.set_weights(</p><p class="source-code">                                          actor_weights)</p><p class="source-code">        self.target_Critic.model.set_weights</p><p class="source-code">                                         (Critic_weights)</p></li>
				<li>Let's implement <a id="_idIndexMarker661"/>a method that will update the target models of our Actor and Critic:<p class="source-code">    def update_target(self):</p><p class="source-code">        actor_weights = self.actor.model.get_weights()</p><p class="source-code">        t_actor_weights = \</p><p class="source-code">            self.target_actor.model.get_weights()</p><p class="source-code">        Critic_weights = self.Critic.model.get_weights()</p><p class="source-code">        t_Critic_weights = \</p><p class="source-code">            self.target_Critic.model.get_weights()</p><p class="source-code">        for i in range(len(actor_weights)):</p><p class="source-code">            t_actor_weights[i] = (args.tau * \</p><p class="source-code">            actor_weights[i] + (1 - args.tau) * \</p><p class="source-code">            t_actor_weights[i])</p><p class="source-code">        for i in range(len(Critic_weights)):</p><p class="source-code">            t_Critic_weights[i] = (args.tau * \</p><p class="source-code">            Critic_weights[i] + (1 - args.tau) * \</p><p class="source-code">            t_Critic_weights[i])</p><p class="source-code">        self.target_actor.model.set_weights(</p><p class="source-code">                                      t_actor_weights)</p><p class="source-code">        self.target_Critic.model.set_weights(</p><p class="source-code">                                     t_Critic_weights)</p></li>
				<li>Next, we will implement a method that will compute the temporal difference targets:<p class="source-code">    def get_td_target(self, rewards, q_values, dones):</p><p class="source-code">        targets = np.asarray(q_values)</p><p class="source-code">        for i in range(q_values.shape[0]):</p><p class="source-code">            if dones[i]:</p><p class="source-code">                targets[i] = rewards[i]</p><p class="source-code">            else:</p><p class="source-code">                targets[i] = args.gamma * q_values[i]</p><p class="source-code">        return targets</p></li>
				<li>Because we are using a deterministic policy gradient and a policy without a distribution to <a id="_idIndexMarker662"/>sample from, we will be <a id="_idIndexMarker663"/>using a noise function to sample around the action predicted by the Actor network. The <strong class="bold">Ornstein Uhlenbeck</strong> (<strong class="bold">OU</strong>) noise process is a popular choice for DDPG Agents. We'll implement this here:<p class="source-code">    def add_ou_noise(self, x, rho=0.15, mu=0, dt=1e-1,</p><p class="source-code">                     sigma=0.2, dim=1):</p><p class="source-code">        return (</p><p class="source-code">            x + rho * (mu - x) * dt + sigma * \</p><p class="source-code">            np.sqrt(dt) * np.random.normal(size=dim))</p></li>
				<li>Next, we will implement a method that will replay the experience from the replay buffer:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, \</p><p class="source-code">                dones = self.buffer.sample()</p><p class="source-code">            target_q_values = self.target_Critic.predict(</p><p class="source-code">                [next_states, </p><p class="source-code">                 self.target_actor.predict(next_states)])</p><p class="source-code">            td_targets = self.get_td_target(rewards,</p><p class="source-code">                                  target_q_values, dones)</p><p class="source-code">            self.Critic.train(states, actions, </p><p class="source-code">                              td_targets)</p><p class="source-code">            s_actions = self.actor.predict(states)</p><p class="source-code">            s_grads = self.Critic.q_gradients(states, </p><p class="source-code">                                              s_actions)</p><p class="source-code">            grads = np.array(s_grads).reshape(</p><p class="source-code">                                   (-1, self.action_dim))</p><p class="source-code">            self.actor.train(states, grads)</p><p class="source-code">            self.update_target()</p></li>
				<li>The last but most crucial thing we must do in our Agent implementation is implement the <strong class="source-inline">train</strong> method. We will split the implementation into a few steps. First, we will <a id="_idIndexMarker664"/>start with the outermost loop, which must run for a maximum number of episodes:<p class="source-code">    def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                step_num, episode_reward, done = 0, 0,\</p><p class="source-code">                                                 False</p><p class="source-code">                state = self.env.reset()</p><p class="source-code">                prev_state = state</p><p class="source-code">                bg_noise = np.random.randint(</p><p class="source-code">                    self.env.action_space.low,</p><p class="source-code">                    self.env.action_space.high,</p><p class="source-code">                    self.env.action_space.shape,</p><p class="source-code">                )</p></li>
				<li>Next, we will implement the inner loop, which will run until the end of an episode:<p class="source-code">                while not done:</p><p class="source-code">                    # self.env.render()</p><p class="source-code">                    action = self.actor.get_action(state)</p><p class="source-code">                    noise = self.add_ou_noise(bg_noise,\</p><p class="source-code">                                     dim=self.action_dim)</p><p class="source-code">                    action = np.clip(action + noise, 0, \</p><p class="source-code">                         self.action_bound).astype("int")</p><p class="source-code">                    next_state, reward, dones, _ = \</p><p class="source-code">                                   self.env.step(action)</p><p class="source-code">                    done = np.all(dones)</p><p class="source-code">                    if done:</p><p class="source-code">                        next_state = prev_state</p><p class="source-code">                    else:</p><p class="source-code">                        prev_state = next_state</p></li>
				<li>We are <a id="_idIndexMarker665"/>not done yet! We still need to update our Replay Buffer with the new experience that the Agent has collected:<p class="source-code">                for (s, a, r, s_n, d) in zip(next_state,\</p><p class="source-code">                action, reward, next_state, dones):</p><p class="source-code">                        self.buffer.store(s, a, \</p><p class="source-code">                                     (r + 8) / 8, s_n, d)</p><p class="source-code">                        episode_reward += r</p><p class="source-code">                    step_num += 1  # 1 across </p><p class="source-code">                    # num_instances</p><p class="source-code">                    print(f"ep#:{ep} step#:{step_num} \</p><p class="source-code">                          step_rew:{reward} \</p><p class="source-code">                          action:{action} dones:{dones}")</p><p class="source-code">                    bg_noise = noise</p><p class="source-code">                    state = next_state</p></li>
				<li>Are we done?! Almost! We just have to remember to replay the experience when the <a id="_idIndexMarker666"/>buffer size is bigger than the batch size we used for training:<p class="source-code">                if (self.buffer.size() &gt;= args.batch_size</p><p class="source-code">                    and self.buffer.size() &gt;= \</p><p class="source-code">                    args.train_start):</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                print(f"Episode#{ep} \</p><p class="source-code">                        Reward:{episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward", \</p><p class="source-code">                                   episode_reward, \</p><p class="source-code">                                   step=ep)</p></li>
				<li>That completes our implementation. Now, we can launch the Agent training on the Visual Flight Booking environment using the following <strong class="source-inline">__main__</strong> function:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "MiniWoBBookFlightVisualEnv-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    Agent = DDPGAgent(env)</p><p class="source-code">    Agent.train()</p></li>
			</ol>
			<p>That's it!</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor184"/>How it works…</h2>
			<p>The DDPG Agent collects a series of samples from the flight booking environment as it explores and <a id="_idIndexMarker667"/>uses this experience to update its policy parameters through the Actor and Critic updates. The OU noise we discussed earlier allows the Agent to explore while using a deterministic action policy. The flight booking environment is quite complex as it requires the Agent to master both the keyboard and the mouse, in addition to understanding the task by looking at visual images of the task description (visual text parsing), inferring the intended task objective, and executing the actions in the correct sequence. The following screenshot shows the performance of the Agent upon completing a sufficiently large number of episodes of training:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B15074_06_13.jpg" alt="Figure 6.13 – A screenshot of the Agent performing the flight booking task at different stages of learning "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – A screenshot of the Agent performing the flight booking task at different stages of learning</p>
			<p>The following screenshot shows the Agent's screen after the Agent progressed to the final stage <a id="_idIndexMarker668"/>of the task (although it's not close to completing the task):</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B15074_06_14.jpg" alt="Figure 6.14 – Screenshot of the Agent progressing all the way to the final stage of the flight booking task "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – Screenshot of the Agent progressing all the way to the final stage of the flight booking task</p>
			<p>With that, we will move on to the next recipe!</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor185"/>Training an RL Agent to manage your emails</h1>
			<p>Email has become an integral part of many people's lives. The number of emails that an average working <a id="_idIndexMarker669"/>professional goes through in a workday <a id="_idIndexMarker670"/>is growing daily. While a lot of email filters exist for spam control, how nice would it be to have an intelligent Agent that can perform a series of email management tasks that just provide a task description (through text or speech via speech-to-text) and are not limited by any APIs that have rate limits? In this recipe, you will develop a deep RL Agent and train it on email management tasks! A set of sample tasks can be seen in the following image:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B15074_06_15.jpg" alt="Figure 6.15 – A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv environment</p>
			<p>Let's get into the details!</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor186"/>Getting ready</h2>
			<p>To complete <a id="_idIndexMarker671"/>this recipe, make sure you have the latest <a id="_idIndexMarker672"/>version. First, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, then you are ready to get started:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import (</p>
			<p class="source-code">    Conv2D,</p>
			<p class="source-code">    Dense,</p>
			<p class="source-code">    Dropout,</p>
			<p class="source-code">    Flatten,</p>
			<p class="source-code">    Input,</p>
			<p class="source-code">    Lambda,</p>
			<p class="source-code">    MaxPool2D,</p>
			<p class="source-code">)</p>
			<p class="source-code">import webgym  # Used to register webgym environments</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor187"/>How to do it…</h2>
			<p>Follow these steps to implement a deep RL Agent and train it to manage important emails: </p>
			<ol>
				<li value="1">First, we will define an <strong class="source-inline">ArgumentParser</strong> so that we can configure the <a id="_idIndexMarker673"/>script from the command line. For a <a id="_idIndexMarker674"/>complete list of configurable hyperparameters, please refer to the source code for this recipe:<p class="source-code">parser = argparse.ArgumentParser(</p><p class="source-code">    prog="TFRL-Cookbook-Ch5-Important-Emails-Manager-Agent"</p><p class="source-code">)</p><p class="source-code">parser.add_argument("--env", default="MiniWoBEmailInboxImportantVisualEnv-v0")</p></li>
				<li>Next, let's set up TensorBoard logging:<p class="source-code">args = parser.parse_args()</p><p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Now, we can initialize the <strong class="source-inline">Actor</strong> class:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound, std_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = np.array(action_bound)</p><p class="source-code">        self.std_bound = std_bound</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.eps = 1e-5</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.model.summary()  # Print a summary of the </p><p class="source-code">        # Actor model</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Nadam(args.actor_lr)</p></li>
				<li>Because the <a id="_idIndexMarker675"/>observations in our email <a id="_idIndexMarker676"/>management environment are visual (images), we will need perception capabilities for the Actor in our Agent. For this, we must make use of convolution-based perception blocks, as follows:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(</p><p class="source-code">            filters=32,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="same",</p><p class="source-code">            input_shape=self.state_dim,</p><p class="source-code">            data_format="channels_last",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                         (conv1)</p><p class="source-code">        conv2 = Conv2D(</p><p class="source-code">            filters=32,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                         (conv2)</p></li>
				<li>Now, let's add <a id="_idIndexMarker677"/>more perception blocks <a id="_idIndexMarker678"/>comprising convolutions, followed by max pooling layers:<p class="source-code">        conv3 = Conv2D(</p><p class="source-code">            filters=16,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool2)</p><p class="source-code">        pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                         (conv3)</p><p class="source-code">        conv4 = Conv2D(</p><p class="source-code">            filters=16,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool3)</p><p class="source-code">        pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                         (conv4)</p></li>
				<li>Now, we are <a id="_idIndexMarker679"/>ready to flatten the DNN output <a id="_idIndexMarker680"/>to produce the mean (mu) and standard deviation that we want as the output from the Actor. First, let's add the flattening layer and the dense layers:<p class="source-code">       flat = Flatten()(pool4)</p><p class="source-code">        dense1 = Dense(</p><p class="source-code">            16, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(</p><p class="source-code">            8, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p><p class="source-code">        # action_dim[0] = 2</p><p class="source-code">        output_val = Dense(</p><p class="source-code">            self.action_dim[0],</p><p class="source-code">            activation="relu",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p></li>
				<li>We are now <a id="_idIndexMarker681"/>ready to define the final layers of <a id="_idIndexMarker682"/>our Actor network. These will helps us produce <strong class="source-inline">mu</strong> and <strong class="source-inline">std</strong>, as we discussed in the previous step:<p class="source-code">        # Scale &amp; clip x[i] to be in range [0, </p><p class="source-code">                                         action_bound[i]]</p><p class="source-code">        mu_output = Lambda(</p><p class="source-code">            lambda x: tf.clip_by_value(x * \</p><p class="source-code">              self.action_bound, 1e-9, self.action_bound)</p><p class="source-code">        )(output_val)</p><p class="source-code">        std_output_1 = Dense(</p><p class="source-code">            self.action_dim[0],</p><p class="source-code">            activation="softplus",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p><p class="source-code">        std_output = Lambda(</p><p class="source-code">            lambda x: tf.clip_by_value(</p><p class="source-code">                x * self.action_bound, 1e-9, \</p><p class="source-code">                self.action_bound / 2))(std_output_1)</p><p class="source-code">        return tf.keras.models.Model(</p><p class="source-code">            inputs=obs_input, outputs=[mu_output, \</p><p class="source-code">                       std_output], name="Actor"</p><p class="source-code">        )</p></li>
				<li>That completes our Actor's DNN model implementation. To implement the remaining <a id="_idIndexMarker683"/>methods to complete the Actor class, please <a id="_idIndexMarker684"/>refer to the full code for this recipe, which can be found in this cookbook's code repository. We will now focus on defining the interfaces for the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">                tf.keras.initializers.he_normal()</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.model.summary()  # Print a summary of the </p><p class="source-code">        # Critic model</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Nadam(args.Critic_lr)</p></li>
				<li>The Critic's DNN model is also based on the same convolutional neural network architecture that we used for the <strong class="source-inline">Actor</strong>. For completeness, please refer to this recipe's full source code, which is available in this cookbook's code repository. We will implement the loss computation and training method here:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p><p class="source-code">    def train(self, states, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model(states, training=True)</p><p class="source-code">            # assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, \</p><p class="source-code">                       tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, \</p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, \</p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>With that, we <a id="_idIndexMarker685"/>can now define our Agent's <a id="_idIndexMarker686"/>class:<p class="source-code">class PPOAgent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = self.env.observation_space.shape</p><p class="source-code">        self.action_dim = self.env.action_space.shape</p><p class="source-code">        # Set action_bounds to be within the actual </p><p class="source-code">        # task-window/browser-view of the Agent</p><p class="source-code">        self.action_bound = [self.env.task_width, \</p><p class="source-code">                             self.env.task_height]</p><p class="source-code">        self.std_bound = [1e-2, 1.0]</p><p class="source-code">        self.actor = Actor(</p><p class="source-code">            self.state_dim, self.action_dim, \</p><p class="source-code">            self.action_bound, self.std_bound</p><p class="source-code">        )</p><p class="source-code">        self.Critic = Critic(self.state_dim)</p></li>
				<li>The preceding code should be familiar to you from the previous Agent implementations in this chapter. You can complete the remaining methods (and the training loop) based on our previous implementations. If you get stuck, you can refer to the <a id="_idIndexMarker687"/>full source code for this recipe by going <a id="_idIndexMarker688"/>to this cookbook's code repository. We will now write the <strong class="source-inline">__main__</strong> function so that we can train the Agent in <strong class="source-inline">MiniWoBEmailInboxImportantVisualEnv</strong>. This will allow us to see the Agent's learning process in action:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "MiniWoBEmailInboxImportantVisualEnv-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    cta_Agent = PPOAgent(env)</p><p class="source-code">    cta_Agent.train()</p></li>
			</ol>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor188"/>How it works…</h2>
			<p>The PPO Agent uses convolutional neural network layers to process the high-dimensional visual inputs in the Actor and Critic classes. The PPO algorithm updates the Agent's policy parameters using a surrogate loss function that prevents the policy parameters from being drastically updated. It then keeps the policy updates within the trust region, which makes it robust to hyperparameter choices and a few other factors that may lead to instability during the Agent's training regime. The email management environment poses as a nice sequential decision-making problem for the deep RL Agent. First, the Agent has <a id="_idIndexMarker689"/>to choose the correct email from a <a id="_idIndexMarker690"/>series of emails in an inbox and then perform the desired action (starring the email and so on). The Agent only has access to the visual rendering of the inbox, so it needs to extract the task specification details, interpret the task specification, and then plan and execute the actions! </p>
			<p>The following is a screenshot of the Agent's performance at different stages of learning (loaded from different checkpoints):</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B15074_06_16.jpg" alt="Figure 6.16 – A series of screenshots showing the Agent's learning progress "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – A series of screenshots showing the Agent's learning progress</p>
			<p>Now, let's move on to the next recipe!</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor189"/>Training an RL Agent to automate your social media account management</h1>
			<p>By the end of this recipe, you will have built a complete deep RL Agent training script that <a id="_idIndexMarker691"/>can be trained to perform management tasks on your social media account!</p>
			<p>The following image shows a series of (randomized) tasks from the environment that we will be training the Agent in:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B15074_06_17.jpg" alt="Figure 6.17 – A sample set of social media account management tasks that the Agent has been  asked to solve "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – A sample set of social media account management tasks that the Agent has been asked to solve</p>
			<p>Note that there is a scroll bar in this task that the Agent needs to learn how to use! The tweet that's relevant to this task may be hidden from the visible part of the screen, so the Agent will have to actively explore (by sliding the scroll bar up/down) in order to progress!</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor190"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, then you are ready to get started:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten, Input, Lambda, MaxPool2D, concatenate,)</p>
			<p class="source-code">import webgym  # Used to register webgym environments</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor191"/>How to do it…</h2>
			<p>Let's <a id="_idIndexMarker692"/>start by configuring the Agent training script. After that, you will be shown how to  complete the implementation. </p>
			<p>Let's get started:</p>
			<ol>
				<li value="1">Let's jump right into the implementation! We will begin with our <strong class="source-inline">ReplayBuffer</strong> implementation:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state,</p><p class="source-code">    done):</p><p class="source-code">        self.buffer.append([state, action, reward, </p><p class="source-code">                            next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, </p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">                            map(np.asarray, zip(*sample))</p><p class="source-code">        states = \</p><p class="source-code">            np.array(states).reshape(args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).\</p><p class="source-code">                       reshape(args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states,\</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>Next, we <a id="_idIndexMarker693"/>will implement our <strong class="source-inline">Actor</strong> class:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = action_bound</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.eps = 1e-5</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.actor_lr)</p></li>
				<li>The next core piece is the DNN definition for our Actor:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(filters=64, kernel_size=(3, 3),\ </p><p class="source-code">                       strides=(1, 1), padding="same", \</p><p class="source-code">                       input_shape=self.state_dim, \</p><p class="source-code">                       data_format="channels_last", \</p><p class="source-code">                       activation="relu")(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), \</p><p class="source-code">                          strides=1)(conv1)</p><p class="source-code">        conv2 = Conv2D(filters=32, kernel_size=(3, 3),\</p><p class="source-code">                       strides=(1, 1), padding="valid", \</p><p class="source-code">                       activation="relu",)(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                          (conv2)</p></li>
				<li>Depending <a id="_idIndexMarker694"/>on the complexity of the task, we can modify (add/reduce) the depth of the DNN. We will start by connecting the output of the pooling layers to the fully connected layers with dropout:<p class="source-code">        flat = Flatten()(pool2)</p><p class="source-code">        dense1 = Dense(</p><p class="source-code">            16, activation="relu", \</p><p class="source-code">            kernel_initializer=self.weight_initializer)\</p><p class="source-code">            (flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(8, activation="relu", \</p><p class="source-code">            kernel_initializer=self.weight_initializer)\</p><p class="source-code">            (dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p><p class="source-code">        # action_dim[0] = 2</p><p class="source-code">        output_val = Dense(self.action_dim[0], </p><p class="source-code">                           activation="relu",</p><p class="source-code">                           kernel_initializer= \</p><p class="source-code">                               self.weight_initializer,)\</p><p class="source-code">                           (dropout2)</p><p class="source-code">        # Scale &amp; clip x[i] to be in range </p><p class="source-code">        # [0, action_bound[i]]</p><p class="source-code">        mu_output=Lambda(lambda x: tf.clip_by_value(x *\</p><p class="source-code">                         self.action_bound, 1e-9, \</p><p class="source-code">                         self.action_bound))(output_val)</p><p class="source-code">        return tf.keras.models.Model(inputs=obs_input, </p><p class="source-code">                                     outputs=mu_output, </p><p class="source-code">                                     name="Actor")</p></li>
				<li>That completes our DNN model implementation for the Actor. Now, let's implement the <a id="_idIndexMarker695"/>methods that will train the Actor and get the predictions from the Actor model:<p class="source-code">    def train(self, states, q_grads):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            grads = tape.gradient(self.model(states),\ </p><p class="source-code">                         self.model.trainable_variables,\</p><p class="source-code">                         -q_grads)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, \</p><p class="source-code">             self.model.trainable_variables))</p><p class="source-code">    def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p></li>
				<li>We can now get the actions from our Actor:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        # Convert [Image] to np.array(np.adarray)</p><p class="source-code">        state_np = np.array([np.array(s) for s in state])</p><p class="source-code">        if len(state_np.shape) == 3:</p><p class="source-code">            # Convert (w, h, c) to (1, w, h, c)</p><p class="source-code">            state_np = np.expand_dims(state_np, 0)</p><p class="source-code">        action = self.model.predict(state_np)</p><p class="source-code">        action = np.clip(action, 0, self.action_bound)</p><p class="source-code">        return action</p></li>
				<li>Let's get started with our Critic implementation. Here, we will need to implement the Agent class that we are after:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim, action_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.weight_initializer = \</p><p class="source-code">            tf.keras.initializers.he_normal()</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.Critic_lr)</p></li>
				<li>Note <a id="_idIndexMarker696"/>that the Critic's model is initialized with <strong class="source-inline">self.nn_model()</strong>. Let's implement this here:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(filters=64, kernel_size=(3, 3), </p><p class="source-code">                       strides=(1, 1), padding="same", </p><p class="source-code">                       input_shape=self.state_dim, </p><p class="source-code">                       data_format="channels_last",</p><p class="source-code">                       activation="relu",)(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\</p><p class="source-code">                         (conv1)</p><p class="source-code">        conv2 = Conv2D(filters=32, kernel_size=(3, 3), </p><p class="source-code">                       strides=(1, 1), padding="valid", </p><p class="source-code">                       activation="relu",)(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), </p><p class="source-code">                          strides=2)(conv2)</p></li>
				<li>We will complete our DNN architecture for the Critic by funneling the output through the fully connected layers with dropout. This way, we receive the necessary action values:<p class="source-code">        flat = Flatten()(pool2)</p><p class="source-code">        dense1 = Dense(16, activation="relu", </p><p class="source-code">                       kernel_initializer= \</p><p class="source-code">                           self.weight_initializer)(flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(8, activation="relu", </p><p class="source-code">                       kernel_initializer= \</p><p class="source-code">                           self.weight_initializer)\</p><p class="source-code">                       (dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p><p class="source-code">        value = Dense(1, activation="linear", </p><p class="source-code">                      kernel_initializer= \</p><p class="source-code">                          self.weight_initializer)\</p><p class="source-code">                      (dropout2)</p><p class="source-code">        return tf.keras.models.Model(inputs=obs_input, </p><p class="source-code">                                     outputs=value,</p><p class="source-code">                                     name="Critic")</p></li>
				<li>Now, let's <a id="_idIndexMarker697"/>implement the <strong class="source-inline">g_gradients</strong> and <strong class="source-inline">compute_loss</strong> methods. This should be pretty straightforward:<p class="source-code">    def q_gradients(self, states, actions):</p><p class="source-code">        actions = tf.convert_to_tensor(actions)</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            tape.watch(actions)</p><p class="source-code">            q_values = self.model([states, actions])</p><p class="source-code">            q_values = tf.squeeze(q_values)</p><p class="source-code">        return tape.gradient(q_values, actions)</p><p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p></li>
				<li>Finally, we <a id="_idIndexMarker698"/>can complete the Critic's implementation by implementing the <strong class="source-inline">predict</strong> and <strong class="source-inline">train</strong> methods:<p class="source-code">    def predict(self, inputs):</p><p class="source-code">        return self.model.predict(inputs)    </p><p class="source-code">    def train(self, states, actions, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model([states, actions],\</p><p class="source-code">                                 training=True)</p><p class="source-code">            assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, \</p><p class="source-code">                            tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, \</p><p class="source-code">                         self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, \</p><p class="source-code">                        self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>We can now utilize the Actor and Critic to implement our Agent:<p class="source-code">class DDPGAgent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = self.env.observation_space.shape</p><p class="source-code">        self.action_dim = self.env.action_space.shape</p><p class="source-code">        self.action_bound = self.env.action_space.high</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">        self.actor = Actor(self.state_dim, </p><p class="source-code">                           self.action_dim, </p><p class="source-code">                           self.action_bound)</p><p class="source-code">        self.Critic = Critic(self.state_dim, </p><p class="source-code">                             self.action_dim)</p><p class="source-code">        self.target_actor = Actor(self.state_dim, </p><p class="source-code">                                  self.action_dim, </p><p class="source-code">                                  self.action_bound)</p><p class="source-code">        self.target_Critic = Critic(self.state_dim, </p><p class="source-code">                                    self.action_dim)</p><p class="source-code">        actor_weights = self.actor.model.get_weights()</p><p class="source-code">        Critic_weights = self.Critic.model.get_weights()</p><p class="source-code">        self.target_actor.model.set_weights(</p><p class="source-code">                                         actor_weights)</p><p class="source-code">        self.target_Critic.model.set_weights(</p><p class="source-code">                                        Critic_weights)</p></li>
				<li>Next, we <a id="_idIndexMarker699"/>will implement the <strong class="source-inline">update_target</strong> method, as per the DDPG algorithm:<p class="source-code">    def update_target(self):</p><p class="source-code">        actor_weights = self.actor.model.get_weights()</p><p class="source-code">        t_actor_weights = \</p><p class="source-code">            self.target_actor.model.get_weights()</p><p class="source-code">        Critic_weights = self.Critic.model.get_weights()</p><p class="source-code">        t_Critic_weights = \</p><p class="source-code">            self.target_Critic.model.get_weights()</p><p class="source-code">        for i in range(len(actor_weights)):</p><p class="source-code">            t_actor_weights[i] = (args.tau * \</p><p class="source-code">                                  actor_weights[i] + \</p><p class="source-code">                                  (1 - args.tau) * \</p><p class="source-code">                                  t_actor_weights[i])</p><p class="source-code">        for i in range(len(Critic_weights)):</p><p class="source-code">            t_Critic_weights[i] = (args.tau * \</p><p class="source-code">                                   Critic_weights[i] + \</p><p class="source-code">                                   (1 - args.tau) * \</p><p class="source-code">                                   t_Critic_weights[i])</p><p class="source-code">        self.target_actor.model.set_weights(</p><p class="source-code">                                        t_actor_weights)</p><p class="source-code">        self.target_Critic.model.set_weights(</p><p class="source-code">                                        t_Critic_weights)</p></li>
				<li>We will <a id="_idIndexMarker700"/>not look at the implementation of the <strong class="source-inline">train</strong> method here. Instead, we will start the outer loop's implementation, before completing it in the following steps:<p class="source-code">    def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                step_num, episode_reward, done = 0, 0, \</p><p class="source-code">                                                 False</p><p class="source-code">                state = self.env.reset()</p><p class="source-code">                prev_state = state</p><p class="source-code">                bg_noise = np.random.randint( </p><p class="source-code">                             self.env.action_space.low, </p><p class="source-code">                             self.env.action_space.high, </p><p class="source-code">                             self.env.action_space.shape)</p></li>
				<li>The <a id="_idIndexMarker701"/>main inner loop's implementation is as follows:<p class="source-code">                while not done:</p><p class="source-code">                    action = self.actor.get_action(state)</p><p class="source-code">                    noise = self.add_ou_noise(bg_noise, </p><p class="source-code">                                     dim=self.action_dim)</p><p class="source-code">                    action = np.clip(action + noise, 0,</p><p class="source-code">                         self.action_bound).astype("int")</p><p class="source-code">                    next_state, reward, dones, _ = \</p><p class="source-code">                         self.env.step(action)</p><p class="source-code">                    done = np.all(dones)</p><p class="source-code">                    if done:</p><p class="source-code">                        next_state = prev_state</p><p class="source-code">                    else:</p><p class="source-code">                        prev_state = next_state</p><p class="source-code">                    for (s, a, r, s_n, d) in zip\</p><p class="source-code">                    (next_state, action, reward, \</p><p class="source-code">                     next_state, dones):</p><p class="source-code">                        self.buffer.store(s, a, \</p><p class="source-code">                                   (r + 8) / 8, s_n, d)</p><p class="source-code">                        episode_reward += r</p><p class="source-code">                    step_num += 1  </p><p class="source-code">                    # 1 across num_instances</p><p class="source-code">                    bg_noise = noise</p><p class="source-code">                    state = next_state</p><p class="source-code">                if (self.buffer.size() &gt;= args.batch_size</p><p class="source-code">                    and self.buffer.size() &gt;= \</p><p class="source-code">                        args.train_start):</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                tf.summary.scalar("episode_reward", </p><p class="source-code">                                   episode_reward, </p><p class="source-code">                                   step=ep)</p></li>
				<li>That completes our training method implementation. For the implementation of the <strong class="source-inline">replay_experience</strong>, <strong class="source-inline">add_ou_noise</strong>, and <strong class="source-inline">get_td_targets</strong> methods, please refer to the full source code of this recipe, which can be found in this cookbook's code repository. </li>
				<li>Let's <a id="_idIndexMarker702"/>write our <strong class="source-inline">__main__</strong> function so that we can start training the Agent in our social media environment:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "MiniWoBSocialMediaMuteUserVisualEnv-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    Agent = DDPGAgent(env)</p><p class="source-code">    Agent.train()</p></li>
			</ol>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor192"/>How it works…</h2>
			<p>Let's visually explore how a well-trained Agent progresses through social media management tasks. The following screenshot shows the Agent learning to use the scroll bar to "navigate" in this environment:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B15074_06_18.jpg" alt="Figure 6.18 – The Agent learning to navigate using the scroll bar  "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – The Agent learning to navigate using the scroll bar </p>
			<p>Note that <a id="_idIndexMarker703"/>the task specification does not imply anything related to the scroll bar or the navigation, and that the Agent was able to explore and figure out that it needs to navigate in order to progress with the task! The following screenshot shows the Agent progressing much further by choosing the correct tweet but clicking on the wrong action; that is, <strong class="source-inline">Embed Tweet</strong> instead of the <strong class="source-inline">Mute</strong> button:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B15074_06_19.jpg" alt="Figure 6.19 – The Agent clicking on Embed Tweet when the goal was to click on Mute "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19 – The Agent clicking on Embed Tweet when the goal was to click on Mute</p>
			<p>After 96 million episodes of training, the Agent was sufficiently able to solve the task. The <a id="_idIndexMarker704"/>following screenshot shows the Agent's performance on an evaluation episode (the Agent was loaded from a checkpoint)</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B15074_06_20.jpg" alt="Figure 6.20 – The Agent loaded from trained parameters about to complete the task successfully  "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20 – The Agent loaded from trained parameters about to complete the task successfully </p>
			<p> That concludes this recipe and this chapter. Happy training!</p>
		</div>
</body></html>