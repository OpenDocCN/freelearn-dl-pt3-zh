<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Speech to Text and Topic Extraction Using NLP</h1>
                </header>
            
            <article>
                
<p class="mce-root">Recognizing and understanding spoken language is a challenging problem due to the complexity and variety of speech data. There have been several different technologies deployed to recognize spoken words in the past. Most of those approaches were very limited in their scope, as they <span>were unable to recognize a wide variety of words, accents, and tones, </span><span>and aspects of spoken language, such as a pause between spoken words</span><span>. Some of the prevalent modeling technique for speech recognition include <strong>Hidden Markov Models</strong> (<strong>HMM</strong>), <strong>Dynamic Time Warping</strong> (<strong>DTW</strong>), <strong>Long Short-Term Memory Network</strong>s (<strong>LSTM</strong>), and <strong>Connectionist Temporal Classification</strong> (<strong>CTC</strong>).</span></p>
<p>In this chapter, we shall learn about various options for speech to text and the prebuilt model from Google's TensorFlow team, using the Speech Commands Dataset. We shall cover the following topics:</p>
<ul>
<li>Speech to text frameworks and toolkits</li>
<li>Google Speech Commands Dataset</li>
<li>CNN based architecture for speech recognition</li>
<li>A TensorFlow speech commands example</li>
</ul>
<p>Download and follow the code for this chapter from <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech-to-text frameworks and toolkits</h1>
                </header>
            
            <article>
                
<p>Many cloud-based AI providers offer speech to text as a service:</p>
<ul>
<li>Amazon's offering for speech recognition is known as <strong>Amazon Transcribe</strong>. Amazon Transcribe allows transcription of the audio files stored in Amazon S3 in four different formats: <kbd>.flac</kbd>, <kbd>.wav</kbd>, <kbd>.mp4</kbd>, and <kbd>.mp3</kbd>. It allows an audio file with a maximum of two hours in length and 1 GB in size. The results of the transcription are created as a JSON file in an Amazon S3 bucket.</li>
<li>Google offers speech to text as part of its Google Cloud ML Services. Google Cloud Speech to Text supports <kbd>FLAC</kbd>, <kbd>Linear16</kbd>, <kbd>MULAW</kbd>, <kbd>AMR</kbd>, <kbd>AMR_WB</kbd>, and <kbd>OGG_OPUS</kbd> file formats. </li>
<li>Microsoft offers a speech to text API as part of its Azure Cognitive Services platform, known as Speech Service SDK. The Speech Service SDK integrates with rest of the Microsoft APIs to transcribe recorded audio. It only allows the WAV or PCM file format with a single channel and sample rate of 6 kHz.</li>
<li>IBM offers a speech to text API as part if its Watson platform. Watson Speech to Text supports eight audio formats: BASIC, FLAC, L16, MP3, MULAW, OGG, WAV, and WEBM. The maximum size and length of the audio files vary depending on the format used. The results of transcription are returned as a JSON file.</li>
</ul>
<p>Apart from the support for various international spoken languages and an extensive global vocabulary, these cloud services support the following features to different extents:</p>
<ul>
<li><strong>Multichannel recognition</strong>: Identifying multiple participants recorded in multiple channels</li>
<li><strong>Speaker diarization</strong>: Prediction of speech of a certain speaker</li>
<li><strong>Custom models and model selection</strong>: Plug in your own models and select from a plethora of pre-built models</li>
<li>Inappropriate content filtering and noise filtering</li>
</ul>
<p>There are also many open source toolkits for speech recognition, such as Kaldi.</p>
<p>Kaldi (<a href="http:/kaldi-asr.org">http:/kaldi-asr.org</a>) is a popular open source speech to text recognition library. It is written in C++ and is available from <a href="https://github.com/kaldi-asr/kaldi">https://github.com/kaldi-asr/kaldi</a>. Kaldi can be integrated into your applications using its C++ API. It also supports Android using NDK, clang++, and OpenBLAS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Speech Commands Dataset</h1>
                </header>
            
            <article>
                
<p>The Google Speech Commands Dataset was created by the TensorFlow and AIY teams to showcase the speech recognition example using the TensorFlow API. The dataset has 65,000 clips of one-second-long duration. Each clip contains one of the 30 different words spoken by thousands of different subjects. </p>
<div class="packt_infobox"><span>The Google Speech Commands Dataset is available from the following link: <a href="http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz">http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz</a>.</span></div>
<p>The clips were recorded in realistic environments with phones and laptops. The 35 words contained noise words and the ten command words most useful in a robotics environment, and are listed as follows:</p>
<ul>
<li>Yes</li>
<li>No</li>
<li>Up</li>
<li>Down</li>
<li>Left</li>
<li>Right</li>
<li>On</li>
<li>Off</li>
<li>Stop</li>
<li>Go</li>
</ul>
<p>More details on how the speech dataset is prepared can be found in the following links:</p>
<ul>
<li><a href="https://arxiv.org/pdf/1804.03209.pdf">https://arxiv.org/pdf/1804.03209.pdf</a></li>
<li><a href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html">https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html</a></li>
</ul>
<p>With this dataset, thus the problem that shown in the example in this chapter is known as Keyword Spotting Task.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network architecture</h1>
                </header>
            
            <article>
                
<p>The network used for this example has three modules:</p>
<ul>
<li> A feature extraction module that processes the audio clips into feature vectors</li>
<li>A deep neural network module that produces softmax probabilities for each word in the input frame of feature vectors</li>
<li>A posterior handling module that combines the frame-level posterior scores into a single score for each keyword</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature extraction module</h1>
                </header>
            
            <article>
                
<p>In order to make the computation easy, the incoming audio signal is run through a voice-activity detection system and the signal is divided into speech and non-speech parts of the signals. The voice activity detector uses a 30-component diagonal covariance GMM model. The input to this model is 13-dimensional PLP features, their deltas, and double deltas. The output of GMM is passed to a State Machine that does temporal smoothing.</p>
<p>The output of this GMM-SM module is speech and non-speech parts of the signal.</p>
<p>The speech parts of the signal are further processed to generate the features. The acoustic features are generated based on 40-dimensional log-filterbank energies computed every 10 ms over a window of 25 ms. 10 Future and 30 Pas frames are added to the signal.</p>
<p>More details on feature extractor can be obtained from the original papers, links provided in the further readings section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep neural network module</h1>
                </header>
            
            <article>
                
<p>The DNN module is implemented with the Convolutional Neural Network (CNN) architecture. The code implements multiple variations of ConvNet, each variation producing different levels of accuracy and taking a different amount of time to train.</p>
<p>The code for building the model is provided in the <kbd>models.py</kbd><span> file.</span> It allows the creation of four different models, depending on the parameter passed at the command line:</p>
<ul>
<li><kbd>single_fc</kbd>: This model has only one fully connected layer.</li>
<li><kbd>conv</kbd>: This model is a full CNN architecture with two pairs of Convolution and MaxPool layers, followed by a fully connected layer.</li>
<li><kbd>low_latency_conv</kbd>: This model has one convolutional layer, followed by three fully connected layers. As the name suggests, it has a lesser number of parameters and computations compared with the <kbd>conv</kbd> architecture.</li>
<li><kbd>low_latency_svdf</kbd>: This model follows the architecture and layers from the paper titled <em>Compressing Deep Neural</em><br/>
<em>Networks using a Rank-Constrained Topology</em> available from <a href="https://research.google.com/pubs/archive/43813.pdf">https://research.google.com/pubs/archive/43813.pdf</a>.</li>
<li><kbd>tiny_conv</kbd>: This model has only one convolutional and one fully connected layer.</li>
</ul>
<p>The default architecture is <kbd>conv</kbd>, if the architecture is not passed from the command line. In our runs, the architectures showed the following accuracies for training, validation, and test sets when running the models with default accuracy and default number of steps of 18,000:</p>
<table border="1" style="border-collapse: collapse;width: 731px;height: 273px">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" rowspan="2" style="width: 251px">Architecture</td>
<td class="CDPAlignCenter CDPAlign" colspan="3" style="width: 469px">Accuracy (in %)</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 127px">Train set</td>
<td class="CDPAlignCenter CDPAlign" style="width: 158px">Validation set</td>
<td class="CDPAlignCenter CDPAlign" style="width: 184px">Test set</td>
</tr>
<tr>
<td style="width: 251px"><kbd>conv</kbd> (default)</td>
<td class="CDPAlignCenter CDPAlign" style="width: 127px">90</td>
<td class="CDPAlignCenter CDPAlign" style="width: 158px">88.5</td>
<td class="CDPAlignCenter CDPAlign" style="width: 184px">87.7</td>
</tr>
<tr>
<td style="width: 251px"><kbd>single_fc</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 127px">50</td>
<td class="CDPAlignCenter CDPAlign" style="width: 158px">48.5</td>
<td class="CDPAlignCenter CDPAlign" style="width: 184px">48.2</td>
</tr>
<tr>
<td style="width: 251px"><kbd>low_latenxy_conv</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 127px">22</td>
<td class="CDPAlignCenter CDPAlign" style="width: 158px">21.6</td>
<td class="CDPAlignCenter CDPAlign" style="width: 184px">23.6</td>
</tr>
<tr>
<td style="width: 251px"><kbd>low_latency_svdf</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 127px">7</td>
<td class="CDPAlignCenter CDPAlign" style="width: 158px">8.9</td>
<td class="CDPAlignCenter CDPAlign" style="width: 184px">8.6</td>
</tr>
<tr>
<td style="width: 251px"><kbd>tiny_conv</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 127px">55</td>
<td class="CDPAlignCenter CDPAlign" style="width: 158px">65.7</td>
<td class="CDPAlignCenter CDPAlign" style="width: 184px">65.4</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>Since the network architecture uses CNN layers that are more suitable for image data, the speech files are converted to a single-channel image by converting the audio signal of a short segment into vectors of frequency strengths.</span></p>
<p class="mce-root">As we can see from the preceding observations, the shortened architectures give lower accuracy for same hyper-parameters, but they run faster. Hence, they can be run for a higher number of epochs, or the learning rate could be increased to get higher accuracy.</p>
<p>Now let's see how to train and use this model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<ol>
<li>Move to the folder where you cloned the code from the repository, and train the model with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>python tensorflow/examples/speech_commands/train.py</strong></pre>
<p style="padding-left: 60px">You will start seeing the output of the training as follows:</p>
<pre style="padding-left: 60px"><strong>I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA</strong><br/><strong>I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</strong><br/><strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:</strong><br/><strong>name: Quadro P5000 major: 6 minor: 1 memoryClockRate(GHz): 1.506</strong><br/><strong>pciBusID: 0000:01:00.0</strong><br/><strong>totalMemory: 15.90GiB freeMemory: 14.63GiB</strong><br/><strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0</strong><br/><strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:</strong><br/><strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0</strong><br/><strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0: N</strong><br/><strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14168 MB memory) -&gt; physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:01:00.0, compute capability: 6.1)</strong></pre>
<ol start="2">
<li>Once the training iterations start, the code prints out the learning rate, along with accuracy and the cross entropy loss on the training set, as follows: </li>
</ol>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:Training from step: 1</strong><br/><strong>INFO:tensorflow:Step #1: rate 0.001000, accuracy 12.0%, cross entropy 2.662751</strong><br/><strong>INFO:tensorflow:Step #2: rate 0.001000, accuracy 6.0%, cross entropy 2.572391</strong><br/><strong>INFO:tensorflow:Step #3: rate 0.001000, accuracy 11.0%, cross entropy 2.547692</strong><br/><strong>INFO:tensorflow:Step #4: rate 0.001000, accuracy 8.0%, cross entropy 2.615582</strong><br/><strong>INFO:tensorflow:Step #5: rate 0.001000, accuracy 5.0%, cross entropy 2.592372</strong></pre>
<ol start="3">
<li>The code also keeps saving the model every 100 steps, so that if training is interrupted, then it can be restarted from the latest checkpoint saved:</li>
</ol>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:Saving to "/tmp/speech_commands_train/conv.ckpt-100"</strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The training runs for several hours for 18,000 steps and at the end prints the final training learning rate, accuracy, loss, and confusion matrix as follows:</p>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:Step #18000: rate 0.000100, accuracy 90.0%, cross entropy 0.420554</strong><br/><strong>INFO:tensorflow:Confusion Matrix:</strong><br/><strong> [[368 2 0 0 1 0 0 0 0 0 0 0]</strong><br/><strong> [ 3 252 9 6 13 15 13 18 17 1 13 11]</strong><br/><strong> [ 0 1 370 12 2 2 7 2 0 0 0 1]</strong><br/><strong> [ 3 8 4 351 8 7 6 0 0 0 3 16]</strong><br/><strong> [ 3 4 0 0 324 1 3 0 1 5 7 2]</strong><br/><strong> [ 4 3 4 19 1 330 3 0 0 1 3 9]</strong><br/><strong> [ 2 2 12 2 4 0 321 7 0 0 2 0]</strong><br/><strong> [ 3 7 1 1 2 0 4 344 1 0 0 0]</strong><br/><strong> [ 5 10 0 0 9 1 1 0 334 3 0 0]</strong><br/><strong> [ 4 2 1 0 33 0 2 2 7 317 4 1]</strong><br/><strong> [ 5 2 0 0 15 0 1 1 0 2 323 1]</strong><br/><strong> [ 4 17 0 33 2 8 0 1 0 2 3 302]]</strong></pre>
<div class="packt_tip">One observation from this output is that, although the code starts with a learning rate of 0.001, it reduces the learning rate to 0.001 towards the end. Since there are 12 command words, it also prints out a 12 x 12 confusion matrix.</div>
<p style="padding-left: 60px">The code also prints the accuracy and confusion matrix on the validation set as follows:</p>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:Step 18000: Validation accuracy = 88.5% (N=4445)</strong><br/><strong>INFO:tensorflow:Saving to "/tmp/speech_commands_train/conv.ckpt-18000"</strong><br/><strong>INFO:tensorflow:set_size=4890</strong><br/><strong>INFO:tensorflow:Confusion Matrix:</strong><br/><strong> [[404 2 0 0 0 0 0 0 0 0 2 0]</strong><br/><strong> [ 1 283 10 3 14 15 15 22 12 4 10 19]</strong><br/><strong> [ 0 7 394 4 1 3 9 0 0 0 1 0]</strong><br/><strong> [ 0 8 7 353 0 7 9 1 0 0 0 20]</strong><br/><strong> [ 2 4 1 0 397 6 2 0 1 6 5 1]</strong><br/><strong> [ 1 8 1 36 2 342 6 1 0 0 0 9]</strong><br/><strong> [ 1 2 14 1 4 0 386 4 0 0 0 0]</strong><br/><strong> [ 1 9 0 2 1 0 10 368 3 0 1 1]</strong><br/><strong> [ 2 13 0 0 7 10 1 0 345 15 3 0]</strong><br/><strong> [ 1 8 0 0 34 0 3 1 14 329 7 5]</strong><br/><strong> [ 0 1 1 0 11 3 0 0 1 2 387 5]</strong><br/><strong> [ 3 16 2 58 6 9 3 2 0 1 1 301]]</strong></pre>
<ol start="4">
<li>Finally, the code prints the test set accuracy as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:Final test accuracy = 87.7% (N=4890)</strong></pre>
<p>That's it. The model has been trained and can be exported for serving through TensorFlow, or embedding in another desktop, web, or mobile app.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about a project for converting audio data to text. There are many open source SDKs and commercial paid cloud services that allow us to convert from audio recordings and files to text data. As an example project, we took Google's Speech Commands Dataset and TensorFlow's deep learning-based example to convert audio files to spoken words in order to recognize the commands.</p>
<p>In the next chapter, we continue this journey to build a project for predicting stock prices using Gaussian Processes, which is a popular algorithm for forecasting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the interpretation of the confusion matrix provided at the end of the training?</li>
<li>Create a dataset of your own recorded voices with your friends and family members. Run the model on this data and observe what the accuracy <span>is</span>.</li>
<li>Retrain the model on your own dataset and check the accuracy for your own train, validation and test sets.</li>
<li>Experiment with different options from train.py and share your findings in a blog.</li>
<li>Add different architectures to the models.py file and see if you can create a better architecture for the speech dataset or your own recorded dataset.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>The following links are helpful in learning more about speech to text:</p>
<ul>
<li><a href="https://arxiv.org/pdf/1804.03209.pdf">https://arxiv.org/pdf/1804.03209.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1804.03209.pdf">https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html</a></li>
<li><a href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html">https://research.google.com/pubs/archive/43813.pdf</a></li>
<li><a href="https://research.google.com/pubs/archive/43813.pdf">https://cloud.google.com/speech-to-text/docs/</a></li>
<li><a href="https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html">https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html</a></li>
<li><a href="https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html">https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/">https://nlp.stanford.edu/projects/speech.shtml</a></li>
</ul>


            </article>

            
        </section>
    </body></html>