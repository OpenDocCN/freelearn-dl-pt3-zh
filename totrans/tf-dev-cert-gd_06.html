<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer100">
<h1 class="chapter-number" id="_idParaDest-112"><a id="_idTextAnchor129"/>6</h1>
<h1 id="_idParaDest-113"><a id="_idTextAnchor130"/>Improving the Model</h1>
<p><a id="_idTextAnchor131"/>The goal of modeling in machine learning is to ensure our model generalizes well on unseen data. Throughout our journey as data professionals who build models with neural networks, we are likely to come across two main issues: underfitting and overfitting. <strong class="bold">Underfitting</strong> is a<a id="_idIndexMarker291"/> scenario in which our model lacks the necessary complexity to capture underlying patterns in our <a id="_idIndexMarker292"/>data, while <strong class="bold">overfitting</strong> occurs when our model is too complex such that it not only learns the patterns but also picks up noise and outliers in our training data. In this case, our model performs exceptionally well on training data but fails to generalize well on unseen data. <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Image Classification with Neural Networks</em>, examined the science behind neural networks. Here, we will explore the art of fine-tuning neural networks to build optimally performing models for image classification. We will explore various network settings in a hands-on fashion to gain an understanding of the impact of each of these settings (hyperparameters) on our <span class="No-Break">model’s performance.</span></p>
<p>Beyond exploring the art of hyperparameter tuning, we will also explore various ways of improving our data quality using data normalization, data augmentation, and the use of synthetic data to improve the generalization capabilities of our model. In the past, there was a lot of emphasis on building complex networks. However, in more recent times, there has been an increased interest in enhancing the performance of neural networks using data-centric strategies. The use of these data-centric strategies does not erode the need for careful model design; rather, we can look at them as complementary strategies working in tandem toward a desired goal, thus enhancing our ability to build optimal models with good <span class="No-Break">generalization capabilities.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Data <span class="No-Break">is key</span></li>
<li>Fine-tuning hyperparameters of a <span class="No-Break">neural network</span></li>
</ul>
<p>By the end of this chapter, you will be equipped to effectively navigate the challenges posed by overfitting and underfitting using a combination of model-centric and data-centric ideas when building models with <span class="No-Break">neural networks.</span></p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor132"/>Technical requirements</h1>
<p>We will be using a <strong class="bold">Google Colaboratory</strong> (<strong class="bold">Google Colab</strong>) notebook as our work environment as it is a free, cloud-based Jupyter Notebook that is easy to use and provides us with GPU and TPU backends. We will be using Google Colab to run the coding exercise that requires <strong class="source-inline">python &gt;= 3.8.0</strong>, along with the following packages that can be installed using the <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> command:</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">tensorflow&gt;=2.7.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">tensorflow-datasets==4.4.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pandas==1.3.4</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">numpy==1.21.4</strong></span><strong class="source-inline"><a id="_idTextAnchor133"/></strong></li>
</ul>
<h1 id="_idParaDest-115"><a id="_idTextAnchor134"/>Data is key</h1>
<p>When it comes<a id="_idIndexMarker293"/> to improving the performance of a neural network, or any other machine learning model for that matter, the importance of good data preparation cannot be overemphasized. In <a href="B18118_03.xhtml#_idTextAnchor065"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Linear Regression with TensorFlow</em>, we saw the impact that normalizing our data had on the model’s performance. Beyond data normalization, there are other data preparation techniques that can make a difference in our <span class="No-Break">modeling process.</span></p>
<p>As you must have recognized by now, machine learning requires investigating, experimenting, and applying different techniques, depending on the problem at hand. To ensure we have an optimally performing model, our journey should start by looking at our data thoroughly. Do we have enough representative samples from each of the target classes? Is our data balanced? Have we ensured the absence of incorrect labels? Do we have the right type of data? How are we dealing with missing data? These are some of the questions we have to ask and handle before the <span class="No-Break">modeling phase.</span></p>
<p>Improving the quality of our data is a multi-faceted endeavor involving different techniques, such as engineering new features from existing ones in our data by applying some data preprocessing techniques such as data normalization. When we are working with imbalanced datasets, where we are short of representative samples of the minority class, the logical thing would be to gather more data on the minority class; however, this is not practical in all instances. In such cases, synthetic data may be an effective alternative. Start-ups <a id="_idIndexMarker294"/>such<a id="_idIndexMarker295"/> as <strong class="bold">Anyverse.ai</strong> and <strong class="bold">Datagen.tech</strong> focus on synthetic data development, thus making it possible to mitigate issues around data imbalance and data scarcity. However, synthetic data may be costly, and it is important that we do a cost-benefit analysis before embarking on <span class="No-Break">this route.</span></p>
<p>Another problem we could face is when our collected samples are not representative enough for our model to function properly. Imagine you train your model to recognize human faces. You gather thousands of images of human faces and split your data into training and test sets. You train your model, and it predicts perfectly on your test set. However, when you ship this <a id="_idIndexMarker296"/>model as a product to the open market, you get a result such as <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 6.1 – The need for data augmentation" height="550" src="image/B18118_06_01.jpg" width="506"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The need for data augmenta<a id="_idTextAnchor135"/>tion</p>
<p>Surprising, right? Even though you trained the model on thousands of images, the model failed to learn to recognize faces if the axis is flipped vertically, horizontally, or otherwise. To mitigate this type of problem, we employ a technique called data augmentation. Data augmentation is a technique that we use to create new training data by altering the existing data in some way, such as randomly cropping, zooming in or out, or rotating or flipping the initial image. The underlying idea behind data augmentation is to enable our model to recognize an object in the image, even under unpredictable conditions such as the ones we saw in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<p>Data augmentation is useful when we want more data samples from a limited training set; we can use data augmentation to efficiently increase the size of our dataset, hence giving our model more data to learn from. Also, because we can simulate various scenarios, our model is less likely to overfit as it learns the underlying patterns in the data rather than the noise in the data because our model learns about the data in multiple ways. Another important benefit of data augmentation is that it is a cost-saving technique that saves us from expensive and sometimes time-consuming data collection processes. We will be applying data augmentation in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Handling Overfitting,</em> where we will be working on real-world image classification problems. Also, should you find yourself working on image or text data in the future, you may find data augmentation a very handy technique <span class="No-Break">to know.</span></p>
<p>Beyond addressing issues around data imbalance and data diversity, we may also want to refine our model itself to make it suitably complex enough to identify patterns in the data, and we want to do this without overfitting the model. Here, the aim is to improve the model’s quality by tweaking one or more settings, such as increasing the number of hidden layers, adding more neurons to each layer, changing the optimizer, or using a more sophisticated activation function. These settings can be tuned via experimentation until an optimal model <span class="No-Break">is achieved.</span></p>
<p>We have discussed <a id="_idIndexMarker297"/>several ideas behind improving the performance of neural networks. Now, let’s see how we can improve the result achieved on the Fashion MNIST dataset in <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Image Classification with </em><span class="No-Break"><em class="italic">Neural Networks</em></span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor136"/>Fine-tuning hyperparameters of a neural network</h1>
<p>It is important<a id="_idIndexMarker298"/> to establish <a id="_idIndexMarker299"/>a <strong class="bold">baseline model</strong> before <a id="_idIndexMarker300"/>making any improvements in machine learning. A baseline model is a simple model that we can use to evaluate the performance of more complex models. In <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Image Classification with Neural Networks</em>, we achieved an accuracy of 88.50% on our training data and 85.67% on our test data in just five epochs. In our quest to try to improve our model’s performance, we will continue with our three-step (<em class="italic">build</em>, <em class="italic">compile</em>, and <em class="italic">fit</em>) process of constructing a neural network<a id="_idIndexMarker301"/> using <strong class="bold">TensorFlow</strong>. In each of the steps we use to build our neural network, there are settings that need to be configured before training. These settings are called <strong class="bold">hyperparameters</strong>. They<a id="_idIndexMarker302"/> control how the network will learn and perform, and mastering the art of fine-tuning them is an essential step in building successful deep learning models. Common hyperparameters include the number of neurons in each layer, the number of hidden layers, the learning rate, the activation functions, and the number of epochs. By iteratively experimenting with these hyperparameters, we can obtain the optimal setting best suited to our <span class="No-Break">use case.</span></p>
<p>When building real-world models, especially when working on domain-specific problems, expert knowledge could prove very useful in pinpointing the best hyperparameter values for the task. Let’s return to our notebook and experiment with different<a id="_idIndexMarker303"/> hyperparameters, and see whether we can <a id="_idIndexMarker304"/>beat our baseline model by tweaking one or <span class="No-Break">more hyperparameters.</span></p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor137"/>Increasing the number of epochs</h2>
<p>Imagine you are<a id="_idIndexMarker305"/> trying to teach a child the multiplication tables; each study session you have with the child can be likened to an epoch in machine learning. If you only have a small number of study sessions with the child, odds are they will not fully grasp the concept of multiplication. Hence, the child will be unable to attempt basic multiplication problems. This scenario in machine learning is underfitting, where the model hasn’t been able to grasp the underlying patterns in the data due to <span class="No-Break">insufficient training.</span></p>
<p>On the other hand, let’s say you spend a good amount of time teaching the child to memorize specific aspects of the times tables, say the 2s, 3s, and 4s. The child becomes skilled at reciting these tables; however, when faced with multiplying numbers such as 10 x 8, the child struggles. This happens because rather than understanding the principle of multiplication such that the child can apply the underlying idea when working with other numbers, the child simply memorized the examples learned during the study session. This scenario in machine learning is like the concept of overfitting, where our model performs well on the training data but fails to generalize well in new situations. In machine learning, when training our model, we need to strike a balance such that our model is trained well enough to learn the underlying patterns in our data and not to memorize the <span class="No-Break">training data.</span></p>
<p>Let’s see what the impact of training the model for longer will have on our result. This time, let’s go for 40 epochs and observe what <span class="No-Break">will happen:</span></p>
<pre class="source-code">
#Step 1:  Model configuration
model=keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(100, activation=»relu»),
    keras.layers.Dense(10,activation=»softmax»)
])
#Step 2: Compiling the model, we add the loss, optimizer and evaluation metrics here
model.compile(optimizer='adam',
    loss=›sparse_categorical_crossentropy›,
    metrics=[‹accuracy›])
#Step 3: We fit our data to the model
history= model.fit(train_images, train_labels, epochs=40)</pre>
<p>Here, we change <a id="_idIndexMarker306"/>the number of epochs in <strong class="source-inline">Step 3</strong> from <strong class="source-inline">5</strong> to <strong class="source-inline">40</strong>, keeping all other hyperparameters of our base model constant. The last five epochs of the output are <span class="No-Break">displayed here:</span></p>
<pre class="source-code">
Epoch 36/40
1875/1875 [==============================] - 5s 2ms/step - loss: 0.1356 - accuracy: 0.9493
Epoch 37/40
1875/1875 [==============================] - 5s 2ms/step - loss: 0.1334 - accuracy: 0.9503
Epoch 38/40
1875/1875 [==============================] - 4s 2ms/step - loss: 0.1305 - accuracy: 0.9502
Epoch 39/40
1875/1875 [==============================] - 4s 2ms/step - loss: 0.1296 - accuracy: 0.9512
Epoch 40/40
1875/1875 [==============================] - 4s 2ms/step - loss: 0.1284 - accuracy: 0.9524</pre>
<p>Notice that, when we increase the number of epochs, it takes a much longer time to train our model. So, it can become computationally expensive when we have to train for a large number of epochs. After 40 epochs, we see that our model’s training accuracy has jumped up to <strong class="source-inline">0.9524</strong>, and it may appear to you that you have found a silver bullet for solving this problem. However, our goal is to ensure generalization; hence, the acid test for our model is to see how it will perform on the unseen data. Let’s check out what the results look like on our <span class="No-Break">test data:</span></p>
<pre class="source-code">
test_loss, test_acc=model.evaluate(test_images,test_labels)
print('Test Accuracy: ', test_acc)</pre>
<p>When we run the <a id="_idIndexMarker307"/>code, we arrive at an accuracy of <strong class="source-inline">0.8692</strong> on our test data. We can see that the longer the model is trained, the more accurate the model can be on the training data. However, if we train the model for too long, it will reach a point of diminishing returns, which is evident when we compare the difference in performance between the training and test set accuracy. It is critical to strike the right balance of epochs so that the model can learn and improve, but not to the point of overfitting on our training data. A practical approach could be to start with a small number of epochs, then increase the number of epochs as required. This approach can be effective; however, it can also be time-consuming as multiple experiments are required to find the optimal number <span class="No-Break">of epochs.</span></p>
<p>What if we could perhaps set a rule to stop training before the point of diminishing returns? Yes, this<a id="_idIndexMarker308"/> is possible. Let’s examine this idea next and see what difference it can make to <span class="No-Break">our result.</span></p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor138"/>Early stopping using callbacks</h2>
<p><strong class="bold">Early stopping</strong> is a<a id="_idIndexMarker309"/> regularization <a id="_idIndexMarker310"/>technique that can be <a id="_idIndexMarker311"/>used to prevent overfitting when training neural networks. When we hardcode the number of epochs into our model, we cannot halt training when the desired metric is attained, or when training begins to degrade or fails to improve any further. We saw this when we increased the number of epochs. However, to mitigate against this scenario, TensorFlow provides us with early stopping callbacks such that we can either use the in-built callback functions or design our own custom callbacks. We can monitor our experiments in real time with more control such that we can halt training before our model begins to overfit, when our model stops learning during training, or in line with other defined criteria. Early stopping can be invoked at various points during training. It can be applied at the start or end of training or based on attaining a <span class="No-Break">specific metric.</span></p>
<h3>Early stopping with built-in callbacks</h3>
<p>Let’s explore <a id="_idIndexMarker312"/>built-in callbacks for early stopping<a id="_idIndexMarker313"/> <span class="No-Break">with TensorFlow:</span></p>
<ol>
<li>We’ll start by importing early stopping <span class="No-Break">from TensorFlow:</span><pre class="source-code">
from tensorflow.keras.callbacks import EarlyStopping</pre></li>
<li>Next, we initialize early stopping. TensorFlow allows us to pass in some arguments, which we use to create a <span class="No-Break"><strong class="source-inline">callbacks</strong></span><span class="No-Break"> object:</span><pre class="source-code">
callbacks = EarlyStopping(monitor='val_loss',</pre><pre class="source-code">
    patience=5, verbose=1, restore_best_weights=True)</pre></li>
</ol>
<p>Let’s unpack some of the arguments used in our early <span class="No-Break">stopping function:</span></p>
<ul>
<li><strong class="source-inline">monitor</strong> can be used to track a metric we want to keep an eye on; in our case, we want to keep track of the validation loss. We could also switch it to track the validation accuracy. It is a good idea to track your experiment on your validation split, hence we set our <strong class="source-inline">callbacks</strong> to monitor the <span class="No-Break">validation loss.</span></li>
<li>The <strong class="source-inline">patience</strong> argument is set at <strong class="source-inline">5</strong>. This means if there is no progress with regards to reducing the validation loss after five epochs, the training <span class="No-Break">will end.</span></li>
<li>We add the <strong class="source-inline">restore_best_weight</strong> argument and set it to <strong class="source-inline">True</strong>. This allows the callback to monitor the entire process and restores the weights from the best epoch found during training. If we set <strong class="source-inline">restore_best_weight</strong> to <strong class="source-inline">False</strong>, the model weights from the last <span class="No-Break">training step.</span></li>
<li>When we set <strong class="source-inline">verbose</strong> to <strong class="source-inline">1</strong>, this ensures we are informed when callback actions take place. If we set <strong class="source-inline">verbose</strong> to <strong class="source-inline">0</strong>, training stops but we get no <span class="No-Break">output message.</span></li>
</ul>
<p>There are a few <a id="_idIndexMarker314"/>other arguments that can be<a id="_idIndexMarker315"/> used here, but these ones work well enough for many instances with regard to applying <span class="No-Break">early stopping.</span></p>
<ol>
<li value="3">We’ll continue with our three-step approach in which we build, compile, and fit <span class="No-Break">the model:</span><pre class="source-code">
#Step 1:  Model configuration</pre><pre class="source-code">
model=keras.Sequential([</pre><pre class="source-code">
    keras.layers.Flatten(input_shape=(28,28)),</pre><pre class="source-code">
    keras.layers.Dense(100, activation=»relu»),</pre><pre class="source-code">
    keras.layers.Dense(10,activation=»softmax»)</pre><pre class="source-code">
])</pre><pre class="source-code">
#Step 2: Compiling the model, we add the loss, optimizer and evaluation metrics here</pre><pre class="source-code">
model.compile(optimizer='adam',</pre><pre class="source-code">
    loss=›sparse_categorical_crossentropy›,</pre><pre class="source-code">
    metrics=[‹accuracy›])</pre><pre class="source-code">
#Step 3: We fit our data to the model</pre><pre class="source-code">
history= model.fit(train_images, train_labels,</pre><pre class="source-code">
    epochs=100, callbacks=[callbacks],</pre><pre class="source-code">
    validation_split=0.2)</pre></li>
</ol>
<p><strong class="source-inline">Step 1</strong> and <strong class="source-inline">Step 2</strong> are the same steps we previously implemented. When building out the model, we trained for longer epochs. However, in <strong class="source-inline">Step 3</strong>, we made a few tweaks to accommodate our validation split and callbacks. We use 20% of our training data for validation and we pass our <strong class="source-inline">callbacks</strong> object into <strong class="source-inline">model.fit()</strong>. This ensures our early stopping callbacks interrupt the training when our <a id="_idIndexMarker316"/>validation loss stops falling. The<a id="_idIndexMarker317"/> output is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
Epoch 14/100
1500/1500 [==============================] - 4s 2ms/step - loss: 0.2197 - accuracy: 0.9172 - val_loss: 0.3194 - val_accuracy: 0.8903
Epoch 15/100
1500/1500 [==============================] - 4s 2ms/step - loss: 0.2133 - accuracy: 0.9204 - val_loss: 0.3301 - val_accuracy: 0.8860
Epoch 16/100
1500/1500 [==============================] - 4s 2ms/step - loss: 0.2064 - accuracy: 0.9225 - val_loss: 0.3267 - val_accuracy: 0.8895
Epoch 17/100
1500/1500 [==============================] - 3s 2ms/step - loss: 0.2018 - accuracy: 0.9246 - val_loss: 0.3475 - val_accuracy: 0.8844
Epoch 18/100
1500/1500 [==============================] - 4s 2ms/step - loss: 0.1959 - accuracy: 0.9273 - val_loss: 0.3203 - val_accuracy: 0.8913
Epoch 19/100
1484/1500 [============================&gt;.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9282 Restoring model weights from the end of the best epoch: 14.
1500/1500 [==============================] - 4s 2ms/step - loss: 0.1928 - accuracy: 0.9281 - val_loss: 0.3347 - val_accuracy: 0.8912
Epoch 19: early stopping</pre>
<p>Because we set <strong class="source-inline">verbose</strong> to <strong class="source-inline">1</strong>, we can see that our experiment ends on epoch 19. Now, rather than worrying about how many epochs we need to train effectively, we can simply select a large number of epochs and implement early stopping. Next, we can also see that because we implemented <strong class="source-inline">restore_best_weights</strong>, the best weights are achieved on epoch 14 where we recorded the lowest validation loss (<strong class="source-inline">0.3194</strong>). With early stopping, we save compute time and take concrete steps <span class="No-Break">against overfitting.</span></p>
<ol>
<li value="4">Let’s see what our test accuracy <span class="No-Break">looks like:</span><pre class="source-code">
test_loss, test_acc = model.evaluate(test_images,</pre><pre class="source-code">
    test_labels)</pre><pre class="source-code">
print('Test Accuracy: ', test_acc)</pre></li>
</ol>
<p>Here, we<a id="_idIndexMarker318"/> achieved<a id="_idIndexMarker319"/> a test accuracy <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.8847</strong></span><span class="No-Break">.</span></p>
<p>Now, let’s see how we can write our own custom callbacks to implement <span class="No-Break">early stopping.</span></p>
<h3>Early stopping with custom callbacks</h3>
<p>We can extend<a id="_idIndexMarker320"/> the<a id="_idIndexMarker321"/> capabilities of callbacks by writing our own custom callbacks for early stopping. This adds flexibility to callbacks so we can implement some desired logic during training. The TensorFlow documentation provides several ways to do this. Let’s implement a simple callback to track our <span class="No-Break">validation accuracy:</span></p>
<pre class="source-code">
class EarlyStop(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('val_accuracy') &gt; 0.85):
            print("\n\n85% validation accuracy has been reached.")
            self.model.stop_training = True
callback = EarlyStop()</pre>
<p>For example, if we want to stop our training process when the model exceeds 85% accuracy on the validation set, we can do this by crafting our own custom callback called <strong class="source-inline">E</strong><strong class="source-inline">arlyStop</strong>, which takes the <strong class="source-inline">tf.keras.callbacks.Callback</strong> parameter. We then define a function called <strong class="source-inline">on_epoch_end</strong>, which returns the logs for each epoch. We set <strong class="source-inline">self.model.stop_training = True</strong> and once the accuracy exceeds 85%, training ends and displays a message similar to what we get with <strong class="source-inline">verbose</strong> set to <strong class="source-inline">1</strong> when we used<a id="_idIndexMarker322"/> built-in callbacks. Now we can pass in <a id="_idIndexMarker323"/>our <strong class="source-inline">callback</strong> into <strong class="source-inline">model.fit()</strong> as we did with built-in callbacks. We then train our model using our <span class="No-Break">three-step approach:</span></p>
<pre class="source-code">
Epoch 1/100
1490/1500 [============================&gt;.] - ETA: 0s - loss: 0.5325 - accuracy: 0.8134/n/n 85% validation accuracy has been reached
1500/1500 [==============================] - 4s 3ms/step - loss: 0.5318 - accuracy: 0.8138 - val_loss: 0.4190 - val_accuracy: 0.8538</pre>
<p>This time, at the end of the first epoch, we arrive at over 85% validation accuracy. Again, this is a smart way of achieving the desired metrics with minimal use of <span class="No-Break">computational resources.</span></p>
<p>Now that we have a good grasp of how to select epochs and apply early stopping, let’s now set our sights on other hyperparameters and see whether by tweaking one or more of them, we can improve our test accuracy of 88%. Perhaps we can start by trying out a more<a id="_idIndexMarker324"/> <span class="No-Break">complex</span><span class="No-Break"><a id="_idIndexMarker325"/></span><span class="No-Break"> model.</span></p>
<p>Let’s see what happens if we add more neurons to our <span class="No-Break">hidden layer.</span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor139"/>Adding neurons in the hidden layer</h2>
<p>The hidden <a id="_idIndexMarker326"/>layers are<a id="_idIndexMarker327"/> responsible for the heavy lifting in neural networks, as we covered when discussing the anatomy of neural networks in <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Image Classification with Neural Networks</em>. Let’s try out different numbers of neurons in the hidden layer. We’ll define a function called <strong class="source-inline">train_model</strong> that will allow us to try out different numbers of neurons. The <strong class="source-inline">train_model</strong> function takes the <strong class="source-inline">hidden_neurons</strong> argument that represents the number of hidden neurons in the model. In addition, the function also takes training images, labels, callbacks, validation splits, and<a id="_idIndexMarker328"/> epochs. The function builds, compiles, and fits the model using <span class="No-Break">these parameters:</span></p>
<pre class="source-code">
def train_model(hidden_neurons, train_images, train_labels, callbacks=None, validation_split=0.2, epochs=100):
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(hidden_neurons, activation=»relu»),
        keras.layers.Dense(10, activation=»softmax»)
    ])
    model.compile(optimizer=›adam›,
        loss=›sparse_categorical_crossentropy›,
        metrics=[‹accuracy›])
    history = model.fit(train_images, train_labels,
        epochs=epochs, callbacks=[callbacks] if callbacks else None,
        validation_split=validation_split)
    return model, history</pre>
<p>To try out a list of neurons, we created a <strong class="source-inline">for</strong> loop to iterate over the neuron list called <strong class="source-inline">neuron_values</strong>. Then it applies the <strong class="source-inline">train_model</strong> function to build and train a model for each of the neurons in <span class="No-Break">the list:</span></p>
<pre class="source-code">
neuron_values = [1, 500]
for neuron in neuron_values:
    model, history = train_model(neurons, train_images,
        train_labels, callbacks=callbacks)
    print(f»Trained model with {neurons} neurons in the hidden layer»)</pre>
<p>The <strong class="source-inline">print</strong> statement returns a message indicating that the model has been trained with 1 and 500 neurons respectively. Let’s examine the results when we run the function, starting <a id="_idIndexMarker329"/>with <a id="_idIndexMarker330"/>the hidden layer with <span class="No-Break">one neuron:</span></p>
<pre class="source-code">
Epoch 36/40
1500/1500 [==============================] - 3s 2ms/step - loss: 1.2382 - accuracy: 0.4581 - val_loss: 1.2705 - val_accuracy: 0.4419
Epoch 37/40
1500/1500 [==============================] - 2s 1ms/step - loss: 1.2360 - accuracy: 0.4578 - val_loss: 1.2562 - val_accuracy: 0.4564
Epoch 38/40
1500/1500 [==============================] - 2s 1ms/step - loss: 1.2340 - accuracy: 0.4559 - val_loss: 1.2531 - val_accuracy: 0.4507
Epoch 39/40
1500/1500 [==============================] - 2s 1ms/step - loss: 1.2317 - accuracy: 0.4552 - val_loss: 1.2553 - val_accuracy: 0.4371
Epoch 40/40
1500/1500 [==============================] - 2s 1ms/step - loss: 1.2292 - accuracy: 0.4552 - val_loss: 1.2523 - val_accuracy: 0.4401
end of experiment with 1 neuron</pre>
<p>From our result, the model with one neuron in the hidden layer was not complex enough to identify patterns in the data. This model performed well below 50%, which is a clear case of<a id="_idIndexMarker331"/> underfitting. Next, let’s look at the result from the model with <span class="No-Break">500 neurons:</span></p>
<pre class="source-code">
Epoch 11/40
1500/1500 [==============================] - 6s 4ms/step - loss: 0.2141 - accuracy: 0.9186 - val_loss: 0.3278 - val_accuracy: 0.8878
Epoch 12/40
1500/1500 [==============================] - 6s 4ms/step - loss: 0.2057 - accuracy: 0.9220 - val_loss: 0.3169 - val_accuracy: 0.8913
Epoch 13/40
1500/1500 [==============================] - 6s 4ms/step - loss: 0.1976 - accuracy: 0.9258 - val_loss: 0.3355 - val_accuracy: 0.8860
Epoch 14/40
1500/1500 [==============================] - 6s 4ms/step - loss: 0.1893 - accuracy: 0.9288 - val_loss: 0.3216 - val_accuracy: 0.8909
Epoch 15/40
1499/1500 [============================&gt;.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9303Restoring model weights from the end of the best epoch: 10.
1500/1500 [==============================] - 6s 4ms/step - loss: 0.1826 - accuracy: 0.9303 - val_loss: 0.3408 - val_accuracy: 0.8838
Epoch 15: early stopping
end of experiment with 500 neurons</pre>
<p>We can see that the <a id="_idIndexMarker332"/>model <a id="_idIndexMarker333"/>is overfitting with more neurons. The model recorded an accuracy of <strong class="source-inline">0.9303</strong> on the<a id="_idTextAnchor140"/> training set but <strong class="source-inline">0.8838</strong> on the test set. In general, a larger hidden layer can learn more complex patterns; however, it would require more computational resources and be more prone to overfitting. When selecting the number of neurons in the hidden layer, it is important to consider the size of the training data. If we have a large training sample, we can afford to have a large number of neurons in the hidden layer. However, when the training sample is quite small, it may be better to consider working with a smaller number of neurons in the hidden layer. A larger number of neurons could lead to overfitting, as we saw in our experiment, and this architecture may perform even worse than a model with a smaller number of neurons in the <span class="No-Break">hidden layer.</span></p>
<p>Another consideration to bear in mind is the type of data we are working with. When we work with linear data, a small number of hidden layers may be sufficient for our neural network. However, with non-linear data, we will need a more complex model to learn the complexities in the data. Finally, we must bear in mind that models with more neurons require longer training time. It is important to consider the trade-off between performance and generalization. As a rule of thumb, you can start training a model with a small number of neurons. This will train faster and <span class="No-Break">avoid overfitting.</span></p>
<p>Alternatively, we can optimize the number of neurons in the hidden layer by identifying neurons that have little or no impact on the performance of the network. This approach is called <strong class="bold">pruning</strong>. This is <a id="_idIndexMarker334"/>outside the scope of the exam, so we will <span class="No-Break">stop there.</span></p>
<p>Let’s see the<a id="_idIndexMarker335"/> impact of <a id="_idIndexMarker336"/>adding more layers to our baseline architecture. So far, we have looked at making the model more complex and training for longer. How about we try out changing the optimizers? Let’s mix things up a bit and see <span class="No-Break">what happens.</span></p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor141"/>Changing the optimizers</h2>
<p>We have<a id="_idIndexMarker337"/> used the <strong class="bold">Adam optimizer</strong> as our<a id="_idIndexMarker338"/> default optimizer; however, there are<a id="_idIndexMarker339"/> other prominent optimizers and they all have their pros and cons. In this book and for your exam, we will focus on Adam, <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>), and <strong class="bold">Root Mean Squared Propagation</strong> (<strong class="bold">RMSprop</strong>). RMSprop<a id="_idIndexMarker340"/> has low memory requirements and offers an adaptive learning rate; on the flip side, it takes a much longer time to converge in comparison to Adam and SGD. RMSprop works well on training very deep networks <a id="_idIndexMarker341"/>such as <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>), which we will talk about later in <span class="No-Break">this book.</span></p>
<p>On the other hand, SGD is another popular optimizer; it is simple to implement and efficient when the data is sparse. However, it is slow to converge and requires careful tuning of the learning rate. If the learning rate is too high, SGD will diverge; if the learning rate is too low, SGD will converge very slowly. SGD works well on a wide variety of problems and converges faster than other optimizers on large datasets, but it could sometimes converge slowly when training very large <span class="No-Break">neural networks.</span></p>
<p>Adam is an improved version of SGD; it has low memory requirements, offers an adaptive learning rate, is a very efficient optimizer, and can converge to a good solution in fewer iterations than to SGD or RMSprop. Adam is also well suited for training large <span class="No-Break">neural networks.</span></p>
<p>Let’s try out these three optimizers and see which one works best on our dataset. We have changed the optimizer from Adam to RMSprop and SGD, and used the same architecture with<a id="_idIndexMarker342"/> built-in callbacks. We can see the results in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Adam</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">RMSProp</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SGD</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Number of epochs before <span class="No-Break">early stopping</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">13</span></p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">39</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Validation accuracy</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8867</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8788</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8836</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Test accuracy</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8787</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8749</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8749</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – The performance of different optimizers</p>
<p>Although Adam required more training epochs, its results were marginally better than those of the other optimizers. Of course, any of these optimizers can be used for this problem. In later chapters, we will work on real-world images that are more complex. There, we will <a id="_idIndexMarker343"/>revisit<a id="_idIndexMarker344"/> <span class="No-Break">these optimizers.</span></p>
<p>Before we close this chapter, let’s look at the learning rate and its impact on our <span class="No-Break">model’s performance.</span></p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor142"/>Changing the learning rate</h2>
<p><strong class="bold">Learning rate</strong> is an important <a id="_idIndexMarker345"/>hyperparameter that controls how well our model will learn and improve <a id="_idIndexMarker346"/>during training. An optimal learning rate will ensure the model converges quickly and accurately, while on the other hand, a poorly selected learning rate can lead to a wide range of issues such as slow convergence, underfitting, overfitting, or <span class="No-Break">network instability.</span></p>
<p>To understand the impact of the learning rate, we need to know how it affects the model’s training process. The learning rate is the step size taken to reach the point where the loss function is at its minimum. In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3(a)</em>, we see that when we choose a low learning rate, the model requires too many steps to reach the minimum point. On the flip side, when the learning rate is too high, the model would likely learn too quickly, taking larger steps and likely overshooting the minimum point, as seen in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3(c)</em>. A high learning rate can lead to instability and overfitting. However, when we find the ideal learning rate as in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3(b)</em>, the model is likely to experience fast<a id="_idIndexMarker347"/> convergence and <span class="No-Break">good generalization:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 6.3 – A plot showing low, optimal, and high learning rates" height="633" src="image/B18118_06_03.jpg" width="1665"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – A plot showing low, optimal, and high learning rates</p>
<p>The question that comes to mind is: how do we find the optimal learning rate? One way is to try out different learning rates and see what works based on evaluating the model on the validation set. Another way is to use a learning rate scheduler. This allows us to dynamically adjust the learning rate during training. We will explore this approach in the later chapters of the book. Here, let’s try out a few different learning rates to see how they impact on <span class="No-Break">our network.</span></p>
<p>Let’s craft a function <a id="_idIndexMarker348"/>that will take a list of different learning rates. In this experiment, we will try out six different learning rates (1, 0.1, 0.01, 0.001, 0.0001, 0.00001, and 0.000001). First, let’s create a function to create <span class="No-Break">our model:</span></p>
<pre class="source-code">
def learning_rate_test(learning_rate):
    #Step 1:  Model configuration
    model=keras.Sequential([
        keras.layers.Flatten(input_shape=(28,28)),
        keras.layers.Dense(64, activation=»relu»),
        keras.layers.Dense(10,activation=»softmax»)
])
    #Step 2: Compiling the model, we add the loss,
         #optimizer and evaluation metrics here
    model.compile(optimizer=tf.keras.optimizers.Adam(
        learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=[‹accuracy›])
    #Step 3: We fit our data to the model
    callbacks = EarlyStopping(monitor='val_loss',
        patience=5, verbose=1, restore_best_weights=True)
    history=model.fit(train_images, train_labels,
        epochs=50, validation_split=0.2,
        callbacks=[callbacks])
    score=model.evaluate(test_images, test_labels)
    return score[1]</pre>
<p>We will use the<a id="_idIndexMarker349"/> function to <a id="_idIndexMarker350"/>build, compile, and fit the model. It also takes in the learning rate as a variable that we pass into our function, and that returns the test accuracy as <span class="No-Break">our result:</span></p>
<pre class="source-code">
# Try out different learning rates
learning_rates = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001,
    0.000001]
# Create an empty list to store the accuracies
accuracies = []
# Loop through the different learning rates
for learning_rate in learning_rates:
    # Get the accuracy for the current learning rate
    accuracy = learning_rate_test(learning_rate)
    # Append the accuracy to the list
    accuracies.append(accuracy)</pre>
<p>We have now <a id="_idIndexMarker351"/>outlined<a id="_idIndexMarker352"/> the different learning rates. Here, we want to experiment with different learning rates, from a very high to a very low learning rate. We created an empty list and appended our test set accuracy. Next, let’s look at the numeric values in a tabular fashion. We use <strong class="source-inline">pandas</strong> to generate a DataFrame with the learning rate <span class="No-Break">and accuracies:</span></p>
<pre class="source-code">
df = pd.DataFrame(list(zip(learning_rates, accuracies)),
    columns =[‹Learning_rates›, ‹Test_Accuracy›])
df</pre>
<p> The output DataFrame is shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 6.4 – Different learning rates and their test accuracies" height="311" src="image/B18118_06_04.jpg" width="317"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Different learning rates and their test accuracies</p>
<p>From the results, we can see that when working with a really high learning rate (1.0), the model performs poorly. As we reduce the learning rate value, we see that the model’s accuracy begins to rise; when the learning rate becomes too small, the model takes too long to converge. There is no silver bullet when it comes to choosing the ideal learning rate for a problem. It depends on several factors such as the model architecture, the data, and the type of optimization <span class="No-Break">technique applied.</span></p>
<p>Now that we have seen various ways of tweaking a model to improve its performance, we have come to the end of the chapter. We’ve tried adjusting different hyperparameters to improve our model’s performance; however, we got stuck on 88% test accuracy. Perhaps<a id="_idIndexMarker353"/> this is a <a id="_idIndexMarker354"/>good time to try something else, which we will do in the next chapter. Take a break, and when you are ready, let’s see how we can improve this result and also try out <span class="No-Break">real-world images.</span></p>
<h1 id="_idParaDest-122"><a id="_idTextAnchor143"/>Summary</h1>
<p>In this chapter, we looked at improving the performance of a neural network. Although we worked with a lightweight dataset, we have learned some important ideas around improving our model’s performance–ideas that will come in handy, both in the exam and on the job. You now know that data quality and model complexity are two sides of the machine learning coin. If you have good-quality data, a poor model will yield subpar results and, on the flip side, even the most advanced model will yield a suboptimal result with <span class="No-Break">bad data.</span></p>
<p>By now, you should have a good understanding and hands-on experience of fine-tuning neural networks. Like a seasoned expert, you should be able to understand the art of fine-tuning hyperparameters and apply this to different machine learning problems and not just image classification. Also, you have seen that model building requires a lot of experimenting. There is no silver bullet, but having a good understanding of the moving parts and various techniques, and how and why to apply them, is what differentiates a star from the <span class="No-Break">average Joe.</span></p>
<p>In the next chapter, we will examine convolutional neural networks. We will see why they are state-of-the-art when it comes to image classification tasks. We will look at the power of convolutions and examine in a hands-on fashion how they do things differently from the simple neural networks we have been using <span class="No-Break">so far.</span></p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor144"/>Questions</h1>
<p>Let’s test what we’ve learned in this chapter using the <span class="No-Break">CIFAR-10 notebook:</span></p>
<ol>
<li>Build a neural network using our <span class="No-Break">three-step approach.</span></li>
<li>Increase the number of neurons from 5 to 100 in the <span class="No-Break">hidden layer.</span></li>
<li>Use a custom callback to stop training when the training accuracy <span class="No-Break">is 90%.</span></li>
<li>Try out the following learning rates: 5, 0.5, 0.01, 0.001. What did <span class="No-Break">you observe?</span></li>
</ol>
<h1 id="_idParaDest-124"><a id="_idTextAnchor145"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Amr, T., 2020. <em class="italic">Hands-On Machine Learning with scikit-learn and Scientific Python Toolkits</em>, <span class="No-Break">Packt Publishing.</span></li>
<li>Gulli, A., Kapoor, A. and Pal, S., 2019. <em class="italic">Deep Learning with TensorFlow 2 and Keras</em>, <span class="No-Break">Packt Publishing.</span></li>
<li><em class="italic">How to Write Custom TensorFlow Callbacks — The Easy </em><span class="No-Break"><em class="italic">Way</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c"><span class="No-Break">https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c</span></a></li>
<li><a href="https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3"><span class="No-Break">https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3</span></a></li>
<li><a href="https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3"><span class="No-Break">https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping</span></a></li>
</ul>
</div>
</div></body></html>