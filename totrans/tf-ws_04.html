<html><head></head><body>
		<div>
			<div id="_idContainer150" class="Content">
			</div>
		</div>
		<div id="_idContainer151" class="Content">
			<h1 id="_idParaDest-77"><a id="_idTextAnchor077"/>4. Regression and Classification Models</h1>
		</div>
		<div id="_idContainer167" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will learn how to build regression and classification models using TensorFlow. You will build models with TensorFlow utilizing Keras layers, which are a simple approach to model building that offer a high-level API for building and training models. You will create models to solve regression and classification tasks, including the classification of the binding properties of various molecules. You will also use TensorBoard to visualize the architecture of TensorFlow models and view the training process.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor078"/>Introduction</h1>
			<p>In the previous chapter, you learned how to use some TensorFlow resources to aid in development. These included TensorBoard (for visualizing computational graphs), TensorFlow Hub (an online repository for machine learning modules), and Google Colab (an online Python development environment for running code on Google servers). All these resources help machine learning practitioners develop models efficiently.</p>
			<p>In this chapter, you will explore how to create ANNs using TensorFlow. You will build ANNs with different architectures to solve regression and classification tasks. Regression tasks aim to predict continuous variables from the input training data, while classification tasks aim to classify the input data into two or more classes. For example, a model to predict whether or not it will rain on a given day is a classification task since the result of the model will be of two classes—rain or no rain. However, a model to predict the amount of rain on a given day would be an example of a regression task since the output of the model would be a continuous variable—the amount of rain.</p>
			<p>Models that are used to tackle these tasks represent a large class of machine learning models, and a huge amount of machine learning problems fall into these two categories. This chapter will demonstrate how regression and classification models can be created, trained, and evaluated in TensorFlow. You will use much of the learning covered in the previous chapters (including using TensorBoard to monitor the model training process) to understand how to build performant models.</p>
			<p>This chapter introduces the various parameters used to build ANNs (known as <strong class="bold">hyperparameters</strong>), which include activation functions, loss functions, and optimizers. Other hyperparameters to select in the model-fitting process include the number of epochs and batch size, which vary the number of times the entire dataset is used to update the weights and the number of data points for each update, respectively. You will also learn how to log variables during the model-fitting process so that they can be visualized in TensorBoard. This allows you to determine whether the model is under- or overfitting the training data. Finally, after building your model, you will learn how to evaluate it on the dataset to see how well it performs.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor079"/>Sequential Models</h1>
			<p>A sequential model is used to build regression and classification models. In sequential models, information propagates through the network from the input layer at the beginning to the output layer at the end. Layers are stacked in the model sequentially, with each layer having an input and an output.</p>
			<p>Other types of ANN models exist, such as recurrent neural networks (in which the output feeds back into the input), which will be covered in later chapters. The difference between sequential and recurrent neural networks is shown in <em class="italic">Figure 4.01</em>. In both the models, the information flows from the input layer through the hidden layers to the output layer, as indicated by the direction of the arrows. However, in recurrent architectures, the output of the hidden layers feeds back into the input of the hidden layers:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B16341_04_01.jpg" alt="Figure 4.1: The architectures of sequential and recurrent ANNs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: The architectures of sequential and recurrent ANNs</p>
			<p>In the following section, you will learn how to create sequential models in TensorFlow that form the basis of regression and classification models. You will utilize the Keras API, which is now included as part of the TensorFlow library for sequential models, since the high-level API provides a simple interface for creating these models. Using the API, you will find that adding more layers to a model is incredibly easy and is great for new practitioners learning the field.</p>
			<p>A sequential model can be initialized as follows:</p>
			<p class="source-code">model = tf.keras.Sequential()</p>
			<p>Once the model has been initialized, layers can be added to the model. In this section, you will also explore how to add Keras layers to the model.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor080"/>Keras Layers</h2>
			<p>Keras layers are included in the TensorFlow package. Keras layers are a collection of commonly used layers that can be added easily to your sequential models.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can check out all the possible options for Keras layers here: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">https://www.tensorflow.org/api_docs/python/tf/keras/layers</a>.</p>
			<p>To add layers to a model of the <strong class="source-inline">Sequential</strong> class, you can use the model's <strong class="source-inline">add</strong> method. One optional layer that can be added to the beginning of a sequential model is an <strong class="bold">input layer</strong> as an entry point to the network. Input layers can take the following common input arguments:</p>
			<ul>
				<li><strong class="source-inline">input_shape</strong> (required): The shape of the input tensor, not including the batch axis</li>
				<li><strong class="source-inline">batch_size</strong>: An optional argument indicating the input batch size</li>
				<li><strong class="source-inline">name</strong>: Optional name of the input layer</li>
			</ul>
			<p>Input layers can be added to a model as follows. The following code snippet is used to add a layer, expecting inputs to have eight features:</p>
			<p class="source-code">model.add(tf.keras.layers.InputLayer(input_shape=(8,), \</p>
			<p class="source-code">                                     name='Input_layer'))</p>
			<p>By providing a <strong class="source-inline">name</strong> argument, you can label the layers, which will be useful when visualizing the model in TensorBoard. Another type of layer that is commonly used when building regression and classification models is the <strong class="bold">dense layer</strong>. The dense layer is a fully connected layer, which means that all the nodes in the layer receive inputs from every node in the layer prior and then connect to every node of the next layer. A dense layer can be used as the first layer of the model with <strong class="source-inline">input_shape</strong> provided as an argument. The following are the common input arguments for layers of the <strong class="source-inline">Dense</strong> class:</p>
			<ul>
				<li><strong class="source-inline">units</strong> (required): This is a positive integer denoting the number of units in the layer.</li>
				<li><strong class="source-inline">input_shape</strong>: This is the shape of the input tensor but is not required unless it is the first layer of the model.</li>
				<li><strong class="source-inline">activation</strong>: This is an optional argument indicating which activation function to apply to the output of the layer.</li>
				<li><strong class="source-inline">use_bias</strong>: This is a Boolean argument indicating whether to use bias in the layer. The default is set to <strong class="source-inline">True</strong>.</li>
				<li><strong class="source-inline">name</strong>: This refers to the name of the layer. One will be generated if this argument is not provided.</li>
				<li><strong class="source-inline">kernel_initializer</strong>: This is the initializer for the kernel weights. The <strong class="bold">Glorot uniform initializer</strong>, which has a normal distribution centered on zero and a standard deviation that is dependent on the number of units in the layer, is used by default.</li>
				<li><strong class="source-inline">bias_initializer</strong>: This is the initializer for the bias. The default of this parameter is used to set the bias values to zero.</li>
				<li><strong class="source-inline">kernel_regularizer</strong>: This is the regularizer to use on the kernel weights. There are none applied by default.</li>
				<li><strong class="source-inline">bias_regularizer</strong>: This is the regularizer to use on the bias. There are none applied by default.</li>
			</ul>
			<p>The following is an example of adding a dense layer to a model with <strong class="source-inline">12</strong> units, adding a <strong class="source-inline">sigmoid</strong> activation function at the output of the layer, and naming the layer <strong class="source-inline">Dense_layer_1</strong>:</p>
			<p class="source-code">model.add(tf.keras.layers.Dense(units=12, name='Dense_layer_1', \</p>
			<p class="source-code">                                activation='sigmoid'))</p>
			<p>Now that you understand how to initialize sequential models and add layers to them, you will create a Keras sequential model using TensorFlow in the first exercise. You will initialize a model, add layers to the model, add activation functions to the output of the model, and pass data through the model to simulate creating a prediction.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/>Exercise 4.01: Creating an ANN with TensorFlow</h2>
			<p>In this exercise, you will create your first sequential ANN in TensorFlow. You will have an input layer, a hidden layer with four units and a ReLU activation function, and an output layer with one unit. Then, you will create some simulation data by generating random numbers and passing it through the model, using the model's <strong class="source-inline">predict</strong> method to simulate a prediction for each data example. </p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>Open a Jupyter notebook and import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Initialize a Keras model of the sequential class:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Add an input layer to the model using the model's <strong class="source-inline">add</strong> method, and add the <strong class="source-inline">input_shape</strong> argument with size <strong class="source-inline">(8,)</strong> to represent input data with eight features:<p class="source-code">model.add(tf.keras.layers.InputLayer(input_shape=(8,), \</p><p class="source-code">                                     name='Input_layer'))</p></li>
				<li>Add two layers of the <strong class="source-inline">Dense</strong> class to the model. The first will represent your hidden layer with four units and a ReLU activation function, and the second will represent your output layer with one unit:<p class="source-code">model.add(tf.keras.layers.Dense(4, activation='relu', \</p><p class="source-code">                                name='First_hidden_layer'))</p><p class="source-code">model.add(tf.keras.layers.Dense(1, name='Output_layer'))</p></li>
				<li>View the weights by calling the <strong class="source-inline">variables</strong> attribute of the model:<p class="source-code">model.variables</p><p>You should get the following output:</p><div id="_idContainer153" class="IMG---Figure"><img src="image/B16341_04_02.jpg" alt="Figure 4.2: The variables of the ANN&#13;&#10;"/></div><p class="figure-caption">Figure 4.2: The variables of the ANN</p><p>This output shows all the variables that compose the model; they include the values for all weights and biases in each layer.</p></li>
				<li>Create a tensor of size <strong class="source-inline">32x8</strong>, which represents a tensor with 32 records and 8 features:<p class="source-code">data = tf.random.normal((32,8))</p></li>
				<li>Call the <strong class="source-inline">predict</strong> method of the model and pass in the sample data:<p class="source-code">model.predict(data)</p><p class="source-code">prediction</p><p>You should get the following result:</p><div id="_idContainer154" class="IMG---Figure"><img src="image/B16341_04_03.jpg" alt="Figure 4.3: The output of the ANN after random inputs have been applied&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.3: The output of the ANN after random inputs have been applied</p>
			<p>Calling the <strong class="source-inline">predict()</strong> method on the sample data will propagate the data through the network. In each layer, there will be a matrix multiplication of the data with the weights, and the bias will be added before the data is passed as input data to the next layer. This process continues until the final output layer.</p>
			<p>In this exercise, you created a sequential model with multiple layers. You initialized a model, added an input layer to accept data with eight features, added a hidden layer with four units, and added an output layer with one unit. Before fitting a model to training data, you must first compile the model with an optimizer and choose a loss function to minimize the value it computes by updating weights in the training process. </p>
			<p>In the next section, you will explore how to compile models, then fit them to training data.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>Model Fitting</h1>
			<p>Once a model has been initialized and layers have been added to the ANN, the model must be configured with an optimizer, losses, and any evaluation metrics through the compilation process. A model can be compiled using the model's <strong class="source-inline">compile</strong> method, as follows:</p>
			<p class="source-code">model.compile(optimizer='adam', loss='binary_crossentropy', \</p>
			<p class="source-code">              metrics=['accuracy'])</p>
			<p>Optimizers can be chosen by simply naming the optimizer as the argument. The following optimizers are available as default for Keras models:</p>
			<ul>
				<li><strong class="bold">Stochastic gradient descent</strong> (<strong class="bold">SGD</strong>): This updates the weights for each example in the dataset. You can find more information about SGD here: <a href="https://keras.io/api/optimizers/sgd/">https://keras.io/api/optimizers/sgd/</a>.</li>
				<li><strong class="bold">RMSprop</strong>: This is an adaptive optimizer that varies the weights during training by using a decaying average of the gradients at each update. You can find more information about RMSprop here: <a href="https://keras.io/api/optimizers/rmsprop/">https://keras.io/api/optimizers/rmsprop/</a>.</li>
				<li><strong class="bold">Adam</strong>: This is also an adaptive optimizer that implements the Adam algorithm, updating the learning rates based on the first- and second-order gradients. You can find more information about Adam here: <a href="https://keras.io/api/optimizers/adam/">https://keras.io/api/optimizers/adam/</a>.</li>
				<li><strong class="bold">Adagrad</strong>: This adaptive gradient optimizer adapts the learning rate at each weight update. The learning rate is adapted for each feature using the prior gradients and observations. You can find more information about Adagrad here: <a href="https://keras.io/api/optimizers/adagrad/">https://keras.io/api/optimizers/adagrad/</a>.</li>
				<li><strong class="bold">Adadelta</strong>: This is a more robust version of Adagrad that uses a sliding window of gradient updates to adapt the learning rate. You can find more information about Adadelta here: <a href="https://keras.io/api/optimizers/adadelta/">https://keras.io/api/optimizers/adadelta/</a>.</li>
				<li><strong class="bold">Adamax</strong>: This is an adaptive optimizer that is a variant of the Adam optimizer. You can find more information about Adamax here: <a href="https://keras.io/api/optimizers/adamax/">https://keras.io/api/optimizers/adamax/</a>.</li>
				<li><strong class="bold">Nadam</strong>: This is another adaptive optimizer that is a variant of the Adam optimizer with Nesterov momentum. You can find more information about Nadam here: <a href="https://keras.io/api/optimizers/Nadam/">https://keras.io/api/optimizers/Nadam/</a>.</li>
				<li><strong class="bold">Ftrl</strong>: This is an optimizer that implements the FTRL algorithm. You can find more information about Ftrl here: <a href="https://keras.io/api/optimizers/ftrl/">https://keras.io/api/optimizers/ftrl/</a>.</li>
			</ul>
			<p>Custom optimizers can also be added to Keras models if the provided ones are not relevant. Selecting the most appropriate optimizer is often a matter of trying each and identifying which optimizer produces the lowest error. This process is known as <strong class="bold">hyperparameter tuning</strong> and will be covered in a later chapter. In the next section, you will uncover another option when compiling models: the loss function. The goal of training a model is to minimize the value computed by the loss function.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor083"/>The Loss Function</h2>
			<p>The loss function is the measure of error between the predicted results and the true results. You use the loss function during the training process to determine whether varying any of the weights and biases will create a better model by minimizing the loss function's value through the optimization process.</p>
			<p>There are many different types of loss functions that can be used, and the specific one will depend on the problem and goal. In general, regression and classification tasks will have different loss functions. Since regression models predict continuous variables, loss functions for regression models typically aim to summarize how far, on average, the predictions are from the true values. For classification models, loss functions aim to determine how the quantity of true positive, true negative, false positive, and false negative classifications of the predicted classes vary compared to the true classes. </p>
			<p><strong class="bold">True positives</strong> are defined as correct predictions labeled positive by the classifier; similarly, <strong class="bold">true negatives</strong> are correct predictions labeled negative. <strong class="bold">False positives</strong> are predictions labeled positive where the true value is negative, and <strong class="bold">false negatives</strong> are predictions labeled negative that are actually positive. Loss functions that are directly available to use in Keras sequential models for regression include the following:</p>
			<ul>
				<li><strong class="bold">Mean squared error</strong>: This is a loss function that calculates the squared difference between the true and predicted value for each data point, <strong class="source-inline">(true value – predicted value)^2</strong>, and returns the average across the entire dataset. This loss function is primarily used for regression problems, and the squaring of the difference between the two values ensures the loss function results in a positive number.</li>
				<li><strong class="bold">Mean absolute error</strong>: This is another loss function primarily used for regression problems that calculates the absolute value of the difference between the true and predicted value for each data point, <strong class="source-inline">|true value – predicted value|</strong>, and returns the average across the dataset. This method also ensures that the result is a positive value.</li>
				<li><strong class="bold">Mean absolute percentage error</strong>: This is another loss function used for regression problems that calculates the absolute value of the percentage error for each data point, <strong class="source-inline">|(true value– predicted value) / true value|</strong>, and returns the average across the dataset as a percentage.</li>
			</ul>
			<p>For classification, loss functions that are available include the following:</p>
			<ul>
				<li><strong class="bold">Binary cross-entropy</strong>: This is a loss function used for binary classification problems that outputs a value between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, with values closer to <strong class="source-inline">1</strong> representing a greater number of true positive classifications.</li>
				<li><strong class="bold">Categorical cross-entropy</strong>: This is a loss function similar to binary cross-entropy; however, it is suitable for multi-class classification problems and also outputs values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>.</li>
			</ul>
			<p>When compiling a model, other metrics can also be passed in as an argument to the method. They will be calculated after each epoch and saved during the training process. The metrics that are available to be calculated for Keras models include the following:</p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: This is the proportion of correct results out of the total results.</li>
				<li><strong class="bold">Precision</strong>: This is the proportion of true positives out of the total positives predicted.</li>
				<li><strong class="bold">Recall</strong>: This is the proportion of true positives out of the actual positives.</li>
				<li><strong class="bold">AUC</strong>: This metric represents the area under the ROC curve.</li>
			</ul>
			<p>These metrics can be incredibly valuable in understanding the performance of the model during the training process. All the metrics have values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, with higher values representing better performance. Once the model has been compiled, it can be fit to the training data. This can be accomplished by calling the <strong class="source-inline">fit</strong> method and passing in the following arguments:</p>
			<ul>
				<li><strong class="source-inline">x</strong>: This is the feature data as a TensorFlow tensor or NumPy array.</li>
				<li><strong class="source-inline">y</strong>: This is the target data as a TensorFlow tensor or NumPy array.</li>
				<li><strong class="source-inline">epochs</strong>: This refers to the number of epochs to run the model for. An epoch is an iteration over the entire training dataset.</li>
				<li><strong class="source-inline">batch_size</strong>: This is the number of training data samples to use per gradient update.</li>
				<li><strong class="source-inline">validation_split</strong>: This is the proportion of the training data to be used for validation that is evaluated after each epoch. This proportion of data is not used in the weight update process.</li>
				<li><strong class="source-inline">shuffle</strong>: This indicates whether to shuffle the training data before each epoch.</li>
			</ul>
			<p>To fit the model to the training data, the <strong class="source-inline">fit</strong> method can be applied to a model in the following way:</p>
			<p class="source-code">model.fit(x=features, y=target, epochs=10, batch_size=32, \</p>
			<p class="source-code">         validation_split=0.2, shuffle=False)</p>
			<p>Once the <strong class="source-inline">fit</strong> method has been called, the model will begin fitting to the training data. After each epoch, the loss is returned for the training. If a validation split is defined, then the loss is also evaluated on the validation split.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor084"/>Model Evaluation</h2>
			<p>Once models are trained, they can be evaluated by utilizing the model's <strong class="source-inline">evaluate</strong> method. The <strong class="source-inline">evaluate</strong> method assesses the performance of the model according to the loss function used to train the model and any metrics that were passed to the model. The method is best used when determining how the model will perform on new, unseen data by passing in a feature and target dataset that has not been used in the training process or out-of-sample dataset. The method can be called as follows:</p>
			<p class="source-code">eval_metrics = model.evaluate(features, target)</p>
			<p>The result of the method is first the loss calculated on the input data, and then, if any metrics were passed in the model compilation process, they will also be calculated when the <strong class="source-inline">evaluate</strong> method is executed. Model evaluation is an important step in determining how well your model is performing. Since there is an enormous number of hyperparameters (such as the number of hidden layers, the number of units in each layer, and the choice of activation functions, to name a few), model evaluation is necessary to determine which combination of hyperparameters is optimal. Effective model evaluation can help provide an unbiased view on which model architecture will perform best overall.</p>
			<p>In the following exercise, you will undertake the process of creating an ANN, compiling the model, fitting the model to training data, and finally, evaluating the model on the training data. You will recreate the linear regression algorithm with an ANN, which can be interpreted as an ANN with only one layer and one unit. Furthermore, you will view the architecture of the model and model training process in TensorBoard.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor085"/>Exercise 4.02: Creating a Linear Regression Model as an ANN with TensorFlow</h2>
			<p>In this exercise, you will create a linear regression model as an ANN using TensorFlow. The dataset, <strong class="source-inline">Bias_correction_ucl.csv</strong>, describes the bias correction of air temperature forecasts of Seoul, South Korea. The fields represent temperature measurements of the given date, the weather station at which the metrics were measured, model forecasts of weather-related metrics such as humidity, and projections for the temperature the following day. You are required to predict the next maximum and minimum temperature given measurements of the prior timepoints and attributes of the weather station.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">Bias_correction_ucl.csv</strong> file can be found here: <a href="https://packt.link/khfeF">https://packt.link/khfeF</a>.</p>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this exercise. </li>
				<li>In a new Jupyter Notebook cell, import the TensorFlow and pandas libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p></li>
				<li>Load in the dataset using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code">df = pd.read_csv(<strong class="bold">'Bias_correction_ucl.csv'</strong>)</p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the CSV file based on its location on your system. If you're running the Jupyter notebook from the same directory where the CSV file is stored, you can run the preceding code without any modification.   </p></li>
				<li>Drop the <strong class="source-inline">date</strong> column and drop any rows that have null values since your model requires numerical values only:<p class="source-code">df.drop('Date', inplace=True, axis=1)</p><p class="source-code">df.dropna(inplace=True)</p></li>
				<li>Create target and feature datasets. The target dataset will contain the columns named <strong class="source-inline">Next_Tmax</strong> and <strong class="source-inline">Next_Tmin</strong>, while the feature dataset will contain all columns except those named <strong class="source-inline">Next_Tmax</strong> and <strong class="source-inline">Next_Tmin</strong>:<p class="source-code">target = df[['Next_Tmax', 'Next_Tmin']]</p><p class="source-code">features = df.drop(['Next_Tmax', 'Next_Tmin'], axis=1)</p></li>
				<li>Rescale the feature dataset:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">feature_array = scaler.fit_transform(features)</p><p class="source-code">features = pd.DataFrame(feature_array, columns=features.columns)</p></li>
				<li>Initialize a Keras model of the <strong class="source-inline">Sequential</strong> class:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Add an input layer to the model using the model's <strong class="source-inline">add</strong> method, and set <strong class="source-inline">input_shape</strong> to be the number of columns in the feature dataset:<p class="source-code">model.add(tf.keras.layers.InputLayer\</p><p class="source-code">         (input_shape=(features.shape[1],), \</p><p class="source-code">                       name='Input_layer'))</p></li>
				<li>Add the output layer of the <strong class="source-inline">Dense</strong> class to the model with a size of <strong class="source-inline">2</strong>, representing the two target variables:<p class="source-code">model.add(tf.keras.layers.Dense(2, name='Output_layer'))</p></li>
				<li>Compile the model with an RMSprop optimizer and a mean squared error loss:<p class="source-code">model.compile(tf.optimizers.RMSprop(0.001), loss='mse')</p></li>
				<li>Add a callback for TensorBoard:<p class="source-code">tensorboard_callback = tf.keras.callbacks\</p><p class="source-code">                         .TensorBoard(log_dir="./logs")</p></li>
				<li>Fit the model to the training data:<p class="source-code">model.fit(x=features.to_numpy(), y=target.to_numpy(),\</p><p class="source-code">          epochs=50, callbacks=[tensorboard_callback])</p><p>You should get the following output:</p><div id="_idContainer155" class="IMG---Figure"><img src="image/B16341_04_04.jpg" alt="Figure 4.4: The output of the fitting process showing the epoch, train time per sample, and loss after each epoch&#13;&#10;"/></div><p class="figure-caption">Figure 4.4: The output of the fitting process showing the epoch, train time per sample, and loss after each epoch</p></li>
				<li>Evaluate the model on the training data:<p class="source-code">loss = model.evaluate(features.to_numpy(), target.to_numpy())</p><p class="source-code">print('loss:', loss)</p><p>This results in the following output:</p><p class="source-code">loss: 3.5468221449764012</p></li>
				<li>View the model architecture and model-fitting process on TensorBoard by calling the following on the command line:<p class="source-code">tensorboard –-logdir=logs/</p><p>You can see its execution in a web browser by visiting the URL that is provided after launching TensorBoard. The default URL provided is <strong class="source-inline">http://localhost:6006/</strong>:</p><div id="_idContainer156" class="IMG---Figure"><img src="image/B16341_04_05.jpg" alt="Figure 4.5: A visual representation of the model architecture in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.5: A visual representation of the model architecture in TensorBoard</p>
			<p>The loss function can be visualized as shown in the following figure:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B16341_04_06.jpg" alt="Figure 4.6: A visual representation of the loss as a function of an epoch in TensorBoard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6: A visual representation of the loss as a function of an epoch in TensorBoard</p>
			<p>You can see the architecture of the model in the <strong class="source-inline">GRAPHS</strong> tab. The architecture shows the input layer and output layer in the model, as well as the calculated loss. During the model-fitting process, the loss is calculated after each epoch and is displayed in TensorBoard in the <strong class="source-inline">SCALARS</strong> tab. The loss is that which is defined in the compilation process; so, in this case, the loss is the mean squared error. From TensorBoard, you can see that the mean squared error reduces after each epoch, indicating that the model is learning from the training data, updating the weights in order to reduce the total loss.</p>
			<p>In this exercise, you have learned how to create, train, and evaluate an ANN with TensorFlow by using Keras layers. You recreated the linear regression algorithm by creating an ANN with an input layer and an output layer that has one unit for each output. Here, there were two outputs representing the maximum and minimum values of the temperature; thus, the output layer has two units.</p>
			<p>In <em class="italic">Exercise 4.01</em>, <em class="italic">Creating an ANN with TensorFlow</em>, you created an ANN with only one layer containing weights and the output layer. This is an example of a <strong class="bold">shallow neural network</strong>. ANNs that have many hidden layers containing weights are called <strong class="bold">deep neural networks</strong>, and the process of training them is called <strong class="bold">deep learning</strong>. By increasing the number of layers and making the ANN deeper, the model becomes more flexible and will be able to model more complex functions. However, to gain this increase in flexibility, you need more training data and more computation power to train the model. </p>
			<p>In the next exercise, you will create and train ANNs that have multiple hidden layers.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor086"/>Exercise 4.03: Creating a Multi-Layer ANN with TensorFlow</h2>
			<p>In this exercise, you will create a multi-layer ANN using TensorFlow. This model will have four hidden layers. You will add multiple layers to the model and activation functions to the output of the layers. The first hidden layer will have <strong class="source-inline">16</strong> units, the second will have <strong class="source-inline">8</strong> units, and the third will have <strong class="source-inline">4</strong> units. The output layer will have <strong class="source-inline">2</strong> units. You will utilize the same dataset as in <em class="italic">Exercise 4.02</em>, <em class="italic">Creating a Linear Regression Model as an ANN with TensorFlow</em>, which describes the bias correction of air temperature forecasts for Seoul, South Korea. The exercise aims to predict the next maximum and minimum temperature given measurements of the prior timepoints and attributes of the weather station.</p>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this exercise. </li>
				<li>In a new Jupyter Notebook cell, import the TensorFlow and pandas libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p></li>
				<li>Load in the dataset using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code">df = pd.read_csv(<strong class="bold">'Bias_correction_ucl.csv'</strong>)</p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the CSV file based on its location on your system. If you're running the Jupyter notebook from the same directory where the CSV file is stored, you can run the preceding code without any modification.   </p></li>
				<li>Drop the <strong class="source-inline">Date</strong> column and drop any rows that have null values:<p class="source-code">df.drop('Date', inplace=True, axis=1)</p><p class="source-code">df.dropna(inplace=True)</p></li>
				<li>Create target and feature datasets:<p class="source-code">target = df[['Next_Tmax', 'Next_Tmin']]</p><p class="source-code">features = df.drop(['Next_Tmax', 'Next_Tmin'], axis=1)</p></li>
				<li>Rescale the feature dataset:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">feature_array = scaler.fit_transform(features)</p><p class="source-code">features = pd.DataFrame(feature_array, columns=features.columns)</p></li>
				<li>Initialize a Keras model of the <strong class="source-inline">Sequential</strong> class:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Add an input layer to the model using the model's <strong class="source-inline">add</strong> method, and set <strong class="source-inline">input_shape</strong> to the number of columns in the feature dataset:<p class="source-code">model.add(tf.keras.layers.InputLayer\</p><p class="source-code">                         (input_shape=(features.shape[1],), \</p><p class="source-code">                          name='Input_layer'))</p></li>
				<li>Add three hidden layers and an output layer of the <strong class="source-inline">Dense</strong> class to the model. The first hidden layer will have <strong class="source-inline">16</strong> units, the second will have <strong class="source-inline">8</strong> units, and the third will have <strong class="source-inline">4</strong> units. Label the layers appropriately. The output layer will have two units to match the target variable that has two columns:<p class="source-code">model.add(tf.keras.layers.Dense(16, name='Dense_layer_1'))</p><p class="source-code">model.add(tf.keras.layers.Dense(8, name='Dense_layer_2'))</p><p class="source-code">model.add(tf.keras.layers.Dense(4, name='Dense_layer_3'))</p><p class="source-code">model.add(tf.keras.layers.Dense(2, name='Output_layer'))</p></li>
				<li>Compile the model with an RMSprop optimizer and mean squared error loss:<p class="source-code">model.compile(tf.optimizers.RMSprop(0.001), loss='mse')</p></li>
				<li>Add a callback for TensorBoard:<p class="source-code">tensorboard_callback = tf.keras.callbacks\</p><p class="source-code">                         .TensorBoard(log_dir="./logs")</p></li>
				<li>Fit the model to the training data for <strong class="source-inline">50</strong> epochs and add a validation split equal to 20%:<p class="source-code">model.fit(x=features.to_numpy(), y=target.to_numpy(),\</p><p class="source-code">          epochs=50, callbacks=[tensorboard_callback] , \</p><p class="source-code">          validation_split=0.2)</p><p>You should get the following output:</p><div id="_idContainer158" class="IMG---Figure"><img src="image/B16341_04_07.jpg" alt="Figure 4.7: The output of the fitting process showing the epoch, training time per sample, and loss after each epoch"/></div><p class="figure-caption">Figure 4.7: The output of the fitting process showing the epoch, training time per sample, and loss after each epoch</p></li>
				<li>Evaluate the model on the training data:<p class="source-code">loss = model.evaluate(features.to_numpy(), target.to_numpy())</p><p class="source-code">print('loss:', loss)</p><p>This will display the following result:</p><p class="source-code">loss: 1.664448248190068</p></li>
				<li>View the model architecture and model-fitting process in TensorBoard:<p class="source-code">tensorboard --logdir=logs/</p><p>You should get something like the following:</p><div id="_idContainer159" class="IMG---Figure"><img src="image/B16341_04_08.jpg" alt="Figure 4.8: A visual representation of the model architecture in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.8: A visual representation of the model architecture in TensorBoard</p>
			<p>You can visualize the loss function as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B16341_04_09.jpg" alt="Figure 4.9: A visual representation of the loss as a function of an epoch in TensorBoard &#13;&#10;on the training and validation split&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: A visual representation of the loss as a function of an epoch in TensorBoard on the training and validation split</p>
			<p>The network architecture shows the input layer and the four hidden layers of the model as well as the calculated loss at the end. During the model-fitting process, the loss is calculated after each epoch and is displayed in TensorBoard in the <strong class="source-inline">SCALARS</strong> tab. Here, the loss is the mean squared error. From TensorBoard, you can see that the mean squared error reduces on the training set (the orange line) and the validation set (the blue line), after each epoch, indicating that the model is learning effectively from the training data.</p>
			<p>In this exercise, you have created an ANN with multiple hidden layers. The loss you obtained was lower than that achieved using linear regression, which demonstrates the power of ANNs. With some tuning to the hyperparameters (such as varying the number of layers, the number of units within each layer, adding activation functions, and changing the loss and optimizer), the loss could be even lower. In the next activity, you will put your model-building skills into action on a new dataset.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor087"/>Activity 4.01: Creating a Multi-Layer ANN with TensorFlow</h2>
			<p>The feature dataset, <strong class="source-inline">superconductivity.csv</strong>, contains the properties of superconductors including the atomic mass of the material and its density. Importantly, the dataset also contains the critical temperature of the material, which is the temperature at which the material exhibits superconductive properties. In this activity, you are tasked with finding the critical temperature of the material or the temperature at which the material gains superconductive properties.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superconductivity.csv</strong> file can be found here: <a href="https://packt.link/sOCPh">https://packt.link/sOCPh</a>.</p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this activity.</li>
				<li>Import the TensorFlow and pandas libraries.</li>
				<li>Load in the <strong class="source-inline">superconductivity.csv</strong> dataset.</li>
				<li>Drop any rows that have null values.</li>
				<li>Set the target as the <strong class="source-inline">critical_temp</strong> column and the feature dataset as the remaining columns.</li>
				<li>Rescale the feature dataset using a standard scaler.</li>
				<li>Initialize a model of the Keras <strong class="source-inline">Sequential</strong> class.</li>
				<li>Add an input layer, four hidden layers of sizes <strong class="source-inline">64</strong>, <strong class="source-inline">32</strong>, <strong class="source-inline">16</strong>, and <strong class="source-inline">8</strong>, and an output layer of size <strong class="source-inline">1</strong> to the model. Add a ReLU activation function to the first hidden layer.</li>
				<li>Compile the model with an RMSprop optimizer with a learning rate equal to <strong class="source-inline">0.001</strong> and the mean squared error for the loss.</li>
				<li>Add a callback to write logs to TensorBoard.</li>
				<li>Fit the model to the training data for <strong class="source-inline">100</strong> epochs, with a batch size equal to <strong class="source-inline">32</strong> and a validation split equal to 20%.</li>
				<li>Evaluate the model on the training data.</li>
				<li>View the model architecture in TensorBoard.<p>You should get an output like the following:</p><div id="_idContainer161" class="IMG---Figure"><img src="image/B16341_04_10.jpg" alt="Figure 4.10: A visual representation of the model architecture in TensorBoard&#13;&#10;"/></div><p class="figure-caption">Figure 4.10: A visual representation of the model architecture in TensorBoard</p></li>
				<li>Visualize the model-fitting process in TensorBoard. You should get the following output:<div id="_idContainer162" class="IMG---Figure"><img src="image/B16341_04_11.jpg" alt="Figure 4.11: A visual representation of the loss as a function of an epoch on the training and validation split in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.11: A visual representation of the loss as a function of an epoch on the training and validation split in TensorBoard</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor262">this link</a>.</p>
			<p>In the next section, you will explore classification models, which attempt to classify data into distinct classes. You will begin with binary classification models that classify data into just two classes. This is the simplest form of a classification model. Once binary classifiers are mastered, more complicated models can be tackled, such as multi-label and multi-class classification.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor088"/>Classification Models</h1>
			<p>The goal of classification models is to classify data into distinct classes. For example, a spam filter is a classification model that aims to classify emails into "spam" (referring to unsolicited and unwanted email) or "ham" (a legitimate email). Spam filters are an example of a binary classifier since there are two classes. The input to the filter may include the content of the email, the email address of the sender, and the subject line, among other features, and the output will be the predicted class, <strong class="source-inline">spam</strong> or <strong class="source-inline">ham</strong>. Classification models can classify data into more than two distinct classes (known as <strong class="bold">multi-class classification</strong>) or classify data with multiple positive labels (known as <strong class="bold">multi-label classification</strong>).</p>
			<p>There are several different algorithms that can be used for classification tasks. Some popular ones include logistic regression, decision trees, and ANNs. ANNs are a great choice for classification models since they can learn complex relationships between the features and the target, and results can be achieved with the appropriate activation function on the output layer of the ANN.</p>
			<p>A common activation function to use for classification models is the sigmoid function, which is the same function used in logistic regression. In fact, a logistic regression model can be created by building an ANN with a single layer with one unit and a sigmoid activation function. The sigmoid function is a transformation in which the input is any real value, and the output is a number strictly between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. A visual representation is shown in the following figure.</p>
			<p>The output of the sigmoid transformation can be interpreted as a probability of a value being in the positive class; a value closer to a value of <strong class="source-inline">1</strong> indicates a higher probability of being in the positive class:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B16341_04_12.jpg" alt="Figure 4.12: A visual representation of the sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12: A visual representation of the sigmoid function</p>
			<p>After the sigmoid function has been applied, a threshold is applied, above which the data is classified as the positive class and below as the negative class. The default threshold for a sigmoid function is <strong class="source-inline">0.5</strong>, meaning that any value at or above <strong class="source-inline">0.5</strong> is classified as positive.</p>
			<p>In the next exercise, you will create a logistic regression model with TensorFlow. You will achieve this by creating a single-layer ANN, the process of which is similar to that of the linear regression model in <em class="italic">Exercise 4.02</em>, <em class="italic">Creating a Linear Regression Model as an ANN with TensorFlow</em>. The difference is that you will add a sigmoid activation function to the output of the ANN. Another difference that separates the two exercises is the loss function that you will use to calculate the loss.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor089"/>Exercise 4.04: Creating a Logistic Regression Model as an ANN with TensorFlow</h2>
			<p>In this exercise, you will create a logistic regression model as an ANN using TensorFlow. The dataset, <strong class="source-inline">qsar_androgen_receptor.csv</strong>, is used to develop classification models for the discrimination of binder/non-binder molecules given various attributes of the molecules. Here, the molecule attributes represent the features of your dataset, and their binding properties represent the target variable, in which a positive value represents a binding molecule, and a negative value represents a non-binding molecule. You will create a logistic regression model to predict the binding properties of the molecule given attributes of the molecule provided in the dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">qsar_androgen_receptor.csv</strong> file can be found here: <a href="https://packt.link/hWvjc">https://packt.link/hWvjc</a>.</p>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this exercise.</li>
				<li>Import the TensorFlow and pandas libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p></li>
				<li>Load in the dataset using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code">df = pd.read_csv(<strong class="bold">'qsar_androgen_receptor.csv'</strong>, \</p><p class="source-code">                 sep=';')</p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the CSV file based on its location on your system. If you're running the Jupyter notebook from the same directory where the CSV file is stored, you can run the preceding code without any modification.   </p></li>
				<li>Drop any rows that have null values:<p class="source-code">df.dropna(inplace=True)</p></li>
				<li>Create target and feature datasets:<p class="source-code">target = df['positive'].apply(lambda x: 1 if x=='positive' else 0)</p><p class="source-code">features = df.drop('positive', axis=1)</p></li>
				<li>Initialize a Keras model of the <strong class="source-inline">Sequential</strong> class:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Add an input layer to the model using the model's <strong class="source-inline">add</strong> method and set <strong class="source-inline">input_shape</strong> to be the number of columns in the feature dataset:<p class="source-code">model.add(tf.keras.layers.InputLayer\</p><p class="source-code">         (input_shape=(features.shape[1],), \</p><p class="source-code">                       name='Input_layer'))</p></li>
				<li>Add the output layer of the <strong class="source-inline">Dense</strong> class to the model with a size of <strong class="source-inline">1</strong>, representing the target variable:<p class="source-code">model.add(tf.keras.layers.Dense(1, name='Output_layer', \</p><p class="source-code">                                activation='sigmoid'))</p></li>
				<li>Compile the model with an RMSprop optimizer and binary cross-entropy for the loss, and compute the accuracy:<p class="source-code">model.compile(tf.optimizers.RMSprop(0.0001), \</p><p class="source-code">              loss='binary_crossentropy', metrics=['accuracy'])</p></li>
				<li>Create a TensorBoard callback:<p class="source-code">tensorboard_callback = tf.keras.callbacks.TensorBoard\</p><p class="source-code">                       (log_dir="./logs")</p></li>
				<li>Fit the model to the training data for <strong class="source-inline">50</strong> epochs, adding the TensorBoard callback with a validation split of 20%:<p class="source-code">model.fit(x=features.to_numpy(), y=target.to_numpy(), \</p><p class="source-code">         epochs=50, callbacks=[tensorboard_callback] , \</p><p class="source-code">         validation_split=0.2)</p><p>Your output should be similar to the following figure:</p><div id="_idContainer164" class="IMG---Figure"><img src="image/B16341_04_13.jpg" alt="Figure 4.13: The output of the fitting process showing the epoch, training time per sample, and loss after each epoch&#13;&#10;"/></div><p class="figure-caption">Figure 4.13: The output of the fitting process showing the epoch, training time per sample, and loss after each epoch</p></li>
				<li>Evaluate the model on the training data:<p class="source-code">loss, accuracy = model.evaluate(features.to_numpy(), \</p><p class="source-code">                                target.to_numpy())</p><p class="source-code">print(f'loss: {loss}, accuracy: {accuracy}')</p><p>You should get output something like the following:</p><p class="source-code">loss: 0.2781583094794838, accuracy: 0.9110320210456848</p></li>
				<li>Visualize the model-fitting process in TensorBoard by calling the following command on the command line:<p class="source-code">tensorboard --logdir=logs/</p><p>You should get a screen similar to the following in the browser:</p><div id="_idContainer165" class="IMG---Figure"><img src="image/B16341_04_14.jpg" alt="Figure 4.14: A visual representation of the model architecture in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.14: A visual representation of the model architecture in TensorBoard</p>
			<p>The loss function can be represented as follows:</p>
			<p>	</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B16341_04_15.jpg" alt="Figure 4.15: A visual representation of the loss and accuracy as a function of an epoch evaluated on the training and validation split in TensorBoard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15: A <a id="_idTextAnchor090"/>visual representation of the loss and accuracy as a function of an epoch evaluated on the training and validation split in TensorBoard</p>
			<p>You can see from TensorBoard that, with the addition of the <strong class="source-inline">metrics</strong> argument that was added in the model compilation process, there is an additional node in the architecture for the calculation of the accuracy metric. There is also an additional chart in the <strong class="source-inline">SCALARS</strong> tab showing the accuracy metric as a function of the epoch for the training and validation split.</p>
			<p>You can see from the charts that, for the training set, the accuracy increases, and the loss decreases over time, which is a positive indication that the model is learning. However, on the validation split, the accuracy begins to decrease, and the loss begins to increase, which is a sign that the model may be overfitting to the training data.</p>
			<p>In this exercise, you have learned how to build a classification model to discriminate between the binding properties of various molecules based on their other molecular attributes. The classification model was equivalent to a logistic regression model since it had only one layer and was preceded by a sigmoid activation function. With only one layer, there is a weight for each input feature and a single value for the bias. The sigmoid activation function transforms the output of the layer into a value between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, which is then rounded to represent your two classes. <strong class="source-inline">0.5</strong> and above represents one class, the molecule with binding properties, and below <strong class="source-inline">0.5</strong> represents the other class, molecules with non-binding properties.</p>
			<p>The next activity will summarize your learning in this chapter by combining your knowledge of creating multi-layer ANNs as you accomplished in <em class="italic">Exercise 4.03</em>, <em class="italic">Creating a Multi-Layer ANN with TensorFlow</em>, and <em class="italic">Activity 4.01</em>, <em class="italic">Creating a Multi-Layer ANN with TensorFlow</em>, with your knowledge of creating classification models from <em class="italic">Exercise 4.04</em>, <em class="italic">Creating a Logistic Regression Model as an ANN with TensorFlow</em>. You will use the same dataset as in the preceding activity but change the target variable to make it more suitable for a classification task.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor091"/>Activity 4.02: Creating a Multi-Layer Classification ANN with TensorFlow</h2>
			<p>The feature dataset, <strong class="source-inline">superconductivity.csv</strong>, contains the properties of superconductors including the atomic mass of the material and its density. Importantly, the dataset also contains the critical temperature of the material, which is the temperature at which the material exhibits superconductive properties. You are required to determine which superconductors will express superconductive properties above the boiling point of nitrogen (77.36 K), thereby allowing superconductivity using liquid nitrogen, which is readily available. Your target variable will have a <strong class="source-inline">true</strong> value when the critical temperature is above 77.36 K and <strong class="source-inline">false</strong> below, indicating whether the material expresses superconductive properties above the boiling point of nitrogen.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superconductivity.csv</strong> file can be found here: <a href="http://packt.link/sOCPh">http://packt.link/sOCPh</a>.</p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Open a Jupyter notebook to complete the activity.</li>
				<li>Import the TensorFlow and pandas libraries.</li>
				<li>Load in the <strong class="source-inline">superconductivity.csv</strong> dataset.</li>
				<li>Drop any rows that have null values.</li>
				<li>Set the target values to <strong class="source-inline">true</strong> when values of the <strong class="source-inline">critical_temp</strong> column are above <strong class="source-inline">77.36</strong> and <strong class="source-inline">false</strong> when below. The feature dataset is the remaining columns in the dataset.</li>
				<li>Rescale the feature dataset using a standard scaler.</li>
				<li>Initialize a model of the Keras <strong class="source-inline">Sequential</strong> class.</li>
				<li>Add an input layer, three hidden layers of sizes <strong class="source-inline">32</strong>, <strong class="source-inline">16</strong>, and <strong class="source-inline">8</strong>, and an output layer with a <strong class="source-inline">sigmoid</strong> activation function of size <strong class="source-inline">1</strong> to the model.</li>
				<li>Compile the model with an RMSprop optimizer with a learning rate equal to <strong class="source-inline">0.0001</strong> and binary cross-entropy for the loss and compute the accuracy metric.</li>
				<li>Add a callback to write logs to TensorBoard.</li>
				<li>Fit the model to the training data for <strong class="source-inline">50</strong> epochs and a validation split equal to 0%.</li>
				<li>Evaluate the model on the training data.</li>
				<li>View the model architecture and model-fitting process in TensorBoard.<p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor263">this link</a>.</p></li>
			</ol>
			<p>In this section, you have begun your foray into building, training, and evaluating classification models using TensorFlow. You have seen that they are built in much the same way as ANNs for regression tasks with the primary difference being the activation function on the output layer.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, you began your journey into creating ANNs in TensorFlow. You saw how simple it is to create regression and classification models by utilizing Keras layers. Keras layers are distinct classes that exist in a separate library that uses TensorFlow in the backend. Due to their popularity and ease of use, they are now included in TensorFlow and can be called in the same way as any other TensorFlow class.</p>
			<p>You created ANNs with fully connected layers, varying layers, beginning with an ANN that resembles a linear regression algorithm, which is equivalent to a single-layer ANN. Then, you added layers to your ANN and added activation functions to the output of the layers. Activation functions can be used to determine whether a unit is fired or can be used to bind the value of the output from a given unit. Regression models aim to predict a continuous variable from the data provided. In the exercises and activities throughout this chapter, you attempted to predict the temperature in Seoul given data from weather stations, and the critical temperature of superconducting materials given various material properties.</p>
			<p>Finally, you explored classification models, which aim to classify data into distinct classes. These models are similar to regression models in the way they are set up; however, an activation is used on the final output to bind the output values between two numbers that represent whether or not the data point is classified into the class. You began with binary classification models, which aim to classify the data into two classes, and demonstrated the concept of binary classification with an exercise in which you classified molecules into classes that represent their binding properties based on other attributes of the molecules' properties.</p>
			<p>In the next chapter, you will explore classification models in more depth. You will learn some of the intricacies and capabilities of classification models, including how to classify data that has more than two distinct classes (known as multi-class classification), and whether data points can have more than one positive label (known as multi-label classification). You will address how to structure the architecture to create these models, the appropriate loss functions to use when training, and the relevant metrics to calculate to understand whether models are performing well.</p>
		</div>
		<div>
			<div id="_idContainer168" class="Content">
			</div>
		</div>
	</body></html>