<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Training on Complex and Scarce Datasets</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>moData is th</span>e lifeblood of deep learnin<span>g applications. As such, training data should be able to flow unobstructed into networks, and it should contain all the meaningful information that is essential to prepare the methods for their tasks. Oftentimes, however, datasets can have complex structures or be stored on heterogeneous devices, complicating the process of efficiently feeding their content to the models. In other cases, relevant training images or annotations can be unavailable, depriving models of the information they need to learn.</span></p>
<p>Thankfully, for the former cases, TensorFlow provides a rich framework to set up optimized data pipelines—<kbd>tf.data</kbd>. For the latter cases, researchers have been proposing multiple alternatives when relevant training data is scarce—data augmentation, generation of synthetic datasets, domain adaptation, and more. These alternatives will also give us the opportunity to elaborate on generative models, such as <strong>variational autoencoders</strong> (<strong>VAEs</strong>) and <strong>generative adversarial networks</strong> (<strong>GANs</strong>).</p>
<p>The following topics will thus be covered in this chapter:</p>
<ul>
<li><span>How to build efficient input pipelines with <kbd>tf.data</kbd>, extracting and processing samples of all kinds</span></li>
<li>How to augment and render images to compensate for training data scarcity</li>
<li>What domain adaptation methods are, and how they can help train more robust models</li>
<li>How to create novel images with generative models such as VAEs and GANs</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Once again, several Jupyter notebooks and related source files to illustrate the chapter can be found in the Git repository dedicated to this book: <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter07">https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter07</a><span>.</span></p>
<p><span>Some additional Python packages are required for the notebook, demonstrating how to render synthetic images from 3D models, such as <kbd>vispy</kbd> (<a href="http://vispy.org">http://vispy.org</a>) <a href="http://vispy.org">and <kbd>plyfile</kbd> (</a><a href="https://github.com/dranjan/python-plyfile">https://github.com/dranjan/python-plyfile</a><a href="http://vispy.org">). Installation instructions are provided in the notebook itself.</a></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Efficient data serving</h1>
                </header>
            
            <article>
                
<p>Well-defined input pipelines cannot only greatly reduce the time needed to train models, but also help to better preprocess the training samples to guide the networks toward more performant configurations. In this section, we will demonstrate how to build such optimized pipelines, diving into the TensorFlow <kbd>tf.data</kbd> API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing the TensorFlow Data API</h1>
                </header>
            
            <article>
                
<p>While <kbd>tf.data</kbd> has already appeared multiple times in the Jupyter notebooks, we have yet to properly introduce this API and its multiple facets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intuition behind the TensorFlow Data API</h1>
                </header>
            
            <article>
                
<p>Before covering <kbd>tf.data</kbd>, we will provide some context to justify its relevance to the training of deep learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feeding fast and data-hungry models</h1>
                </header>
            
            <article>
                
<p><strong>Neural networks</strong> (<strong>NNs</strong>) are <em>data-hungry</em> models. The larger the datasets they can iterate on during training, the more accurate and robust these neural networks will become. As we have already noticed in our experiments, training a network is thus a heavy task, which can take hours, if not days.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As GPU/TPU hardware architectures are becoming more and more performant, the time needed to feed forward and backpropagate for each training iteration keeps decreasing (for those who can afford these devices). The speed is such nowadays that NNs tend to <em>consume</em> training batches faster than typical input pipelines can <em>produce</em> them. This is especially true in computer vision. Image datasets are commonly too heavy to be entirely preprocessed, and reading/decoding image files on the fly can cause significant delays (especially when repeated millions of times per training).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inspiration from lazy structures</h1>
                </header>
            
            <article>
                
<p>More generally, with the rise of <em>big data</em> some years ago, plenty of literature, frameworks, best practices, and more have appeared, offering new solutions to the processing and serving of huge amounts of data for all kinds of applications. The <kbd>tf.data</kbd> API was built by TensorFlow developers with those frameworks and practices in mind, in order to provide <em>a clear and efficient framework to feed data to neural networks</em>. More precisely, the goal of this API is to define input pipelines that are able to <em>deliver data for the next step before the current step has finished</em> (refer to the official API guide, <a href="https://www.tensorflow.org/guide/performance/datasets">https://www.tensorflow.org/guide/performance/datasets</a>).</p>
<p>As explained in several online presentations by Derek Murray, one of the Google experts working on TensorFlow (one of his presentations was video recorded and is available at <a href="https://www.youtube.com/watch?v=uIcqeP7MFH0">https://www.youtube.com/watch?v=uIcqeP7MFH0</a>), pipelines built with the <kbd>tf.data</kbd> API are comparable to <em>lazy lists</em> in functional languages. They can iterate over huge or infinite datasets batch by batch in a call-by-need fashion (<em>infinite</em>, for instance, when new data samples are generated on the fly). They provide operations such as <kbd>map()</kbd>, <kbd>reduce()</kbd>, <kbd>filter()</kbd>, and <kbd>repeat()</kbd> to process data and control its flow. They can be compared to Python generators<em>,</em> but with a more advanced interface and, more importantly, with a C++ backbone for computational performance. Though you could manually implement a multithreaded Python generator to process and serve batches in parallel with the main training loop, <kbd>tf.data</kbd> does all this out of the box (and most probably in a more optimized manner).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structure of TensorFlow data pipelines</h1>
                </header>
            
            <article>
                
<p>As indicated in the previous paragraphs, data scientists have already developed extensive know-how regarding the processing and pipelining of large datasets, and the structure of <kbd>tf.data</kbd> pipelines directly follows these best practices.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extract, Transform, Load</h1>
                </header>
            
            <article>
                
<p>The API guide also makes the parallel between data pipelines for training and <strong>Extract, Transform, Load</strong> (<strong>ETL</strong>) processes. ETL is a common paradigm for data processing in computer science. In computer vision, ETL pipelines in charge of feeding models with training data usually look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5a5940f7-93b6-4d4d-8ae2-84f5506b44e8.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-1: A typical ETL pipeline to provide data for the training of computer vision models</div>
<p>The <strong>extraction</strong> step consists of selecting data sources and extracting their content. These sources may be listed explicitly by a document (for instance, a CSV file containing the filenames for all the images), or implicitly (for instance, with all the dataset's images already stored in a specific folder). Sources may be <em>stored on different devices</em> (local or remote), and it is also the task of the extractor to list these different sources and extract their content. For example, it is common in computer vision to have datasets so big that they have to be stored on multiple hard drives. To train NNs in a supervised manner, we also need to extract the annotations/ground truths along the images (for instance, class labels contained in CSV files, and ground truth segmentation masks stored in another folder).</p>
<p>The fetched data samples should then be <em>transformed</em>. One of the most common transformations is the parsing of extracted data samples into a common format. For instance, this means parsing the bytes read from image files into a matrix representation (for instance, to decode JPEG or PNG bytes into image tensors). Other heavy transformations can be applied in this step, such as <em>cropping/scaling</em> images to the same dimensions, or <em>augmenting</em> them with various random operations. Again, the same applies to annotations for supervised learning. They should also be parsed, for instance, into tensors that could later be handed to loss functions.</p>
<p>Once ready, the data is <em>loaded</em> into the target structure. For the training of machine learning methods, this means sending the batch samples into the device in charge of running the model, such as the selected GPU(s). The processed dataset can also be cached/saved somewhere for later use.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This ETL process can already be observed, for instance, in the Jupyter notebook setting up the <em>Cityscapes</em> input pipeline in <a href="c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml">Chapter 6</a>, <em>Enhancing and Segmenting Images</em>. The input pipeline was iterating over the input/ground truth filenames provided, and parsing and augmenting their content, before passing the results as batches to our training processes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">API interface</h1>
                </header>
            
            <article>
                
<p><kbd>tf.data.Dataset</kbd> is the central class provided by the <kbd>tf.data</kbd> API (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a>). Instances of this class (which are simply called <strong>datasets</strong>) represent data sources, following the lazy list paradigm we just presented.</p>
<p>Datasets can be initialized in a multitude of ways, depending on how their content is initially stored (in files, NumPy arrays, tensors, and others). For example, a dataset can be based on a list of image files, as follows:</p>
<pre class=""><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">data</span><span class="pun">.</span><span class="typ">Dataset</span><span class="pun">.</span><span class="pln">list_files</span><span class="pun">(</span><span class="str">"/path/to/dataset/*.png"</span><span class="pun">)</span></pre>
<p>Datasets also have numerous methods they can apply to themselves in order to provide a transformed dataset. For example, the following function returns a new dataset instance with the file's contents properly transformed (that is, parsed) into homogeneously resized image tensors:</p>
<pre class=""><span class="kwd">def</span><span class="pln"> parse_fn</span><span class="pun">(</span><span class="pln">filename</span><span class="pun">):</span><span class="pln"><br/>    img_bytes = tf.io.read_file(filename)<br/>    </span><span class="pln">img </span><span class="pun">=</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">io</span><span class="pun">.</span><span class="pln">decode_png</span><span class="pun">(</span><span class="pun">img_bytes, channels=3)<br/>    img = tf.image.resize(img, [64, 64])<br/>    </span><span class="kwd">return</span><span class="pln"> img</span><span class="pln">  # or for instance, `{'image': img}` if we want to name this input<br/></span><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> dataset</span><span class="pun">.</span><span class="pln">map</span><span class="pun">(</span><span class="pln">map_func</span><span class="pun">=</span><span class="pln">parse_fn</span><span class="pun">)</span></pre>
<p>The function passed to <kbd>.map()</kbd> will be applied to every sample in the dataset when iterating. Indeed, once all the necessary transformations are applied, datasets can be used as any lazy lists/generators, as follows:</p>
<pre class="lang-python"><span class="kwd">print</span><span class="pun">(</span><span class="pln">dataset</span><span class="pun">.</span><span class="pln">output_types</span><span class="pun">)</span><span class="pln">  </span><span class="com"># &gt; "tf.uint8"</span><span class="pln"><br/></span><span class="kwd">print</span><span class="pun">(</span><span class="pln">dataset</span><span class="pun">.</span><span class="pln">output_shapes</span><span class="pun">)</span><span class="pln"> </span><span class="com"># &gt; "(64, 64, 3)"</span><span class="pln"><br/>for image in dataset:<br/>     # do something with the image</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>All the data samples are already returned as <kbd>Tensor</kbd>, and can easily be loaded into the device(s) in charge of the training. To make things even more straightforward, <kbd>tf.estimator.Estimator</kbd> and <kbd>tf.keras.Model</kbd> instances can directly receive a <kbd>tf.data.Dataset</kbd> object as input for their training (for estimators, the dataset operations have to be wrapped into a function returning the dataset) as follows:</p>
<pre>keras_model.fit(dataset, ...)     # to train a Keras model on the data<br/>def input_fn():<br/>    # ... build dataset<br/>    return dataset<br/>tf_estimator.train(input_fn, ...) # ... or to train a TF estimator</pre>
<p>With estimators and models tightly integrating the <kbd>tf.data</kbd> API, TensorFlow 2 has made data preprocessing and data loading both modular and clear.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up input pipelines</h1>
                </header>
            
            <article>
                
<p>Keeping in mind the ETL procedure, we will develop some of the most common and important methods provided by <kbd>tf.data</kbd>, at least for computer vision applications. For an exhaustive list, we invite our readers to refer to the documentation (<a href="https://www.tensorflow.org/api_docs/python/tf/data">https://www.tensorflow.org/api_docs/python/tf/data</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting (from tensors, text files, TFRecord files, and more)</h1>
                </header>
            
            <article>
                
<p>Datasets are usually built for specific needs (companies gathering images to train smarter algorithms, researchers setting up benchmarks, and so on), so it is rare to find two datasets with the same structure and format. Thankfully for us, TensorFlow developers are well aware of this and have provided plenty of tools to list and extract data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From NumPy and TensorFlow data</h1>
                </header>
            
            <article>
                
<p>First of all, if data samples were already somehow loaded by the program (for instance, as NumPy or TensorFlow structures), they can be passed directly to <kbd>tf.data</kbd> using the <kbd>.from_tensors()</kbd> or <kbd>.from_tensor_slices()</kbd> static methods.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Both accept nested array/tensor structures, but the latter will slice the data into samples along the first axis as follows:</p>
<pre>x, y = np.array([1, 2, 3, 4]), np.array([5, 6, 7, 8])<br/>d = tf.data.Dataset.from_tensors((x,y))<br/>print(d.output_shapes) # &gt; (TensorShape([4]), TensorShape([4]))<br/>d_sliced = tf.data.Dataset.from_tensor_slices((x,y))<br/>print(d_sliced.output_shapes) # &gt; (TensorShape([]), TensorShape([]))</pre>
<p>As we can observe, the second dataset, <kbd>d_sliced</kbd>, ends up containing four pairs of samples, each containing only one value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From files</h1>
                </header>
            
            <article>
                
<p>As seen in a previous example, datasets can iterate over files using the <kbd>.list_files()</kbd> static method. This method creates a dataset of string tensors, each containing the path of one of the listed files. Each file can then be opened using, for instance, <kbd>tf.io.read_file()</kbd> (<kbd>tf.io</kbd> contains file-related operations).</p>
<p class="mce-root">The <kbd>tf.data</kbd> API also provides some specific datasets to iterate over binary or text files. <kbd>tf.data.TextLineDataset()</kbd> can be used to read documents line by line (useful for some public datasets that are listing their image files and/or labels in text files); <kbd>tf.data.experimental.CsvDataset()</kbd> can parse CSV files and return their content line by line too.</p>
<div class="packt_infobox"><kbd>tf.data.experimental</kbd> does not ensure the same backward compatibility as other modules. By the time this book reaches our readers, methods may have been moved to <kbd>tf.data.Dataset</kbd> or simply removed (for methods that are temporary solutions to some TensorFlow limitations). We invite our readers to check the documentation.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From other inputs (generator, SQL database, range, and others)</h1>
                </header>
            
            <article>
                
<p class="mce-root">Although we will not list them all, it is good to keep in mind that <kbd>tf.data.Dataset</kbd> can be defined from a wide range of input sources. For example, datasets simply iterating over numbers can be initialized with the <kbd>.range()</kbd> <span>static method</span><span>. Datasets can also be built upon</span> Python generators wi<span>th</span> <kbd>.from_generator()</kbd><span>. Finally, even if elements are stored in a</span> SQL database<span>, TensorFlow provides some (experimental) tools to query it, including the following:</span></p>
<pre class="lang-python"><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">data</span><span class="pun">.</span><span class="pln">experimental</span><span class="pun">.</span><span class="typ">SqlDataset</span><span class="pun">(<br/>    </span><span class="str">"sqlite"</span><span class="pun">,</span><span class="pln"> </span><span class="str">"/path/to/my_db.sqlite3"</span><span class="pun">,<br/>    </span><span class="str">"SELECT img_filename, label FROM images"</span><span class="pun">,</span><span class="pln"> </span><span class="pun">(</span><span class="pln">tf</span><span class="pun">.</span><span class="kwd">string</span><span class="pun">,</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">int32</span><span class="pun">))</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>For more specific dataset instantiators, we invite our readers to check the <kbd>tf.data</kbd> documentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming the samples (parsing, augmenting, and more)</h1>
                </header>
            
            <article>
                
<p>The second step of ETL pipelines is <strong>transform</strong>. Transformations can be split into two types—those that affect data samples individually, and those that affect a dataset as a whole. In the following paragraphs, we will cover the former transformations and explain how our samples can be preprocessed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parsing images and labels</h1>
                </header>
            
            <article>
                
<p>In the <kbd>parse_fn()</kbd> method we wrote in the previous subsection for <kbd>dataset.map()</kbd>, <kbd>tf.io.read_file()</kbd> was called to read the file corresponding to each filename listed by the dataset, and then <kbd>tf.io.decode_png()</kbd> converted the bytes into an image tensor.</p>
<div class="packt_tip"><strong><kbd>tf.io</kbd></strong> <span>also</span> contains <kbd>decode_jpeg()</kbd>, <kbd>decode_gif()</kbd>, and more. It also provides the more generic <kbd>decode_image()</kbd>, which can infer which image format to use (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/io">https://www.tensorflow.org/api_docs/python/tf/io</a>).</div>
<p>Furthermore, numerous methods can be applied to parsing computer vision labels. Obviously, if the labels are also images (for instance, for image segmentation or edition), the methods we just listed can be reused all the same. If the labels are stored in text files, <kbd>TextLineDataset</kbd> or <kbd>FixedLengthRecordDataset</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data">https://www.tensorflow.org/api_docs/python/tf/data</a>) can be used to iterate over them, and modules such as <kbd>tf.strings</kbd> can help parse the lines/records. For example, let's imagine we have a training dataset with a text file listing the filename of an image and its class identifier <span>on each line</span>, separated by a comma. Each pair of images/labels could be parsed this way:</p>
<pre class=""><span class="kwd">def</span><span class="pln"> parse_fn</span><span class="pun">(</span><span class="pln">line</span><span class="pun">):</span><span class="pln"><br/>    img_filename, img_label = tf.strings.split(line, sep=',')<br/>    img = tf.io.decode_image(tf.io.read_file(img_filename))[0]</span><span class="pun"><br/>    </span><span class="kwd">return</span><span class="pln"> </span><span class="pln">{'image': img, 'label': tf.strings.to_number(img_label)}<br/></span><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> tf.data.TextLineDataset('/path/to/file.txt').map</span><span class="pun">(</span><span class="pln">parse_fn)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As we can observe, TensorFlow provides multiple helper functions to process and convert strings, to read binary files, to decode PNG or JPEG bytes into images, and so on. With these functions, pipelines to handle heterogeneous data can be set up with minimal effort.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parsing TFRecord files</h1>
                </header>
            
            <article>
                
<p>While listing all the image files and then iterating to open and parse them is a straightforward pipeline solution, it can be suboptimal. Loading and parsing image files <span>one by one</span> <span>i</span><span>s resource-consuming. Storing a large number of images together into a binary file would make the read-from-disk operations (or streaming operations for remote files) much more efficient. Therefore, TensorFlow users are often advised to use the</span> TFRecord <span>file format, based on Google's</span> Protocol Buffers<span>, a</span> language-neutral, platform-neutral extensible mechanism for serializing structured data <span>(</span><span>refer to the documentation at</span> <a href="https://developers.google.com/protocol-buffers/">https://developers.google.com/protocol-buffers</a><span>).</span></p>
<p>TFRecord files are binary files aggregating data samples (such as images, labels, and metadata). A TFRecord file contains serialized <kbd>tf.train.Example</kbd> instances, which are basically dictionaries naming each data element (called <strong>features</strong> according to this API) composing the sample (for example, <kbd>{'img': image_sample1, 'label': label_sample1, ...}</kbd>). Each element/feature that a sample contains is an instance of <kbd>tf.train.Feature</kbd> or of its subclasses. These objects store the data content as lists of bytes, of floats, or of integers (refer to the documentation at <a href="https://www.tensorflow.org//api_docs/python/tf/train">https://www.tensorflow.org//api_docs/python/tf/train</a>).</p>
<p>Because it was developed specifically for TensorFlow, this file format is very well supported by <kbd>tf.data</kbd>. In order to use TFRecord files as data source for input pipelines, TensorFlow users can pass the files to <kbd>tf.data.TFRecordDataset(filenames)</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset">https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset</a>), which can iterate over the serialized <kbd>tf.train.Example</kbd> elements they contain. To parse their content, the following should be done:</p>
<pre class="lang-python"><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">data</span><span class="pun">.</span><span class="typ">TFRecordDataset</span><span class="pun">([</span><span class="str">'file1.tfrecords','file2.tfrecords']</span><span class="pun">)</span><span class="pln"><br/></span><span class="com"># Dictionary describing the features/tf.trainExample structure:</span><span class="pln"><br/><strong>feat_dic </strong></span><span class="pun">=</span><span class="pln"> </span><span class="pun">{</span><span class="str">'img'</span><span class="pun">:</span><span class="pln"> </span><span class="pun">tf.io.FixedLenFeature([], tf.string), # image's bytes<br/></span><span class="pln">            </span><span class="str">'label'</span><span class="pun">:</span><span class="pln"> tf</span><span class="pun">.io.</span><span class="typ">FixedLenFeature</span><span class="pun">([1],</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">int64</span><span class="pun">)}</span><span class="pln"> # class label</span><span class="pln"><br/></span><span class="kwd">def</span><span class="pln"> parse_fn</span><span class="pun">(</span><span class="pln">example_proto</span><span class="pun">):</span><span class="pln"> # Parse a serialized tf.train.Example<br/></span><span class="pln">    sample = <strong>tf</strong></span><strong><span class="pun">.</span><span class="pln">parse_single_example</span></strong><span class="pun">(</span><span class="pln">example_proto</span><span class="pun">,</span><span class="pln"> feat_dic</span><span class="pun">)</span><span class="pln"><br/>    return </span><span class="pln">tf.io.decode_image(</span><span class="pln">sample['img])[0], sample['label']<br/>dataset </span><span class="pun">=</span><span class="pln"> dataset</span><span class="pun">.</span><span class="pln">map</span><span class="pun">(</span><span class="pln">parse_fn</span><span class="pun">)</span></pre>
<p><kbd>tf.io.FixedLenFeature(shape, dtype, default_value)</kbd> lets the pipeline know what kind of data to expect out of the serialized sample, which can then be parsed with a single command.</p>
<div class="packt_tip">In one of the Jupyter notebooks, we cover TFRecord in more detail, explaining step by step how data can be preprocessed and stored as TFRecord files, and how these files can then be used as a data source for <kbd>tf.data</kbd> pipelines.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Editing samples</h1>
                </header>
            
            <article>
                
<p>The <kbd>.map()</kbd> method is central to <kbd>tf.data</kbd> pipelines. Besides parsing samples, it is also applied to edit them further. For example, in computer vision, it is common for some applications to crop/resize input images to the same dimensions (for instance, applying <kbd>tf.image.resize()</kbd>) or to one-hot target labels (<kbd>tf.one_hot()</kbd>).</p>
<p>As we will detail later in this chapter, it is also recommended to wrap the optional augmentations for training data into a function passed to <kbd>.map()</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming the datasets (shuffling, zipping, parallelizing, and more)</h1>
                </header>
            
            <article>
                
<p>The API also provides numerous functions to transform one dataset into another, to adapt its structure, or to merge it with other data sources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structuring datasets</h1>
                </header>
            
            <article>
                
<p>In data science and machine learning, operations such as filtering data, shuffling samples, and stacking samples into batches are extremely common. The <kbd>tf.data</kbd> API offers simple solutions to most of those (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a>). For example, some of the most frequently used datasets' methods are as follows:</p>
<ul>
<li>
<p><kbd>.batch(batch_size, ...)</kbd>, which returns a new dataset, with the data samples batched accordingly (<kbd>tf.data.experimental.unbatch()</kbd> does the opposite). Note that if <kbd>.map()</kbd> is called after <kbd>.batch()</kbd>, the mapping function will therefore receive batched data as input.</p>
</li>
<li>
<p><kbd>.repeat(count=None)</kbd>, which repeats the data <kbd>count</kbd> times (infinitely if <kbd>count = None</kbd>).</p>
</li>
<li>
<p><kbd>.shuffle(buffer_size, seed, ...)</kbd>, which shuffles elements after filling a buffer accordingly (for instance, if <kbd>buffer_size = 10</kbd>, the dataset will virtually divide the dataset into subsets of 10 elements, and randomly permute the elements in each, before returning them one by one). The larger the buffer size is, the more stochastic the shuffling becomes, but also the heavier the process is.</p>
</li>
<li><kbd>.filter(predicate)</kbd>, which keeps/removes elements depending on the Boolean output of the <kbd>predicate</kbd> function provided. For example, if we wanted to filter a dataset to remove elements stored online, we could use this method as follows:</li>
</ul>
<pre style="padding-left: 60px">url_regex = "(?i)([a-z][a-z0-9]*)://([^ /]+)(/[^ ]*)?|([^ @]+)@([^ @]+)"<br/>def is_not_url(filename): #NB: the regex isn't 100% sure/covering all cases<br/>    return ~(tf.strings.regex_full_match(filename, url_regex))<br/>dataset = dataset.filter(is_not_url)</pre>
<ul>
<li class="mce-root"><kbd><strong>.</strong>take(count)</kbd>, which returns a dataset containing the first <kbd>count</kbd> elements at most.</li>
<li class="mce-root"><kbd><strong>.</strong>skip(count)</kbd>, which returns a dataset without the <span>first</span> <kbd>count</kbd> elements. Both methods can be used to split a dataset, for instance, into training and validation sets as follows:</li>
</ul>
<pre style="padding-left: 60px">num_training_samples, num_epochs = 10000, 100<br/>dataset_train = dataset.take(num_training_samples)<br/>dataset_train = dataset_train.repeat(num_epochs)<br/>dataset_val   = dataset.skip(num_training_samples)</pre>
<p class="mce-root">Many other methods are available to structure data or to control its flow, usually inspired by other data processing frameworks (such as <kbd>.unique()</kbd>, <kbd>.reduce()</kbd>, and <kbd>.group_by_reducer()</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Merging datasets</h1>
                </header>
            
            <article>
                
<p class="mce-root">Some methods can also be used to merge datasets together. The two most straightforward ones are <kbd>.concatenate(dataset)</kbd> and the static <kbd>.zip(datasets)</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a>). The former <em>concatenates</em> the samples of the dataset <span>provided </span>with those of the current one, while the latter <em>combines</em> the dataset's elements into tuples (similar to Python's <kbd>zip()</kbd>) as follows:</p>
<pre>d1 = tf.data.Dataset.range(3)<br/>d2 = tf.data.Dataset.from_tensor_slices([[4, 5], [6, 7], [8, 9]])<br/>d = tf.data.Dataset.zip((d1, d2))<br/># d will return [0, [4, 5]], [1, [6, 7]], and [2, [8, 9]]</pre>
<p class="mce-root">Another method often used to merge data from different sources is <kbd>.interleave(map_func, cycle_length, block_length, ...)</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave</a>). This applies the <kbd>map_func</kbd> function to the elements of the datasets and <em>interleaves</em> the results. Let's now go back to the example presented in the <em>Parsing images and labels</em> section, with image files and classes listed in a text file. If we have several such text files and want to combine all their images into a single dataset, <kbd>.interleave()</kbd> could be applied as follows:</p>
<pre>filenames = ['/path/to/file1.txt', '/path/to/file2.txt', ...]<br/>d = tf.data.Dataset.from_tensor_slices(filenames)<br/>d = d.interleave(lambda f: <span class="pln">tf.data.TextLineDataset(f).map</span><span class="pun">(</span><span class="pln">parse_fn), <br/>                 cycle_length=2, block_length=5)</span></pre>
<p>The <kbd>cycle_length</kbd> parameter fixes the number of elements processed concurrently. In our preceding example, <kbd>cycle_length = 2</kbd> means that the function will concurrently iterate over the lines of the first two files, before iterating over the lines of the third and fourth files, and so on. The <kbd>block_length</kbd> parameter controls the number of consecutive samples returned per element. Here, <kbd>block_length = 5</kbd> means that the method will yield a maximum of <kbd>5</kbd> consecutive lines from one file before iterating over another.</p>
<p>With all these methods and much more available, complex pipelines for data extraction and transformation can be set up with minimal effort, as already illustrated in some previous notebooks (for instance, for the <em>CIFAR</em> and <em>Cityscapes</em> datasets).</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another advantage of <kbd>tf.data</kbd> is that all its operations are registered in the TensorFlow operational graph, and the extracted and processed samples are returned as <kbd>Tensor</kbd> instances. Therefore, we do not have much to do regarding the final step of ETL, that is, the <em>loading</em>. As with any other TensorFlow operation or tensor, the library will take care of loading them into the target devices—unless we want to choose them ourselves (for instance, wrapping the creation of datasets with <kbd>tf.device()</kbd>). When we start iterating over a <kbd>tf.data</kbd> dataset, generated samples can be directly passed to the models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing and monitoring input pipelines</h1>
                </header>
            
            <article>
                
<p>While this API simplifies the setting up of efficient input pipelines, some best practices should be followed to fully harness its power. After sharing some recommendations from TensorFlow creators, we will also present how to monitor and reuse pipelines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Following best practices for optimization</h1>
                </header>
            
            <article>
                
<p>The API provides several methods and options to optimize the data processing and flow, which we will now cover in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parallelizing and prefetching</h1>
                </header>
            
            <article>
                
<p>By default, most of the dataset methods are processing samples one by one, with no parallelism. However, this behavior can be easily changed, for example, to take advantage of multiple CPU cores. For instance, the <kbd>.interleave()</kbd> and <kbd>.map()</kbd> methods both have a <strong><kbd>num_parallel_calls</kbd></strong> parameter to specify the number of threads they can create (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a>). <strong>Parallelizing</strong> the extraction and transformation of images can greatly decrease the time needed to generate training batches, so it is important to always properly set <kbd>num_parallel_calls</kbd> (for instance, to the number of CPU cores the processing machine has).</p>
<p class="mce-root"/>
<div class="packt_tip">TensorFlow also provides <kbd>tf.data.experimental.parallel_interleave()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/parallel_interleave">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/parallel_interleave</a>), a parallelized version of <kbd>.interleave()</kbd> with some additional options. For instance, it has a <kbd>sloppy</kbd> parameter, which, if set to <kbd>True</kbd>, allows each thread to return its output as soon as it is ready. On the one hand, this means that the data will no longer be returned in a deterministic order, but, on the other hand, this can further improve the pipeline performance.</div>
<p>Another performance-related feature of <kbd>tf.data</kbd> is the possibility to <em>prefetch</em> data samples. When applied through the dataset's <kbd>.prefetch(buffer_size)</kbd> method, this feature allows the input pipelines to start preparing the next samples while the current ones are being consumed, instead of waiting for the next dataset call. Concretely, this allows TensorFlow to start preparing the next training batch(es) on the CPU(s), while the current batch is being used by the model running on the GPU(s), for instance.</p>
<p>Prefetching basically enables the <em>parallelization of the data preparation and training operations</em> in a <em>producer-consumer</em> fashion. Enabling parallel calls and prefetching can thus be done with minor changes, while greatly reducing the training time, as follows:</p>
<pre class=""><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> tf.data.TextLineDataset('/path/to/file.txt')<br/>dataset = dataset.map</span><span class="pun">(</span><span class="pln">parse_fn, num_threads).batch(batch_size).prefetch(1)</span></pre>
<p>Inspired by TensorFlow's official guide (<a href="https://www.tensorflow.org/guide/performance/datasets">https://www.tensorflow.org/guide/performance/datasets</a>), <em>Figure 7-2</em> illustrates the performance gain these best practices can bring:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2fb1ae26-bf70-4028-9b91-08c69ac59a8d.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7-2: Visual representation of the performance gain obtained from parallelizing and prefetching</div>
<p class="mce-root">By combining these different optimizations, CPU/GPU idle time can be reduced <span>further</span>. The performance gain in terms of preprocessing time can become really significant, as demonstrated in one of the Jupyter notebooks for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fusing operations</h1>
                </header>
            
            <article>
                
<p>It is also useful to know that <kbd>tf.data</kbd> offers functions that combine some key operations for greater performance or more reliable results.</p>
<p>For example, <kbd>tf.data.experimental.shuffle_and_repeat(buffer_size, count, seed)</kbd> fuses together the shuffling and repeating operations, making it easy to have datasets shuffled differently at each epoch (refer to the documentation at <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/shuffle_and_repeat">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/shuffle_and_repeat</a>).</p>
<p>Back to optimization matters, <kbd>tf.data.experimental.map_and_batch(map_func, batch_size, num_parallel_batches, ...)</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/map_and_batch">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/map_and_batch</a>) applies the <kbd>map_func</kbd> function and then batches the results together. By fusing these two operations, this solution prevents some computational overheads and should thus be preferred.</p>
<p class="mce-root"/>
<div class="mce-root packt_infobox"><kbd>map_and_batch()</kbd> is meant to disappear, as TensorFlow 2 is implementing several tools to automatically optimize the <kbd>tf.data</kbd> operations, for instance, grouping multiple <kbd>.map()</kbd> calls together, vectorizing the <kbd>.map()</kbd> operations and fusing them directly with <kbd>.batch()</kbd>, fusing <kbd>.map()</kbd> and <kbd>.filter()</kbd>, and more. Once this automatic optimization has been fully implemented and validated by the TensorFlow community, there will be no further need for <kbd>map_and_batch()</kbd> (once again, this may already be the case by the time you reach this chapter).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Passing options to ensure global properties</h1>
                </header>
            
            <article>
                
<p>In TensorFlow 2, it is also possible to configure datasets by <em>setting global options,</em> which will affect all their operations. <kbd>tf.data.Options</kbd> is a structure that can be passed to datasets through their <kbd>.with_options(options)</kbd> <span>method</span> and that has several attributes to parametrize the datasets (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/Options">https://www.tensorflow.org/api_docs/python/tf/data/Options</a>).</p>
<p>For instance, if the <kbd>.experimental_autotune</kbd> Boolean attribute is set to <kbd>True</kbd>, TensorFlow will automatically tune the values of <kbd>num_parallel_calls</kbd> for all the dataset's operations, according to the capacity of the target machine(s).</p>
<p>The attribute currently named <kbd>.experimental_optimization</kbd> contains a set of <em>sub-options</em> related to the automatic optimization of the dataset's operations (refer to the previous information box). For example, its own <kbd>.map_and_batch_fusion</kbd> attribute can be set to <kbd>True</kbd> to let TensorFlow automatically fuse the <kbd>.map()</kbd> and <kbd>.batch()</kbd> calls; <kbd>.map_parallelization</kbd> can be set to <kbd>True</kbd> to let TensorFlow automatically parallelize some of the mapping functions, and so on, as follows:</p>
<pre class="lang-python"><span class="pln">options </span><span class="pun">=</span><span class="pln"> tf</span><span class="pun">.</span><span class="pln">data</span><span class="pun">.</span><span class="typ">Options</span><span class="pun">()</span><span class="pln"><br/>options</span><span class="pun">.</span><span class="pln">experimental_optimization</span><span class="pun">.</span><span class="pun">map_and_batch_fusion =</span><span class="pln"> </span><span class="kwd">True</span><span class="pln"><br/>dataset </span><span class="pun">=</span><span class="pln"> dataset</span><span class="pun">.</span><span class="pln">with_options</span><span class="pun">(</span><span class="pln">options</span><span class="pun">)</span></pre>
<p>Plenty of other options are available (and more may come). We invite our readers to have a look at the documentation, especially if the performance of their input pipelines is a key matter.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring and reusing datasets</h1>
                </header>
            
            <article>
                
<p>We presented multiple tools to optimize <kbd>tf.data</kbd> pipelines, but how can we make sure they positively affect the performance? Are there other tools to figure out which operations may be slowing down the data flow? In the following paragraphs, we will answer these questions by demonstrating how input pipelines can be monitored, as well as how they can be cached and restored for later use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Aggregating performance statistics</h1>
                </header>
            
            <article>
                
<p>One of the novelties of TensorFlow 2 is the possibility to aggregate some statistics regarding <kbd>tf.data</kbd> pipelines, such as their latency (for the whole process and/or for each operation) or the number of bytes <span class="pl-s">produced by each of their elements.</span></p>
<p>TensorFlow can be notified to gather these metric values for a dataset <em>through its global options</em> (refer to previous paragraphs). The <kbd>tf.data.Options</kbd> instances have a <kbd>.experimental_stats</kbd> <span class="pl-s">field from the <kbd>tf.data.experimental.StatsOption</kbd> class (refer to the documentation at <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/StatsOptions">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/StatsOptions</a>). This class defines several options related to the aforementioned dataset metrics (for instance, setting <kbd>.latency_all_edges</kbd> to <kbd>True</kbd> to measure the latency). It also has a <kbd>.aggregator</kbd> attribute, which can receive an instance of <kbd>tf.data.experimental.StatsAggregator</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/StatsAggregator">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/StatsAggregator</a>). As its name implies, this object will be attached to the dataset and aggregate the requested statistics, providing summaries that can be logged and visualized in TensorBoard, as shown in the following code sample.</span></p>
<div class="mce-root packt_infobox">At the time of writing this book, these features are still highly experimental and are not fully implemented yet. For example, there is no easy way to log the summaries containing the aggregated statistics. Given how important monitoring tools are, we still covered these features, believing they should soon be fully available.</div>
<p>Dataset statistics can, therefore, be aggregated and saved (for instance, for TensorBoard) as follows:</p>
<pre class="lang-python"><span class="pln"># Use utility function to tell TF to gather latency stats for this dataset:<br/>dataset = dataset.apply(tf.data.experimental.latency_stats("data_latency"))<br/># Link stats aggregator to dataset through the global options:<br/>stats_aggregator = tf.data.experimental.StatsAggregator()<br/>options = tf.data.Options()<br/>options.experimental_stats.aggregator = stats_aggregator<br/>dataset = dataset.with_options(options)<br/># Later, aggregated stats can be obtained as summary, for instance, to log them:<br/>summary_writer = tf.summary.create_file_writer('/path/to/summaries/folder')<br/>with summary_writer.as_default():<br/>    stats_summary = stats_aggregator.get_summary()<br/>    # ... log summary with `summary_writer` for Tensorboard (TF2 support coming soon)</span></pre>
<p>Note that it is possible to obtain statistics not only for the input pipeline as a whole, but also for each of its inner operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caching and reusing datasets</h1>
                </header>
            
            <article>
                
<p>Finally, TensorFlow offers several functions to <em>cache</em> generated samples or to save <kbd>tf.data</kbd> pipeline states.</p>
<p>Samples can be cached by calling the dataset's <kbd>.cache(filename)</kbd> method. If cached, data will not have to undergo the same transformations when iterated over again (that is, for the next epochs). Note that the content of the cached data will not be the same depending on when the method is applied. Take the following example:</p>
<pre class=""><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> tf.data.TextLineDataset('/path/to/file.txt')<br/>dataset_v1 = dataset.cache('cached_textlines.temp').map</span><span class="pun">(</span><span class="pln">parse_fn)</span><span class="pln"><br/>dataset_v2 = dataset.map(parse_fn).cache('cached_images.temp')</span></pre>
<p>The first dataset will cache the samples returned by <kbd>TextLineDataset</kbd>, that is, the text lines (the cached data is stored in the specified file, <kbd>cached_textlines.temp</kbd>). The transformation done by <kbd>parse_fn</kbd> (for instance, opening and decoding the corresponding image file for each text line) will have to be repeated for each epoch. On the other hand, the second dataset is caching the samples returned by <kbd>parse_fn</kbd>, that is, the images. While this may save precious computational time for the next epochs, this also means caching all the resulting images, which may be memory inefficient. Therefore, caching should be carefully thought through.</p>
<p>Finally, it is also possible to <em>save the state of a dataset</em>, for instance, so that if the training is somehow stopped, it can be resumed without re-iterating over the precedent input batches. As mentioned in the documentation, this feature can have a positive impact on models being trained on a small number of different batches (and thus with a risk of overfitting). For estimators, one solution to save the iterator state of a dataset is to set up the following hook—<kbd>tf.data.experimental.CheckpointInputPipelineHook</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/CheckpointInputPipelineHook">https://www.tensorflow.org/api_docs/python/tf/data/experimental/CheckpointInputPipelineHook</a>).</p>
<p class="mce-root"/>
<p>Aware of how important a configurable and optimized data flow is to machine learning applications, TensorFlow developers are continuously providing new features to refine the <kbd>tf.data</kbd> API. As covered in this past section and illustrated in the related Jupyter Notebook, taking advantage of these features—even the experimental ones—can greatly reduce implementation overheads and training time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to deal with data scarcity</h1>
                </header>
            
            <article>
                
<p class="mce-root">Being able to efficiently extract and transform data for the training of complex applications is primordial, but this is assuming that <em>enough data</em> is available for such tasks in the first place. After all, NNs are <em>data-hungry</em> methods and even though we are in the big data era, large enough datasets are still tenuous to gather and even more difficult to annotate. It can take several minutes to annotate a single image (for instance, to create the ground truth label map for semantic segmentation models), and some annotations may have to be validated/corrected by experts (for instance, when labeling medical pictures). In some cases, images themselves may not be easily available. For instance, it would be too time- and money-consuming to take pictures of every manufactured object and their components when building automation models for industrial plants.</p>
<p><strong>Data scarcity</strong> is, therefore, a common problem in computer vision, and much effort has been expended trying to train robust models despite the lack of training images or rigorous annotations. In this section, we will cover several solutions proposed over the years, and we will demonstrate their benefits and limitations in relation to various tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Augmenting datasets</h1>
                </header>
            
            <article>
                
<p>We have been mentioning this first approach since <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>, and we have already put it into use for some applications in previous notebooks. This is finally the opportunity for us to properly present what <strong>data augmentation</strong> is and how to apply it with TensorFlow 2.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>A</span>s indicated before, <span><em>augmenting</em> datasets means applying random transformations to their content in order to obtain different-looking versions for each. We will present the benefits of this procedure, as well as some related best practices.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why augment datasets?</h1>
                </header>
            
            <article>
                
<p>Data augmentation is probably the most common and simple method to deal with overly small training sets. It can virtually multiply their number of images by providing different looking versions of each. These various versions are obtained by applying a combination of random transformations, <span>such as scale jittering, random flipping, rotation, and color shift. Data augmentation can incidentally help</span> <em>prevent overfitting</em><span>, which would usually happen when training a large model on a small set of images.</span></p>
<p>But even when enough training images are available, this procedure should still be considered. Indeed, data augmentation has other benefits. Even large datasets can suffer from <em>biases</em>, and data augmentation can compensate for some of them. We will illustrate this concept with an example. Let's imagine we want to build a classifier for brush versus pen pictures. However, the pictures for each class were gathered by two different teams that did not agree on a precise acquisition protocol beforehand (for instance, which camera model or lighting conditions to opt for). As a result, the <em>brush</em> training images are clearly darker and noisier than the <em>pen</em> ones. Since NNs are trained to use any visual cues to predict correctly, the models learning on such a dataset may end up relying on these obvious lighting/noise differences to classify the objects, instead of purely focusing on the object representations (such as their shape and texture). Once in production, these models will fare poorly, no longer being able to rely on these biases. This example is illustrated in <em>Figure 7-3</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0cb9e43e-e94d-4a5e-ac61-911dcd808178.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-3: Example of a classifier trained on a biased dataset, unable to apply its knowledge to the target data</div>
<p>Randomly adding some noise to the pictures or randomly adjusting their brightness would prevent the networks from relying on these cues. These augmentations would thus partially compensate for the dataset's biases, and make these visual differences too unpredictable to be used by the networks (that is, preventing models from overfitting biased datasets).</p>
<p class="mce-root"/>
<p>Augmentations can also be used to improve the dataset's coverage. Training datasets cannot cover all image variations (otherwise we would not have to build machine learning models to deal with new different images). If, for example, all the images of a dataset were shot under the same light, then the recognition models trained on them would fare really poorly with images taken under different lighting conditions. These models were basically not taught that <em>lighting conditions is a thing</em> and that they should learn to ignore it and focus on the actual image content. Therefore, randomly editing the brightness of the training images before passing them to the networks would educate them on this visual property. By better preparing them for the variability of target images, data augmentation helps us to train more robust solutions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considerations</h1>
                </header>
            
            <article>
                
<p>Data augmentation can take multiple forms, and several options should be considered when performing this procedure. First of all, data augmentation can be done either offline or online. Offline augmentation means transforming all the images before the training even starts, and saving the various versions for later use. Online means applying the transformations when generating each new batch inside the training input pipelines.</p>
<p>Since augmentation operations can be computationally heavy, applying them beforehand and storing the results can be advantageous in terms of latency for the input pipelines. However, this implies having enough memory space to store the augmented dataset, often limiting the number of different versions generated. By randomly transforming the images on the fly, online solutions can provide different looking versions for every epoch. While computationally more expensive, this means presenting more variation to the networks. The choice between offline and online augmentation is thus conditioned by the memory/processing capacity of the available devices, and by the desired variability.</p>
<p>The variability is itself conditioned by the choice of transformations to be applied. For example, if only random horizontal and vertical flipping operations are applied, then this means a maximum of four different versions per image. Depending on the size of the original dataset, you could consider applying the transformations offline and storing the four-times-larger dataset. On the other hand, if operations such as random cropping and random color shift are considered, then the number of possible variations can become almost infinite.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When setting up data augmentation, the first thing to do, therefore, is to shortlist the relevant transformations (and their parameters when applicable). The list of possible operations is huge, but not all make sense with regard to the target data and use cases. For instance, vertical flipping should only be considered if the content of images can be naturally found upside down (such as close-up images of larger systems or birdview/satellite images). Vertically flipping images of urban scenes (such as the <em>Cityscapes</em> images) would not help the models at all, since they would (hopefully) never be confronted with such upside down images.</p>
<p>Similarly, you should be careful to properly parameterize some transformations such as cropping or brightness adjustment. If an image becomes so dark/bright that its content cannot be identified anymore, or if the key elements are cropped out, then the models won't learn anything from training on this edited picture (it may even confuse them if too many images are inappropriately augmented). Therefore, it is important to shortlist and parametrize transformations that add meaningful variations to the dataset (with respect to the target use cases) while preserving its semantic content.</p>
<p><em>Figure 7-4</em> provides some examples of what invalid and valid augmentations can be for an autonomous driving application:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/78a99a2c-eca2-41e8-b965-002b99c13e19.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-4: Valid/invalid augmentations for an autonomous driving application</div>
<p>It is also important to keep in mind that data augmentation cannot fully compensate for data scarcity. If we want a model to be able to recognize cats, but only have training images of Persian cats, no straightforward image transformations will help our model identify other cat breeds (for instance, Sphynx cats).</p>
<div class="packt_infobox">Some advanced data augmentation solutions include applying computer graphics or encoder-decoder methods to alter images. For example, computer graphics algorithms could be used to add fake sun blares or motion blur, and CNNs could be trained to transform daytime images into nighttime ones. We will develop some of these techniques later in this chapter.</div>
<p class="mce-root"/>
<p>Finally, you should not forget to transform the labels accordingly, when applicable. This especially concerns detection and segmentation labels, when geometrical transformations are performed. If an image is resized or rotated, its related label map or bounding boxes should undergo the same operation(s) to stay aligned (refer to the <em>Cityscapes</em> experiments in <a href="c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml">Chapter 6</a>, <em>Enhancing and Segmenting Images</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Augmenting images with TensorFlow</h1>
                </header>
            
            <article>
                
<p>Having clarified <em>why</em> and <em>when</em> images should be augmented, it is time to properly explain <em>how</em>. We will introduce some useful tools provided by TensorFlow to transform images, sharing a number of concrete examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Image module</h1>
                </header>
            
            <article>
                
<p>Python offers a huge variety of frameworks to manipulate and transform images. Besides the generic ones such as <em>OpenCV</em> (<a href="https://opencv.org">https://opencv.org</a>) and <em>Python Imaging Library</em> (PIL—<a href="http://effbot.org/zone/pil-index.htm">http://effbot.org/zone/pil-index.htm</a>), some packages specialize in providing data augmentation methods for machine learning systems. Among those, <kbd>imgaug</kbd> by Alexander Jung (<a href="https://github.com/aleju/imgaug">https://github.com/aleju/imgaug</a>) and <kbd>Augmentor</kbd> by Marcus D. Bloice (<a href="https://github.com/mdbloice/Augmentor">https://github.com/mdbloice/Augmentor</a>) are probably the most widely used, both offering a wide range of operations and a neat interface. Even Keras provides functions to preprocess and augment image datasets. <kbd>ImageDataGenerator</kbd> (<a href="https://keras.io/preprocessing/image">https://keras.io/preprocessing/image</a>) can be used to instantiate an image batch generator covering data augmentation (such as image rotation, zoom, or channel shifting).</p>
<p>However, TensorFlow has its own module for image processing that can seamlessly integrate <kbd>tf.data</kbd> pipelines—<kbd>tf.image</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/image">https://www.tensorflow.org/api_docs/python/tf/image</a>). This module contains all sorts of functions. Some of them implement common image-related metrics (for instance, <kbd>tf.image.psnr()</kbd> and <kbd>tf.image.ssim()</kbd>), and others can be used to convert images from one format to another (for instance, <kbd>tf.image.rgb_to_grayscale()</kbd>). But before all else, <kbd>tf.image</kbd> implements multiple image transformations. Most of these functions come in pairs—one function implementing a fixed version of the operation (such as <kbd>tf.image.central_crop()</kbd>, <kbd>tf.image.flip_left_right()</kbd> and <kbd>tf.image.adjust_jpeg_quality()</kbd>) and the other a randomized version (such as <kbd>tf.image.random_crop()</kbd>, <kbd>tf.image.random_flip_left_right()</kbd>, and <kbd>tf.image.random_jpeg_quality()</kbd>). The randomized functions usually take for arguments a range of values from which the attributes of the transformation are randomly sampled (such as <kbd>min_jpeg_quality</kbd> and <kbd>max_jpeg_quality</kbd> for <kbd>tf.image.random_jpeg_quality()</kbd> parameters).</p>
<p class="mce-root"/>
<p>Directly applicable to image tensors (single or batched), the <kbd>tf.image</kbd> functions are recommended within <kbd>tf.data</kbd> pipelines for online augmentation (grouping the operations into a function passed to <kbd>.map()</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – augmenting images for our autonomous driving application</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we introduced some state-of-the-art models for semantic segmentation and applied them to urban scenes in order to guide self-driving cars. In the related Jupyter notebooks, we provided an <kbd>_augmentation_fn(img, gt_img)</kbd> function passed to <kbd>dataset.map()</kbd> to augment the pictures and their ground truth label maps. Though we did not provide detailed explanations back then, this augmentation function illustrates well how <kbd>tf.image</kbd> can augment complex data.</p>
<p>For example, it offers a simple solution to the problem of transforming both the input images and their dense labels. Imagine we want some of the samples to be randomly horizontally flipped. If we call <kbd>tf.image.random_flip_left_right()</kbd> once for the input image and once for the ground truth label map, there is only a half chance that both images will undergo the same transformation.</p>
<p>One solution to ensure that the same set of geometrical transformations are applied to  the image pairs is the following:</p>
<pre>img_dim, img_ch = tf.shape(img)[-3:-1], tf.shape(img)[-1]<br/># Stack/concatenate the image pairs along the channel axis:<br/>stacked_imgs = tf.concat([img, tf.cast(gt_img, img.dtype)], -1)<br/># Apply the random operations, for instance, horizontal flipping:<br/>stacked_imgs = tf.image.random_flip_left_right(stacked_imgs)<br/># ... or random cropping (for instance, keeping from 80 to 100% of the images):<br/>rand_factor = tf.random.uniform([], minval=0.8, maxval=1.)<br/>crop_shape = tf.cast(tf.cast(img_dim, tf.float32) * rand_factor, tf.int32)<br/>crop_shape = tf.concat([crop_shape, tf.shape(stacked_imgs)[-1]], axis=0)<br/>stacked_imgs = tf.image.random_crop(stacked_imgs, crop_shape)<br/># [...] (apply additional geometrical transformations)<br/># Unstack to recover the 2 augmented tensors:<br/>img = stacked_imgs[..., :img_ch]<br/>gt_img = tf.cast(stacked_imgs[..., img_ch:], gt_img.dtype)<br/># Apply other transformations in the pixel domain, for instance:<br/>img = tf.image.random_brightness(image, max_delta=0.15)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Since most <kbd>tf.image</kbd> geometrical functions do not have any limitations regarding the number of channels the images can have, concatenating images along the channel axis beforehand is a simple trick to ensure that they undergo the same geometrical operations.</p>
<p>The preceding example also illustrates how some operations can be further randomized by sampling some parameters from random distributions. <kbd>tf.image.random_crop(images, size)</kbd> returns crops of a fixed size, extracted from random positions in the images. Picking a size factor with <kbd>tf.random.uniform()</kbd>, we obtain crops that are not only randomly positioned in the original images, but also randomly dimensioned.</p>
<p>Finally, this example is also a reminder that <em>not</em> all transformations should be applied to both the input images and their label maps. Trying to adjust the brightness or saturation of label maps would not make sense (and would, in some cases, raise an exception).</p>
<p>We will conclude this subsection on data augmentation by emphasizing that this procedure should always be considered. Even when training on large datasets, augmenting their images can only make the models more robust—as long as the random transformations are selected and applied with care.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rendering synthetic datasets</h1>
                </header>
            
            <article>
                
<p>However, what if we have no images to train on, at all? A common solution in computer vision is the use of <em>synthetic datasets</em>. In the following subsection, we will explain what synthetic images are, how they can be generated, and what their limitations are.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's first clarify what is meant by <em>synthetic images</em>, and why they are so often used in computer vision.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rise of 3D databases</h1>
                </header>
            
            <article>
                
<p>As mentioned in the introduction of this section on data scarcity, the complete lack of training images is not that uncommon a situation, especially in industry. Gathering hundreds of images for each new element to recognize is costly, and sometimes completely impractical (for instance, when the target objects are not produced yet or are only available at some remote location).</p>
<p class="mce-root"/>
<p>However, for industrial applications and others, it is increasingly common to have access to 3D models of the target objects or scenes (such as 3D <strong>computer-aided design</strong> (<strong>CAD</strong>) blueprints or 3D scenes captured with depth sensors). Large datasets of 3D models have even multiplied on the web. With the coincidental development of computer graphics, this led more and more experts to use such 3D databases to <em>render</em> synthetic images on which to train their recognition models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits of synthetic data</h1>
                </header>
            
            <article>
                
<p><strong>Synthetic images</strong> are thus images generated by computer graphics libraries from 3D models. Thanks to the lucrative entertainment industry, computer graphics have indeed come a long way, and rendering engines can nowadays generate highly realistic images from 3D models (such as for video games, 3D animated movies, and special effects). It did not take long for scientists to see the potential for computer vision.</p>
<p>Given some detailed 3D models of the target objects/scenes, it is possible with modern 3D engines to render huge datasets of pseudo-realistic images. With proper scripting, you can, for instance, render images of target objects from every angle, at various distances, with different lighting conditions or backgrounds, and so on. Using various rendering methods, it is even possible to simulate different types of cameras and sensors (for instance, depth sensors such as the <em>Microsoft Kinect</em> or <em>Occipital Structure</em> sensors).</p>
<p>Having full control over the scene/image content, you can also easily obtain all kinds of ground truth labels for each synthetic image (such as precise 3D positions of the rendered models or object masks). For example, targeting driving scenarios, a team of researchers from the Universitat Autònoma de Barcelona built virtual replicas of city environments and used them to render multiple datasets of urban scenes, named <em>SYNTHIA</em> (<a href="http://synthia-dataset.net">http://synthia-dataset.net</a>). This dataset is similar to <em>Cityscapes</em> (<a href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</a>), though larger.</p>
<p>Another team from the Technical University of Darmstadt and Intel Labs successfully demonstrated self-driving models trained on images taken from the realistic looking video game <em><span>Grand Theft Auto</span> V (GTA 5)</em> (<a href="https://download.visinf.tu-darmstadt.de/data/from_games">https://download.visinf.tu-darmstadt.de/data/from_games</a>).</p>
<p>These three datasets are presented in <em>Figure 7-5</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6d823e2d-4485-48f1-8f92-16b2bd8eab75.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-5: Samples from the <em>Cityscapes</em>, <em>SYNTHIA</em>, and <em>Playing for Data</em> datasets (links to the datasets are provided in the section). Images and their class labels are superposed</div>
<p>Besides the generation of static datasets, 3D models and game engines can also be used to create <em>interactive simulation environments</em>. After all, <em>simulation-based learning</em> is commonly used to teach humans complex skills, for instance, when it would be too dangerous or complicated to learn in real conditions (for instance, simulating zero gravity environments to teach astronauts how to perform some tasks once in space, and building game-based platforms to help surgeons learning on virtual patients). If it works for humans, why not machines? Companies and research labs have been developing a multitude of simulation frameworks covering various applications (robotics, autonomous driving, surveillance, and so on).</p>
<p>In these virtual environments, people can train and test their models. At each time step, the models receive some visual inputs from the environments, which they can use to take further action, affecting the simulation, and so on (this kind of interactive training is actually central to <em>reinforcement learning</em> as mentioned in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>).</p>
<p>Synthetic datasets and virtual environments are used to compensate for the lack of real training data or to avoid the consequences of directly applying immature solutions to complex or dangerous situations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating synthetic images from 3D models</h1>
                </header>
            
            <article>
                
<p><strong>Computer graphics</strong> is a vast and fascinating domain by itself. In the following paragraphs, we will simply point out some useful tools and ready-to-use frameworks for those in need of rendering data for their applications.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rendering from 3D models</h1>
                </header>
            
            <article>
                
<p class="mce-root">Generating images from 3D models is a complex, multi-step process. Most 3D models are represented by a <em>mesh</em>, a set of small <em>faces</em> (usually triangles) delimited by <em>vertices</em> (that is, points in the 3D space) representing the model's surface. Some models also contain some <em>texture or color information</em>, indicating which color each vertex or small surface should be. Finally, models can be placed into a larger 3D scene (translated/rotated). Given a virtual camera defined by its <strong>intrinsic parameters</strong> (such as its focal length and principal point) and its own pose in the 3D scene, the task is to render what the camera sees of the scene. This procedure is presented in a simplified manner in the following <em>Figure 7-6</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/68241f0c-18fb-4cf0-8240-9c05f503c504.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-6: Simplistic representation of a 3D rendering pipeline (3D models are from the LineMOD dataset—http://campar.in.tum.de/Main/StefanHinterstoisser)</div>
<p class="mce-root">Converting a 3D scene into a 2D image thus implies multiple transformations, projecting the faces of each model from 3D coordinates relative to the object to coordinates relative to the whole scene (world coordinates), then, relative to the camera (camera coordinates), and finally to 2D coordinates relative to the image space (image coordinates). All these projections can be expressed as direct <em>matrix multiplications</em>, but constitute (alas) only a small part of the rendering process. Surface colors should also be properly interpolated, <em>visibility</em> should be respected (elements occluded by others should not be drawn), realistic light effects should be applied (for instance, illumination, reflection, and refraction), and so on.</p>
<p class="mce-root">Operations are numerous and computationally heavy. Thankfully for us, GPUs were originally built to efficiently perform them, and frameworks such as <em>OpenGL</em> (<a href="https://www.opengl.org">https://www.opengl.org</a>) have been developed to help interface with the GPUs for computer graphics (for instance, to load vertices/faces in the GPUs as <em>buffers</em>, or to define programs named <em>shaders</em> to specify how to project and color scenes) and streamline some of the process.</p>
<p class="mce-root"/>
<p>Most of the modern computer languages offer libraries built on top of <em>OpenGL</em>, such as <kbd>PyOpenGL</kbd> (<a href="http://pyopengl.sourceforge.net">http://pyopengl.sourceforge.net</a>) or the object-oriented <kbd>vispy</kbd> (<a href="http://vispy.org">http://vispy.org</a>) for Python. Applications such as <em>Blender</em> (<a href="https://www.blender.org">https://www.blender.org</a>) provide graphical interfaces to also build and render 3D scenes. While it requires some effort to master all these tools, they are extremely versatile and can be immensely helpful to render any kind of synthetic data.</p>
<p>However, it is good to keep in mind that, as we previously mentioned, labs and companies have been sharing many higher-level frameworks to render synthetic datasets specifically for machine learning applications. For example, Michael Gschwandtner and Roland Kwitt from the University of Salzburg developed <em>BlenSor</em> (<a href="https://www.blensor.org">https://www.blensor.org</a>), a Blender-based application to simulate all kinds of sensors (<em>BlenSor: blender sensor simulation toolbox</em>, Springer, 2011); more recently, <span>Simon Brodeur</span> and a group of researchers from various backgrounds shared the <em>HoME-Platform</em>, simulating a variety of indoor environments for intelligent systems (<em>HoME: A household multimodal environment</em>, ArXiv, 2017).</p>
<p>When manually setting up a complete rendering pipeline or using a specific simulation system, in both cases, the end goal is to render a large amount of training data with ground truths and enough variation (viewpoints, lighting conditions, textures, and more).</p>
<div class="packt_infobox">To better illustrate these notions, a complete notebook is dedicated to rendering synthetic datasets from 3D models, briefly covering concepts such as <em>3D meshes</em>, <em>shaders</em>, and <em>view matrices</em>. A simple renderer is implemented using <kbd>vispy</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Post-processing synthetic images</h1>
                </header>
            
            <article>
                
<p>While 3D models of target objects are often available in industrial contexts, it is rare to have a 3D representation of the environments they will be found in (for instance, a 3D model of the industrial plant). The 3D objects/scenes then appear isolated, with no proper background. But, like any other visual content, if models are not trained to deal with background/clutter, they won't be able to perform properly once confronted with real images. Therefore, it is common for researchers to post-process synthetic images, for instance, to merge them with relevant background pictures (replacing the blank background with pixel values from images of related environments).</p>
<p>While some augmentation operations could be taken care of by the rendering pipeline (such as brightness changes or motion blur), other 2D transformations are still commonly applied to synthetic data during training. This additional post-processing is once again done to reduce the risk of overfitting and to increase the robustness of the models.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox">In May 2019, <strong>TensorFlow Graphics</strong> was released. This module provides a computer graphics pipeline to generate images from 3D models. Because this rendering pipeline is composed of novel differentiable operations, it can be tightly combined with—or integrated into—NNs (these graphics operations are differentiable, so the training loss can be backpropagated through them, like any other NN layer). With more and more features being added to TensorFlow Graphics (such as 3D visualization add-ons for TensorBoard and additional rendering options), it will certainly become a central component of solutions dealing with 3D applications or applications relying on synthetic training data. More information, as well as detailed tutorials, can be found in the related GitHub repository (<a href="https://github.com/tensorflow/graphics">https://github.com/tensorflow/graphics</a>).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem – realism gap</h1>
                </header>
            
            <article>
                
<p>Though rendering synthetic images has enabled a variety of computer vision applications, it is, however, not the perfect remedy for data scarcity (or at least not yet). While computer graphics frameworks can nowadays render hyper-realistic images, they <em>need detailed 3D models</em> for that (with precise surfaces and high-quality texture information). Gathering the data to build such models is as <em>expensive</em> as—if not more than—directly building a dataset of real images for the target objects.</p>
<p>Because 3D models sometimes have simplified geometries or lack texture-related information, realistic synthetic datasets are not that common. This <strong>realism gap</strong> between the rendered training data and the real target images <em>harms the performance</em> of the models. The visual cues they have learned to rely on while training on synthetic data may not appear in real images (which may have differently saturated colors, more complex textures or surfaces, and so on).</p>
<div class="packt_infobox">Even when the 3D models are properly depicting the original objects, it often happens that the appearance of these objects changes over time (for instance, from wear and tear).</div>
<p>Currently, a lot of effort is being devoted to tackling the realism gap for computer vision. While some experts are working on building more realistic 3D databases or developing more advanced simulation tools, others are coming up with new machine learning models that are able to transfer the knowledge they acquired from synthetic environments to real situations. The latter approach will be the topic of this chapter's final subsection.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Leveraging domain adaptation and generative models (VAEs and GANs)</h1>
                </header>
            
            <article>
                
<p><strong>Domain adaptation</strong> methods were briefly mentioned in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>, among transfer learning strategies. Their goal is to transpose the knowledge acquired by models from one <em>source domain</em> (that is, one data distribution) to another <em>target domain</em>. Resulting models should be able to properly recognize samples from the new distribution, even if they were not directly trained on it. This fits scenarios when training samples from the target domain are unavailable, but other related datasets are considered as training substitutes.</p>
<p>Suppose we want to train a model to classify household tools in real scenes, but we only have access to uncluttered product pictures provided by the manufacturers. Without domain adaptation, models trained on these advertising pictures will not perform properly on target images with actual clutter, poor lighting, and other discrepancies.</p>
<p>Training recognition models on synthetic data so that they can be applied to real images has also become a common application for domain adaptation methods. Indeed, synthetic images and real pictures of the same semantic content can be considered as two different data distributions, that is, two domains with different levels of detail, noise, and so on.</p>
<p>In this section, we will consider the following two different flavors of approaches:</p>
<ul>
<li>Domain adaptation methods that aim to train models so that they perform indifferently on the source and target domains</li>
<li>Methods for adapting the training images to make them more similar to the target images</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training models to be robust to domain changes</h1>
                </header>
            
            <article>
                
<p class="mce-root">A first approach to domain adaptation is to encourage the models to focus on robust features, which can be found in both the source and target domains. Multiple solutions following this approach have been proposed, contingent on the availability of target data during training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised domain adaptation</h1>
                </header>
            
            <article>
                
<p>Sometimes, you may be lucky enough to have access to some pictures from the target domain and relevant annotations, besides a larger source dataset (for instance, of synthetic images). This is typically the case in industry, where companies have to find a compromise between the high cost of gathering enough target images to train recognition models, and the performance drop they would experience if models are taught on synthetic data only.</p>
<p>Thankfully, multiple studies have demonstrated that adding even a small number of target samples to training sets can boost the final performance of the algorithms. The following two main reasons are usually put forward:</p>
<ul>
<li>Even if scarce, this provides the models with some information on the target domain. To minimize their training loss over all samples, the networks will have to learn how to process this handful of added images (this can even be accentuated by weighing the loss more for these images).</li>
<li>Since source and target distributions are, by definition, different, mixed datasets display <em>greater visual variability</em>. As previously explained, models will have to learn more robust features, which can be beneficial once applied to target images only (for example, models become better prepared to deal with varied data, and thus better prepared for whatever the target image distribution is).</li>
</ul>
<p>A direct parallel can also be made with the transfer learning methods we explored in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em> (training models first on a large source dataset, and then fine-tuning them on the smaller target training set). As mentioned then, the closer the source data is to the target domain, the more efficient such a training scheme becomes—and the other way around (in a Jupyter Notebook, we highlight these limitations, training our segmentation model for self-driving cars on synthetic images too far removed from the target distribution).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised domain adaptation</h1>
                </header>
            
            <article>
                
<p class="mce-root">When preparing training datasets, gathering images is often not the main problem. But properly annotating these images is, as it is a tedious and therefore costly procedure. Plenty of domain adaptation methods are thus targeting these scenarios when only source images, their corresponding annotations, and target images are available. With no ground truth, these target samples cannot be directly used to train the models in the usual <em>supervised</em> manner. Instead, researchers have been exploring <em>unsupervised</em> schemes to take advantage of the visual information these images still provide of the target domain.</p>
<p>For example, works such as <em>Learning Transferable Features with Deep Adaptation Networks</em>, by Mingsheng Long et al. (from Tsinghua University, China) are adding constraints to some layers of the models, so that the feature maps they generate have the same distribution, whichever domain the input images belong to. The training scheme proposed by this flavor of approach can be oversimplified as the following:</p>
<ol>
<li>For several iterations, train the model on source batches in a supervised manner.</li>
<li>Once in a while, feed the training set to the model and compute the distribution (for instance, mean and variance) of the feature maps generated by the layers we want to adapt.</li>
<li>Similarly, feed the set of target images to the model and compute the distribution of the resulting feature maps.</li>
<li>Optimize each layer to reduce the distance between the two distributions.</li>
<li>Repeat the whole process until you achieve convergence.</li>
</ol>
<p>Without the need for target labels, these solutions force the networks to learn features that can transfer to both domains while the networks are trained on the source data (the constraints are usually added to the last convolutional layers in charge of feature extraction, as the first ones are often generic enough already).</p>
<p class="mce-root">Other methods are taking into account an implicit label always available in these training scenarios—the domain each image belongs to (that is, <em>source</em> or <em>target</em>). This information can be used to train a supervised binary classifier—given an image or feature volume, its task is to predict whether it comes from the source or target domain. This secondary model can be trained along with the main one, to guide it toward extracting features that could belong to any of the two domains.</p>
<p>For example, in their <strong><span>Domain-Adversarial Neural Networks</span></strong> <span>(<strong>DANN</strong>)</span> <span>paper</span> <span>(published in JMLR, 2016), Hana Ajakan, Yaroslav Ganin, et al. (from Skoltech) proposed adding a secondary head to the models to train (right after their feature extraction layers) whose t</span>ask is to identify the domain of the input data (binary classification). The training then proceeds as follows (once again, we simplify):</p>
<ol>
<li>Generate a batch of source images and their task-related ground truths to train the main network on it (normal feed-forwarding and backpropagation through the main branch).</li>
<li>Generate a batch mixing source and target images with their domain labels and feed <span>it</span> forward through the feature extractor and the secondary branch, which tries to predict the correct domain for each input <em>(source</em> or <em>target</em>).</li>
</ol>
<p> </p>
<ol start="3">
<li>Backpropagate the domain classification loss normally through the layers of the secondary branch, but then <em>reverse the gradient</em> before backpropagating through the feature extractor.</li>
<li>Repeat the whole process until convergence, that is, until the main network can perform its task as expected, whereas the domain classification branch can no longer properly predict the domains.</li>
</ol>
<p>This training procedure is illustrated in <em>Figure 7-7</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b1deb4e7-a851-4237-9cfa-93e3a30535c8.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-7: DANN concept applied to the training of a classifier</div>
<div class="packt_tip">With proper control of the data flow or weighting of the main loss, the three steps can be executed at once in a single iteration. This is demonstrated in the Jupyter Notebook we dedicate to this method.</div>
<p>This scheme got a lot of attention for its cleverness. By <em>reversing</em> the gradient from the domain classification loss (that is, multiplying it by <em>-1</em>) before propagating it through the feature extractor, its layers will <em>learn to maximize this loss</em>, not to <em>minimize</em> it. This method is called <strong>adversarial</strong> because the secondary head will keep trying to properly predict the domains, while the upstream feature extractor will learn to <em>confuse</em> it. Concretely, this leads the feature extractor to learn features that <em>cannot</em> be used to <em>discriminate</em> the domains of the input images but are useful to the network's main task (since the normal training of the main head is done in parallel). After training, the domain classification head can simply be discarded.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Note that with TensorFlow 2, it is quite straightforward to manipulate the gradients of specific operations. This can be done by applying the <kbd>@tf.custom_gradient</kbd> decorator (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/custom_gradient">https://www.tensorflow.org/api_docs/python/tf/custom_gradient</a>) to functions and by providing the custom gradient operations. Doing so, we can implement the following operation for <em>DANN</em>, to be called after the feature extractor and before the domain classification layers in order to reverse the gradient at that point during backpropagation:</p>
<pre class="mce-root"># This decorator specifies the method has a custom gradient. Along with its normal output, the method should return the function to compute its gradient:<br/>@tf.custom_gradient <br/>def reverse_gradient(x): # Flip the gradient's sign.<br/>    y = tf.identity(x) # the value of the tensor itself isn't changed<br/>    return y, lambda dy: tf.math.negative(dy) # output + gradient method</pre>
<p>Since <em>DANN</em>, a multitude of other domain adaptation methods have been released (for instance, <em>ADDA</em> and <em>CyCaDa</em>), following similar adversarial schemes.</p>
<div class="packt_infobox">In some cases, annotations for the target images are available, but not with the desired <em>density</em> (for instance, with only image-level class labels when the target task is pixel-level semantic segmentation). <strong>Auto-labeling methods</strong> have been proposed for such scenarios. For example, guided by the sparse labels, the models trained on source data are used to predict the denser labels of the target training images. Then, these source labels are added to the training set to refine the models. This process is repeated iteratively until the target labels look correct enough and the models trained on the mixed data have converged.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Domain randomization</h1>
                </header>
            
            <article>
                
<p>Finally, it may happen that <em>no target data is available</em> at all for training (no image, no annotation). The performance of the models then relies entirely on the relevance of the source dataset (for instance, how realistic looking and relevant to the task the rendered synthetic images are).</p>
<p>Pushing the concept of data augmentation for synthetic images to the extreme, <em>domain randomization</em> can also be considered. Mostly explored by industrial experts, the idea is to train models on large data variations (as described in <em>Domain randomization for transferring deep neural networks from simulation to the real world</em>, IEEE, 2017). For example, if we only have access to 3D models of the objects we want the networks to recognize, but we do not know in what kind of scenes these objects may appear, we could use a 3D simulation engine to generate images with a significant number of <em>random</em> backgrounds, lights, scene layouts, and so on. <span>The claim is that with</span> <span>enough variability in the simulation, real data may appear</span> <span>just as another variation to the models.</span> As long as the target domain somehow overlaps the randomized training one, the networks would not be completely clueless after training.</p>
<div class="packt_tip">Obviously, we cannot expect such NNs to perform as well as any trained on target samples, but domain randomization is a fair solution to desperate situations.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating larger or more realistic datasets with VAEs and GANs</h1>
                </header>
            
            <article>
                
<p class="mce-root">The second main type of domain adaptation methods we will cover in this chapter will give us the opportunity to introduce what many call the most interesting development in machine learning these past years—generative models, and, more precisely <em>VAEs</em> and <em>GANs</em>. Highly popular since they were proposed, these models have been incorporated into a large variety of solutions. Therefore, we will confine ourselves here to a generic introduction, before presenting how these models are applied to dataset generation and domain adaptation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminative versus generative models</h1>
                </header>
            
            <article>
                
<p>So far, most of the models we have been studying are <strong>discriminative</strong>. Given an input, <em>x</em>, they learn the proper parameters, <em>W</em>, in order to return/discriminate the correct label, <em>y</em>, out of those considered (for instance, <em>x</em> may be an input image and <em>y</em> may be the image class label). A discriminative model can be interpreted as a <em>function f(x ; W) = y</em>. They can also be interpreted as models trying to learn the <em>conditional probability distribution,</em> <em>p</em>(<em>y</em>|<em>x</em>) (meaning <em>the probability of y given x</em>; for instance<em>,</em> given a specific picture <em>x</em>, what is the probability that its label is <em>y</em> = "<em>cat picture</em>"?).</p>
<p class="mce-root"/>
<p>There is a second category of models we have yet to introduce—<em>generative</em> models. Given some samples, <em>x</em>, drawn from an unknown probability distribution, <em>p</em>(<em>x</em>), generative models are trying to <em>model this distribution</em>. For example, given some images, <em>x</em>, representing cats, a generative model will attempt to infer the data distribution (what makes these cat pictures, out of all possible pixel combinations) in order to generate new cat images that could belong to the same set as <em>x</em>.</p>
<p>In other words, a discriminative model learns to recognize a picture based on specific features (for instance, it is probably a cat picture because it depicts something with whiskers, paws, and a tail). A generative model learns to sample new images from the input domain, reproducing its typical features (for instance, here is a plausible new cat picture, obtained by generating and combining typical cat features).</p>
<p>As functions, generative CNNs need an input they can process into a new picture. Oftentimes, they are <em>conditioned by a noise vector</em>, that is, a tensor, <em>z</em>, sampled from a random distribution (such as <img class="fm-editor-equation" src="assets/7a246ff9-7a7c-4048-9ee3-e3b1c3f6d26a.png" style="width:5.25em;height:1.33em;"/>, meaning <em>z</em> is randomly sampled from a normal distribution of mean <img class="fm-editor-equation" src="assets/c9fb8172-71ed-4c36-a993-f3cd685595fe.png" style="width:2.75em;height:1.17em;"/> and standard deviation <img class="fm-editor-equation" src="assets/32ef4265-9515-4bbb-9313-e7620826e376.png" style="width:2.42em;height:0.83em;"/>). For each random input they receive, the models provide a new image from the distribution they learned to model. When available, generative networks can also be conditioned by the labels, <em>y</em>. In such cases, they have to model the conditional distribution, <em>p</em>(<em>x</em>|<em>y</em>) (for instance, considering the label <em>y</em> = "<em>cat</em>", what is the probability of sampling the specific image <em>x</em>)?</p>
<div class="packt_infobox">According to the majority of experts, generative models hold the key to the next stage of machine learning. To be able to generate a large and varied amount of new data despite their limited number of parameters, networks have to distill the dataset to uncover its structure and key features. They have to <em>understand</em> the data.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VAEs</h1>
                </header>
            
            <article>
                
<p class="mce-root">While auto-encoders can also learn some aspects of a data distribution, their goal is only to reconstruct encoded samples, that is, to <em>discriminate</em> the original image out of all possible pixel combinations, based on the encoded features. Standard auto-encoders are not meant to <em>generate</em> new samples. If we randomly sample a <em>code</em> vector from their latent space, chances are high that we will obtain a gibberish image out of their decoder. This is because their latent space is unconstrained and typically <em>not continuous</em> (that is, there are usually large regions in the latent space that are not corresponding to any valid image).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"><strong>Variational auto-encoders</strong> (<strong>VAEs</strong>) are particular auto-encoders designed to have continuous latent space, and they are therefore used as generative models. Instead of directly extracting the code corresponding to an image, <em>x</em>, the encoder of a VAE is tasked to provide a simplified estimation of the distribution in the latent space that the image belongs to.</p>
<p class="mce-root">Typically, the encoder is built to return two vectors, respectively representing the mean, <img class="fm-editor-equation" src="assets/accc4133-bc87-4d2f-beb6-a330201bdb97.png" style="width:3.33em;height:1.17em;"/>, and the standard deviation, <img class="fm-editor-equation" src="assets/9d724ce2-f5ff-4e06-af6b-d6f96bd95f60.png" style="width:3.08em;height:0.92em;"/>, of a multivariate normal distribution (for an <em>n</em>-dimensional latent space). Figuratively speaking, the mean represents the <em>most likely</em> position of the image in the latent space, and the standard deviation controls the size of the circular area, around that position, where the image <em>could also be</em>. From this distribution defined by the encoder, a random code, <em>z</em>, is picked and passed to the decoder. The decoder's task is then to recover image <em>x</em> based on <em>z</em>. Since <em>z</em> can slightly vary for the same image, the decoder has to learn to deal with these variations to return the input image.</p>
<p class="mce-root">To illustrate the differences between them, auto-encoders and VAEs are depicted side by side in <em>Figure 7-8</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4a7a545-0b37-4b43-bfae-9891708b12a4.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7-8: Comparison of standard auto-encoders and variational ones</div>
<div class="packt_infobox">Gradients cannot flow back through random sampling operations. To be able to backpropagate the loss through the encoder despite the sampling of <em>z</em>, a <strong>reparameterization trick</strong> is used. Instead of directly sampling <img class="fm-editor-equation" src="assets/7beb4539-c3b2-466a-90c7-4688a35e79f4.png" style="width:4.50em;height:1.00em;"/>, this operation is approximated by <img class="fm-editor-equation" src="assets/5af5992a-0698-4e75-be89-cd9e7f50832d.png" style="width:5.25em;height:1.08em;"/>, with <img class="fm-editor-equation" src="assets/cc41dbfb-9fb3-4e08-bbd5-8840150e7c3d.png" style="width:3.92em;height:1.00em;"/><span>. This way, <em>z</em> can be obtained through a derivable operation, considering <img class="fm-editor-equation" src="assets/f54a9975-144a-4880-a099-4cb1a2a96e7a.png" style="width:0.58em;height:0.83em;"/> as a random vector passed as an additional input to the model.</span></div>
<p class="mce-root">During training, a loss—usually the <strong>mean-squared error</strong> (<strong>MSE</strong>)—measures how similar the output image is to the input one, as we do for standard auto-encoders. However, another loss is added to VAE models, to make sure the distribution estimated by their encoder is well-defined. Without this constraint, the VAEs could otherwise end up behaving like normal auto-encoders, returning <img class="fm-editor-equation" src="assets/f91213f6-448c-4269-a794-3bfc73c58928.png" style="width:0.83em;height:0.83em;"/> null and <img class="fm-editor-equation" src="assets/5a0c2520-c3d2-44ad-9a50-a6fbd79e38b9.png" style="width:0.83em;height:1.08em;"/> as the images' code. This second loss is based on the <strong>Kullback–Leibler divergence</strong> (named after its creators and usually contracted to <em>KL divergence</em>). The KL divergence measures the difference between two probability distributions. It is adapted into a loss, to ensure that the distributions defined by the encoder are close enough to the standard normal distribution, <img class="fm-editor-equation" src="assets/626ed271-78e9-497e-b49f-80420808668a.png" style="width:3.00em;height:1.17em;"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6b4bceea-891d-4683-a149-7237c8cb8180.png" style="width:23.17em;height:2.25em;"/></p>
<p>With this reparameterization trick and KL divergence, auto-encoders become powerful generative models. Once the models are trained, their encoders can be discarded, and their decoders can be directly used to generate new images, given random vectors, <img class="fm-editor-equation" src="assets/1e481d38-a9c2-48fb-978f-aa41debe27a4.png" style="width:4.92em;height:1.25em;"/>, as inputs. For example, <em>Figure 7-9</em> shows a grid of results for a simple convolutional VAE with a latent space of dimension <em>n = 2</em>, trained to generate MNIST-like images (additional details and source code are available as a Jupyter Notebook):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fe38d124-f850-4285-9f97-7a16fdc2aa1a.png" style="width:24.50em;height:24.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7-9: Grid of images generated by a simple VAE trained to create MNIST-like results</div>
<p>To generate this grid, the different vectors, <em>z</em>, are not randomly picked, but are sampled to homogeneously cover part of the 2D latent space, hence the grid figure that shows the output images for <em>z</em> varying from (<em>-1.5, -1.5</em>) to (<em>1.5, 1.5</em>). We can thus observe the continuity of the latent space, with the content of the resulting images varying from one digit to another.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GANs</h1>
                </header>
            
            <article>
                
<p>First proposed in 2014 by Ian Goodfellow et al. from the University of Montreal, GANs are certainly the most popular solution for generative tasks.</p>
<p>As their name indicates, GANs use an adversarial scheme so they can be trained in an unsupervised manner (this scheme inspired the <em>DANN</em> method introduced earlier in this chapter). Having only a number of images, <em>x</em>, we want to train a <em>generator</em> network to model <em>p</em>(<em>x</em>), that is, to create new valid images. We thus have no proper ground truth data to directly compare the new images with (since they are <em>new</em>). Not able to use a typical loss function, we pit the generator against another network—the <strong>discriminator</strong>.</p>
<p>The discriminator's task is to evaluate whether an image comes from the original dataset (<em>real</em> image) or if it was generated by the other network (<em>fake</em> image). Like the domain discriminating head in <em>DANN</em>, the discriminator is trained in a supervised manner as a binary classifier using the implicit image labels (<em>real</em> versus <em>fake</em>). Playing against the discriminator, the generator tries to fool it, generating new images conditioned by noise vectors, <em>z</em>, so the discriminator believes they are <em>real</em> images (that is, sampled from <em>p(x)</em>).</p>
<p>When the discriminator predicts the binary class of generated images, its results are backpropagated all the way into the generator. The generator thus learns purely from the <em>discriminator's feedback</em>. For example, if the discriminator learns to check whether an image contains whiskers to label it as <em>real</em> (if we want to create cat images), then the generator will receive this feedback from backpropagation and learn to draw whiskers (even though only the discriminator was fed with actual cat images!). <em>Figure 7-10</em> illustrates the <span>concept of</span> GANs with the generation of handwritten digit images:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6bd33ea0-a99a-497f-bb34-9dfbcb576cc3.png" style="width:35.33em;height:15.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7-10: GAN representation</div>
<p>GANs were inspired by <em>game theory</em>, and their training can be interpreted as a <em>two-player zero-sum minimax game</em><span>. Each phase of the game (that is, each training iteration) takes place as follows:</span></p>
<ol>
<li>The generator, <em>G</em>, receives <em>N</em> noise vectors<span>,</span> <em>z<span>,</span></em> and outputs as many images<span>,</span> <em>x<sub>G</sub></em><sub>.</sub></li>
<li>These 𝑁 <em>fake</em> images are mixed with <em>N</em> <em>real</em> images<span>,</span> <em>x<span>,</span></em> picked from the training set.</li>
<li>The discriminator<span>,</span> <em>D<span>,</span></em> is trained on this mixed batch, trying to estimate which images are <em>real</em> and which are <em>fake</em>.</li>
<li>The generator<span>,</span> <em>G<span>,</span></em> is trained on another batch of <em>N</em> noise vectors, trying to generate images so that <em>D</em> assumes they are real.</li>
</ol>
<p><span>Therefore, at each iteration, the discriminator, <em>D</em> (parameterized by <em>P</em><sub><em>D</em></sub>), tries to maximize the game reward, <em>V</em>(<em>G</em>, <em>D</em>), while the generator, <em>G</em> (parameterized by <em>P</em><sub><em>G</em></sub>), tries to minimize it:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/79233327-5b4e-4320-94aa-b844811db30d.png" style="width:37.50em;height:2.00em;"/></p>
<p>Note that this equation assumes that the label <em>real</em> is 1 and the label <em>fake</em> is 0. The first term of <span><em>V(G, D</em>)</span> represents the averaged log probability estimated by the discriminator<span>,</span> <em>D<span>,</span></em> that the images<span>,</span> <em>x<span>,</span></em> are <em>real</em> (<em>D</em> should return 1 for each). Its second term represents the averaged log probability estimated by <em>D</em> that the generator's outputs are <em>fake</em> (<em>D</em> should return 0 for each). Therefore, this reward<span>,</span> <span><em>V(G, D</em>),</span> is used to train the discriminator<span>,</span> <em>D<span>,</span></em> as a classification metric that <em>D</em> has to maximize (although in practice, people rather train the network to minimize -<span><em>V(G, D</em>)</span>, out of habit for decreasing losses).</p>
<p>Theoretically, <span><em>V(G, D</em>)</span> should also be used to train the generator<span>,</span> <em>G</em>, as a value to minimize this time. However, the gradient of its second term would <em>vanish</em> toward 0 if <em>D</em> becomes too confident (and the derivative of the first term with respect to <span><em>P</em><sub><em>G</em></sub></span> is always null, since <span><em>P</em><sub><em>G</em></sub></span> does not play any role in it). This vanishing gradient can be avoided with a small mathematical change, using instead the following loss to train <em>G</em><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/10e48567-7e5d-4dcd-8524-507b3d6405f7.png" style="width:16.50em;height:1.50em;"/></p>
<p>According to game theory, the outcome of this <em>mini</em><em>max</em> <em>game</em> is an <em>equilibrium</em> between <em>G</em> and <em>D</em> (called <strong>Nash equilibrium</strong>, after the mathematician John Forbes Nash Jr, who defined it). Though hard to achieve in practice with GANs, the training should end with <em>D</em> unable to differentiate <em>real</em> from <em>fake</em> (that is, <em>D</em>(<em>x</em>) = <sup>1</sup>/<sub>2</sub> and <em>D(G(z))</em> = <sup>1</sup>/<sub>2</sub> for all samples) and with <em>G</em> modeling the target distribution, <em>p(x).</em></p>
<p class="mce-root">Though difficult to train, GANs can lead to highly realistic results and are therefore commonly used to generate new data samples (GANs can be applied to any data modality: image, video, speech, text, and more.)</p>
<div class="packt_infobox">While VAEs are easier to train, GANs usually return crisper results. Using the MSE to evaluate the generated images, VAE results can be slightly blurry, as the models tend to return averaged images to minimize this loss. Generators in GANs cannot cheat this way, as the discriminators would easily spot blurry images as <em>fake</em>. Both VAEs and GANs can be used to generate larger training datasets for image-level recognition (for instance, preparing one GAN to create new <em>dog</em> images and another to create new <em>cat</em> images, to train a <em>dog</em> versus <em>cat</em> classifier on a larger dataset).<br/>
<br/>
Both VAEs and GANs are implemented in the Jupyter Notebooks provided.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Augmenting datasets with conditional GANs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another great advantage GANs have is that they can be conditioned by any kind of data. <strong>Conditional GANs</strong> (<strong>cGANs</strong>) can be trained to model the <em>conditional distribution, p(x|y),</em> that is<span>,</span> to generate images conditioned by a set of input values<span>,</span> <em>y</em> (refer to the introduction to generative models). The conditional input<span>,</span> <em>y<span>,</span></em> can be an image, a categorical or continuous label, a noise vector, and more, or any combination of those.</p>
<p>In conditional GANs, the discriminator is edited to receive both an image<span>,</span> <em>x</em> (real or fake)<span>,</span> and its corresponding conditional variable<span>,</span> <em>y<span>,</span></em> as a paired input (that is, <em>D</em>(<em>x</em>, <em>y</em>)). Though its output is still a value between <em>0</em> and <em>1</em> measuring how <em>real</em> the input seems, its task is slightly changed. To be considered as <em>real</em>, an image should not only look as if drawn from the training dataset; it should also correspond to its paired variable.</p>
<p>Imagine, for instance, that we want to train a generator<span>,</span> <em>G<span>,</span></em> to create images of handwritten digits. Such a generator would be much more useful if, instead of outputting images of random digits, it could be conditioned to output images of requested ones (that is, <em>draw an image whose</em> <em>y</em> <em>= 3</em>, with <em>y</em> the categorical digit label). If the discriminator is not given <em>y</em>, the generator would learn to generate realistic images, but with no certainty that these images would be depicting the desired digits (for instance, we could <span>receive from <em>G</em> a realistic image of a <em>5</em> instead of a <em>3</em>). Giving the conditioning information to <em>D</em>, this network would immediately spot a fake image that does not correspond to its <em>y</em>, forcing <em>G</em> to effectively model <em>p(x|y).</em></span></p>
<p>The <em>Pix2Pix</em> model by Phillip Isola and others from Berkeley AI Research is a famous image-to-image conditional GAN (that is, with <em>y</em> being an image), demonstrated on several tasks, such as converting hand-drawn sketches into pictures, semantic labels into actual pictures, and more (<em>Image-to-image translation with conditional adversarial networks</em>, IEEE, 2017). While <em>Pix2Pix</em> works best in supervised contexts, when the target images were made available to add an MSE loss to the GAN objective, more recent solutions removed this constraint. This is, for instance, the case of <em>CycleGAN</em>, by Jun-Yan Zhu et al. from Berkeley AI Research (published by IEEE in 2017, in collaboration with the <em>Pix2Pix</em> authors) or <em>PixelDA</em> by Konstantinos Bousmalis and colleagues from Google Brain (<em>Unsupervised pixel-level domain adaptation with generative adversarial networks</em>, IEEE, 2017).</p>
<p>Like other recent conditional GANs, <em>PixelDA</em> can be used as a domain adaptation method, to map training images from the source domain to the target domain. For example, the <em>PixelDA</em> generator can be applied to generating realistic-looking versions of synthetic images, learning from a small set of unlabeled real images. It can thus be used to augment synthetic datasets so that the models trained on them do not suffer as much from the realism gap.</p>
<p>Though mostly known for their artistic applications (GAN-generated portraits are already being exhibited in many art galleries), generative models are powerful tools that, in the long term, could become central to the understanding of complex datasets. But nowadays, they are <span>already</span> <span>being used by companies to train more robust recognition models despite scarce training data.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">Although the exponential increase in computational power and the availability of larger datasets have led to the deep learning era, this certainly does not mean that best practices in data science should be ignored or that relevant datasets will be easily available for all applications.</p>
<p>In this chapter, we took a deep dive into the <kbd>tf.data</kbd> API, learning how to optimize the data flow. We then covered different, yet compatible, solutions to tackle the problem of data scarcity: data augmentation, synthetic data generation, and domain adaptation. The latter solution gave us the opportunity to present VAEs and GANs, which are powerful generative models.</p>
<p>The importance of well-defined input pipelines will be highlighted in the next chapter, as we will apply NNs to data of higher dimensionality: image sequences and videos.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Given a tensor, <kbd>a = [1, 2, 3]</kbd>, and another tensor, <kbd>b = [4, 5, 6]</kbd>, how do you build a <kbd>tf.data</kbd> pipeline that would output each value separately, from <kbd>1</kbd> to <kbd>6</kbd>?</li>
<li>According to the documentation of <kbd>tf.data.Options</kbd>, how do you make sure that a dataset always returns samples in the same order, run after run?</li>
<li>Which domain adaptation methods that we introduced can be used when no target annotations are available for training?</li>
<li>What role does the discriminator play in GANs?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Learn OpenGL</em> (<a href="https://www.packtpub.com/game-development/learn-opengl">https://www.packtpub.com/game-development/learn-opengl</a>), by Frahaan Hussain: For readers interested in computer graphics and eager to learn how to use OpenCV, this book is a nice place to start.</li>
<li class="mce-root"><em>Hands-On Artificial Intelligence for Beginners</em> (<a href="https://www.packtpub.com/big-data-and-business-intelligence/hands-artificial-intelligence-beginners">https://www.packtpub.com/big-data-and-business-intelligence/hands-artificial-intelligence-beginners</a>), by Patrick D. Smith: Though written for TensorFlow 1, this book dedicates a complete chapter to generative networks.</li>
</ul>


            </article>

            
        </section>
    </body></html>