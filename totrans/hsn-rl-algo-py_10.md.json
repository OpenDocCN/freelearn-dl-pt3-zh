["```\n---------------------------------------------------------------------------------\nDDPG Algorithm\n---------------------------------------------------------------------------------\n\nInitialize online networks  and \nInitialize target networks  and  with the same weights as the online networks\nInitialize empty replay buffer \nInitialize environment \n\nfor  do\n    > Run an episode\n    while not d:\n\n        > Store the transition in the buffer\n\n        > Sample a minibatch \n\n        > Calculate the target value for every i in b\n         (8.4)\n\n        > Update the critic \n         (8.5)\n\n        > Update the policy\n         (8.6)\n\n        > Targets update\n\nif :\n\n```", "```\ndef deterministic_actor_critic(x, a, hidden_sizes, act_dim, max_act):\n    with tf.variable_scope('p_mlp'):\n        p_means = max_act * mlp(x, hidden_sizes, act_dim, last_activation=tf.tanh)\n    with tf.variable_scope('q_mlp'):\n        q_d = mlp(tf.concat([x,p_means], axis=-1), hidden_sizes, 1, last_activation=None)\n    with tf.variable_scope('q_mlp', reuse=True): # reuse the weights\n        q_a = mlp(tf.concat([x,a], axis=-1), hidden_sizes, 1, last_activation=None)\n    return p_means, tf.squeeze(q_d), tf.squeeze(q_a)\n```", "```\nobs_dim = env.observation_space.shape\nact_dim = env.action_space.shape\n\nobs_ph = tf.placeholder(shape=(None, obs_dim[0]), dtype=tf.float32, name='obs')\nact_ph = tf.placeholder(shape=(None, act_dim[0]), dtype=tf.float32, name='act')\ny_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='y')\n```", "```\nwith tf.variable_scope('online'):\n    p_onl, qd_onl, qa_onl = deterministic_actor_critic(obs_ph, act_ph, hidden_sizes, act_dim[0], np.max(env.action_space.high))\n\nwith tf.variable_scope('target'):\n    _, qd_tar, _ = deterministic_actor_critic(obs_ph, act_ph, hidden_sizes, act_dim[0], np.max(env.action_space.high))\n```", "```\nq_loss = tf.reduce_mean((qa_onl - y_ph)**2)\n```", "```\nq_opt = tf.train.AdamOptimizer(cr_lr).minimize(q_loss)\n```", "```\np_loss = -tf.reduce_mean(qd_onl)\n```", "```\np_opt = tf.train.AdamOptimizer(ac_lr).minimize(p_loss, var_list=variables_in_scope('online/p_mlp'))\n```", "```\ndef variables_in_scope(scope):\n    return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope)\n```", "```\nupdate_target = [target_var.assign(tau*online_var + (1-tau)*target_var) for target_var, online_var in zip(variables_in_scope('target'), variables_in_scope('online'))]\nupdate_target_op = tf.group(*update_target)\n```", "```\n    ... \n\n    mb_obs, mb_rew, mb_act, mb_obs2, mb_done = buffer.sample_minibatch(batch_size)\n\n    q_target_mb = sess.run(qd_tar, feed_dict={obs_ph:mb_obs2})\n    y_r = np.array(mb_rew) + discount*(1-np.array(mb_done))*q_target_mb\n\n    _, q_train_loss = sess.run([q_opt, q_loss], feed_dict={obs_ph:mb_obs, y_ph:y_r, act_ph: mb_act})\n\n    _, p_train_loss = sess.run([p_opt, p_loss], feed_dict={obs_ph:mb_obs})\n\n    sess.run(update_target_op)\n\n     ...\n\n```", "```\ndef deterministic_actor_double_critic(x, a, hidden_sizes, act_dim, max_act):\n    with tf.variable_scope('p_mlp'):\n        p_means = max_act * mlp(x, hidden_sizes, act_dim, last_activation=tf.tanh)\n\n    # First critic\n    with tf.variable_scope('q1_mlp'):\n        q1_d = mlp(tf.concat([x,p_means], axis=-1), hidden_sizes, 1, last_activation=None)\n    with tf.variable_scope('q1_mlp', reuse=True): # Use the weights of the mlp just defined\n        q1_a = mlp(tf.concat([x,a], axis=-1), hidden_sizes, 1, last_activation=None)\n\n    # Second critic\n    with tf.variable_scope('q2_mlp'):\n        q2_d = mlp(tf.concat([x,p_means], axis=-1), hidden_sizes, 1, last_activation=None)\n    with tf.variable_scope('q2_mlp', reuse=True):\n        q2_a = mlp(tf.concat([x,a], axis=-1), hidden_sizes, 1, last_activation=None)\n\n    return p_means, tf.squeeze(q1_d), tf.squeeze(q1_a), tf.squeeze(q2_d), tf.squeeze(q2_a)\n```", "```\n            ...            \n            double_actions = sess.run(p_tar, feed_dict={obs_ph:mb_obs2})\n\n            q1_target_mb, q2_target_mb = sess.run([qa1_tar,qa2_tar], feed_dict={obs_ph:mb_obs2, act_ph:double_actions})\n            q_target_mb = np.min([q1_target_mb, q2_target_mb], axis=0) \n            y_r = np.array(mb_rew) + discount*(1-np.array(mb_done))*q_target_mb\n            ..\n```", "```\n            ...\n            q1_train_loss, q2_train_loss = sess.run([q1_opt, q2_opt], feed_dict={obs_ph:mb_obs, y_ph:y_r, act_ph: mb_act})\n            ...\n\n```", "```\n            ...\n            q1_train_loss, q2_train_loss = sess.run([q1_opt, q2_opt], feed_dict={obs_ph:mb_obs, y_ph:y_r, act_ph: mb_act})\n            if step_count % policy_update_freq == 0:\n                sess.run(p_opt, feed_dict={obs_ph:mb_obs})\n                sess.run(update_target_op)\n            ...\n```", "```\ndef add_normal_noise(x, noise_scale):\n    return x + np.clip(np.random.normal(loc=0.0, scale=noise_scale, size=x.shape), -0.5, 0.5)\n```", "```\n            ...            \n            double_actions = sess.run(p_tar, feed_dict={obs_ph:mb_obs2})\n            double_noisy_actions = np.clip(add_normal_noise(double_actions, target_noise), env.action_space.low, env.action_space.high)\n\n            q1_target_mb, q2_target_mb = sess.run([qa1_tar,qa2_tar], feed_dict={obs_ph:mb_obs2, act_ph:double_noisy_actions})\n            q_target_mb = np.min([q1_target_mb, q2_target_mb], axis=0) \n            y_r = np.array(mb_rew) + discount*(1-np.array(mb_done))*q_target_mb\n            ..\n```", "```\n---------------------------------------------------------------------------------\nTD 3 Algorithm\n---------------------------------------------------------------------------------\n\nInitialize online networks  and \nInitialize target networks  and  with the same weights as the online networks\nInitialize empty replay buffer \nInitialize environment \n\nfor  do\n    > Run an episode\n    while not d:\n\n        > Store the transition in the buffer\n\n        > Sample a minibatch \n\n        > Calculate the target value for every i in b\n\n        > Update the critics \n\n        if iter % policy_update_frequency == 0:\n            > Update the policy\n\n            > Targets update\n\n if :\n\n```"]