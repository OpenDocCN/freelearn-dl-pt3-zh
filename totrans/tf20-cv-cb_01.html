<html><head></head><body>
		<div id="_idContainer014">
			<h1 id="_idParaDest-22"><em class="italic"><a id="_idTextAnchor021"/>Chapter 1</em>: Getting Started with TensorFlow 2.x for Computer Vision</h1>
			<p>One of the greatest features of TensorFlow 2.x is that it finally incorporates Keras as its high-level API. Why is this so important? While it's true that Keras and TensorFlow have had very good compatibility for a while, they have remained separate libraries with different development cycles, which causes frequent compatibility issues. Now that the relationship between these two immensely popular tools is official, they'll grow in the same direction, following a single roadmap and making the interoperability between them completely seamless. In the end, Keras is TensorFlow and TensorFlow is Keras.</p>
			<p>Perhaps the biggest advantage of this merger is that by using Keras' high-level features, we are not sacrificing performance by any means. Simply put, Keras code is production-ready!</p>
			<p>Unless the requirements of a particular project demand otherwise, in the vast majority of the recipes in this book, we'll rely on TensorFlow's Keras API. </p>
			<p>The reason behind this decision is twofold: </p>
			<ul>
				<li>Keras is easier to understand and work with.</li>
				<li>It's the encouraged way to develop using TensorFlow 2.x.</li>
			</ul>
			<p>In this chapter, we will cover the following recipes:</p>
			<ul>
				<li>Working with the basic building blocks of the Keras API</li>
				<li>Loading images using the Keras API</li>
				<li>Loading images using the tf.data.Dataset API</li>
				<li>Saving and loading a model</li>
				<li>Visualizing a model's architecture</li>
				<li>Creating a basic image classifier</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Technical requirements</h1>
			<p>For this chapter, you will need a working installation of TensorFlow 2.x. If you can access a GPU, either physical or via a cloud provider, your experience will be much more enjoyable. In each recipe, in the <em class="italic">Getting ready</em> section, you will find the specific preliminary steps and dependencies to complete it. Finally, all the code shown in this chapter is available in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch1">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch1</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/39wkpGN">https://bit.ly/39wkpGN</a>.</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Working with the basic building blocks of the Keras API</h1>
			<p>Keras is the official<a id="_idIndexMarker000"/> high-level API for TensorFlow 2.x and its use is highly encouraged for both experimental and production-ready code. Therefore, in this first recipe, we'll review the basic building blocks of Keras by creating a very simple fully connected neural network.</p>
			<p>Are you ready? Let's begin!</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Getting ready</h2>
			<p>At the most basic level, a working installation of TensorFlow 2.x is all you need. </p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>How to do it…</h2>
			<p>In the following sections, we'll go over the sequence of steps required to complete this recipe. Let's get started:</p>
			<ol>
				<li>Import the required libraries from the Keras API:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Input</p><p class="source-code">from tensorflow.keras.datasets import mnist</p><p class="source-code">from tensorflow.keras.layers import Dense</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.models import Sequential</p></li>
				<li>Create a model using the Sequential API by passing a list of layers to the Sequential constructor. The numbers in each layer correspond to the number of neurons or<a id="_idIndexMarker001"/> units it contains:<p class="source-code">layers = [Dense(256, input_shape=(28 * 28 * 1,), </p><p class="source-code">                activation='sigmoid'),</p><p class="source-code">          Dense(128, activation='sigmoid'),</p><p class="source-code">          Dense(10, activation='softmax')]</p><p class="source-code">sequential_model_list = Sequential(layers)</p></li>
				<li>Create a model using the <strong class="source-inline">add()</strong> method to add one layer at a time. The numbers in each layer correspond to the number of neurons or units it contains:<p class="source-code">sequential_model = Sequential()</p><p class="source-code">sequential_model.add(Dense(256, </p><p class="source-code">                     input_shape=(28 * 28 * 1,), </p><p class="source-code">                     activation='sigmoid'))</p><p class="source-code">sequential_model.add(Dense(128, activation='sigmoid'))</p><p class="source-code">sequential_model.add(Dense(10, activation='softmax'))</p></li>
				<li>Create a model using the Functional API. The numbers in each layer correspond to the number of neurons or units it contains:<p class="source-code">input_layer = Input(shape=(28 * 28 * 1,))</p><p class="source-code">dense_1 = Dense(256, activation='sigmoid')(input_layer)</p><p class="source-code">dense_2 = Dense(128, activation='sigmoid')(dense_1)</p><p class="source-code">predictions = Dense(10, activation='softmax')(dense_2)</p><p class="source-code">functional_model = Model(inputs=input_layer, </p><p class="source-code">                         outputs=predictions)</p></li>
				<li>Create a model using an object-oriented approach by sub-classing <strong class="source-inline">tensorflow.keras.models.Model</strong>. The numbers in each layer correspond to <a id="_idIndexMarker002"/>the number of neurons or units it contains:<p class="source-code">class ClassModel(Model):</p><p class="source-code">    def __init__(self):</p><p class="source-code">        super(ClassModel, self).__init__()</p><p class="source-code">        self.dense_1 = Dense(256, activation='sigmoid')</p><p class="source-code">        self.dense_2 = Dense(256, activation='sigmoid')</p><p class="source-code">        self.predictions = Dense(10,activation='softmax')</p><p class="source-code">    def call(self, inputs, **kwargs):</p><p class="source-code">        x = self.dense_1(inputs)</p><p class="source-code">        x = self.dense_2(x)</p><p class="source-code">  return self.predictions(x)</p><p class="source-code">class_model = ClassModel()</p></li>
				<li>Prepare the data so that we can train all the models we defined previously. We must reshape the images into vector format because that's the format that's expected by a fully connected network:<p class="source-code">(X_train, y_train), (X_test, y_test) = mnist.load_data()</p><p class="source-code">X_train = X_train.reshape((X_train.shape[0], 28 * 28 * </p><p class="source-code">                           1))</p><p class="source-code">X_test = X_test.reshape((X_test.shape[0], 28 * 28 * </p><p class="source-code">                          1))</p><p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p></li>
				<li>One-hot <a id="_idIndexMarker003"/>encode the labels to break any undesired ordering bias:<p class="source-code">label_binarizer = LabelBinarizer()</p><p class="source-code">y_train = label_binarizer.fit_transform(y_train)</p><p class="source-code">y_test = label_binarizer.fit_transform(y_test)</p></li>
				<li>Take 20% of the data for validation:<p class="source-code">X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)</p></li>
				<li>Compile, train the models for 50 epochs, and evaluate them on the test set:<p class="source-code">models = {</p><p class="source-code">    'sequential_model': sequential_model,</p><p class="source-code">    'sequential_model_list': sequential_model_list,</p><p class="source-code">    'functional_model': functional_model,</p><p class="source-code">    'class_model': class_model</p><p class="source-code">}</p><p class="source-code">for name, model in models.items():</p><p class="source-code">    print(f'Compiling model: {name}')</p><p class="source-code">    model.compile(loss='categorical_crossentropy', </p><p class="source-code">                  optimizer='adam', </p><p class="source-code">                  metrics=['accuracy'])</p><p class="source-code">    print(f'Training model: {name}')</p><p class="source-code">    model.fit(X_train, y_train,</p><p class="source-code">              validation_data=(X_valid, y_valid),</p><p class="source-code">              epochs=50,</p><p class="source-code">              batch_size=256,</p><p class="source-code">              verbose=0)</p><p class="source-code">    _, accuracy = model.evaluate(X_test, y_test, </p><p class="source-code">                                 verbose=0)</p><p class="source-code">    print(f'Testing model: {name}. \nAccuracy: </p><p class="source-code">          {accuracy}')</p><p class="source-code">    print('---')</p></li>
			</ol>
			<p>After 50 epochs, all<a id="_idIndexMarker004"/> three models should have obtained around 98% accuracy on the test set.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>How it works…</h2>
			<p>In the previous section, we went over the basic building blocks we'll need to build most deep learning-powered computer vision projects using TensorFlow 2.x.</p>
			<p>First, we imported the Keras API, the high-level interface for the second version of TensorFlow. We learned that all Keras-related functionality is located inside the <strong class="source-inline">tensorflow</strong> package. </p>
			<p>Next, we found that TensorFlow 2.x offers great flexibility when it comes to defining models. In particular, we have two main APIs we can use to build models:</p>
			<ul>
				<li><strong class="bold">Symbolic</strong>: Also known as the declarative API, it allows us to define a model as a <strong class="bold">Directed Acyclic Graph (DAG)</strong>, where each layer constitutes a node and the interactions<a id="_idIndexMarker005"/> or connections between layers are the edges. The pros of this API are that you can examine the model by plotting it or printing its architecture; compatibility checks are run by the framework, diminishing the probability of runtime errors; and if the model compiles, it runs. On the other hand, the main con is that it's not suited for non-DAG architectures (networks with loops), such as Tree-LSTMs.</li>
				<li><strong class="bold">Imperative</strong>: Also known <a id="_idIndexMarker006"/>as the <strong class="bold">model sub-classing API</strong>, this API is a more Pythonic, developer-friendly way of specifying a model. It also allows for more flexibility in the forward pass than its symbolic counterpart. The pros of this API are that developing models becomes no different than any other object-oriented task, which speeds up the process of trying out new ideas; specifying a control flow is easy using Python's built-in constructs; and it's suited for non-DAG architectures, such as Tree-RNNs. In terms of its cons, reusability is lost because the architecture is hidden within the class; almost no inter-layer compatibility checks are run, thus moving most of the debugging responsibility from the framework to the developer; and there's loss of transparency because information about the interconnectedness between layers is not available.</li>
			</ul>
			<p>We defined the same<a id="_idIndexMarker007"/> architecture using both the Sequential and Functional APIs, which correspond to the symbolic or declarative way of implementing networks, and also a third time using an imperative approach.</p>
			<p>To make it clear that, in the end, the three networks are the same, no matter which approach we took, we trained and evaluated them on the famous <strong class="source-inline">MNIST</strong> dataset, obtaining a decent 98% accuracy on the test set.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>See also</h2>
			<p>If you're interested in learning more about Tree-LSTMs, you can read the paper where they were first<a id="_idIndexMarker008"/> introduced here: https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf.</p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Loading images using the Keras API</h1>
			<p>In this recipe, we<a id="_idIndexMarker009"/> will learn how to load images using the Keras<a id="_idIndexMarker010"/> API, a very important task considering that, in computer vision, we'll always work with visual data. In particular, we'll learn how to open, explore, and visualize a single image, as well as a batch of them. Additionally, we will learn how to programmatically download a dataset.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Getting ready</h2>
			<p>Keras relies on the <strong class="source-inline">Pillow</strong> library to manipulate images. You can install it easily using <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>How to do it…</h2>
			<p>Now, let's begin this recipe:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">import glob</p><p class="source-code">import os</p><p class="source-code">import tarfile</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from tensorflow.keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">from tensorflow.keras.preprocessing.image </p><p class="source-code">import load_img, img_to_array</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Define the URL and destination of the <strong class="source-inline">CINIC-10</strong> dataset, an alternative to the famous <strong class="source-inline">CIFAR-10</strong> dataset:<p class="source-code">DATASET_URL = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&amp;isAllowed=y'</p><p class="source-code">DATA_NAME = 'cinic10'</p><p class="source-code">FILE_EXTENSION = 'tar.gz'</p><p class="source-code">FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])</p></li>
				<li>Download <a id="_idIndexMarker011"/>and decompress the data. By default, it<a id="_idIndexMarker012"/> will <a id="_idTextAnchor031"/><a id="_idTextAnchor032"/>be stored in <strong class="source-inline">~/.keras/datasets/&lt;FILE_NAME&gt;</strong>:<p class="source-code">downloaded_file_location = get_file(origin=DATASET_URL, fname=FILE_NAME, extract=False)</p><p class="source-code"># Build the path to the data directory based on the location of the downloaded file.</p><p class="source-code">data_directory, _ = downloaded_file_location.rsplit(os.path.sep, maxsplit=1)</p><p class="source-code">data_directory = os.path.sep.join([data_directory, </p><p class="source-code">                                   DATA_NAME])</p><p class="source-code"># Only extract the data if it hasn't been extracted already</p><p class="source-code">if not os.path.exists(data_directory):</p><p class="source-code">    tar = tarfile.open(downloaded_file_location)</p><p class="source-code">    tar.extractall(data_directory)</p></li>
				<li>Load all image paths and print the number of images found:<p class="source-code">data_pattern = os.path.sep.join([data_directory, </p><p class="source-code">                                 '*/*/*.png'])</p><p class="source-code">image_paths = list(glob.glob(data_pattern))</p><p class="source-code">print(f'There are {len(image_paths):,} images in the </p><p class="source-code">      dataset')</p><p>The<a id="_idIndexMarker013"/> output <a id="_idIndexMarker014"/>should be as follows:</p><p class="source-code">There are 270,000 images in the dataset</p></li>
				<li>Load a single image from the dataset and print its metadata:<p class="source-code">sample_image = load_img(image_paths[0])</p><p class="source-code">print(f'Image type: {type(sample_image)}')</p><p class="source-code">print(f'Image format: {sample_image.format}')</p><p class="source-code">print(f'Image mode: {sample_image.mode}')</p><p class="source-code">print(f'Image size: {sample_image.size}')</p><p>The output should be as follows:</p><p class="source-code">Image type: &lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;</p><p class="source-code">Image format: PNG</p><p class="source-code">Image mode: RGB</p><p class="source-code">Image size: (32, 32)</p></li>
				<li>Convert an image into a <strong class="source-inline">NumPy</strong> array:<p class="source-code">sample_image_array = img_to_array(sample_image)</p><p class="source-code">print(f'Image type: {type(sample_image_array)}')</p><p class="source-code">print(f'Image array shape: {sample_image_array.shape}')</p><p>Here's the output:</p><p class="source-code">Image type: &lt;class 'numpy.ndarray'&gt;</p><p class="source-code">Image array shape: (32, 32, 3)</p></li>
				<li>Display an image using <strong class="source-inline">matplotlib</strong>:<p class="source-code">plt.imshow(sample_image_array / 255.0)</p><p>This<a id="_idIndexMarker015"/> gives us<a id="_idIndexMarker016"/> the following image:</p><div id="_idContainer005" class="IMG---Figure"><img src="image/B14768_01_001.jpg" alt="Figure 1.1 – Sample image&#13;&#10;"/></div><p class="figure-caption">Figure 1.1<a id="_idTextAnchor033"/> – Sample image</p></li>
				<li>Load a batch of images using <strong class="source-inline">ImageDataGenerator</strong>. As in the previous step, each image will be rescaled to the range [0, 1]:<p class="source-code">image_generator = ImageDataGenerator(horizontal_flip=True, rescale=1.0 / 255.0)</p></li>
				<li>Using <strong class="source-inline">image_generator</strong>, we'll pick and display a random batch of 10 images directly from the directory they are stored in:<p class="source-code">iterator = (image_generator</p><p class="source-code">        .flow_from_directory(directory=data_directory, </p><p class="source-code">                                 batch_size=10))</p><p class="source-code">for batch, _ in iterator:</p><p class="source-code">    plt.figure(figsize=(5, 5))</p><p class="source-code">    for index, image in enumerate(batch, start=1):</p><p class="source-code">        ax = plt.subplot(5, 5, index)</p><p class="source-code">        plt.imshow(image)</p><p class="source-code">        plt.axis('off')</p><p class="source-code">    plt.show()</p><p class="source-code">    break</p><p>The <a id="_idIndexMarker017"/>displayed<a id="_idIndexMarker018"/> batch is shown here:</p></li>
			</ol>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B14768_01_002.jpg" alt="Figure 1.2 – Batch of images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – Batch of images</p>
			<p>Let's see how it all works.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor034"/>How it works…</h2>
			<p>First, we downloaded a visual dataset with the help of the <strong class="source-inline">get_file()</strong> function, which, by default, stores the file with a name of our choosing inside the <strong class="source-inline">~/.keras/datasets</strong> directory. If the file already exists in this location, <strong class="source-inline">get_files()</strong> is intelligent enough to not download it again. </p>
			<p>Next, we decompressed the <strong class="source-inline">CINIC-10</strong> dataset using <strong class="source-inline">untar</strong>. Although these steps are not required to load images (we can manually download and decompress a dataset), it's often a good idea to automate as many steps as we can.</p>
			<p>We then loaded a single image into memory with <strong class="source-inline">load_img()</strong>, a function that uses <strong class="source-inline">Pillow</strong> underneath. Because the result of this invocation is in a format a neural network won't understand, we transformed it into a <strong class="source-inline">NumPy</strong> array with <strong class="source-inline">img_to_array()</strong>.</p>
			<p>Finally, to<a id="_idIndexMarker019"/> load batches of images instead of each one individually, we<a id="_idIndexMarker020"/> used <strong class="source-inline">ImageDataGenerator</strong>, which had been configured to also normalize each image. <strong class="source-inline">ImageDataGenerator</strong> is capable of much more, and we'll often use it whenever we want to implement data augmentation, but for this recipe, we only used it to load groups of 10 images at a time directly from disk, thanks to the <strong class="source-inline">flow_from_directory()</strong> method. As a final remark, this last method returns batches of images and labels, but we ignored the latter as we're only interested in the former.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor035"/>See also</h2>
			<p>To learn more <a id="_idIndexMarker021"/>about processing images with Keras, please consult the official documentation here: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image. For more information on the <strong class="source-inline">CINIC-10</strong> dataset, visit this link: <a href="https://datashare.is.ed.ac.uk/handle/10283/3192">https://datashare.is.ed.ac.uk/handle/10283/3192</a>.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor036"/>Loading images using the tf.data.Dataset API</h1>
			<p>In this recipe, we<a id="_idIndexMarker022"/> will learn how to load images<a id="_idIndexMarker023"/> using the <strong class="source-inline">tf.data.Dataset</strong> API, one of the most important innovations that TensorFlow 2.x brings. Its functional style interface, as well as its high level of optimization, makes it a better alternative than the traditional Keras API for large projects, where efficiency and performance is a must. </p>
			<p>In particular, we'll learn how to open, explore, and visualize a single image, as well as a batch of them. Additionally, we will learn how to programmatically download a dataset. </p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor037"/>How to do it…</h2>
			<p>Let's begin this recipe:</p>
			<ol>
				<li value="1">First, we need to import all the packages we'll need for this recipe:<p class="source-code">import os</p><p class="source-code">import tarfile</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Define the<a id="_idIndexMarker024"/> URL and destination of the <strong class="source-inline">CINIC-10</strong> dataset, an<a id="_idIndexMarker025"/> alternative to the famous <strong class="source-inline">CIFAR-10</strong> dataset:<p class="source-code">DATASET_URL = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&amp;isAllowed=y'</p><p class="source-code">DATA_NAME = 'cinic10'</p><p class="source-code">FILE_EXTENSION = 'tar.gz'</p><p class="source-code">FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])</p></li>
				<li>Download and decompress the data. By default, it will be stored in <strong class="source-inline">~/keras/dataset/&lt;FILE_NAME&gt;</strong>:<p class="source-code">downloaded_file_location = get_file(origin=DATASET_URL, fname=FILE_NAME, extract=False)</p><p class="source-code"># Build the path to the data directory based on the location of the downloaded file.</p><p class="source-code">data_directory, _ = downloaded_file_location.rsplit(os.path.sep, maxsplit=1)</p><p class="source-code">data_directory = os.path.sep.join([data_directory, </p><p class="source-code">                                  DATA_NAME])</p><p class="source-code"># Only extract the data if it hasn't been extracted already</p><p class="source-code">if not os.path.exists(data_directory):</p><p class="source-code">    tar = tarfile.open(downloaded_file_location)</p><p class="source-code">    tar.extractall(data_directory)</p></li>
				<li>Create<a id="_idIndexMarker026"/> a dataset of image paths using a<a id="_idIndexMarker027"/> glob-like pattern:<p class="source-code">data_pattern = os.path.sep.join([data_directory, '*/*/*.png'])</p><p class="source-code">image_dataset = tf.data.Dataset.list_files(data_pattern)</p></li>
				<li>Take a single path from the dataset and use it to read the corresponding image:<p class="source-code">for file_path in image_dataset.take(1):</p><p class="source-code">    sample_path = file_path.numpy()</p><p class="source-code">sample_image = tf.io.read_file(sample_path)</p></li>
				<li>Even though the image is now in memory, we must convert it into a format a neural network can work with. For this, we must decode it from its PNG format into a <strong class="source-inline">NumPy</strong> array, as follows:<p class="source-code">sample_image = tf.image.decode_png(sample_image, </p><p class="source-code">                                   channels=3)</p><p class="source-code">sample_image = sample_image.numpy()</p></li>
				<li>Display the image using <strong class="source-inline">matplotlib</strong>:<p class="source-code">plt.imshow(sample_image / 255.0)</p><p>Here's <a id="_idIndexMarker028"/>the<a id="_idIndexMarker029"/> result:</p><div id="_idContainer007" class="IMG---Figure"><img src="image/B14768_01_003.jpg" alt="Figure 1.3 – Sample image&#13;&#10;"/></div><p class="figure-caption">Figure 1.3 – Sample image</p></li>
				<li>Take the first 10 elements of <strong class="source-inline">image_dataset</strong>, decode and normalize them, and then display them using <strong class="source-inline">matplotlib</strong>:<p class="source-code">plt.figure(figsize=(5, 5))</p><p class="source-code">for index, image_path in enumerate(image_dataset.take(10), start=1):</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_png(image, channels=3)</p><p class="source-code">    image = tf.image.convert_image_dtype(image, </p><p class="source-code">                                         np.float32)</p><p class="source-code">    ax = plt.subplot(5, 5, index)</p><p class="source-code">    plt.imshow(image)</p><p class="source-code">    plt.axis('off')</p><p class="source-code">plt.show()</p><p class="source-code">plt.close()</p><p>Here's<a id="_idIndexMarker030"/> the <a id="_idIndexMarker031"/>output:</p></li>
			</ol>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B14768_01_004.jpg" alt="Figure 1.4 – Batch of images"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – Batch of images</p>
			<p>Let's explain this in more detail.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor038"/>How it works…</h2>
			<p>First, we downloaded the <strong class="source-inline">CINIC-10</strong> dataset using the <strong class="source-inline">get_file()</strong> helper function, which saves the fetched file with a name we give it inside the <strong class="source-inline">~/.keras/datasets</strong> directory by default. If the file was downloaded before, <strong class="source-inline">get_files()</strong> won't download it again. </p>
			<p>Because CINIC-10 is compressed, we used <strong class="source-inline">untar</strong> to extract its contents. This is certainly not required to execute these steps each time we want to load images, given that we can manually download and decompress a dataset, but it's a good practice to automate as many steps as possible.</p>
			<p>To load the images into memory, we created a dataset of their file paths, which enabled us to follow almost the same process to display single or multiple images. We did this using the path to load the image into memory. Then, we decoded it from its source format (PNG, in <a id="_idIndexMarker032"/>this recipe), converted it into a <strong class="source-inline">NumPy</strong> array, and <a id="_idIndexMarker033"/>then pre-processed it as needed.</p>
			<p>Finally, we took the first 10 images in the dataset and displayed them with <strong class="source-inline">matplotlib</strong>.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor039"/>See also</h2>
			<p>If you want to learn<a id="_idIndexMarker034"/> more about the <strong class="source-inline">tf.data.Dataset</strong> API, please refer to the official documentation here: <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a>. For more information regarding the CINIC-10 dataset, go to this link: https://datashare.is.ed.ac.uk/handle/10283/3192.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor040"/>Saving and loading a model</h1>
			<p>Training a<a id="_idIndexMarker035"/> neural network is hard work and time-consuming. That's why retraining a model <a id="_idIndexMarker036"/>every time is impractical. The good news is that we can save a network to disk and load it whenever we need it, whether to improve its performance with more training or to use it to make predictions on fresh data. In this recipe, we'll learn about different ways to persist a model.</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor041"/>How to do it…</h2>
			<p>In this recipe, we'll train a <strong class="bold">CNN</strong> on <strong class="source-inline">mnist</strong> just to illustrate our point. Let's get started:</p>
			<ol>
				<li value="1">Import everything we will need:<p class="source-code">import json</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import mnist</p><p class="source-code">from tensorflow.keras.layers import BatchNormalization</p><p class="source-code">from tensorflow.keras.layers import Conv2D</p><p class="source-code">from tensorflow.keras.layers import Dense</p><p class="source-code">from tensorflow.keras.layers import Dropout</p><p class="source-code">from tensorflow.keras.layers import Flatten</p><p class="source-code">from tensorflow.keras.layers import Input</p><p class="source-code">from tensorflow.keras.layers import MaxPooling2D</p><p class="source-code">from tensorflow.keras.layers import ReLU</p><p class="source-code">from tensorflow.keras.layers import Softmax</p><p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">from tensorflow.keras.models import model_from_json</p></li>
				<li>Define a<a id="_idIndexMarker037"/> function that will download and prepare the data by normalizing<a id="_idIndexMarker038"/> the train and test sets and one-hot encoding the labels:<p class="source-code">def load_data():</p><p class="source-code"> (X_train, y_train), (X_test, y_test) = mnist.load_data()</p><p class="source-code">    # Normalize data.</p><p class="source-code">    X_train = X_train.astype('float32') / 255.0</p><p class="source-code">    X_test = X_test.astype('float32') / 255.0</p><p class="source-code">    # Reshape grayscale to include channel dimension.</p><p class="source-code">    X_train = np.expand_dims(X_train, axis=3)</p><p class="source-code">    X_test = np.expand_dims(X_test, axis=3)</p><p class="source-code">    # Process labels.</p><p class="source-code">    label_binarizer = LabelBinarizer()</p><p class="source-code">    y_train = label_binarizer.fit_transform(y_train)</p><p class="source-code">    y_test = label_binarizer.fit_transform(y_test)</p><p class="source-code">    return X_train, y_train, X_test, y_test</p></li>
				<li>Define a function <a id="_idIndexMarker039"/>for building a network. The architecture comprises a<a id="_idIndexMarker040"/> single convolutional layer and two fully connected layers:<p class="source-code">def build_network():</p><p class="source-code">    input_layer = Input(shape=(28, 28, 1))</p><p class="source-code">    convolution_1 = Conv2D(kernel_size=(2, 2),</p><p class="source-code">                           padding='same',</p><p class="source-code">                           strides=(2, 2),</p><p class="source-code">                           filters=32)(input_layer)</p><p class="source-code">    activation_1 = ReLU()(convolution_1)</p><p class="source-code">    batch_normalization_1 = BatchNormalization()    </p><p class="source-code">                            (activation_1)</p><p class="source-code">    pooling_1 = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                              strides=(1, 1),</p><p class="source-code">       padding='same')(batch_normalization_1)</p><p class="source-code">    dropout = Dropout(rate=0.5)(pooling_1)</p><p class="source-code">    flatten = Flatten()(dropout)</p><p class="source-code">    dense_1 = Dense(units=128)(flatten)</p><p class="source-code">    activation_2 = ReLU()(dense_1)</p><p class="source-code">    dense_2 = Dense(units=10)(activation_2)</p><p class="source-code">    output = Softmax()(dense_2)</p><p class="source-code">    network = Model(inputs=input_layer, outputs=output)</p><p class="source-code">    return network</p></li>
				<li>Implement a<a id="_idIndexMarker041"/> function that will evaluate a network using the test <a id="_idIndexMarker042"/>set:<p class="source-code">def evaluate(model, X_test, y_test):</p><p class="source-code">    _, accuracy = model.evaluate(X_test, y_test, </p><p class="source-code">                                 verbose=0)</p><p class="source-code">    print(f'Accuracy: {accuracy}')</p></li>
				<li>Prepare the data, create a validation split, and instantiate the neural network:<p class="source-code">X_train, y_train, X_test, y_test = load_data()</p><p class="source-code">X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)</p><p class="source-code">model = build_network()</p></li>
				<li>Compile and train the model for 50 epochs, with a batch size of <strong class="source-inline">1024</strong>. Feel free to tune these values according to the capacity of your machine:<p class="source-code">model.compile(loss='categorical_crossentropy', </p><p class="source-code">              optimizer='adam', </p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">model.fit(X_train, y_train, </p><p class="source-code">          validation_data=(X_valid, y_valid), </p><p class="source-code">          epochs=50, </p><p class="source-code">          batch_size=1024, </p><p class="source-code">          verbose=0)</p></li>
				<li>Save the model, along<a id="_idIndexMarker043"/> with its weights, in HDF5 format using<a id="_idIndexMarker044"/> the <strong class="source-inline">save()</strong> method. Then, load the persisted model using <strong class="source-inline">load_model()</strong> and evaluate the network's performance on the test set:<p class="source-code"># Saving model and weights as HDF5.</p><p class="source-code">model.save('model_and_weights.hdf5')</p><p class="source-code"># Loading model and weights as HDF5.</p><p class="source-code">loaded_model = load_model('model_and_weights.hdf5')</p><p class="source-code"># Predicting using loaded model.</p><p class="source-code">evaluate(loaded_model, X_test, y_test)</p><p>The output is as follows:</p><p class="source-code">Accuracy: 0.9836000204086304</p></li>
			</ol>
			<p>Here, we can see that our loaded model obtains 98.36% accuracy on the test set. Let's take a look at this in more detail.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor042"/>How it works…</h2>
			<p>We just learned how to persist a model to disk and back into memory using TensorFlow's 2.0 Keras API, which consists of saving both the model and its weights in a single <strong class="bold">HDF5</strong> file using the <strong class="source-inline">save()</strong> method. Although there are other ways to achieve the same goal, this is <a id="_idIndexMarker045"/>the preferred and most commonly used method because we can<a id="_idIndexMarker046"/> simply restore a network to its saved state using the <strong class="source-inline">load_model()</strong> function, and then resume training or use it for inference.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor043"/>There's more…</h2>
			<p>You can also store the model separately from the weights – the former in <strong class="bold">JSON</strong> format and the latter in HDF5 – using <strong class="source-inline">to_json()</strong> and  <strong class="source-inline">save_weights()</strong>, respectively. The advantage of this approach is that we can copy a network with the same architecture from scratch using the <strong class="source-inline">model_from_json()</strong> function. The downside, though, is that we need more function calls, and this effort is rarely worth it. </p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor044"/>Visualizing a model's architecture</h1>
			<p>Due to their <a id="_idIndexMarker047"/>complexity, one of the most effective ways to debug a neural network is by visualizing its architecture. In this recipe, we'll learn about two different ways we can display a model's architecture:</p>
			<ul>
				<li>Using a text summary</li>
				<li>Using a visual diagram</li>
			</ul>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor045"/>Getting ready</h2>
			<p>We'll need both <strong class="source-inline">Pillow</strong> and <strong class="source-inline">pydot</strong> to generate a visual representation of a network's architecture. We can install both libraries using pip, as follows:</p>
			<p class="source-code">$&gt; pip install Pillow pydot</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor046"/>How to do it…</h2>
			<p>Visualizing a model's architecture is pretty easy, as we'll learn in the following steps:</p>
			<ol>
				<li value="1">Import all the required libraries:<p class="source-code">from PIL import Image</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import BatchNormalization</p><p class="source-code">from tensorflow.keras.layers import Conv2D</p><p class="source-code">from tensorflow.keras.layers import Dense</p><p class="source-code">from tensorflow.keras.layers import Dropout</p><p class="source-code">from tensorflow.keras.layers import Flatten</p><p class="source-code">from tensorflow.keras.layers import Input</p><p class="source-code">from tensorflow.keras.layers import LeakyReLU</p><p class="source-code">from tensorflow.keras.layers import MaxPooling2D</p><p class="source-code">from tensorflow.keras.layers import Softmax</p><p class="source-code">from tensorflow.keras.utils import plot_model</p></li>
				<li>Implement <a id="_idIndexMarker048"/>a model using all the layers we imported in the previous step. Notice that we are naming each layer for ease of reference later on. First, let's define the input:<p class="source-code">input_layer = Input(shape=(64, 64, 3), </p><p class="source-code">                    name='input_layer')</p><p>Here's the first convolution block:</p><p class="source-code">convolution_1 = Conv2D(kernel_size=(2, 2),</p><p class="source-code">                       padding='same',</p><p class="source-code">                       strides=(2, 2),</p><p class="source-code">                       filters=32,</p><p class="source-code">                       name='convolution_1')(input_layer)</p><p class="source-code">activation_1 = LeakyReLU(name='activation_1')(convolution_1)</p><p class="source-code">batch_normalization_1 = BatchNormalization(name='batch_normalization_1')(activation_1)</p><p class="source-code">pooling_1 = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                         strides=(1, 1),</p><p class="source-code">                         padding='same',</p><p class="source-code">                         name='pooling_1')(batch_</p><p class="source-code">                         normalization_1)</p><p>Here's the <a id="_idIndexMarker049"/>second convolution block:</p><p class="source-code">convolution_2 = Conv2D(kernel_size=(2, 2),</p><p class="source-code">                       padding='same',</p><p class="source-code">                       strides=(2, 2),</p><p class="source-code">                       filters=64,</p><p class="source-code">                       name='convolution_2')(pooling_1)</p><p class="source-code">activation_2 = LeakyReLU(name='activation_2')(convolution_2)</p><p class="source-code">batch_normalization_2 = BatchNormalization(name='batch_normalization_2')(activation_2)</p><p class="source-code">pooling_2 = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                         strides=(1, 1),</p><p class="source-code">                         padding='same',</p><p class="source-code">                         name='pooling_2')</p><p class="source-code">                        (batch_normalization_2)</p><p class="source-code">dropout = Dropout(rate=0.5, name='dropout')(pooling_2)</p><p>Finally, we'll define the dense layers and the model itself:</p><p class="source-code">flatten = Flatten(name='flatten')(dropout)</p><p class="source-code">dense_1 = Dense(units=256, name='dense_1')(flatten)</p><p class="source-code">activation_3 = LeakyReLU(name='activation_3')(dense_1)</p><p class="source-code">dense_2 = Dense(units=128, name='dense_2')(activation_3)</p><p class="source-code">activation_4 = LeakyReLU(name='activation_4')(dense_2)</p><p class="source-code">dense_3 = Dense(units=3, name='dense_3')(activation_4)</p><p class="source-code">output = Softmax(name='output')(dense_3)</p><p class="source-code">model = Model(inputs=input_layer, outputs=output, </p><p class="source-code">              name='my_model')</p></li>
				<li>Summarize the<a id="_idIndexMarker050"/> model by printing a text representation of its architecture, as follows:<p class="source-code">print(model.summary())</p><p>Here's the summary. The numbers in the <strong class="bold">Output Shape</strong> column describe the dimensions of the volume produced by that layer, while the number in the <strong class="bold">Param #</strong> column states the number of parameters in that layer:</p><div id="_idContainer009" class="IMG---Figure"><img src="image/B14768_01_005.jpg" alt="Figure 1.5 – Text representation of the network&#13;&#10;"/></div><p class="figure-caption">Figure 1.5 – Text representation of the network</p><p>The last few <a id="_idIndexMarker051"/>lines summarize the number of trainable and non-trainable parameters. The more parameters a model has, the harder and slower it is to train.</p></li>
				<li>Plot a diagram of the network's architecture:<p class="source-code">plot_model(model, </p><p class="source-code">           show_shapes=True, </p><p class="source-code">           show_layer_names=True, </p><p class="source-code">           to_file='my_model.jpg')</p><p class="source-code">model_diagram = Image.open('my_model.jpg')</p><p>This produces <a id="_idIndexMarker052"/>the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B14768_01_006.jpg" alt="Figure 1.6 – Visual representation of the network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Visual representation of the network</p>
			<p>Now, let's learn how this all works.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor047"/>How it works…</h2>
			<p>Visualizing a model is as simple as calling <strong class="source-inline">plot_model()</strong> on the variable that holds it. For it to work, however, we must ensure we have the required dependencies installed; for instance, <strong class="source-inline">pydot</strong>. Nevertheless, if we want a more detailed summary of the number of parameters in our<a id="_idIndexMarker053"/> network layer-wise, we must invoke the <strong class="source-inline">summarize()</strong> method.</p>
			<p>Finally, naming each layer is a good convention to follow. This makes the architecture more readable and easier to reuse in the feature because we can simply retrieve a layer by its name. One <a id="_idIndexMarker054"/>remarkable application of this feature is <strong class="bold">neural style transfer</strong>.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor048"/>Creating a basic image classifier</h1>
			<p>We'll close this<a id="_idIndexMarker055"/> chapter by implementing an image classifier on <strong class="source-inline">Fashion-MNIST</strong>, a popular alternative to <strong class="source-inline">mnist</strong>. This will help us consolidate the knowledge we've acquired from the previous recipes. If, at any point, you need more details on a particular step, please refer to the previous recipes.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor049"/>Getting ready</h2>
			<p>I encourage you to complete the five previous recipes before tackling this one since our goal is to come full circle with the lessons we've learned throughout this chapter. Also, make sure you have <strong class="source-inline">Pillow</strong> and <strong class="source-inline">pydot</strong> on your system. You can install them using pip:</p>
			<p class="source-code">$&gt; pip install Pillow pydot</p>
			<p>Finally, we'll use the <strong class="source-inline">tensorflow_docs</strong> package to plot the loss and accuracy curves of the model. You can install this library with the following command:</p>
			<p class="source-code">$&gt; pip install git+https://github.com/tensorflow/docs</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor050"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the <a id="_idIndexMarker056"/>necessary packages:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_docs as tfdocs</p><p class="source-code">import tensorflow_docs.plots</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fm</p><p class="source-code">from tensorflow.keras.layers import BatchNormalization</p><p class="source-code">from tensorflow.keras.layers import Conv2D</p><p class="source-code">from tensorflow.keras.layers import Dense</p><p class="source-code">from tensorflow.keras.layers import Dropout</p><p class="source-code">from tensorflow.keras.layers import ELU</p><p class="source-code">from tensorflow.keras.layers import Flatten</p><p class="source-code">from tensorflow.keras.layers import Input</p><p class="source-code">from tensorflow.keras.layers import MaxPooling2D</p><p class="source-code">from tensorflow.keras.layers import Softmax</p><p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">from tensorflow.keras.utils import plot_model</p></li>
				<li>Define a function that will load and prepare the dataset. It will normalize the data, one-hot encode the labels, take a portion of the training set for validation, and wrap the three data subsets into three separate <strong class="source-inline">tf.data.Dataset</strong> instances to <a id="_idIndexMarker057"/>increase performance using <strong class="source-inline">from_tensor_slices()</strong>:<p class="source-code">def load_dataset():</p><p class="source-code">    (X_train, y_train), (X_test, y_test) = fm.load_data()</p><p class="source-code">    X_train = X_train.astype('float32') / 255.0</p><p class="source-code">    X_test = X_test.astype('float32') / 255.0</p><p class="source-code">    # Reshape grayscale to include channel dimension.</p><p class="source-code">    X_train = np.expand_dims(X_train, axis=3)</p><p class="source-code">    X_test = np.expand_dims(X_test, axis=3)</p><p class="source-code">    label_binarizer = LabelBinarizer()</p><p class="source-code">    y_train = label_binarizer.fit_transform(y_train)</p><p class="source-code">    y_test = label_binarizer.fit_transform(y_test)</p><p class="source-code">    (X_train, X_val,</p><p class="source-code">     y_train, y_val) = train_test_split(X_train, y_train, </p><p class="source-code">                                        </p><p class="source-code">                        train_size=0.8)</p><p class="source-code">    train_ds = (tf.data.Dataset</p><p class="source-code">                .from_tensor_slices((X_train, </p><p class="source-code">                                     y_train)))</p><p class="source-code">    val_ds = (tf.data.Dataset</p><p class="source-code">              .from_tensor_slices((X_val, y_val)))</p><p class="source-code">    test_ds = (tf.data.Dataset</p><p class="source-code">               .from_tensor_slices((X_test, y_test)))</p></li>
				<li>Implement a function that<a id="_idIndexMarker058"/> will build a network similar to <strong class="bold">LeNet</strong> with the addition of <strong class="source-inline">BatchNormalization</strong>, which we'll use to make the network faster and<a id="_idIndexMarker059"/> most stable, and <strong class="source-inline">Dropout</strong> layers, which will help us combat overfitting, a situation where the network loses generalization power due to high variance:<p class="source-code">def build_network():</p><p class="source-code">    input_layer = Input(shape=(28, 28, 1))</p><p class="source-code">    x = Conv2D(filters=20, </p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same', </p><p class="source-code">               strides=(1, 1))(input_layer)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2), </p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Conv2D(filters=50, </p><p class="source-code">               kernel_size=(5, 5), </p><p class="source-code">               padding='same', </p><p class="source-code">               strides=(1, 1))(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2), </p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=500)(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Dense(10)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    model = Model(inputs=input_layer, outputs=output)</p><p class="source-code">    return model</p></li>
				<li>Define a function <a id="_idIndexMarker060"/>that takes a model's training history, along with a metric of interest, to create a plot corresponding to the training and validation of the curves of such a metric:<p class="source-code">def plot_model_history(model_history, metric, ylim=None):</p><p class="source-code">    plt.style.use('seaborn-darkgrid')</p><p class="source-code">    plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">    plotter.plot({'Model': model_history}, metric=metric)</p><p class="source-code">    plt.title(f'{metric.upper()}')</p><p class="source-code">    if ylim is None:</p><p class="source-code">        plt.ylim([0, 1])</p><p class="source-code">    else:</p><p class="source-code">        plt.ylim(ylim)</p><p class="source-code">    plt.savefig(f'{metric}.png')</p><p class="source-code">    plt.close()</p></li>
				<li>Consume the training <a id="_idIndexMarker061"/>and validation datasets in batches of 256 images at a time. The <strong class="source-inline">prefetch()</strong> method spawns a background thread that populates a buffer of size <strong class="source-inline">1024</strong> with image batches:<p class="source-code">BATCH_SIZE = 256</p><p class="source-code">BUFFER_SIZE = 1024</p><p class="source-code">train_dataset, val_dataset, test_dataset = load_dataset()</p><p class="source-code">train_dataset = (train_dataset</p><p class="source-code">                 .shuffle(buffer_size=BUFFER_SIZE)</p><p class="source-code">                 .batch(BATCH_SIZE)</p><p class="source-code">                 .prefetch(buffer_size=BUFFER_SIZE))</p><p class="source-code">val_dataset = (val_dataset</p><p class="source-code">               .batch(BATCH_SIZE)</p><p class="source-code">               .prefetch(buffer_size=BUFFER_SIZE))</p><p class="source-code">test_dataset = test_dataset.batch(BATCH_SIZE)</p></li>
				<li>Build and train the network:<p class="source-code">EPOCHS = 100</p><p class="source-code">model = build_network()</p><p class="source-code">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</p><p class="source-code">model_history = model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS, verbose=0)</p></li>
				<li>Plot the training<a id="_idIndexMarker062"/> and validation loss and accuracy:<p class="source-code">plot_model_history(model_history, 'loss', [0., 2.0])</p><p class="source-code">plot_model_history(model_history, 'accuracy')</p><p>The first graph corresponds to the loss curve both on the training and validation sets:</p><div id="_idContainer011" class="IMG---Figure"><img src="image/B14768_01_007.jpg" alt="Figure 1.7 – Loss plot&#13;&#10;"/></div><p class="figure-caption">Figure 1.7 – Loss plot</p><p>The second plot shows the accuracy curve for the training and validation sets:</p><div id="_idContainer012" class="IMG---Figure"><img src="image/B14768_01_008.jpg" alt="Figure 1.8 – Accuracy plot&#13;&#10;"/></div><p class="figure-caption">Figure 1.8 – Accuracy plot</p></li>
				<li>Visualize the<a id="_idIndexMarker063"/> model's architecture:<p class="source-code">plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')</p><p>The following is a diagram of our model:</p><div id="_idContainer013" class="IMG---Figure"><img src="image/B14768_01_009.jpg" alt="Figure 1.9 – Model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 1.9 – Model architecture</p></li>
				<li>Save the model:<p class="source-code">model.save('model.hdf5')</p></li>
				<li>Load and evaluate the model:<p class="source-code">loaded_model = load_model('model.hdf5')</p><p class="source-code">results = loaded_model.evaluate(test_dataset, verbose=0)</p><p class="source-code">print(f'Loss: {results[0]}, Accuracy: {results[1]}')</p><p>The output is as follows:</p><p class="source-code">Loss: 0.2943768735975027, Accuracy: 0.9132000207901001</p></li>
			</ol>
			<p>That completes the <a id="_idIndexMarker064"/>final recipe of this chapter. Let's review how it all works.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor051"/>How it works…</h2>
			<p>In this recipe, we used all the lessons learned in this chapter. We started by downloading <strong class="source-inline">Fashion-MNIST</strong> and used the <strong class="source-inline">tf.data.Dataset</strong> API to load its images so that we could feed them to our network, which we implemented using the declarative Functional high-level Keras API. After fitting our model to the data, we examined its performance by reviewing the loss and accuracy curves on the training and validation sets with the help of <strong class="source-inline">matplotlib</strong> and <strong class="source-inline">tensorflow_docs</strong>. To gain a better understanding of the network, we visualized its architecture with <strong class="source-inline">plot_model()</strong> and then saved it to disk, along with its weights, in the convenient HDF5 format. Finally, we loaded the model with <strong class="source-inline">load_model()</strong> to evaluate it on new, unseen data – that is, the test set – obtaining a respectable 91.3% accuracy rating.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor052"/>See also</h2>
			<p>For a deeper <a id="_idIndexMarker065"/>explanation of <strong class="source-inline">Fashion-MNIST</strong>, visit this site: <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a>. The GitHub repository for the TensorFlow docs is available here: <a href="https://github.com/tensorflow/docs">https://github.com/tensorflow/docs</a>.</p>
		</div>
	</body></html>