<html><head></head><body>
		<div id="_idContainer153">
			<h1 id="_idParaDest-116"><em class="italic"><a id="_idTextAnchor124"/>Chapter 6</em>: AI Painter </h1>
			<p>In this chapter, we are going to look at two <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>) that could be used to generate and edit images interactively; they are iGAN and GauGAN . The <strong class="bold">iGAN</strong> (<strong class="bold">interactive GAN</strong>) was the first network to demonstrate how to use GANs for interactive image editing and transformation, back in 2016. As GANs were still in fancy at that time, the generated image quality was not impressive as that of today's networks, but the door was opened to the incorporation of GANs into mainstream image editing. </p>
			<p>In this chapter, you will be introduced to the concepts behind iGANs and some websites that feature video demonstrations of them. There won't be any code in that section. Then, we will go over a more recent award-winning application called <strong class="bold">GauGAN</strong>, produced by Nvidia in 2019, that gives impressive results in converting semantic segmentation masks into real landscape photos. </p>
			<p>We will implement GauGAN from scratch, starting with a new normalization technique known as <strong class="bold">spatially adaptive normalization</strong>. We will also learn about a new loss known as <strong class="bold">hinge loss</strong> and will go on to build a full-size GauGAN. The quality of the image generated by GauGAN is far superior to that of the general-purpose image-to-image translation networks that we covered in previous chapters. </p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Introduction to iGAN</li>
				<li>Segmentation map-to-image translation with GauGAN</li>
			</ul>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor125"/>Technical requirements</h1>
			<p>The relevant Jupyter notebooks and code can be found here:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06</a> </p>
			<p>The notebook used in this chapter is <strong class="source-inline">ch6_gaugan.ipynb</strong>.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor126"/>Introduction to iGAN</h1>
			<p>We are now familiar with using generative models such as pix2pix (see <a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation</em>)to generate images from sketch or segmentation masks. However, as most <a id="_idIndexMarker395"/>of us are not skilled artists, we are only able to draw simple sketches, and as a result, our generated images also have simple shapes. What if we could use a real image as input and use sketches to change the appearance of the real image? </p>
			<p>In the early days of GANs, a paper titled <em class="italic">Generative Visual Manipulation on the Natural Image Manifold</em> by J-Y. Zhu (inventor of CycleGAN) et al. was published that explored how to use a learned latent representation to perform image editing and morphing. The authors made a website, <a href="http://efrosgans.eecs.berkeley.edu/iGAN/">http://efrosgans.eecs.berkeley.edu/iGAN/</a>, that contains videos that demonstrate a few of the following use cases:</p>
			<ul>
				<li><strong class="bold">Interactive image generation</strong>: This involves generating images from sketches <a id="_idIndexMarker396"/>in real time, as shown here:</li>
			</ul>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B14538_06_01.jpg" alt="Figure 6.1 – Interactive image generation, where an image is generated only from simple brush strokes (Source: J-Y. Zhu et al., 2016, &quot;Generative Visual Manipulation on the Natural Image Manifold&quot;, https://arxiv.org/abs/1609.03552)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Interactive image generation, where an image is generated only from simple brush strokes (Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation on the Natural Image Manifold", <a href="https://arxiv.org/abs/1609.03552">https://arxiv.org/abs/1609.03552</a>)</p>
			<ul>
				<li><strong class="bold">Interactive image editing</strong>: A picture is imported and we perform image editing <a id="_idIndexMarker397"/>using a GAN. Early GANs generated images uses only noise as input. Even BicycleGAN (which was invented a few years after the iGAN) could only change the appearance of generated images randomly without direct manipulation. iGANs allow us to specify changes in color and texture, which is impressive. </li>
				<li><strong class="bold">Interactive image transformation</strong> (<strong class="bold">morphing</strong>): Given two images, an iGAN can <a id="_idIndexMarker398"/>create sequences of images that show a morphing process from one image to the other, as follows:</li>
			</ul>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B14538_06_02.jpg" alt="Figure 6.2 – Interactive image transformation (morphing). Given two images, sequences of intermediates images can be generated &#13;&#10;(Source: J-Y. Zhu et al., 2016, &quot;Generative Visual Manipulation on the Natural Image Manifold&quot;, https://arxiv.org/abs/1609.03552)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Interactive image transformation (morphing). Given two images, sequences of intermediates images can be generated (Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation on the Natural Image Manifold", <a href="https://arxiv.org/abs/1609.03552">https://arxiv.org/abs/1609.03552</a>)</p>
			<p>The term <strong class="bold">manifold</strong> appears <a id="_idIndexMarker399"/>in the paper a lot. It also appears in other machine learning literature, so let's spend some time understanding it.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor127"/>Understanding manifold</h2>
			<p>We can understand manifold from the perspective of a natural image. A color pixel can be represented <a id="_idIndexMarker400"/>by 8-bit or 256-bit number; a single RGB pixel alone can have 256x256x256 = 1.6 million different possible combinations! Using the same logic, the total possibilities for all pixels in an image is astronomically high! </p>
			<p>However, we know <a id="_idIndexMarker401"/>that the pixels are not independent of each other; for example, the pixels of grassland are confined to the green color range. Thus, the high dimensionality of an image is not as daunting as it seems. In other words, the dimension space is a lot smaller than we might think at first. Thus, we can say that a high-dimensional image space is supported by a low-dimensional manifold. </p>
			<p>Manifold is a term in physics and mathematics that's used to describe smooth geometric surfaces. They can exist in any dimension. One-dimensional manifolds include lines and circles; two-dimensional <a id="_idIndexMarker402"/>manifolds are called <strong class="bold">surfaces</strong>. A <em class="italic">sphere</em> is a three-dimensional manifold that is smooth everywhere. In contrast, <em class="italic">cubes</em> are not manifolds as they are not smooth at the vertices. In fact, we saw in <a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Variational Autoencoder</em>, that a latent space of an autoencoder with a latent dimension of 2 was a 2D manifold of the MNIST digits projected. The following diagram shows a 2D latent space of digits:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B14538_06_03.jpg" alt="Figure 6.3 – Illustration of a 2D manifold of digits. &#13;&#10;(Source: https://scikit-learn.org/stable/modules/manifold.html)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Illustration of a 2D manifold of digits. (Source: https://scikit-learn.org/stable/modules/manifold.html)</p>
			<p>A good <a id="_idIndexMarker403"/>resource to <a id="_idIndexMarker404"/>visualize manifolds in GAN is the interactive tool at <a href="https://poloclub.github.io/ganlab/">https://poloclub.github.io/ganlab/</a>. In the following example, a GAN is trained to map uniformly distributed 2D samples into 2D samples that have a circular distribution:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B14538_06_04.jpg" alt="Figure 6.4 – The generator's data transformation is visualized as a manifold, which turns input noise (on the left) into fake samples (on the right).&#13;&#10;(Source: M. Kahng, 2019, &quot;GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation,&quot; IEEE Transactions on Visualization and Computer Graphics, 25(1) (VAST 2018) https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – The generator's data transformation is visualized as a manifold, which turns input noise (on the left) into fake samples (on the right).(Source: M. Kahng, 2019, "GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation," IEEE Transactions on Visualization and Computer Graphics, 25(1) (VAST 2018) <a href="https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf">https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf</a>)</p>
			<p>We can visualize this mapping using a manifold, where the input is represented as a uniform square grid. The generator wraps the high-dimensional input grid into a warped version with fewer dimensions. The output shown at the top right of the figure is the manifold <a id="_idIndexMarker405"/>approximated by the generator. The generator <a id="_idIndexMarker406"/>output, or the fake image (bottom right in the figure), is the samples sampled from the manifold, where an area with smaller grid blocks means a higher sampling probability. </p>
			<p>The assumption of the paper is that a GAN's output sampled from random noise <strong class="bold">z</strong>, <strong class="bold">G(z)</strong>, lies in on a smooth manifold. Therefore, given two images on the manifold, <strong class="bold">G(z</strong><span class="subscript">0</span><strong class="bold">)</strong> and <strong class="bold">G(z</strong><span class="subscript">N</span><strong class="bold">)</strong>, we could get a sequence of <em class="italic">N</em> + 1 images <em class="italic">[G(z</em><span class="subscript">0</span><em class="italic">) , G(z</em><span class="subscript">0</span><em class="italic">), ..., G(z</em><span class="subscript">N</span><em class="italic">)]</em> with a smooth transition by interpolation in the latent space. This approximation of the manifold of natural images is used for performing <em class="italic">realistic image editing</em>. </p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor128"/>Image editing</h2>
			<p>Now that <a id="_idIndexMarker407"/>we know what a manifold is, let's see how <a id="_idIndexMarker408"/>to use that knowledge to perform image editing. The first step in image editing is to project an image onto the manifold. </p>
			<h3>Projecting an image onto a manifold</h3>
			<p>What projecting an image onto a manifold means is using a pre-trained GAN to generate an image that is close to the given image. In this book, we will use a pre-trained DCGAN where the input to the generator is a 100-dimension latent vector. Therefore, we will need to <a id="_idIndexMarker409"/>find a latent vector that generates an image manifold that is as close as possible to the original image. One way to do this is to <a id="_idIndexMarker410"/>use optimization such as <strong class="bold">style transfer</strong>, a topic we covered in detail in <a href="B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 5</em></a><em class="italic">, Style Transfer</em>. </p>
			<ol>
				<li>We first <a id="_idIndexMarker411"/>extract the features of the original image using a pretrained <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>), such as the output of the <em class="italic">block5_conv1</em> layer in VGG (see <a href="B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 5</em></a><em class="italic">, Style Transfer</em>), and use it as a target. </li>
				<li>Then, we use the pre-trained DCGAN's generator with frozen weights and optimize on the input latent vector by minimizing the L2 loss between the features. </li>
			</ol>
			<p>As we have learned regarding style transfer, optimization can be slow to run and hence not responsive when it comes to interactive drawing. </p>
			<p>Another method is to train a feedforward network to predict the latent vector from the image, which is a lot faster. If the GAN is to translate a segmentation mask into an image, then we can use a network such as U-Net to predict the segmentation mask from the image. </p>
			<p>Manifold projection using a feedforward network looks similar to using an autoencoder. The encoder encodes (predicts) the latent variable from the original image, then the decoder (the generator, in the case of a GAN) projects the latent variable onto the image manifold. However, this method is not always perfect. This is where the <em class="italic">hybrid</em> method comes in. We use the feedforward network to predict the latent variables, which we then fine-tune using optimization. The following figure shows the images generated using different techniques:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B14538_06_05.jpg" alt="Figure 6.5 – Projecting real photos onto an image manifold using a GAN. (Source: J-Y. Zhu et al, 2016, &quot;Generative Visual Manipulation on the Natural Image Manifold,&quot; https://arxiv.org/abs/1609.03552)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Projecting real photos onto an image manifold using a GAN. (Source: J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold," <a href="https://arxiv.org/abs/1609.03552">https://arxiv.org/abs/1609.03552</a>)</p>
			<p>As we <a id="_idIndexMarker412"/>have now obtained the latent vector, we will use it to edit the manifold.</p>
			<h3>Editing the manifold using latent vector</h3>
			<p>Now that <a id="_idIndexMarker413"/>we have obtained the latent variable <em class="italic">z</em><span class="subscript">0</span> and the image manifold <em class="italic">x</em><span class="subscript">0</span><em class="italic"> = G(z</em><span class="subscript">0</span><em class="italic">)</em>, the next step is to manipulate <em class="italic">z</em><span class="subscript">0</span> to modify the image. Now, let's say the image is a red shoe and we want to change the color to black – how can we do that? The simplest and crudest method is to open an image editing software package to select all the red pixels in the picture and change them to black. The resulting picture is likely to not look very natural as some details may be lost. Traditional image editing tools' algorithms tend to not work that well on natural images with complex shapes and fine texture details. </p>
			<p>On the other hand, we know we probably could change the latent vector and feed that to the generator and change the color. In practice, we do not know how to modify the latent variables to get the results we want. </p>
			<p>Therefore, instead of changing the latent vector directly, we could attack the problem from a different direction. We can edit the manifold, for example, by drawing a black stripe on the shoes, then use that to optimize the latent variables, and then project that to generate another image on the manifold. </p>
			<p>Essentially, we are performing optimization as previously described for manifold projection <a id="_idIndexMarker414"/>but with different loss functions. We want to find an image manifold <em class="italic">x</em> that minimizes the following equation:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/Formula_06_001.jpg" alt=""/>
				</div>
			</div>
			<p>Let's start with the second loss term <em class="italic">S(x, x</em><span class="subscript">0</span><em class="italic">)</em> for manifold smoothness. It is L2 loss, used to encourage the new manifold to not deviate too much from the original manifold. This loss term keeps the global appearance of the image in check. The first loss term is the data term, which sums up all the editing operation loss. This is best described using the following images:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B14538_06_06.jpg" alt="Figure 6.6 – Projecting real photos onto the image manifold using GAN. (Source: J-Y. Zhu et al, 2016, &quot;Generative Visual Manipulation on the Natural Image Manifold,&quot; https://arxiv.org/abs/1609.03552)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Projecting real photos onto the image manifold using GAN. (Source: J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold," <a href="https://arxiv.org/abs/1609.03552">https://arxiv.org/abs/1609.03552</a>)</p>
			<p>This example uses a color brush to change the color of the shoe. The color change may not be <a id="_idIndexMarker415"/>obvious in the grayscale print of this book and you are encouraged to check out the color version of the paper, which you can download from <a href="https://arxiv.org/abs/1609.03552">https://arxiv.org/abs/1609.03552</a>. The top row of the preceding figure shows the brush stroke as the constraint <em class="italic">v</em><span class="subscript">g</span> and <em class="italic">f</em><span class="subscript">g</span> as the editing operation. We want every manifold pixel in the brush stroke <em class="italic">f</em><span class="subscript">g</span><em class="italic">(x)</em> to be as close to <em class="italic">v</em><span class="subscript">g</span> as possible. </p>
			<p>In other words, if we put a black stroke on the shoe, we want that part in the image manifold to be black. That is the intention, but to execute it, we will need to do the optimization of the latent variables. Thus, we reformulate the preceding equation from pixel space into latent space. The equation is as follows: </p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/Formula_06_002.jpg" alt=""/>
				</div>
			</div>
			<p>The last term <img src="image/Formula_06_003.png" alt=""/> is the <em class="italic">GAN's adversarial loss</em>. This is used for making the manifold look real and improve the visual quality slightly. By default, this term is <a id="_idIndexMarker416"/>not used for increasing the frame rate. With all the loss terms defined, we can use a TensorFlow optimizer such as Adam to run the optimization.</p>
			<h3>Edit transfer</h3>
			<p><strong class="bold">Edit transfer</strong> is the last <a id="_idIndexMarker417"/>step of image editing. Now we have two manifolds, <em class="italic">G(z</em><span class="subscript">0</span><em class="italic">)</em> and <em class="italic">G(z</em><span class="subscript">1</span><em class="italic">)</em>, and we can generate sequences of intermediate images using linear <a id="_idIndexMarker418"/>interpolation in latent space between <em class="italic">z</em><span class="subscript">0</span> and <em class="italic">z</em><span class="subscript">1</span>. Due to the capacity limitations of the DCGAN, the generated manifolds can appear blurry and may not look as realistic as we hoped them to be. </p>
			<p>The way the authors of the previously mentioned paper address this problem is to not use the manifold as the final image but instead to estimate the color and geometric changes between the manifolds and apply the changes to the original image. The estimation of color and motion flow is performed using optical flow; this is a traditional computer vision technique that is beyond the scope of this book.</p>
			<p>Using the preceding shoe example, if all we are interested in is the color change, we estimate the color change of the pixel between manifolds, then transfer the color changes on the pixels in the original image. Similarly, if the transformation involves warping, that is, a change in shape, we measure the motion of pixels and apply them to original image to perform morphing. The demonstration video on the website was created using both motion and color flow.</p>
			<p>To recap, we have now learned that an iGAN is not a GAN but a method of using a GAN to perform image editing. This is done by first projecting a real image onto a manifold using either optimization or a feedforward network. Next, we use brush strokes as constraints to modify the manifold generated by the latent vector. Finally, we transfer the color and motion flow of the interpolated manifolds onto the real image to complete the image editing. </p>
			<p>As there <a id="_idIndexMarker419"/>aren't any new GAN architectures, we will not be implementing an iGAN. Instead, we are going to implement GauGAN, which includes some new innovations that are exciting for code implementation.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor129"/>Segmentation map-to-image translation with GauGAN</h1>
			<p><strong class="bold">GauGAN</strong> (named after 19th-century painter Paul Gauguin) is a GAN from <strong class="bold">Nvidia</strong>. Speaking of <a id="_idIndexMarker420"/>Nvidia, it is one of the handful of companies that has invested heavily in GANs. They have achieved several breakthroughs in this space, including <strong class="bold">ProgressiveGAN</strong> (we'll cover that in <a href="B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a><em class="italic">, High Fidelity Face Generation</em>), to generate <a id="_idIndexMarker421"/>high-resolution images, and <strong class="bold">StyleGAN</strong> for <a id="_idIndexMarker422"/>high-fidelity faces. </p>
			<p>Their <a id="_idIndexMarker423"/>main business is in making <a id="_idIndexMarker424"/>graphics chips rather than AI software. Therefore, unlike some other companies, who keep their code and trained models as closely guarded secrets, Nvidia tends to open source their software code to the general public. They hav<a id="_idTextAnchor130"/>e built a web page (<a href="http://nvidia-research-mingyuliu.com/gaugan/">http://nvidia-research-mingyuliu.com/gaugan/</a>) to <a id="_idIndexMarker425"/>showcase GauGAN, which can generate photorealistic landscape photos from segmentation maps. The following screenshot is taken from their web page. </p>
			<p>Feel free to pause this chapter for a bit and have a play with the application to see how good it is:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B14538_06_07.jpg" alt="Figure 6.7 –From brush stroke to photo with GauGAN &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 –From brush stroke to photo with GauGAN </p>
			<p>We will now learn about pix2pixHD.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor131"/>Introduction to pix2pixHD</h2>
			<p>GauGAN uses <strong class="bold">pix2pixHD</strong> as a base <a id="_idIndexMarker426"/>and adds new features to it. pix2pixHD is <a id="_idIndexMarker427"/>an upgraded version of pix2pix that can generate <strong class="bold">high-definition</strong> (<strong class="bold">HD</strong>) images. As we haven't covered pix2pixHD in this book and we <a id="_idIndexMarker428"/>won't be using an HD dataset, we will build our GauGAN base on pix2pix's architecture and the code base that we are already familiar with. Nevertheless, it is good to know the high-level architecture of pix2pixHD, and I'll walk you through some of the high-level concepts. The following diagram shows the architecture of pix2pixHD's generators:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B14538_06_08.jpg" alt="Figure 6.8 – The network architecture of the pix2pixHD generator. &#13;&#10;(Source: T-C. W et al., 2018, &quot;High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,&quot; https://arxiv.org/abs/1711.11585)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – The network architecture of the pix2pixHD generator. (Source: T-C. W et al., 2018, "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs," <a href="https://arxiv.org/abs/1711.11585">https://arxiv.org/abs/1711.11585</a>)</p>
			<p>In order to generate high-resolution images, pix2pixHD uses two generators at different image resolutions at coarse- and fine-scale. The coarse generator <strong class="bold">G1</strong> works at half the image resolution; that is, the input and target images are downsampled into half the resolution. When that is trained, we start training the coarse generator <strong class="bold">G</strong><span class="subscript">1</span> together with the fine <a id="_idIndexMarker429"/>generator <strong class="bold">G2</strong>, which works on the full-image scale. From <a id="_idIndexMarker430"/>the preceding architecture diagram, we can see that <strong class="bold">G</strong><span class="subscript">1</span>'s encoder output concatenates with <strong class="bold">G</strong><span class="subscript">1</span>'s features and feeds into the decoder part of <strong class="bold">G</strong><span class="subscript">2</span> to generate <a id="_idIndexMarker431"/>high-resolution images. This setting is also known as a <strong class="bold">coarse-to-fine generator</strong>.</p>
			<p>pix2pixHD uses three PatchGAN discriminators that operate at different image scales. A new loss, known as feature matching loss, is used to match the layer features between the real and fake images. This is used in style transfer, where we use a pre-trained VGG for feature extraction and optimize on the style features.</p>
			<p>Now that we've had a quick introduction to pix2pixHD, we can move on to GauGAN. But first, we will implement a normalization technique that demonstrates GauGAN.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor132"/>Spatial Adaptive Normalization (SPADE)</h2>
			<p>The main <a id="_idIndexMarker432"/>innovation in GauGAN <a id="_idIndexMarker433"/>is a layer normalization method for segmentation map known as <strong class="bold">Spatial-Adaptive Normalization</strong> (<strong class="bold">SPADE</strong>). That's right, another entry into the already-long list of normalization techniques <a id="_idIndexMarker434"/>in the GAN's toolbox. We will <a id="_idIndexMarker435"/>dive deep into SPADE, but before <a id="_idIndexMarker436"/>that, we should learn about the format of the network input – the <strong class="bold">semantic segmentation map</strong>.</p>
			<h3>One-hot encoded segmentation masks</h3>
			<p>We will use the <strong class="source-inline">facades</strong> dataset to train our GauGAN. In previous experiments, the segmentation <a id="_idIndexMarker437"/>map was encoded as different colors in an RGB image; for example, a wall was represented by a purple mask and a door was green. This representation <a id="_idIndexMarker438"/>is visually easy for us to understand but it is not that helpful for the neural network to learn. This is because the colors do not have semantic meaning. </p>
			<p>Colors being closer in color space does not mean they are close in semantic meaning. We could use light green to represent grass and dark green to represent an airplane, and their semantic meanings would not be related even though the segmentation maps would be close in color shade. </p>
			<p>For this reason, instead of labeling pixels using colors, we should use class labels. However, this still does not solve the problem as the class labels are numbers assigned randomly and <a id="_idIndexMarker439"/>they also do not have semantic meaning. Therefore, a better way is to use a <strong class="bold">segmentation mask</strong> with a label of 1 when an object is present in that pixel and a label of 0 otherwise. In other words, we one-hot encode the labels in a segmentation map into a segmentation mask with the shape (H, W, number of classes). The following figure shows an example of semantic segmentation masks for a building image:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B14538_06_09.jpg" alt="Figure 6.9 – On the left is a segmentation map encoded in RGB. On the right, the segmentation maps are separated into individual classes of window, façade, and pillar "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – On the left is a segmentation map encoded in RGB. On the right, the segmentation maps are separated into individual classes of window, façade, and pillar </p>
			<p>The data in the <strong class="source-inline">facades</strong> dataset that we used in the previous chapters was encoded as JPEG, so we cannot use that to train GauGAN. In JPEG encoding, some visual information <a id="_idIndexMarker440"/>that is less important to the visuals <a id="_idIndexMarker441"/>is removed in the compression process. The resulting pixels may have different values even if they should belong to the same class and appear to be the same color. As a result, we cannot map the colors in a JPEG image to classes. To tackle this problem, I got the raw dataset from the original source and created a new dataset that includes three different image file types for each sample as follows:</p>
			<ul>
				<li>JPEG – real photo</li>
				<li>PNG – segmentation map using RGB color</li>
				<li>BMP – segmentation map using class labels</li>
			</ul>
			<p>BMP is uncompressed. We can think of a BMP image as the image in RGB format in the preceding figure, except that the pixel values are 1-channel class labels rather than 3-channel RGB colors. In image loading and pre-processing, we will load all three files and convert them from BMP into one-hot encoded segmentation masks. </p>
			<p>Sometimes, TensorFlow's basic image pre-processing APIs are not able to do some of the more complex tasks, so we need to resort to using other Python libraries. Luckily, <strong class="source-inline">tf.py_function</strong> allows us to run a generic Python function within a TensorFlow training pipeline. </p>
			<p>In this <a id="_idIndexMarker442"/>file-loading function, as shown in the following code, we use <strong class="source-inline">.numpy()</strong> to convert <a id="_idIndexMarker443"/>TensorFlow tensors into Python objects. The function name is a bit misleading as it applies not only to numerical values but also string values:</p>
			<p class="source-code">def load(image_file): </p>
			<p class="source-code">    def load_data(image_file):</p>
			<p class="source-code">        jpg_file = image_file.numpy().decode("utf-8")</p>
			<p class="source-code">        bmp_file = jpg_file.replace('.jpg','.bmp')</p>
			<p class="source-code">        png_file = jpg_file.replace('.jpg','.png')        </p>
			<p class="source-code">        image = np.array(Image.open(jpg_file))/127.5 - 1</p>
			<p class="source-code">        map = np.array(Image.open(png_file))/127.5 - 1</p>
			<p class="source-code">        labels = np.array(Image.open(bmp_file),  							dtype=np.uint8)</p>
			<p class="source-code">        h, w, _ = image.shape</p>
			<p class="source-code">        n_class = 12</p>
			<p class="source-code">        mask = np.zeros((h, w, n_class), dtype=np.float32)</p>
			<p class="source-code">        for i in range(n_class):</p>
			<p class="source-code">            one_hot[labels==i, i] = 1        </p>
			<p class="source-code">        return map, image, mask </p>
			<p class="source-code">    [mask, image, label] = tf.py_function( 						load_data, [image_file], 						 [tf.float32, tf.float32, 						  tf.float32])</p>
			<p>Now that we understand the format of one-hot encoded semantic segmentation masks, we will <a id="_idIndexMarker444"/>look at how SPADE can help us generate <a id="_idIndexMarker445"/>better images from segmentation masks.</p>
			<h3>Implementing SPADE</h3>
			<p>Instance normalization has become popular in image generation, but it tends to wash away the <a id="_idIndexMarker446"/>semantic meaning of segmentation masks. What does that mean? Let's assume an input image consists of only one single segmentation label; for example, say the entire image is of the sky. As the input has uniform values, the output, after passing through the convolution layer, will also have uniform values. </p>
			<p>Recall that instance normalization calculates the mean across dimensions (H, W) for each channel. Therefore, the mean for that channel will be the same uniform value, and the normalized activation after subtraction with mean will become zero. Obviously, the semantic meaning is lost and the sky has just vanished into thin air. This is an extreme example, but using the same logic, we can see that a segmentation mask loses its semantic meaning as its area grows larger. </p>
			<p>To solve this problem, SPADE normalizes on local areas confined by the segmentation mask rather than on the entire mask. The following diagram shows the high-level architecture of SPADE:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B14538_06_10.jpg" alt="Figure 6.10 – High-level SPADE architecture. &#13;&#10;(Redrawn from: T. Park et al., 2019, &quot;Semantic Image Synthesis with Spatially-Adaptive Normalization,&quot; https://arxiv.org/abs/1903.07291)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – High-level SPADE architecture. (Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," <a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a>)</p>
			<p>In batch normalization, we calculate the means and standard deviations of channels across dimensions (N, H, W). This is the same for SPADE, as shown in the preceding figure. The difference is that gamma and beta for each channel are no longer scalar values (or vectors of C channels) but two-dimensional (H, W). In other words, there is a gamma and <a id="_idIndexMarker447"/>a beta for every activation that is learned from the semantic segmentation map. So, normalization is applied differently to different segmentation areas. These two parameters are learned by using two convolutional layers, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B14538_06_11.jpg" alt="Figure 6.11 – SPADE design diagram, where k denotes the number of convolutional filters &#13;&#10;(Redrawn from: T. Park et al., 2019, &quot;Semantic Image Synthesis with Spatially-Adaptive Normalization,&quot; https://arxiv.org/abs/1903.07291)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – SPADE design diagram, where k denotes the number of convolutional filters (Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," <a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a>)</p>
			<p>SPADE is used not only at the network input stage but also in the internal layers. The resize layer <a id="_idIndexMarker448"/>is to resize the segmentation map to match the dimensions of the layer's activation. We can now implement a TensorFlow custom layer for SPADE. </p>
			<p>We will first define the convolutional layers in the <strong class="source-inline">__init__</strong> constructor as follows:</p>
			<p class="source-code">class SPADE(layers.Layer):</p>
			<p class="source-code">    def __init__(self, filters, epsilon=1e-5):</p>
			<p class="source-code">        super(SPADE, self).__init__()</p>
			<p class="source-code">        self.epsilon = epsilon</p>
			<p class="source-code">        self.conv = layers.Conv2D(128, 3, padding='same', 							activation='relu')</p>
			<p class="source-code">        self.conv_gamma = layers.Conv2D(filters, 3,  							  padding='same')</p>
			<p class="source-code">        self.conv_beta = layers.Conv2D(filters, 3, 							 padding='same')</p>
			<p>Next, we will get the activation map dimensions to be used in resizing later:</p>
			<p class="source-code">    def build(self, input_shape):</p>
			<p class="source-code">        self.resize_shape = input_shape[1:3]</p>
			<p>Finally, we will connect the layers and operations together in <strong class="source-inline">call()</strong> as follows:</p>
			<p class="source-code">    def call(self, input_tensor, raw_mask):</p>
			<p class="source-code">        mask = tf.image.resize(raw_mask, self.resize_shape, 					    method='nearest')</p>
			<p class="source-code">        x = self.conv(mask)</p>
			<p class="source-code">        gamma = self.conv_gamma(x)</p>
			<p class="source-code">        beta = self.conv_beta(x)        </p>
			<p class="source-code">        mean, var = tf.nn.moments(input_tensor, 					   axes=(0,1,2), keepdims=True)</p>
			<p class="source-code">        std = tf.sqrt(var + self.epsilon)</p>
			<p class="source-code">        normalized = (input_tensor - mean)/std        </p>
			<p class="source-code">        output = gamma * normalized + beta</p>
			<p class="source-code">        return output</p>
			<p>This is a <a id="_idIndexMarker449"/>straightforward implementation based on the SPADE design diagram. Next, we will look at how to make use of SPADE.</p>
			<h3>Inserting SPADE into residual blocks</h3>
			<p>GauGAN <a id="_idIndexMarker450"/>uses <strong class="bold">residual blocks</strong> in the <a id="_idIndexMarker451"/>generator. We will now look at how to insert SPADE into residual blocks:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B14538_06_12.jpg" alt="Figure 6.12 – SPADE residual blocks&#13;&#10;(Redrawn from: T. Park et. al., 2019, &quot;Semantic Image Synthesis with Spatially-Adaptive Normalization,&quot; https://arxiv.org/abs/1903.07291)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – SPADE residual blocks (Redrawn from: T. Park et. al., 2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," <a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a>)</p>
			<p>The basic <a id="_idIndexMarker452"/>building block within the SPADE residual block is the <strong class="bold">SPADE-ReLU-Conv layer</strong>. Each SPADE takes two inputs – the activation from the previous layer and the semantic segmentation map. </p>
			<p>As <a id="_idIndexMarker453"/>with the standard residual block, there are two convolution-ReLU layers and a skip path. Whenever there is a change in the number of channels before and after the residual block, the skip connection is learned via the sub-block in the dashed-line box shown in the preceding diagram. When this happens, the activation maps at the inputs of the two SPADEs in forward path will have different dimensions. That is alright as we have built-in resizing within SPADE block. The following is the code for the SPADE residual block to build the needed layers:</p>
			<p class="source-code">class Resblock(layers.Layer):</p>
			<p class="source-code">    def __init__(self, filters):</p>
			<p class="source-code">        super(Resblock, self).__init__()</p>
			<p class="source-code">        self.filters = filters        </p>
			<p class="source-code">    def build(self, input_shape):</p>
			<p class="source-code">        input_filter = input_shape[-1]</p>
			<p class="source-code">        self.spade_1 = SPADE(input_filter)</p>
			<p class="source-code">        self.spade_2 = SPADE(self.filters)</p>
			<p class="source-code">        self.conv_1 = layers.Conv2D(self.filters, 3, 							padding='same')</p>
			<p class="source-code">        self.conv_2 = layers.Conv2D(self.filters, 3,  							padding='same')</p>
			<p class="source-code">        self.learned_skip = False        </p>
			<p class="source-code">        if self.filters != input_filter:</p>
			<p class="source-code">            self.learned_skip = True</p>
			<p class="source-code">            self.spade_3 = SPADE(input_filter)</p>
			<p class="source-code">            self.conv_3 = layers.Conv2D(self.filters,  				    			3, padding='same')</p>
			<p>Then, we connect the layers up in <strong class="source-inline">call()</strong>:</p>
			<p class="source-code">    def call(self, input_tensor, mask):</p>
			<p class="source-code">        x = self.spade_1(input_tensor, mask)</p>
			<p class="source-code">        x = self.conv_1(tf.nn.leaky_relu(x, 0.2))</p>
			<p class="source-code">        x = self.spade_2(x, mask)</p>
			<p class="source-code">        x = self.conv_2(tf.nn.leaky_relu(x, 0.2))        </p>
			<p class="source-code">        if self.learned_skip:</p>
			<p class="source-code">            skip = self.spade_3(input_tensor, mask)</p>
			<p class="source-code">            skip = self.conv_3(tf.nn.leaky_relu(skip, 0.2))</p>
			<p class="source-code">        else:</p>
			<p class="source-code">            skip = input_tensor            </p>
			<p class="source-code">        output = skip + x</p>
			<p class="source-code">        return output</p>
			<p>In <a id="_idIndexMarker454"/>the original GauGAN implementation, spectral normalization is applied after the convolutional layer. It is yet another normalization that we will cover in <a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a><em class="italic">, Self-Attention for Image Generation</em>, when we talk about self-attention GANs. Therefore, we will skip over that and put the residual blocks together to implement GauGAN.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor133"/>Implementing GauGAN</h2>
			<p>We will <a id="_idIndexMarker455"/>first build the generator, followed by the discriminator. Finally, we will implement the loss functions and start training GauGAN. </p>
			<h3>Building the GauGAN generator</h3>
			<p>Before diving <a id="_idIndexMarker456"/>into the GauGAN generator, let's revise what we know about some of its predecessors. In pix2pix, the generator takes in only one input – the semantic segmentation map. As there is no randomness in the network, given the same input, it will always generate building facades with the same color and texture. The naive way of concatenating the input with random noise doesn't work. </p>
			<p>One of the <a id="_idIndexMarker457"/>two methods used by <strong class="bold">BicycleGAN</strong> (<a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation</em>) to address this problem is using an encoder to encode the target image (the real photo) into latent vectors, which are then used to sample random noise for the generator input. This <strong class="bold">cVAE-GAN</strong> structure <a id="_idIndexMarker458"/>is used in the GauGAN generator. There are two inputs to the generator – the semantic segmentation mask and the real photo. </p>
			<p>In the GauGAN web application, we can select a photo (the generated image will resemble the style of the photo). This is made possible by using the encoder to encode style information into latent variables. The code for the encoder is the same as that which we used in the previous chapters, so we will move on to look at the generator architecture. Feel free to revisit <a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation</em>, to refresh the encoder implementation. In the following diagram, we can see the GauGAN generator architecture:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B14538_06_13.jpg" alt="Figure 6.13 – GauGAN generator architecture &#13;&#10;(Redrawn from: T. Park et al., 2019, &quot;Semantic Image Synthesis with Spatially-Adaptive Normalization,&quot; https://arxiv.org/abs/1903.07291)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – GauGAN generator architecture (Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," <a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a>)</p>
			<p>The generator is a decoder-like architecture. The main difference is that the segmentation mask goes into every residual block via SPADE. The latent variable dimension chosen for GauGAN is 256. </p>
			<p class="callout-heading">Note </p>
			<p class="callout">The encoder is not an integral part of the generator; we can choose not to use any style image and sample from a standard multivariate Gaussian distribution.</p>
			<p>The following <a id="_idIndexMarker459"/>is the code to build the generator using the residual block that we wrote previously:</p>
			<p class="source-code">def build_generator(self):</p>
			<p class="source-code">    DIM = 64</p>
			<p class="source-code">    z = Input(shape=(self.z_dim))</p>
			<p class="source-code">    mask = Input(shape=self.input_shape)</p>
			<p class="source-code">    x = Dense(16384)(z)</p>
			<p class="source-code">    x = Reshape((4, 4, 1024))(x)</p>
			<p class="source-code">    x = UpSampling2D((2,2))(Resblock(filters=1024)(x, mask))</p>
			<p class="source-code">    x = UpSampling2D((2,2))(Resblock(filters=1024)(x, mask))</p>
			<p class="source-code">    x = UpSampling2D((2,2))(Resblock(filters=1024)(x, mask))</p>
			<p class="source-code">    x = UpSampling2D((2,2))(Resblock(filters=512)(x, mask))</p>
			<p class="source-code">    x = UpSampling2D((2,2))(Resblock(filters=256)(x, mask))</p>
			<p class="source-code">    x = UpSampling2D((2,2))(Resblock(filters=128)(x, mask))</p>
			<p class="source-code">    x = tf.nn.leaky_relu(x, 0.2)</p>
			<p class="source-code">    output_image = tanh(Conv2D(3, 4, padding='same')(x)) </p>
			<p class="source-code">    return Model([z, mask], output_image, name='generator')   </p>
			<p>You now know about everything that makes GauGAN work – SPADE and the generator. The rest of <a id="_idIndexMarker460"/>the network architecture are ideas borrowed from other GANs that we have previously learned. Next, we will look at how to build the discriminator.</p>
			<h3>Building the discriminator</h3>
			<p>The discriminator is PatchGAN, where the input is a concatenation of the segmentation map and the <a id="_idIndexMarker461"/>generated image. The segmentation map has to have the same number of channels as the generated RGB image; therefore, we will use the RGB segmentation map instead of a one-hot encoded segmentation mask. The architecture of the GauGAN discriminator is as follows:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B14538_06_14.jpg" alt="Figure 6.14 – GauGAN discriminator architecture&#13;&#10;(Redrawn from: T. Park et al., 2019, &quot;Semantic Image Synthesis with Spatially-Adaptive Normalization,&quot; https://arxiv.org/abs/1903.07291)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – GauGAN discriminator architecture (Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," <a href="https://arxiv.org/abs/1903.07291">https://arxiv.org/abs/1903.07291</a>)</p>
			<p>Except for the last layer, the discriminator layers consist of the following:</p>
			<ul>
				<li>A convolutional layer with a kernel size of 4x4 and a stride of 2 for downsampling</li>
				<li>Instance normalization (except for the first layer)</li>
				<li>Leaky ReLU</li>
			</ul>
			<p>GauGAN uses multiple discriminators at different scales. Since our dataset image has a low resolution of 256x256, one discriminator is sufficient. If we were to use multiple discriminators, all we would need to do is downsample the input size by half for the next discriminator <a id="_idIndexMarker462"/>and calculate the average loss from all the discriminators. </p>
			<p>The code implementation for a single PatchGAN is as follows:</p>
			<p class="source-code">def build_discriminator(self):</p>
			<p class="source-code">    DIM = 64</p>
			<p class="source-code">    model = tf.keras.Sequential(name='discriminators') </p>
			<p class="source-code">    input_image_A = layers.Input(shape=self.image_shape, 					name='discriminator_image_A')</p>
			<p class="source-code">    input_image_B = layers.Input(shape=self.image_shape, 					name='discriminator_image_B') </p>
			<p class="source-code">    x = layers.Concatenate()([input_image_A, input_image_B]) </p>
			<p class="source-code">    x1 = self.downsample(DIM, 4, norm=False)(x) # 128</p>
			<p class="source-code">    x2 = self.downsample(2*DIM, 4)(x1) # 64</p>
			<p class="source-code">    x3 = self.downsample(4*DIM, 4)(x2) # 32</p>
			<p class="source-code">    x4 = self.downsample(8*DIM, 4, strides=1)(x3) # 29</p>
			<p class="source-code">    x5 = layers.Conv2D(1, 4)(x4) </p>
			<p class="source-code">    outputs = [x1, x2, x3, x4, x5]</p>
			<p class="source-code">    return Model([input_image_A, input_image_B], outputs)</p>
			<p>This is identical to pix2pix, except that the discriminator returns all downsampling blocks' output. Why <a id="_idIndexMarker463"/>do we need that? Well, this brings us to a discussion about loss functions.</p>
			<h3>Feature matching loss</h3>
			<p><strong class="bold">Feature matching loss</strong> has been successfully used in style transfer. The content and style features <a id="_idIndexMarker464"/>are extracted using a pre-trained VGG and <a id="_idIndexMarker465"/>the losses are calculated between the target image and the generated image. The content features are simply the outputs from multiple convolutional blocks in VGG. GauGAN employs content loss to replace L1 reconstruction loss, which was common among GANs. The reason is that reconstruction loss makes comparisons pixel by pixel, and the loss can be large if the image shifts in location despite still looking the same to human eyes. </p>
			<p>On the other hand, the content features of convolutional layers are spatially invariant. As a result, when using the content loss to train on the <strong class="source-inline">facades</strong> dataset, our generated buildings will look a lot less blurry and the lines will look straighter. The content loss in style transfer literature is <a id="_idIndexMarker466"/>sometimes known as <strong class="bold">VGG loss</strong> in code, as people love to use VGG for feature extraction. </p>
			<p class="callout-heading">Why do people still love using the old VGG?</p>
			<p class="callout">Newer CNN architectures such as ResNet have long surpassed VGG's performance and achieve much higher accuracy in image classification. So, why do people still use VGG for feature extraction? Some have tried Inception and ResNet for neural style transfer but have found that the results generated using VGG were more visually pleasant. This is likely due to the hierarchy of VGG's architecture, with its monotonically increasing channel numbers across layers. This allows feature extraction to happen smoothly from low-level to high-level representation. </p>
			<p class="callout">In contrast, the residual blocks of ResNet have a bottleneck design that squeezes the input activation channels (say, 256) to a lower number (say, 64), before restoring it back to a higher number (256 again). Residual blocks also have a skip connection that could <em class="italic">smuggle</em> information for the classification task and bypass feature extraction in the convolutional layers.</p>
			<p>The code <a id="_idIndexMarker467"/>to calculate the VGG feature loss is as follows:</p>
			<p class="source-code">def VGG_loss(self, real_image, fake_image):</p>
			<p class="source-code">    # RGB to BGR</p>
			<p class="source-code">    x = tf.reverse(real_image, axis=[-1])</p>
			<p class="source-code">    y = tf.reverse(fake_image, axis=[-1])</p>
			<p class="source-code">    # [-1, +1] to [0, 255]</p>
			<p class="source-code">    x = tf.keras.applications.vgg19.preprocess_input( 								127.5*(x+1))</p>
			<p class="source-code">    y = tf.keras.applications.vgg19.preprocess_input( 								127.5*(y+1))</p>
			<p class="source-code">    # extract features</p>
			<p class="source-code">    feat_real = self.vgg(x)</p>
			<p class="source-code">    feat_fake = self.vgg(y) </p>
			<p class="source-code">    weights = [1./32, 1./16, 1./8, 1./4, 1.]</p>
			<p class="source-code">    loss = 0</p>
			<p class="source-code">    mae = tf.keras.losses.MeanAbsoluteError()</p>
			<p class="source-code">    for i in range(len(feat_real)):</p>
			<p class="source-code">        loss += weights[i] * mae(feat_real[i], feat_fake[i])</p>
			<p class="source-code">    return loss</p>
			<p>When <a id="_idIndexMarker468"/>calculating VGG loss, we first convert the images from <strong class="source-inline">[-1, +1]</strong> to <strong class="source-inline">[0, 255]</strong> and from <strong class="source-inline">RGB</strong> to <strong class="source-inline">BGR</strong>, which is the image format expected by Keras' VGG <strong class="source-inline">preprocess</strong> function. GauGAN gives more weights to higher layers to emphasize structural accuracy. This is to align the generated image with the segmentation mask. Anyway, this is not set in stone and you are welcome to try different weights. </p>
			<p>Feature matching is also used on the discriminator, where we extract the discriminator layer <a id="_idIndexMarker469"/>outputs of real and fake images. The following code is used to calculate the L1 feature matching loss in the discriminator:</p>
			<p class="source-code">def feature_matching_loss(self, feat_real, feat_fake):</p>
			<p class="source-code">    loss = 0</p>
			<p class="source-code">    mae = tf.keras.losses.MeanAbsoluteError()</p>
			<p class="source-code">    for i in range(len(feat_real)-1):</p>
			<p class="source-code">        loss +=  mae(feat_real[i], feat_fake[i])</p>
			<p class="source-code">    return loss</p>
			<p>Apart <a id="_idIndexMarker470"/>from this, we will also have <strong class="bold">KL divergence loss</strong> for <a id="_idIndexMarker471"/>the encoder. The last loss is <strong class="bold">hinge loss</strong> as the new adversarial loss.</p>
			<h3>Hinge loss</h3>
			<p>Hinge loss may <a id="_idIndexMarker472"/>be a newcomer in the GAN world, but it has long been used in <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>) for <a id="_idIndexMarker473"/>classification. It <a id="_idIndexMarker474"/>maximizes the margin of the decision boundary. The following plots show the hinge loss for positive (real) and negative (fake) labels:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B14538_06_15.jpg" alt="Figure 6.15 – Hinge loss for a discriminator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – Hinge loss for a discriminator</p>
			<p>On the left is the hinge loss for the discriminator when the image is real. When we use hinge loss <a id="_idIndexMarker475"/>for the discriminator,the loss is bounded to 0 when prediction is over 1. If it is anything under 1, the loss increases to penalize for not predicting <a id="_idIndexMarker476"/>the image as real. It's similar for fake images but in the opposite direction: the hinge loss is 0 when the prediction of fake image is under -1 and it increases linearly once above that threshold.</p>
			<p>We can implement the hinge loss using basic mathematic operations as follows:</p>
			<p class="source-code">def d_hinge_loss(y, is_real):</p>
			<p class="source-code">    if is_real:</p>
			<p class="source-code">        loss = tf.reduce_mean(tf.maximum(0., 1-y))</p>
			<p class="source-code">    else:</p>
			<p class="source-code">        loss = tf.reduce_mean (tf.maximum(0., 1+y))</p>
			<p class="source-code">    return loss</p>
			<p>Another way of doing this is by using TensorFlow's hinge loss API:</p>
			<p class="source-code">def hinge_loss_d(self, y, is_real):</p>
			<p class="source-code">    label = 1. if is_real else -1.</p>
			<p class="source-code">    loss = tf.keras.losses.Hinge()(y, label)</p>
			<p class="source-code">    return loss</p>
			<p>The loss <a id="_idIndexMarker477"/>for the generator isn't really hinge loss; it is simply a negative <a id="_idIndexMarker478"/>mean of prediction. This is unbounded, so the loss is lower when the prediction score is higher:</p>
			<p class="source-code">def g_hinge_loss(y):</p>
			<p class="source-code">    return –tf.reduce_mean(y)</p>
			<p>We now have everything we need to train GauGAN using a training framework as we did in the previous chapter. The following figure shows the images generated using the segmentation mask:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B14538_06_16.jpg" alt="Figure 6.16 – Example of images generated by our GauGAN implementation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – Example of images generated by our GauGAN implementation</p>
			<p>They look a <a id="_idIndexMarker479"/>lot better than the pix2pix and CycleGAN results! If we <a id="_idIndexMarker480"/>encode the ground truth image's style into random noise, the generated images will be almost indistinguishable from the ground truth. It is really impressive to look at on the computer!</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor134"/>Summary</h1>
			<p>Using AI in image editing is already prevalent now, and all this started at around the time that the iGAN was introduced. We learned about the key principle of the iGAN being to first project an image onto a manifold and then directly perform editing on the manifold. We then optimize this on the latent variables and generate an edited image that is natural-looking. This is in contrast with previous methods that could only change generated images indirectly by manipulating latent variables. </p>
			<p>GauGAN incorporates many advanced techniques to generate crisp images from semantic segmentation masks. This includes the use of hinge loss and feature matching loss. However, the key ingredient is SPADE, which provides superior performance when using a segmentation mask as input. SPADE performs normalization on a local segmentation map to preserve its semantic meaning, which helps us to produce high-quality images. So far, we have been using images with up to 256x256 resolution to train our networks. We now have techniques that are mature enough to generate high-resolution images, as we briefly discussed when introducing pix2pixHD. </p>
			<p>In the next chapter, we will move to the realm of high-resolution images with advanced models such as ProgressiveGAN and StyleGAN.</p>
		</div>
	</body></html>