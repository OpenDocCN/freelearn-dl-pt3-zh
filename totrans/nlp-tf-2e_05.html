<html><head></head><body>
  <div id="_idContainer216" class="Basic-Text-Frame">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-101" class="chapterTitle">Sentence Classification with Convolutional Neural Networks</h1>
    <p class="normal">In this chapter, we will discuss a type of neural network known as <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>). CNNs are quite different from fully connected neural networks and have achieved state-of-the-art performance in numerous tasks. These tasks include image classification, object detection, speech recognition, and of course, sentence classification. One of the main advantages of CNNs is that, compared to a fully connected layer, a convolution layer in a CNN has a much smaller number of parameters. This allows us to build deeper models without worrying about memory overflow. Also, deeper models usually lead to better performance.</p>
    <p class="normal">We will introduce you to what a CNN is in detail by discussing different components found in a CNN and what makes CNNs different from their fully connected counterparts. Then we will discuss the various operations used in CNNs, such as the convolution and pooling operations, and certain hyperparameters related to these operations, such as filter size, padding, and stride. We will also look at some of the mathematics behind the actual operations. After establishing a good understanding of CNNs, we will look at the practical side of implementing a CNN with TensorFlow. First, we will implement a CNN to classify images and then use a CNN for sentence classification. Specifically, we’ll go through the following topics:</p>
    <ul>
      <li class="bulletList">Learning the fundamentals of CNNs</li>
      <li class="bulletList">Classifying images with CNNs</li>
      <li class="bulletList">Classifying sentences with CNNs</li>
    </ul>
    <h1 id="_idParaDest-102" class="heading-1">Introducing CNNs</h1>
    <p class="normal">In this section, you will learn about CNNs. Specifically, you will first get an understanding of the sort of operations present in a<a id="_idIndexMarker429"/> CNN, such as convolution layers, pooling layers, and fully connected layers. Next, we will briefly see how all of these are connected to form an end-to-end model. </p>
    <p class="normal">It is important to note that the first use case we’ll be solving with CNNs is an image classification task. CNNs were originally used to solve computer vision tasks and were adopted for NLP much later. Furthermore, CNNs have a stronger presence in the computer vision domain than the NLP domain, making it easier to explain the underlying concepts in a vision context. For this reason, we will first learn how CNNs are used in computer vision and then move on to NLP.</p>
    <h3 id="_idParaDest-103" class="heading-3">CNN fundamentals</h3>
    <p class="normal">Now, let’s explore the fundamental ideas behind a CNN without delving into too much technical detail. A CNN is a stack of layers, such <a id="_idIndexMarker430"/>as convolution layers, pooling layers, and fully connected layers. We will discuss each of these to understand their role in the CNN.</p>
    <p class="normal">Initially, the input is connected to a set of convolution layers. These convolution layers slide a patch of weights (sometimes called the convolution <a id="_idIndexMarker431"/>window or filter) over the input and produce an output by means of the convolution operation. Convolution layers use a small number of weights, organized to cover only a small patch of input in each layer, unlike fully connected neural networks, and these weights are shared across certain dimensions (for example, the width and height dimensions of an image). Also, CNNs use the convolution operations to share the weights from the output by sliding this small set of weights along the desired dimension. What we ultimately get from this convolution operation is illustrated in <em class="italic">Figure 5.1</em>. If the pattern present in a convolution filter is present in a patch of image, the convolution will output a high value for that location; if not, it will output a low value. Also, by convolving the full image, we get a matrix indicating whether a pattern was present or not in a given location. Finally, we will get a matrix as the convolution output:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_01.png" alt="CNN fundamentals"/></figure>
    <p class="packt_figref">Figure 5.1: What the convolution operation does to an image</p>
    <p class="normal">Also, these convolution layers are optionally interleaved with pooling/subsampling layers, which reduces the dimensionality of the input. While reducing the dimensionality, we make the<a id="_idIndexMarker432"/> translation of CNNs invariant, as well as force the CNN to learn with less information, leading to better generalization and regularization of the model. The dimensionality is reduced by dividing the input into several patches and transforming each patch into a single element. For example, such transformations include picking the maximum element of a patch or averaging all the values in a patch. We will illustrate how pooling can make the translation of CNNs invariant in <em class="italic">Figure 5.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_02.png" alt="CNN fundamentals"/></figure>
    <p class="packt_figref">Figure 5.2: How the pooling operation helps to make data translation invariant</p>
    <p class="normal">Here, we have the original image and an image slightly translated on the <em class="italic">y</em> axis. We have convolution output for both images, and you can see that the value <strong class="keyWord">10</strong> appears at slightly different places in<a id="_idIndexMarker433"/> the convolution output. However, using max pooling (which takes the maximum value of each thick square), we can get the same output at the end. We will discuss these operations in detail later.</p>
    <p class="normal">Finally, the output is fed to a set of fully connected layers, which then forward the output to the final classification/regression layer (for example, sentence/image classification). Fully connected layers contain a significant fraction of the total number of weights of the CNN, as convolution layers have a small number of weights. However, it has been found that CNNs perform better with fully connected layers than without them. This could be because convolution layers learn more localized features due to their small size, whereas fully connected layers provide a global picture of how these localized features should be <a id="_idIndexMarker434"/>connected together to produce a desirable final output. </p>
    <p class="normal"><em class="italic">Figure 5.3 </em>shows a typical CNN used to classify images:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_03.png" alt="CNN fundamentals"/></figure>
    <p class="packt_figref">Figure 5.3: A typical CNN architecture</p>
    <p class="normal">As is evident from the figure, CNNs, by design, preserve the spatial structure of the inputs during learning. In other words, for a two-dimensional input, a CNN will mostly have two-dimensional layers, whereas it will only have fully connected layers close to the output layer. Preserving the spatial structure allows CNNs to exploit valuable spatial information of the inputs and learn about inputs with fewer parameters. The value of spatial information is illustrated in <em class="italic">Figure 5.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_04.png" alt="CNN fundamentals"/></figure>
    <p class="packt_figref">Figure 5.4: Unwrapping an image into a one-dimensional vector loses some of the important spatial information</p>
    <p class="normal">As you can see, when a two-dimensional image of a cat is unwrapped to be a one-dimensional vector, ears are<a id="_idIndexMarker435"/> no longer close to the eyes, and the nose is far away from the eyes as well. This means we have destroyed some of the useful spatial information during the unwrapping. This is why preserving the two-dimensional nature of the inputs is so important.</p>
    <h3 id="_idParaDest-104" class="heading-3">The power of CNNs</h3>
    <p class="normal">CNNs are a very versatile family of<a id="_idIndexMarker436"/> models and have shown a remarkable performance in many types of tasks. Such versatility is attributed to the ability of CNNs to perform feature extraction and learning simultaneously, leading to greater efficiency and generalizability. Let’s discuss a few examples of the utility of CNNs.</p>
    <p class="normal">In the <strong class="keyWord">ImageNet Large Scale Visual Recognition Challenge</strong> (<strong class="keyWord">ILSVRC</strong>) 2020, which involved classifying images, detecting <a id="_idIndexMarker437"/>objects, and localizing objects in an image, CNNs were used to achieve incredible test accuracies. For example, for image-classification tasks, its top-1 test accuracy was approximately 90% for 1,000 different object classes, which means that the CNN was able to correctly identify around 900 different objects correctly.</p>
    <p class="normal">CNNs also have been used for image segmentation. Image segmentation involves segmenting an image into different areas. For example, in an urbanscape image that includes buildings, a road, vehicles, and passengers, isolating the road from the buildings is a segmentation<a id="_idIndexMarker438"/> task. Moreover, CNNs have made incredible strides, demonstrating their performance in NLP tasks such as sentence classification, text generation, and machine translation.</p>
    <h1 id="_idParaDest-105" class="heading-1">Understanding CNNs</h1>
    <p class="normal">Now that we understand the<a id="_idIndexMarker439"/> high level concepts governing CNNs, let’s walk through the technical details of a CNN. First, we will discuss the convolution operation and introduce some terminology, such as filter size, stride, and padding. In brief, <strong class="keyWord">filter size</strong> refers to the<a id="_idIndexMarker440"/> window size of the convolution operation, <strong class="keyWord">stride</strong> refers to the <a id="_idIndexMarker441"/>distance between two movements of the convolution window, and <strong class="keyWord">padding</strong> refers to<a id="_idIndexMarker442"/> the way you handle the boundaries of the input. We will also discuss an operation that is known as deconvolution or transposed convolution. Then we will discuss the details of the pooling operation. Finally, we will discuss how to add fully connected layers, which produce the classification or regression output.</p>
    <h2 id="_idParaDest-106" class="heading-2">Convolution operation</h2>
    <p class="normal">In this section, we will discuss the convolution operation in detail. First, we will discuss the convolution operation without <a id="_idIndexMarker443"/>stride and padding, then we will describe the convolution operation with stride, and then we will discuss the convolution operation with padding. Finally, we will discuss <a id="_idIndexMarker444"/>something called transposed convolution. For all<a id="_idIndexMarker445"/> the operations in this chapter, we consider the index starting from one, and not from zero.</p>
    <h3 id="_idParaDest-107" class="heading-3">Standard convolution operation</h3>
    <p class="normal">The convolution operation is a central<a id="_idIndexMarker446"/> part of CNNs. For an input of size <img src="../Images/B14070_05_001.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> and a weight patch (also known as a <em class="italic">filter</em> or a <em class="italic">kernel</em>) of <img src="../Images/B14070_05_002.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, where <img src="../Images/B14070_05_003.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, the convolution operation slides the patch of weights over the input. Let’s denote the input by <code class="inlineCode">X</code>, the patch of weights by <code class="inlineCode">W</code>, and the output by <code class="inlineCode">H</code>. Also, at each location <em class="italic">i, j</em>, the output is calculated as follows: </p>
    <p class="center"><img src="../Images/B14070_05_004.png" alt="" style="height: 3.33em !important;"/></p>
    <p class="normal">Here, <em class="italic">x</em> <sub class="subscript">i,j</sub>, <em class="italic">w</em> <sub class="subscript">i,j</sub>, and <em class="italic">h</em><sub class="subscript">i,j</sub> denote the value at the <em class="italic">(i,j)</em><sup class="superscript">th</sup> location of <em class="italic">X</em>, <em class="italic">W</em>, and <em class="italic">H</em>, respectively. As already <a id="_idIndexMarker447"/>shown by the equation, though the input size is <img src="../Images/B14070_05_001.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, the output in this case will be <img src="../Images/B14070_05_006.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>. Also, <em class="italic">m</em> is known as the filter size. This means the width and height of the output will be slightly less than of the original. Let’s look at this through a visualization (see <em class="italic">Figure 5.5</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_05.png" alt="Standard convolution operation"/></figure>
    <p class="packt_figref">Figure 5.5: The convolution operation with a filter size (m) = 3, stride = 1, and no padding</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">The output produced by the<a id="_idIndexMarker448"/> convolution operation (the rectangle at the top in <em class="italic">Figure 5.5</em>) is sometimes called a <strong class="keyWord">features map</strong>.</p>
    </div>
    <p class="normal">Next let’s discuss the stride parameter in convolution.</p>
    <h3 id="_idParaDest-108" class="heading-3">Convolving with stride</h3>
    <p class="normal">In the preceding example, we shifted the filter by a single step. However, this is not mandatory; we can take large steps<a id="_idIndexMarker449"/> or strides while convolving the input. Therefore, the size of the step is known as the stride. </p>
    <p class="normal">Let’s modify the<a id="_idIndexMarker450"/> previous equation to include the <em class="italic">s</em> <sub class="subscript">i</sub> and <em class="italic">s</em> <sub class="subscript">j</sub> strides: </p>
    <p class="center"><img src="../Images/B14070_05_007.png" alt="" style="height: 3.65em !important; vertical-align: 0.11em !important;"/></p>
    <p class="normal"><img src="../Images/B14070_05_007.1.png" alt=""/></p>
    <p class="normal">In this case, the output will be smaller as the size of <em class="italic">s</em><sub class="subscript">i</sub> and <em class="italic">s</em><sub class="subscript">j</sub> increases. Comparing <em class="italic">Figure 5.5</em> (<em class="italic">stride = 1</em>) and <em class="italic">Figure 5.6</em> (<em class="italic">stride = 2</em>) illustrates the effect of different strides:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_06.png" alt="Convolving with stride"/></figure>
    <p class="packt_figref">Figure 5.6: The convolution operation with a filter size (m) = 2, stride = 2, and no padding</p>
    <p class="normal">As you can see, doing convolution with stride helps to reduce the dimensionality of the input similar to a pooling layer. Therefore, sometimes convolution with stride is used instead of pooling in the CNNs as it reduces the computational complexity. Also note that the dimensionality reduction achieved by stride can be tuned or controlled as opposed to the inherent dimensionality reduction from the standard convolution operation. We will now discuss another important concept in<a id="_idIndexMarker451"/> convolution known as padding.</p>
    <h3 id="_idParaDest-109" class="heading-3">Convolving with padding</h3>
    <p class="normal">The inevitable output size reduction<a id="_idIndexMarker452"/> resulting from each convolution (without stride) is an undesirable property. This greatly limits the number of layers we can have in a network. Also, it is known that deeper networks perform better than shallow networks. This should not be confused with the dimensionality reduction achieved by stride, as this is a design choice and we can decide to have a stride of 1 if necessary. Therefore, padding is used to circumvent this issue. This is achieved by padding zeros to the boundary of the input so that the output size and the input size are equal. Let’s assume a stride of 1: </p>
    <p class="center"><img src="../Images/B14070_05_008.png" alt="" style="height: 3.33em !important;"/></p>
    <p class="normal">Here: </p>
    <p class="center"><img src="../Images/B14070_05_009.png" alt="" style="height: 1.35em !important; vertical-align: 0.01em !important;"/></p>
    <p class="normal"><em class="italic">Figure 5.7</em> depicts the result of<a id="_idIndexMarker453"/> the padding:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_07.png" alt="Convolving with padding"/></figure>
    <p class="packt_figref">Figure 5.7: The convolution operation with a filter size (m=3), stride (s=1), and zero padding</p>
    <p class="normal">We will now discuss the transposed convolution operation.</p>
    <h3 id="_idParaDest-110" class="heading-3">Transposed convolution</h3>
    <p class="normal">Though the convolution operation looks complicated in terms of mathematics, it can be simplified to a matrix multiplication. For this<a id="_idIndexMarker454"/> reason, we can define the transpose of the convolution operation or <strong class="keyWord">deconvolution</strong>, as it is <a id="_idIndexMarker455"/>sometimes called. However, we will use the term <strong class="keyWord">transposed convolution</strong> as it <a id="_idIndexMarker456"/>sounds more natural. In addition, deconvolution refers to a different mathematical concept. The transposed convolution operation plays an important role in CNNs for the reverse accumulation of the gradients during backpropagation. Let’s go through an example.</p>
    <p class="normal">For an input of size <img src="../Images/B14070_05_001.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> and a weight patch, or filter, of <img src="../Images/B14070_05_002.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, where <img src="../Images/B14070_05_003.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, the convolution operation slides the patch of weights over the input. Let’s denote the input by <em class="italic">X</em>, the patch of weights by <em class="italic">W</em>, and the output by <em class="italic">H</em>. The output <em class="italic">H </em>can be calculated as a matrix multiplication as follows.</p>
    <p class="normal">Let’s assume <img src="../Images/B14070_05_013.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> and <img src="../Images/B14070_05_014.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> for clarity and unwrap the input <em class="italic">X</em> from left to right, top to bottom, resulting in this: </p>
    <p class="center"><img src="../Images/B14070_05_015.png" alt="" style="height: 1.35em !important; vertical-align: 0.01em !important;"/></p>
    <p class="normal">Let’s define a new matrix <em class="italic">A</em> from <em class="italic">W</em>: </p>
    <p class="center"><img src="../Images/B14070_05_016.png" alt="" style="height: 5.42em !important;"/></p>
    <p class="normal">Then, if we perform the following matrix multiplication, we obtain <em class="italic">H</em>: </p>
    <p class="center"><img src="../Images/B14070_05_017.png" alt="" style="height: 1.05em !important; vertical-align: 0.27em !important;"/></p>
    <p class="normal">Now, by reshaping the output <img src="../Images/B14070_05_018.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/> to <img src="../Images/B14070_05_019.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/> we obtain the convolved output. Now let’s project this result back to <em class="italic">n</em> and <em class="italic">m</em>.</p>
    <p class="normal">By unwrapping the input <img src="../Images/B14070_05_020.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/> to <img src="../Images/B14070_05_021.png" alt="" style="height: 1.15em !important;"/> and by creating a matrix <img src="../Images/B14070_05_022.png" alt="" style="height: 1.15em !important;"/> from <em class="italic">w</em>, as we showed earlier, we obtain <img src="../Images/B14070_05_023.png" alt="" style="height: 1.15em !important;"/>, which will then be reshaped to <img src="../Images/B14070_05_024.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/>.</p>
    <p class="normal">Next, to obtain the transposed convolution, we simply transpose <em class="italic">A</em> and arrive at the following: </p>
    <p class="center"><img src="../Images/B14070_05_025.png" alt="" style="height: 1.35em !important; vertical-align: 0.08em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_05_026.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/> is the resultant output of the transposed convolution.</p>
    <p class="normal">We end our discussion about the<a id="_idIndexMarker457"/> convolution operation here. We discussed the convolution operation, convolution operation with stride, convolution operation with padding, and how to calculate the transposed convolution. Next, we will discuss the pooling operation in more detail.</p>
    <h2 id="_idParaDest-111" class="heading-2">Pooling operation</h2>
    <p class="normal">The pooling operation, which is<a id="_idIndexMarker458"/> sometimes known as the subsampling<a id="_idIndexMarker459"/> operation, was introduced to CNNs mainly for reducing the size of the intermediate outputs as well as for making the translation of CNNs invariant. This is preferred over the natural dimensionality reduction caused by convolution without <a id="_idIndexMarker460"/>padding, as we can decide where to reduce the size of the output with the pooling layer, in contrast to forcing it to happen every time. Forcing the dimensionality to decrease without padding would strictly limit the number of layers we can have in our CNN models.</p>
    <p class="normal">We define the pooling operation mathematically in the following sections. More precisely, we will discuss two types of pooling: max pooling and average pooling. First, however, we will define the notation. For an input of size <img src="../Images/B14070_05_001.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> and a kernel (analogous to the filter of a convolution layer) of size <img src="../Images/B14070_05_002.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, where <img src="../Images/B14070_05_003.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, the convolution operation slides the patch of weights over the input. Let’s denote the input by <em class="italic">X</em>, the patch of weights by <em class="italic">W</em>, and the output by <em class="italic">H</em>. Then let us use, <em class="italic">x</em> <sub class="subscript">i,j</sub>, <em class="italic">w</em><sub class="subscript">i,j</sub>, and <em class="italic">h</em><sub class="subscript">i,j</sub> to denote the value at the (<em class="italic">i</em>,<em class="italic">j</em>)<sup class="superscript">th</sup> location of <em class="italic">X</em>, <em class="italic">W</em>, and <em class="italic">H</em>, respectively. We will now look at specific implementations of pooling commonly used.</p>
    <h3 id="_idParaDest-112" class="heading-3">Max pooling</h3>
    <p class="normal">The max pooling operation picks the maximum element within the defined kernel of an input to produce the output. The max pooling<a id="_idIndexMarker461"/> operation shifts are windows over the input (the middle squares in<em class="italic"> Figure 5.8</em>) and take the maximum at each time. Mathematically, we define the pooling equation as follows: </p>
    <p class="center"><img src="../Images/B14070_05_030.png" alt="" style="height: 1.45em !important; vertical-align: 0.20em !important;"/></p>
    <p class="normal"><img src="../Images/B14070_05_030.1.png" alt=""/></p>
    <p class="normal"><em class="italic">Figure 5.8 </em>shows this operation:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_08.png" alt="Max pooling"/></figure>
    <p class="packt_figref">Figure 5.8: The max pooling operation with a filter size of 3, stride of 1, and no padding</p>
    <p class="normal">Next, let’s discuss how to perform max pooling with stride.</p>
    <h3 id="_idParaDest-113" class="heading-3">Max pooling with stride</h3>
    <p class="normal">Max pooling with stride is similar to<a id="_idIndexMarker462"/> convolution with stride. Here is the equation: </p>
    <p class="center"><img src="../Images/B14070_05_031.png" alt="" style="height: 2.60em !important; vertical-align: 0.03em !important;"/></p>
    <p class="normal"><img src="../Images/B14070_05_031.1.png" alt=""/></p>
    <p class="normal"><em class="italic">Figure 5.9</em> shows the result:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_09.png" alt="Max pooling with stride"/></figure>
    <p class="packt_figref">Figure 5.9: The max pooling operation for an input of size (n=4) with a filter size of (m=2), stride (s=2), and no padding</p>
    <p class="normal">We will discuss another variant of pooling<a id="_idIndexMarker463"/> known as average<a id="_idIndexMarker464"/> pooling, below.</p>
    <h3 id="_idParaDest-114" class="heading-3">Average pooling</h3>
    <p class="normal">Average pooling works similar to <a id="_idIndexMarker465"/>max pooling, except that instead of only taking the maximum, the average of all the inputs falling within the kernel is taken. Consider the following equation: </p>
    <p class="center"><img src="../Images/B14070_05_033.png" alt="" style="height: 2.08em !important; vertical-align: 0.04em !important;"/></p>
    <p class="normal">The average pooling operation is shown in <em class="italic">Figure 5.10</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_10.png" alt="Average pooling"/></figure>
    <p class="packt_figref">Figure 5.10: The average pooling operation for an input of size (n=4) with a filter size of (m=2), stride (s=1), and no padding</p>
    <p class="normal">We have so far discussed the operations directly performed on the two-dimensional inputs like images. Next we will <a id="_idIndexMarker466"/>discuss how they are connected to one-dimensional fully connected layers.</p>
    <h2 id="_idParaDest-115" class="heading-2">Fully connected layers</h2>
    <p class="normal">Fully connected layers are<a id="_idIndexMarker467"/> a fully connected set of weights from the input to the output. These fully connected weights are able to learn global information as they are connected from each input to each output. Also, having such layers of full connectedness allows us to combine features learned by the convolution layers preceding the fully connected layers, globally, to produce meaningful outputs.</p>
    <p class="normal">Let’s define the output of the last convolution or pooling layer to be of size <img src="../Images/B14070_05_034.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, where <em class="italic">p</em> is the height of the input, <em class="italic">o</em> is the width of the input, and <em class="italic">d</em> is the depth of the input. As an example, think of an RGB image, which will have a fixed height, fixed width, and a depth of 3 (one depth channel for each RGB component).</p>
    <p class="normal">Then, for the initial fully connected layer found immediately after the last convolution or pooling layer, the weight matrix will be <img src="../Images/B14070_05_035.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/>, where<em class="italic"> height </em>x<em class="italic"> width </em>x<em class="italic"> depth</em> of the layer output is the number of output units produced by that last layer and <em class="italic">m</em> is the number of hidden units in the fully connected layer. Then, during inference (or prediction), we reshape the output of the last convolution/pooling layer to be of size <img src="../Images/B14070_05_036.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> and perform the following matrix multiplication to obtain <em class="italic">h</em>: </p>
    <p class="center"><img src="../Images/B14070_05_037.png" alt="" style="height: 1.05em !important; vertical-align: 0.27em !important;"/></p>
    <p class="normal">The resultant fully connected layers will behave as in a fully connected neural network, where you have several fully connected layers and an output layer. The output layer can be a softmax classification layer for a classification problem or a linear layer for a regression problem.</p>
    <h2 id="_idParaDest-116" class="heading-2">Putting everything together</h2>
    <p class="normal">Now we will discuss <a id="_idIndexMarker468"/>how the convolutional, pooling, and fully connected layers come together to form a complete CNN.</p>
    <p class="normal">As shown in<em class="italic"> Figure 5.11</em>, the convolution, pooling, and fully connected layers come together to form an end-to-end learning model that takes raw data, which can be high-dimensional (for example, RGB images) and produce meaningful output (for example, the class of the object). First, the convolution layers learn the spatial features of the images. </p>
    <p class="normal">The lower convolution layers learn low-level features such as differently oriented edges present in the images, and the higher layers learn more high-level features such as shapes present in the images (for example, circles and triangles) or bigger parts of an object (for example, the face of a dog, tail of a dog, and front section of a car). The pooling layers in the middle make each of these learned features slightly translation invariant. This means that, in a new image, even if the feature appears a bit offset compared to the location in which the feature appeared in the learned images, the CNN will still recognize that feature. Finally, the fully connected layers combine the high-level features learned by the CNN to produce global representations that will be used by the final output layer to determine the class the object belongs to:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_11.png" alt="Putting everything together"/></figure>
    <p class="packt_figref">Figure 5.11: Combining convolution layers, pooling layers, and fully connected layers to form a CNN</p>
    <p class="normal">With a strong conceptual understanding of a CNN, we will now get started on our first use case: classifying images with a CNN model.</p>
    <h1 id="_idParaDest-117" class="heading-1">Exercise – image classification on Fashion-MNIST with CNN</h1>
    <p class="normal">This will be our first example of <a id="_idIndexMarker469"/>using a CNN for a real-world machine learning task. We will classify images using a CNN. The reason for not starting with an NLP task is that applying CNNs to NLP tasks (for example, sentence classification) is not very<a id="_idIndexMarker470"/> straightforward. There are several tricks involved in<a id="_idIndexMarker471"/> using CNNs for such a task. However, originally, CNNs were designed to cope with image data. Therefore, let’s start there, and then find our way through to see how CNNs apply to NLP tasks in the <em class="italic">Using CNNs for sentence classification</em> section.</p>
    <h2 id="_idParaDest-118" class="heading-2">About the data</h2>
    <p class="normal">In this exercise, we will use a dataset well-known in the computer vision community: the Fashion-MNIST dataset. Fashion-MNIST was<a id="_idIndexMarker472"/> inspired by the famous MNIST dataset (<a href="http://yann.lecun.com/exdb/mnist/"><span class="url">http://yann.lecun.com/exdb/mnist/</span></a>). MNIST is a database of labeled images of handwritten digits from 0 to 9 (i.e. 10 digits). However, due to the simplicity of the MNIST image classification task, test accuracy on MNIST is just shy of 100%. At the time of writing, the popular research benchmarking site <em class="italic">paperswithcode.com</em> has published a test accuracy of 99.87% (<a href="https://paperswithcode.com/sota/image-classification-on-mnist"><span class="url">https://paperswithcode.com/sota/image-classification-on-mnist</span></a>). Because of this, Fashion-MNIST came to life.</p>
    <p class="normal">Fashion-MNIST consists of images of clothing garments. Our task is to classify each garment into a category (e.g. dress, t-shirt). The dataset contains two sets: the training set, and the test set. We will train on the training set and evaluate the performance of our model on the unseen test dataset. We will further split the training set into two sets: training and validation sets. We will use the validation dataset as a continuous performance monitoring mechanism for our model. We will discuss the details later, but we will see that we can reach up to approximately 88% test accuracy without any special regularization or tricks.</p>
    <h2 id="_idParaDest-119" class="heading-2">Downloading and exploring the data</h2>
    <p class="normal">The very first task will be to <a id="_idIndexMarker473"/>download and explore the data. To download the data, we will simply tap into the <code class="inlineCode">tf.keras.datasets</code> module, as it provides several datasets to be downloaded conveniently through TensorFlow. To see what other datasets are available, visit <a href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/datasets</span></a>. The full code for this chapter is available in <code class="inlineCode">ch5_image_classification_fashion_mnist.ipynb</code> in the <code class="inlineCode">Ch05-Sentence-Classification</code> folder. Simply call the following function to download the data:</p>
    <pre class="programlisting code"><code class="hljs-code">(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()
</code></pre>
    <p class="normal">The data will be downloaded to a default cache directory specified by TensorFlow (for example: <code class="inlineCode">~/.keras/dataset/fasion_minst</code>).</p>
    <p class="normal">We will then see the sizes of the <a id="_idIndexMarker474"/>data by printing their shapes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">"train_images is of shape: {}"</span>.<span class="hljs-built_in">format</span>(train_images.shape))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"train_labels is of shape: {}"</span>.<span class="hljs-built_in">format</span>(train_labels.shape))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"test_images is of shape: {}"</span>.<span class="hljs-built_in">format</span>(test_images.shape))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"test_labels is of shape: {}"</span>.<span class="hljs-built_in">format</span>(test_labels.shape))
</code></pre>
    <p class="normal">This will produce:</p>
    <pre class="programlisting con"><code class="hljs-con">train_images is of shape: (60000, 28, 28)
train_labels is of shape: (60000,)
test_images is of shape: (10000, 28, 28)
test_labels is of shape: (10000,)
</code></pre>
    <p class="normal">We can see that we have 60,000 training images, each of size 28x28, and 10,000 testing images of the same dimensions. The labels are simple class IDs ranging from 0 to 9. We will also create a variable to contain the class ID to class name mapping, which will help us during explorations and post-training analysis:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Available at: https://www.tensorflow.org/api_docs/python/tf/keras/</span>
<span class="hljs-comment"># datasets/fashion_mnist/load_data</span>
label_map = {
    <span class="hljs-number">0</span>: <span class="hljs-string">"T-shirt/top"</span>, <span class="hljs-number">1</span>: <span class="hljs-string">"Trouser"</span>, <span class="hljs-number">2</span>: <span class="hljs-string">"Pullover"</span>, <span class="hljs-number">3</span>: <span class="hljs-string">"Dress"</span>, <span class="hljs-number">4</span>: <span class="hljs-string">"Coat"</span>,
    <span class="hljs-number">5</span>: <span class="hljs-string">"Sandal"</span>, <span class="hljs-number">6</span>: <span class="hljs-string">"Shirt"</span>, <span class="hljs-number">7</span>: <span class="hljs-string">"Sneaker"</span>,  <span class="hljs-number">8</span>: <span class="hljs-string">"Bag"</span>, <span class="hljs-number">9</span>: <span class="hljs-string">"Ankle boot"</span>
}
</code></pre>
    <p class="normal">We can also plot the images, which will give the following plot of images (<em class="italic">Figure 5.12</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.12: An overview of the images found in the Fashion-MNIST dataset</p>
    <p class="normal">Finally, we are going to extend <code class="inlineCode">train_images</code> and <code class="inlineCode">test_images</code> by adding a new dimension (of size 1) to the end of each tensor. Standard implementation of the convolution operation in TensorFlow is<a id="_idIndexMarker475"/> designed to work on a four-dimensional input (i.e. batch, height, width, and channel dimensions). </p>
    <p class="normal">Here, the channel dimension is omitted in the images as they are black and white images. Therefore, to comply with the dimensional requirement of TensorFlow’s convolution operation, we add this additional dimension to the images. This is a necessity for using the convolution operation in CNNs. You can do this as follows: </p>
    <pre class="programlisting code"><code class="hljs-code">train_images = train_images[:, : , :, <span class="hljs-literal">None</span>]
test_images = test_images[:, : ,: , <span class="hljs-literal">None</span>]
</code></pre>
    <p class="normal">Using the indexing and slicing capabilities available in NumPy, you can simply add a <code class="inlineCode">None</code> dimension to the tensor when indexing as above. Let’s now check the shapes of the tensors:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">"train_images is of shape: {}"</span>.<span class="hljs-built_in">format</span>(train_images.shape))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"test_images is of shape: {}"</span>.<span class="hljs-built_in">format</span>(test_images.shape))
</code></pre>
    <p class="normal">This gives:</p>
    <pre class="programlisting code"><code class="hljs-code">train_images <span class="hljs-keyword">is</span> of shape: (<span class="hljs-number">60000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)
test_images <span class="hljs-keyword">is</span> of shape: (<span class="hljs-number">10000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Let’s have a crack at<a id="_idIndexMarker476"/> implementing a CNN model that can learn from this data.</p>
    <h2 id="_idParaDest-120" class="heading-2">Implementing the CNN</h2>
    <p class="normal">In this subsection, we will look at<a id="_idIndexMarker477"/> some important code snippets from the TensorFlow implementation of the CNN. The full code is available in <code class="inlineCode">ch5_image_classification_mnist.ipynb</code> in the <code class="inlineCode">Ch05-Sentence-Classification</code> folder. First, we will define several important hyperparameters. The code comments are self-explanatory for the purpose of these hyperparameters:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">100</span> <span class="hljs-comment"># This is the typical batch size we've been using</span>
image_size = <span class="hljs-number">28</span> <span class="hljs-comment"># This is the width/height of a single image</span>
<span class="hljs-comment"># Number of color channels in an image. These are black and white images </span>
n_channels = <span class="hljs-number">1</span> 
<span class="hljs-comment"># Number of different digits we have images for (i.e. classes)</span>
n_classes = <span class="hljs-number">10</span>
</code></pre>
    <p class="normal">With that, we can start to implement the model. We will find inspiration from one of the earliest CNN models, known as LeNet, introduced<a id="_idIndexMarker478"/> in the paper <em class="italic">Gradient-Based Learning Applied to Document Recognition</em> by LeCun et al. (<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"><span class="url">http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf</span></a>). This model will be a great start as it is a simple model yet gives a reasonably good performance on the dataset. We will introduce some slight modifications to the original model, because the original model operated on a 32x32-sized image, whereas in our case, the image is a 28x28-sized image. </p>
    <p class="normal">Let’s go through some quick details of the model. It has the following sequence of layers:</p>
    <ul>
      <li class="bulletList">A convolutional layer with a 5x5 kernel, 1x1 stride, and valid padding</li>
      <li class="bulletList">A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling</li>
      <li class="bulletList">A convolutional layer with a 5x5 kernel, 1x1 stride, and valid pooling</li>
      <li class="bulletList">A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling</li>
      <li class="bulletList">A convolutional layer with a 4x4 kernel, 1x1 stride, and valid pooling</li>
      <li class="bulletList">A layer that flattens the 2D output to a 1D vector</li>
      <li class="bulletList">A Dense layer with 84 nodes</li>
      <li class="bulletList">A final softmax prediction layer with 10 nodes</li>
    </ul>
    <p class="normal">Here, all the layers except the last have ReLU (Rectified Linear Unit) activation. A convolutional layer in a CNN model generalizes the convolution operation we discussed, to work on multi-channel inputs and produce multi-channel outputs. Let’s understand what we meant by that. The<a id="_idIndexMarker479"/> original convolution operation we saw operated on a simple 2D plane with a height <em class="italic">h</em> and width <em class="italic">w</em>. Next, the kernel moves over the plane while producing a single value at each position. This process produces another 2D plane. But in practice, CNN models operate on four-dimensional inputs, i.e. an input of size <code class="inlineCode">[batch size, height, width, in channels]</code>, and produce an output that is a four-dimensional, i.e. an output of size <code class="inlineCode">[batch size, height, width, out channels]</code>. To produce this output, the kernel would need to be a four-dimensional tensor having the dimensions <code class="inlineCode">[kernel height, kernel width, in channels, out channels]</code>. </p>
    <p class="normal">It might not be entirely clear why inputs, outputs, and kernels would be in this format. <em class="italic">Figure 5.13</em> clarifies this.</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.13: How input and output shapes look for a two-dimensional convolution layer</p>
    <p class="normal">Below, we will outline the full model. Don’t worry if you don’t understand it at first glance. We will go through line by<a id="_idIndexMarker480"/> line to understand how the model comes to be:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Conv2D, MaxPool2D, Flatten, Dense
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
K.clear_session()
lenet_like_model = Sequential([
    <span class="hljs-comment"># 1st convolutional layer</span>
    Conv2D(
        filters=<span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">'valid'</span>, 
        activation=<span class="hljs-string">'relu'</span>, 
        input_shape=(image_size,image_size,n_channels)
    ), <span class="hljs-comment"># in 28x28 / out 24x24</span>
    <span class="hljs-comment"># 1st max pooling layer</span>
    MaxPool2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">'valid'</span>), 
    <span class="hljs-comment"># in 24x24 / out 12x12</span>
    <span class="hljs-comment"># 2nd convolutional layer</span>
    Conv2D(filters=<span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), 
    padding=<span class="hljs-string">'valid'</span>, activation=<span class="hljs-string">'relu'</span>), <span class="hljs-comment"># in 12x12 / out 8x8</span>
    <span class="hljs-comment"># 2nd max pooling layer</span>
    MaxPool2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">'valid'</span>), 
    <span class="hljs-comment"># in 8x8 / out 4x4</span>
    <span class="hljs-comment"># 3rd convolutional layer</span>
    Conv2D(filters=<span class="hljs-number">120</span>, kernel_size=(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), 
    padding=<span class="hljs-string">'valid'</span>, activation=<span class="hljs-string">'relu'</span>), <span class="hljs-comment"># in 4x4 / out 1x1</span>
    <span class="hljs-comment"># flatten the output of the last layer to suit a fully connected layer</span>
    Flatten(),
    <span class="hljs-comment"># First dense (fully-connected) layer</span>
    Dense(<span class="hljs-number">84</span>, activation=<span class="hljs-string">'relu'</span>),
    <span class="hljs-comment"># Final prediction layer</span>
    Dense(n_classes, activation=<span class="hljs-string">'softmax'</span>)
])
</code></pre>
    <p class="normal">The very first thing to notice is that we are using the Keras Sequential API. The CNN we are implementing here has <a id="_idIndexMarker481"/>a series of layers connected one after the other. Therefore, we will use the simplest API possible. We then have our first convolutional layer. We have already discussed the convolution operation. Let’s take the first line:</p>
    <pre class="programlisting code"><code class="hljs-code">Conv2D(
        filters=<span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">'valid'</span>, 
        activation=<span class="hljs-string">'relu'</span>, 
        input_shape=(image_size,image_size,n_channels)
    )
</code></pre>
    <p class="normal">The <code class="inlineCode">tensorflow.keras.layers.Conv2D</code> layer takes the following argument values in that order:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">filters</code> (<code class="inlineCode">int</code>): This is the number of output filters (i.e. the number of out channels).</li>
      <li class="bulletList"><code class="inlineCode">kernel_size</code> (<code class="inlineCode">Tuple[int]</code>): This is the (height, width) of the convolution kernel.</li>
      <li class="bulletList"><code class="inlineCode">strides</code> (<code class="inlineCode">Tuple[int]</code>): This denotes the stride on the height and width dimension of the input.</li>
      <li class="bulletList"><code class="inlineCode">padding</code> (<code class="inlineCode">str</code>): This denotes the type of padding (can be <code class="inlineCode">'</code><code class="inlineCode">SAME'</code> or <code class="inlineCode">'VALID'</code>).</li>
      <li class="bulletList"><code class="inlineCode">activation</code> (<code class="inlineCode">str</code>): The non-linear activation used.</li>
      <li class="bulletList"><code class="inlineCode">input_shape</code> (<code class="inlineCode">Tuple[int]</code>): The shape of the input. When defining <code class="inlineCode">input_shape</code>, we do not specify the batch dimension as it’s automatically added.</li>
    </ul>
    <p class="normal">Next, we have the first max-pooling layer, which looks as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">MaxPool2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">'valid'</span>)
</code></pre>
    <p class="normal">The arguments are quite similar to the ones in <code class="inlineCode">tf.keras.layers.Conv2D</code>. The <code class="inlineCode">pool_size</code> argument corresponds to the <code class="inlineCode">kernel_size</code> argument that specifies the (height, width) of the pool window. Following a similar pattern, the following convolutional and pooling layers are defined. The final <a id="_idIndexMarker482"/>convolution layer produces a <code class="inlineCode">[batch size, 1, 1, 120]</code>-sized output. The height and width dimensions are equal to 1, because LeNet is designed in a way that the last convolutional kernel has the same height and width as the output. Before this input is fed to a fully connected layer, we need to flatten this output, such that it has the shape <code class="inlineCode">[batch size, 120]</code>. This is because a standard Dense layer takes a two-dimensional input. For that, we use the <code class="inlineCode">tf.keras.layers.Flatten()</code> layer:</p>
    <pre class="programlisting code"><code class="hljs-code">Flatten(),
</code></pre>
    <p class="normal">Finally, we define two Dense layers as follows. </p>
    <pre class="programlisting code"><code class="hljs-code">Dense(<span class="hljs-number">84</span>, activation=<span class="hljs-string">'relu'</span>),
Dense(n_classes, activation=<span class="hljs-string">'softmax'</span>)
</code></pre>
    <p class="normal">As the final step, we will compile the model using the sparse categorical cross-entropy loss and the Adam optimizer. We will also track the accuracy on the data:</p>
    <pre class="programlisting code"><code class="hljs-code">lenet_like_model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">With the data prepared and the model defined fully, we are good to train our model. Model training is as simple as calling one function:</p>
    <pre class="programlisting code"><code class="hljs-code">lenet_like_model.fit(train_images, train_labels, validation_split=<span class="hljs-number">0.2</span>, batch_size=batch_size, epochs=<span class="hljs-number">5</span>)
</code></pre>
    <p class="normal">The <code class="inlineCode">tf.keras.layers.Model.fit()</code> takes many arguments. But let’s only discuss the ones we have used here:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">x </code>(<code class="inlineCode">np.ndarray</code> / <code class="inlineCode">tf.Tensor</code> / other): Takes in a tensor that will act as input to the model (implemented as a NumPy array or a TensorFlow tensor). But the accepted values are not limited just to tensors. To see the full list, please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</span></a>.</li>
      <li class="bulletList"><code class="inlineCode">y </code>(<code class="inlineCode">np.ndarray</code> / <code class="inlineCode">tf.Tensor</code>): Takes in a tensor that will act as the labels (targets) for the model.</li>
      <li class="bulletList"><code class="inlineCode">validation_split</code> (<code class="inlineCode">float</code>): Setting this argument means a fraction of training data (e.g. 0.2 translates to 20%) will be used as validation data.</li>
      <li class="bulletList"><code class="inlineCode">epochs</code> (<code class="inlineCode">int</code>): The number of epochs to train the model for.</li>
    </ul>
    <p class="normal">You can evaluate the trained model on the test data by calling:</p>
    <pre class="programlisting code"><code class="hljs-code">lenet_like_model.evaluate(test_images, test_labels)
</code></pre>
    <p class="normal">Once run, you’ll see an output as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">313/313 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8806
</code></pre>
    <p class="normal">The model should get up to around 88% accuracy when trained.</p>
    <p class="normal">You just finished learning <a id="_idIndexMarker483"/>about the functions that we used to create our first CNN. You learned to use the functions to implement the CNN structure as well as define the loss, minimize the loss, and get predictions for unseen data. We used a simple CNN to see if it could learn to classify clothing items. Also, we were able to achieve an accuracy above 88% with a reasonably simple CNN. Next, we will analyze some of the results produced by the CNN. We will see why the CNN couldn’t recognize some of the images correctly.</p>
    <h2 id="_idParaDest-121" class="heading-2">Analyzing the predictions produced with a CNN</h2>
    <p class="normal">Here, we can randomly <a id="_idIndexMarker484"/>pick some correctly and incorrectly classified samples from the test set to evaluate the learning power of CNNs (see <em class="italic">Figure 5.14</em>). </p>
    <p class="normal">We can see that for the correctly classified instances, the CNN is very confident about the output, most of the time. This is a good sign that the model is making very confident and accurate decisions. However, when we evaluate the incorrectly classified examples, we can see that some of them are in fact difficult, and even a human can get some of them wrong. For example, for an ankle boot that’s classified as a sandal, there is a large black patch that can indicate the presence of straps, which makes it more likely to be a sandal (the third image from the right in<a id="_idIndexMarker485"/> the third row). Also, in the fifth image from the right in the third row, it’s difficult to say whether it’s a shirt or a collared t-shirt:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.14: Fashion-MNIST correctly classified and misclassified instances</p>
    <h1 id="_idParaDest-122" class="heading-1">Using CNNs for sentence classification</h1>
    <p class="normal">Though CNNs have mostly been<a id="_idIndexMarker486"/> used for computer vision tasks, nothing stops them from being used in NLP applications. But as we highlighted earlier, CNNs were originally designed for visual content. Therefore, using <a id="_idIndexMarker487"/>CNNs for NLP tasks requires somewhat more effort. This is why we started out learning about CNNs with a simple computer vision problem. CNNs are an attractive choice for machine learning problems due to the low parameter count of convolution layers. One such NLP application for which CNNs have been used effectively is sentence classification.</p>
    <p class="normal">In sentence classification, a given sentence should be classified with a class. We will use a question database, where each question is labeled by what the question is about. For example, the question “Who was Abraham Lincoln?” will be a question and its label will be <em class="italic">Person</em>. For this we will use a sentence classification dataset available at <a href="http://cogcomp.org/Data/QA/QC/"><span class="url">http://cogcomp.org/Data/QA/QC/</span></a>; here you will find several datasets. We are using the set with ~5,500 training questions and their respective labels and 500 testing sentences.</p>
    <p class="normal">We will use the CNN network introduced in a paper by Yoon Kim, <em class="italic">Convolutional Neural Networks for Sentence Classification</em>, to understand the value of CNNs for NLP tasks. However, using CNNs for sentence classification is somewhat different from the Fashion-MNIST example we discussed, because operations (for example, convolution and pooling) now happen in one dimension (length) rather than two dimensions (height and width). Furthermore, the pooling operations will also have a different flavor to the normal pooling operation, as we will see soon. You can find the code for this exercise in the <code class="inlineCode">ch5_cnn_sentence_classification.ipynb</code> file in the <code class="inlineCode">Ch5-Sentence-Classification</code> folder. As the first step, we will understand the data.</p>
    <h2 id="_idParaDest-123" class="heading-2">How data is transformed for sentence classification</h2>
    <p class="normal">Let’s assume a sentence of <em class="italic">p</em> words. First, we will <a id="_idIndexMarker488"/>pad the sentence with some special words (if the length of the sentence is &lt; <em class="italic">n</em>) to set the sentence length to <em class="italic">n</em> words, where <img src="../Images/B14070_05_038.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>. Next, we will represent each word in the sentence by a vector of size <em class="italic">k</em>, where this vector can either be a one-hot-encoded representation, or Word2vec word vectors learned using skip-gram, CBOW, or GloVe. Then a batch of sentences of size <em class="italic">b</em> can be represented by a <img src="../Images/B14070_05_039.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> matrix.</p>
    <p class="normal">Let’s walk through an example. Let’s consider the following three sentences:</p>
    <ul>
      <li class="bulletList"><em class="italic">Bob and Mary are friends.</em></li>
      <li class="bulletList"><em class="italic">Bob plays soccer.</em></li>
      <li class="bulletList"><em class="italic">Mary likes to sing in the choir.</em></li>
    </ul>
    <p class="normal">In this example, the third sentence has the most words, so let’s set <em class="italic">n</em> = <em class="italic">7</em>, which is the number of words in the<a id="_idIndexMarker489"/> third sentence. Next, let’s look at the one-hot-encoded representation for each word. In this case, there are 13 distinct words. Therefore, we get this:</p>
    <p class="normal"><em class="italic">Bob</em>: 1,0,0,0,0,0,0,0,0,0,0,0,0</p>
    <p class="normal"><em class="italic">and</em>: 0,1,0,0,0,0,0,0,0,0,0,0,0</p>
    <p class="normal"><em class="italic">Mary</em>: 0,0,1,0,0,0,0,0,0,0,0,0,0</p>
    <p class="normal">Also, <em class="italic">k</em> = <em class="italic">13</em> for the same reason. With this representation, we can represent the three sentences as a three-dimensional matrix of size <em class="italic">3 x 7 x 13</em>, as shown in <em class="italic">Figure 5.15</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_15.png" alt="Data transformation"/></figure>
    <p class="packt_figref">Figure 5.15: A batch of sentences represented as a sentence matrix</p>
    <p class="normal">You could also utilize word embeddings instead of one-hot encoding here. Representing each word as a one-hot-encoded feature introduces sparsity and wastes computational memory. By using embeddings, we are enabling the model to learn more compact and powerful word representations than one-hot-encoded representations. This also means that <img src="../Images/B14070_04_020.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> becomes a<a id="_idIndexMarker490"/> hyperparameter (i.e. the embedding size), as opposed to being driven by the size of the vocabulary. This means that, in <em class="italic">Figure 5.15</em>, each column will be a distributed continuous vector, not a combination of 0s and 1s.</p>
    <div class="note">
      <p class="normal">We know that one-hot vectors lead to high-dimensional and highly sparse representations that are sub-optimal. On the other hand, word vectors give richer representations of words. However, learning word vectors is computationally costly. There is another alternative called the hashing trick. The beauty of the hashing trick is that it is <a id="_idIndexMarker491"/>extremely simple but gives a powerful and economical alternative that sits between one-hot vectors and word vectors. The idea behind the hashing trick is to use a hash function that converts a given token to an integer. </p>
      <p class="center"><em class="italic">f(&lt;token&gt;)--&gt;hash value</em></p>
      <p class="normal">Here <em class="italic">f</em> is a chosen hash function. Some example popular hash functions are SHA (<a href="https://brilliant.org/wiki/secure-hashing-algorithms/"><span class="url">https://brilliant.org/wiki/secure-hashing-algorithms/</span></a>) and MD5 (<a href="https://searchsecurity.techtarget.com/definition/MD5"><span class="url">https://searchsecurity.techtarget.com/definition/MD5</span></a>). There’s also more advanced hashing such as locality-sensitive hashing (<a href="https://www.pinecone.io/learn/locality-sensitive-hashing/"><span class="url">https://www.pinecone.io/learn/locality-sensitive-hashing/</span></a>) to give out similar IDs for morphologically similar words. You can easily use the hashing trick via TensorFlow (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick</span></a>).</p>
    </div>
    <h2 id="_idParaDest-124" class="heading-2">Implementation – downloading and preparing data</h2>
    <p class="normal">First we will download the<a id="_idIndexMarker492"/> data from the web. The data download functions are provided in the notebook and are simply downloading two files: training and testing data (the paths to the files are retained in <code class="inlineCode">train_filename</code> and <code class="inlineCode">test_filename</code>). </p>
    <p class="normal">If you open these files you will see that they contain a collection of lines of text. Each line has the format:</p>
    <p class="center"><code class="inlineCode">&lt;Category&gt;: &lt;sub-category&gt; &lt;question&gt;</code></p>
    <p class="normal">There are two pieces of meta information for each question: a category and a sub-category. A category is a macro-level classification, where sub-category is a finer grain identification of the type of the question. There are six categories available: <code class="inlineCode">DESC</code> (description-related), <code class="inlineCode">ENTY</code> (entity-related), <code class="inlineCode">HUM</code> (human-related), <code class="inlineCode">ABBR</code> (abbreviation related), <code class="inlineCode">NUM</code> (numerical), and <code class="inlineCode">LOC</code> (location related). Each category has several sub-categories associated with them. For example, the <code class="inlineCode">ENTY</code> category is further broken down to animal, currency, events, food, etc. For our problem, we will be focusing on high-level classification (i.e. six classes), but you could also leverage the same model with minimal changes to classify on the sub-category level.</p>
    <p class="normal">Once the files are downloaded, we’ll read the data<a id="_idIndexMarker493"/> into the memory. For that, we will implement the <code class="inlineCode">read_data()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">read_data</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-string">'''</span>
<span class="hljs-string">    Read data from a file with given filename</span>
<span class="hljs-string">    Returns a list of strings where each string is a lower case word</span>
<span class="hljs-string">    '''</span>
    <span class="hljs-comment"># Holds question strings, categories and sub categories</span>
    <span class="hljs-comment"># category/sub_cateory definitions: https://cogcomp.seas.upenn.edu/</span>
<span class="hljs-comment">    # Data/QA/QC/definition.html</span>
    questions, categories, sub_categories = [], [], []     
    
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename,<span class="hljs-string">'r'</span>,encoding=<span class="hljs-string">'latin-1'</span>) <span class="hljs-keyword">as</span> f:        
        <span class="hljs-comment"># Read each line</span>
        <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> f:   
            <span class="hljs-comment"># Each string has format &lt;cat&gt;:&lt;sub cat&gt; &lt;question&gt;</span>
            <span class="hljs-comment"># Split by : to separate cat and (sub_cat + question)</span>
            row_str = row.split(<span class="hljs-string">":"</span>)        
            cat, sub_cat_and_question = row_str[<span class="hljs-number">0</span>], row_str[<span class="hljs-number">1</span>]
            tokens = sub_cat_and_question.split(<span class="hljs-string">' '</span>)
            <span class="hljs-comment"># The first word in sub_cat_and_question is the sub </span>
<span class="hljs-comment">            # category rest is the question</span>
            sub_cat, question = tokens[<span class="hljs-number">0</span>], <span class="hljs-string">' '</span>.join(tokens[<span class="hljs-number">1</span>:])        
            
            questions.append(question.lower().strip())
            categories.append(cat)
            sub_categories.append(sub_cat)
            
    <span class="hljs-keyword">return</span> questions, categories, sub_categories
train_questions, train_categories, train_sub_categories = read_data(train_filename)
test_questions, test_categories, test_sub_categories = read_data(test_filename)
</code></pre>
    <p class="normal">This function simply goes through each line in the file and separates the question, category, and sub-category, using the format of each line elucidated above. After that, each question, category, and sub-category is written to the lists <code class="inlineCode">questions</code>, <code class="inlineCode">categories</code>, and <code class="inlineCode">sub_categories</code> respectively. Finally, the function returns these lists. With the <code class="inlineCode">questions</code>, <code class="inlineCode">categories</code>, and <code class="inlineCode">sub_categories</code> available for both training and testing data, we will create <code class="inlineCode">pandas</code> DataFrames for training and testing data. </p>
    <p class="normal"><code class="inlineCode">pandas</code> DataFrames are an expressive<a id="_idIndexMarker494"/> data structure for storing multi-dimensional data. A DataFrame can have indices, columns, and values. Each value has a specific index and a column. It is quite simple to create a DataFrame: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define training and testing</span>
train_df = pd.DataFrame(
    {<span class="hljs-string">'question'</span>: train_questions, <span class="hljs-string">'category'</span>: train_categories, 
    <span class="hljs-string">'sub_category'</span>: train_sub_categories}
)
test_df = pd.DataFrame(
    {<span class="hljs-string">'question'</span>: test_questions, <span class="hljs-string">'category'</span>: test_categories,
    <span class="hljs-string">'sub_category'</span>: test_sub_categories}
)
</code></pre>
    <p class="normal">We call the <code class="inlineCode">pd.DataFrame</code> construct with a dictionary. The keys of the dictionary represent columns of the DataFrame, and the values represent the elements in each column. Here we create three columns: <code class="inlineCode">question</code>, <code class="inlineCode">category</code>, and <code class="inlineCode">sub_category</code>. </p>
    <p class="normal"><em class="italic">Figure 5.16</em> depicts what the <code class="inlineCode">train_df</code> looks like.</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.16: A sample of data captured in the pandas DataFrame</p>
    <p class="normal">We will do a simple shuffle of rows in the<a id="_idIndexMarker495"/> training set, to make sure we are not introducing any unintentional ordering in the data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Shuffle the data for better randomization</span>
train_df = train_df.sample(frac=<span class="hljs-number">1.0</span>, random_state=seed)
</code></pre>
    <p class="normal">This process will sample 100% of the data from the DataFrame randomly. In other words, it will shuffle the order of the rows. From this point onward, we will not consider the <code class="inlineCode">sub_category</code> column. We will first map each class label to a class ID:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Generate the label to ID mapping</span>
unique_cats = train_df[<span class="hljs-string">"category"</span>].unique()
labels_map = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(unique_cats, np.arange(unique_cats.shape[<span class="hljs-number">0</span>])))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Label-&gt;ID mapping: {}"</span>.<span class="hljs-built_in">format</span>(labels_map))
n_classes = <span class="hljs-built_in">len</span>(labels_map)
<span class="hljs-comment"># Convert all string labels to IDs</span>
train_df[<span class="hljs-string">"category"</span>] = train_df[<span class="hljs-string">"category"</span>].<span class="hljs-built_in">map</span>(labels_map)
test_df[<span class="hljs-string">"category"</span>] = test_df[<span class="hljs-string">"category"</span>].<span class="hljs-built_in">map</span>(labels_map)
</code></pre>
    <p class="normal">We first identify the unique values present in the <code class="inlineCode">train_df["category"]</code>. Then we will create a dictionary by mapping from the unique values to a list of numerical IDs (0 to 5). The <code class="inlineCode">np.arange() </code>function can be used to generate a series of integers in a specified range (here, the range is from 0 to the length of <code class="inlineCode">unique_cats</code>). This process will give us the following <code class="inlineCode">labels_map</code>.</p>
    <p class="normal"><code class="inlineCode">Label-&gt;ID mapping: {0: 0, 1: 1, 2: 2, 4: 3, 3: 4, 5: 5}</code></p>
    <p class="normal">Then we simply apply this <a id="_idIndexMarker496"/>mapping to the category column of both the train and test DataFrames to convert string labels to numerical labels. The data would look as follows, after the transformation (<em class="italic">Figure 5.17</em>).</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.17: A sample of data in the DataFrame after mapping categories to integers</p>
    <p class="normal">We create a validation set, stemming from the original training set, to monitor model performance while it trains. We will use the <code class="inlineCode">train_test_split()</code> function from the scikit-learn library. 10% of the data will be separated as validation data, while 90% is kept as training data.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
train_df, valid_df = train_test_split(train_df, test_size=<span class="hljs-number">0.1</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Train size: {}"</span>.<span class="hljs-built_in">format</span>(train_df.shape))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Valid size: {}"</span>.<span class="hljs-built_in">format</span>(valid_df.shape))
</code></pre>
    <p class="normal">This outputs:</p>
    <pre class="programlisting con"><code class="hljs-con">Train size: (4906, 3)
Valid size: (546, 3)
</code></pre>
    <p class="normal">We can see that approximately 4,900 examples are used as training and the rest as validation. In the next <a id="_idIndexMarker497"/>section, we will build a tokenizer to tokenize the questions and assign individual tokens numerical IDs.</p>
    <h3 id="_idParaDest-125" class="heading-3">Implementation – building a tokenizer</h3>
    <p class="normal">Moving on, now it’s time to <a id="_idIndexMarker498"/>build a tokenizer that can map words to numerical IDs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-comment"># Define a tokenizer and fit on train data</span>
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_df[<span class="hljs-string">"question"</span>].tolist())
</code></pre>
    <p class="normal">Here we simply create a <code class="inlineCode">Tokenizer</code> object and use the <code class="inlineCode">fit_on_texts()</code> function to train it on the training corpus. In this process, the tokenizer will map words in the vocabulary to IDs. We will convert all of the train, validation, and test inputs to sequences of word IDs. Simply call the <code class="inlineCode">tokenizer.texts_to_sequences()</code> function with a list of strings, where each string represents a question:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Convert each list of tokens to a list of IDs, using tokenizer's mapping</span>
train_sequences = tokenizer.texts_to_sequences(train_df[<span class="hljs-string">"question"</span>].tolist())
valid_sequences = tokenizer.texts_to_sequences(valid_df[<span class="hljs-string">"</span><span class="hljs-string">question"</span>].tolist())
test_sequences = tokenizer.texts_to_sequences(test_df[<span class="hljs-string">"question"</span>].tolist())
</code></pre>
    <p class="normal">It’s important to understand that we are feeding our model a batch of questions at a given time. It is very unlikely that all of the questions have the same number of tokens. If all questions do not have the same number of tokens, we cannot form a tensor due to the uneven lengths of different questions. To solve this, we have to pad shorter sequences with special tokens and truncate sequences longer than a specified length. To achieve this we can easily use the <code class="inlineCode">tf.keras.preprocessing.sequence.pad_sequences()</code> function. It would be worthwhile going through the arguments accepted by this function:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">sequences (List[List[int]])</code> – List of list integers; each list of integers is a sequence</li>
      <li class="bulletList"><code class="inlineCode">maxlen (int)</code> – The maximum padding length</li>
      <li class="bulletList"><code class="inlineCode">padding (string) </code>– Whether to pad at the beginning <code class="inlineCode">(pre)</code> or end <code class="inlineCode">(post)</code></li>
      <li class="bulletList"><code class="inlineCode">truncating (string)</code> – Whether to truncate at the beginning <code class="inlineCode">(pre)</code> or end <code class="inlineCode">(post)</code></li>
      <li class="bulletList"><code class="inlineCode">value (int)</code> – What value is to be used for padding (defaults to 0)</li>
    </ul>
    <p class="normal">Below we use this function to create sequence matrices for training, validation, and testing data:</p>
    <pre class="programlisting code"><code class="hljs-code">max_seq_length = <span class="hljs-number">22</span>
<span class="hljs-comment"># Pad shorter sentences and truncate longer ones (maximum length: max_seq_</span>
<span class="hljs-comment"># length)</span>
preprocessed_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(
    train_sequences, maxlen=max_seq_length, padding=<span class="hljs-string">'post'</span>,
    truncating=<span class="hljs-string">'post'</span>
)
preprocessed_valid_sequences = tf.keras.preprocessing.sequence.pad_sequences(
    valid_sequences, maxlen=max_seq_length, padding=<span class="hljs-string">'post'</span>, 
    truncating=<span class="hljs-string">'post'</span>
)
preprocessed_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(
    test_sequences, maxlen=max_seq_length, padding=<span class="hljs-string">'post'</span>, 
    truncating=<span class="hljs-string">'post'</span>
)
</code></pre>
    <p class="normal">The reason we picked 22 as the <a id="_idIndexMarker499"/>sequence length is through a simple analysis. The 99% percentile of the sequence lengths of the training corpus is equal to 22. Therefore, we have picked that. Another important statistic is that the vocabulary size will be approximately 7,880 words. Now we will discuss the model.</p>
    <h2 id="_idParaDest-126" class="heading-2">The sentence classification CNN model</h2>
    <p class="normal">Now we will discuss the technical details of the <a id="_idIndexMarker500"/>CNN used for sentence classification. First, we will discuss how data or sentences are transformed into a preferred format that can easily be dealt with by CNNs. Next, we will discuss how the convolution and pooling operations are adapted for sentence classification, and finally, we will discuss how all these components are connected.</p>
    <h3 id="_idParaDest-127" class="heading-3">The convolution operation</h3>
    <p class="normal">If we ignore the batch size, that is, if we <a id="_idIndexMarker501"/>assume that we are only processing a single sentence at a time, our data is a <img src="../Images/B14070_05_042.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> matrix, where <em class="italic">n</em> is the number of words per sentence after padding, and <em class="italic">k</em> is the dimension of a single word vector. In our example, this would be <em class="italic">7 </em>x<em class="italic"> 13</em>.</p>
    <p class="normal">Now we will define our convolution weight matrix to be of size <img src="../Images/B14070_05_043.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, where <em class="italic">m</em> is the filter size for a one-dimensional convolution operation. By convolving the input <em class="italic">x</em> of size <img src="../Images/B14070_05_042.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> with a weight matrix <em class="italic">W</em> of size <img src="../Images/B14070_05_043.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, we will produce an output of <em class="italic">h</em> of size <img src="../Images/B14070_05_046.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/> as follows:</p>
    <p class="center"><img src="../Images/B14070_05_047.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">Here, <em class="italic">w</em><sub class="subscript">i,j</sub> is the <em class="italic">(i,j)</em><sup class="superscript">th</sup> element of <em class="italic">W</em> and we will pad <em class="italic">x</em> with zeros so that <em class="italic">h</em> is of size <img src="../Images/B14070_05_046.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>. Also, we will define this operation more simply, as shown here:</p>
    <p class="center"><img src="../Images/B14070_05_049.png" alt="" style="height: 1.05em !important; vertical-align: 0.20em !important;"/></p>
    <p class="normal">Here, <em class="italic">*</em> defines the convolution operation (with padding) and we will add an additional scalar bias <em class="italic">b</em>. <em class="italic">Figure 5.18</em> illustrates this operation:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_18.png" alt="The convolution operation"/></figure>
    <p class="packt_figref">Figure 5.18: A convolution operation for sentence classification. Convolution layers with different kernel widths are used to convolve over the sentence (i.e. sequence of tokens)</p>
    <p class="normal">Then, to learn a rich set of features, we have parallel layers with different convolution filter sizes. Each convolution layer<a id="_idIndexMarker502"/> outputs a hidden vector of size <img src="../Images/B14070_05_046.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>, and we will concatenate these outputs to form the input to the next layer of size <img src="../Images/B14070_05_051.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, where <em class="italic">q</em> is the number of parallel layers we will use. The larger <em class="italic">q</em> is, the better the performance of the model.</p>
    <p class="normal">The value of convolving can be understood in the following manner. Think about the movie rating learning problem (with two classes, positive or negative), and we have the following sentences:</p>
    <ul>
      <li class="bulletList"><em class="italic">I like the movie, not too bad</em></li>
      <li class="bulletList"><em class="italic">I did not like the movie, bad</em></li>
    </ul>
    <p class="normal">Now imagine a convolution window of size 5. Let’s bin the words according to the movement of the convolution window.</p>
    <p class="normal">The sentence <em class="italic">I like the movie, not too bad</em> gives:</p>
    <p class="normal"><em class="italic">[I, like, the, movie, ‘,’]</em></p>
    <p class="normal"><em class="italic">[like, the, movie, ‘,’, not]</em></p>
    <p class="normal"><em class="italic">[the, movie, ‘,’, not, too]</em></p>
    <p class="normal"><em class="italic">[movie, ‘,’, not, too, bad]</em></p>
    <p class="normal">The sentence <em class="italic">I did not like the movie, bad</em> gives the following:</p>
    <p class="normal"><em class="italic">[I, did, not, like, the]</em></p>
    <p class="normal"><em class="italic">[did, not ,like, the, movie]</em></p>
    <p class="normal"><em class="italic">[not, like, the, movie, ‘,’]</em></p>
    <p class="normal"><em class="italic">[like, the, movie, ‘,’, bad]</em></p>
    <p class="normal">For the first sentence, windows such as the following convey that the rating is positive:</p>
    <p class="normal"><em class="italic">[I, like, the, movie, ‘,’]</em></p>
    <p class="normal"><em class="italic">[movie, ‘,’, not, too, bad]</em></p>
    <p class="normal">However, for the second sentence, windows<a id="_idIndexMarker503"/> such as the following convey negativity in the rating:</p>
    <p class="normal"><em class="italic">[did, not, like, the, movie]</em></p>
    <p class="normal">We are able to see such patterns that help to classify ratings thanks to the preserved spatiality. For example, if you use a technique such as <em class="italic">bag-of-words</em> to calculate sentence representations that lose spatial information, the sentence representations of the above two sentences would be highly similar. The convolution operation plays an important role in preserving the spatial information of the sentences.</p>
    <p class="normal">Having <em class="italic">q</em> different layers with different filter sizes, the network learns to extract the rating with different size phrases, leading to an improved performance.</p>
    <h3 id="_idParaDest-128" class="heading-3">Pooling over time</h3>
    <p class="normal">The pooling operation is designed to subsample the outputs produced by the previously discussed parallel convolution layers. This is <a id="_idIndexMarker504"/>achieved as follows.</p>
    <p class="normal">Let’s assume the output of the last layer <em class="italic">h</em> is of size <img src="../Images/B14070_05_051.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>. The pooling over time layer would produce an output <em class="italic">h’</em> of size <img src="../Images/B14070_05_053.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> output. The precise calculation would be as follows:</p>
    <p class="center"><img src="../Images/B14070_05_054.png" alt="" style="height: 1.35em !important; vertical-align: 0.01em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_05_055.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/> and <em class="italic">h</em><sup class="superscript">(i)</sup> is the output produced by the <em class="italic">i</em><sup class="superscript">th</sup> convolution layer and <img src="../Images/B14070_05_056.png" alt="" style="height: 1.05em !important; vertical-align: -0.02em !important;"/> is the set of weights belonging to that layer. Simply put, the pooling over time operation creates a vector by concatenating the maximum element of each convolution layer. </p>
    <p class="normal">We will illustrate this operation in <em class="italic">Figure 5.19</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_19.png" alt="Pooling over time"/></figure>
    <p class="packt_figref">Figure 5.19: The pooling over time operation for sentence classification</p>
    <p class="normal">By combining these operations, we finally <a id="_idIndexMarker505"/>arrive at the architecture shown in <em class="italic">Figure 5.20</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_05_20.png" alt="Pooling over time"/></figure>
    <p class="packt_figref">Figure 5. 20: A sentence classification CNN architecture. The pool of convolution layers having different kernel widths produces a set of output sequences. They are fed into the Pooling Over Time Layer that produces a compact representation of that input. This is finally connected to a classification layer with softmax activation</p>
    <h2 id="_idParaDest-129" class="heading-2">Implementation – sentence classification with CNNs</h2>
    <p class="normal">We are off implementing the model in <a id="_idIndexMarker506"/>TensorFlow 2. As <a id="_idIndexMarker507"/>a prerequisite, let’s import several necessary modules from TensorFlow:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">import</span> tensorflow.keras.layers <span class="hljs-keyword">as</span> layers
<span class="hljs-keyword">import</span> tensorflow.keras.regularizers <span class="hljs-keyword">as</span> regularizers
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Model
</code></pre>
    <p class="normal">Clear the running session to make sure previous runs are not interfering with the current run: </p>
    <pre class="programlisting code"><code class="hljs-code">K.clear_session()
</code></pre>
    <p class="normal">Before we start, we will be using the Functional API from Keras. The reason for this is that the model we will be building here cannot be built with the Sequential API, due to intricate pathways <a id="_idIndexMarker508"/>present in the model. Let’s start off by creating an input layer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">Input layer takes word IDs as inputs</span>
word_id_inputs = layers.Input(shape=(max_seq_length,), dtype=<span class="hljs-string">'int32'</span>)
</code></pre>
    <p class="normal">The input layer simply takes a batch of <code class="inlineCode">max_seq_length</code> word IDs. That is, a batch of sequences, where each<a id="_idIndexMarker509"/> sequence is padded/truncated to a max length. We specify the <code class="inlineCode">dtype</code> as <code class="inlineCode">int32</code>, since they are word IDs. Next, we define an embedding layer, from which we will look up embeddings corresponding to the word IDs coming through the <code class="inlineCode">word_id_inputs</code> layer: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Get the embeddings of the inputs / out [batch_size, sent_length, </span>
<span class="hljs-comment"># output_dim]</span>
embedding_out = layers.Embedding(input_dim=n_vocab, output_dim=<span class="hljs-number">64</span>)(word_id_inputs)
</code></pre>
    <p class="normal">This is a randomly initialized embedding layer. It contains a large matrix of size <code class="inlineCode">[n_vocab, 64]</code>, where each row represents the word vector of the word indexed by that row number. The embeddings will be jointly learned with the model, while the model is trained on the supervised task. For the next part, we will define three different one-dimensional convolution layers with three different kernel (filter) sizes of <code class="inlineCode">3</code>, <code class="inlineCode">4</code>, and <code class="inlineCode">5</code>, having 100 feature maps each:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># For all layers: in [batch_size, sent_length, emb_size] / out [batch_</span>
<span class="hljs-comment"># size, sent_length, 100]</span>
conv1_1 = layers.Conv1D(
    <span class="hljs-number">100</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'same'</span>, 
    activation=<span class="hljs-string">'relu'</span>
)(embedding_out)
conv1_2 = layers.Conv1D(
    <span class="hljs-number">100</span>, kernel_size=<span class="hljs-number">4</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'same'</span>, 
    activation=<span class="hljs-string">'</span><span class="hljs-string">relu'</span>
)(embedding_out)
conv1_3 = layers.Conv1D(
    <span class="hljs-number">100</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'same'</span>, 
    activation=<span class="hljs-string">'relu'</span>
)(embedding_out)
</code></pre>
    <p class="normal">An important distinction to make here is that we are using one-dimensional convolution as opposed to the two-dimensional <a id="_idIndexMarker510"/>convolution we used in the earlier exercise. However, most of the concepts remain the same. The main difference is that, unlike <code class="inlineCode">tf.keras.layers.Conv2D,</code> which works on four-dimensional inputs, <code class="inlineCode">tf.keras.layers.Conv1D</code> operates on three-dimensional inputs (i.e. inputs with shape <code class="inlineCode">[batch size, width, in channels]</code>). In other words, the convolution kernel moves only in one direction over the inputs. Each of these layers produces a <code class="inlineCode">[batch size, sentence length, 100]</code>-sized output. Afterward, these outputs are concatenated on the last axis to produce a single tensor:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># in previous conv outputs / out [batch_size, sent_length, 300]</span>
conv_out = layers.Concatenate(axis=-<span class="hljs-number">1</span>)([conv1_1, conv1_2, conv1_3])
</code></pre>
    <p class="normal">Subsequently, the new tensor of size <code class="inlineCode">[batch size, sentence length, 300]</code> will be used to perform the pooling over time operation. We can implement the pooling over time operation by defining a one-dimensional max-pooling layer (i.e. <code class="inlineCode">tf.keras.layers.MaxPool1D</code>) with a window as wide as the sequence length. This will produce a single value as the output, for each feature map in <code class="inlineCode">conv_out</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Pooling over time operation. </span>
<span class="hljs-comment"># This is doing the max pooling over sequence length</span>
<span class="hljs-comment"># in other words, each feature map results in a single output</span>
<span class="hljs-comment"># in [batch_size, sent_length, 300] / out [batch_size, 1, 300]</span>
pool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, padding=<span class="hljs-string">'valid'</span>)(conv_out)
</code></pre>
    <p class="normal">Here we get a <code class="inlineCode">[batch_size, 1, 300]</code>-sized output after performing the operation. Next, we will convert this output to a <code class="inlineCode">[batch_size, 300]</code>-sized output, by using the <code class="inlineCode">tf.keras.layers.Flatten</code> layer. The Flatten layer simply collapses all the dimensions (except the batch dimension) to a single dimension:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Flatten the unit length dimension</span>
flatten_out = layers.Flatten()(pool_over_time_out)
</code></pre>
    <p class="normal">Finally, <code class="inlineCode">flatten_out </code>is passed to a Dense layer that has <code class="inlineCode">n_classes</code> (i.e. six) nodes as the output and has a softmax activation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Compute the final output</span>
out = layers.Dense(
    n_classes, activation=<span class="hljs-string">'softmax'</span>,
    kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>)
)(flatten_out)
</code></pre>
    <p class="normal">Note the use of the <code class="inlineCode">kernel_regularizer</code> argument. We can use this argument to add any special regularization (e.g. L1 or L2 regularization) to a given layer. Finally, we define a model as,</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define the model</span>
cnn_model = Model(inputs=word_id_inputs, outputs=out)
</code></pre>
    <p class="normal">and compile the model with<a id="_idIndexMarker511"/> the desired loss function, an<a id="_idIndexMarker512"/> optimizer, and metrics:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Compile the model with loss/optimzier/metrics</span>
cnn_model.<span class="hljs-built_in">compile</span>(
    loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, 
    optimizer=<span class="hljs-string">'adam'</span>, 
    metrics=[<span class="hljs-string">'accuracy'</span>]
)
</code></pre>
    <p class="normal">You can view the model by running the following line:</p>
    <pre class="programlisting code"><code class="hljs-code">cnn_model.summary()
</code></pre>
    <p class="normal">which gives,</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "model"
______________________________________________________________________
Layer (type)            Output Shape         Param #     Connected to 
======================================================================
input_1 (InputLayer)    [(None, 22)]         0                        
______________________________________________________________________
embedding (Embedding)   (None, 22, 64)       504320      input_1[0][0]
______________________________________________________________________
conv1d (Conv1D)         (None, 22, 100)      19300     embedding[0][0]
______________________________________________________________________
conv1d_1 (Conv1D)       (None, 22, 100)      25700     embedding[0][0]
______________________________________________________________________
conv1d_2 (Conv1D)       (None, 22, 100)      32100     embedding[0][0]
______________________________________________________________________
concatenate (Concatenate) (None, 22, 300)    0            conv1d[0][0]
                                                        conv1d_1[0][0]
                                                        conv1d_2[0][0]
______________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 1, 300)    0     concatenate[0][0]
______________________________________________________________________
flatten (Flatten)          (None, 300)         0   max_pooling1d[0][0]
______________________________________________________________________
dense (Dense)              (None, 6)           1806    flatten[0][0] 
======================================================================
Total params: 583,226
Trainable params: 583,226
Non-trainable params: 0
______________________________________________________________________
</code></pre>
    <p class="normal">Next, we will train the model on<a id="_idIndexMarker513"/> the data we already <a id="_idIndexMarker514"/>prepared.</p>
    <h2 id="_idParaDest-130" class="heading-2">Training the model </h2>
    <p class="normal">Since we have done the hard yard at<a id="_idIndexMarker515"/> the beginning, by making sure the data is transformed, training the model is simple. All we need to do is call the <code class="inlineCode">tf.keras.layers.Model.fit()</code> function. However, let’s leverage a few techniques to improve model performance. This will be done by leveraging a built-in callback of <a id="_idIndexMarker516"/>TensorFlow. The technique we’ll be using is known as “decaying the learning rate.” The idea is to reduce the learning rate (by some fraction) whenever the model has stopped to improve performance. The following callback assists us to do this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Call backs</span>
lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(
    monitor=<span class="hljs-string">'val_loss'</span>, factor=<span class="hljs-number">0.1</span>, patience=<span class="hljs-number">3</span>, verbose=<span class="hljs-number">1</span>,
    mode=<span class="hljs-string">'auto'</span>, min_delta=<span class="hljs-number">0.0001</span>, min_lr=<span class="hljs-number">0.000001</span>
)
</code></pre>
    <p class="normal">The parameters can be set as you wish, to control the learning rate reduction. Let’s understand the arguments above:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">monitor (str)</code> – Which metric to monitor in order to decay the learning rate. We will monitor the validation loss</li>
      <li class="bulletList"><code class="inlineCode">factor (float)</code> – By how much to reduce the learning rate. For example, a factor of 0.1 means that the learning rate will be reduced by 10 times (e.g. 0.01 will be stepped down to 0.001)</li>
      <li class="bulletList"><code class="inlineCode">patience (int)</code> – How many epochs to wait without an improvement, before reducing the learning rate</li>
      <li class="bulletList"><code class="inlineCode">mode (string)</code> – Whether to look for an increase or decrease of the metric; ‘auto’ means that the direction will be determined by looking at the metric name</li>
      <li class="bulletList"><code class="inlineCode">min_delta (float)</code> – How much of an increase/decrease to consider as an improvement</li>
      <li class="bulletList"><code class="inlineCode">min_lr (float)</code> – Minimum learning rate (floor)</li>
    </ul>
    <p class="normal">Let’s train the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Train the model</span>
cnn_model.fit(
    preprocessed_train_sequences, train_labels, 
    validation_data=(preprocessed_valid_sequences, valid_labels),
    batch_size=<span class="hljs-number">128</span>, 
    epochs=<span class="hljs-number">25</span>,
    callbacks=[lr_reduce_callback]
)
</code></pre>
    <p class="normal">We will see the accuracy quickly <a id="_idIndexMarker517"/>going up and the validation accuracy plateauing around 88%. Here’s a snippet of the output produced:</p>
    <pre class="programlisting con"><code class="hljs-con">Epoch 1/50
39/39 [==============================] - 1s 9ms/step - loss: 1.7147 - accuracy: 0.3063 - val_loss: 1.3912 - val_accuracy: 0.5696
Epoch 2/50
39/39 [==============================] - 0s 6ms/step - loss: 1.2268 - accuracy: 0.6052 - val_loss: 0.7832 - val_accuracy: 0.7509
...
Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 16/50
39/39 [==============================] - 0s 6ms/step - loss: 0.0487 - accuracy: 0.9999 - val_loss: 0.3639 - val_accuracy: 0.8846
Restoring model weights from the end of the best epoch.
Epoch 00016: early stopping
</code></pre>
    <p class="normal">Next, let’s test the model on the testing dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">cnn_model.evaluate(preprocessed_test_sequences, test_labels, return_dict=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Evaluating the test data as given in the exercise gives us a test accuracy of close to 88% (for 500 test sentences) in this sentence classification task.</p>
    <p class="normal">Here we end our discussion about using CNNs for sentence classification. We first discussed how one-dimensional convolution <a id="_idIndexMarker518"/>operations combined with a special pooling <a id="_idIndexMarker519"/>operation called <em class="italic">pooling over time</em> can be used to implement a sentence classifier based on the CNN architecture. Finally, we discussed how to use TensorFlow to implement such a CNN and saw that it in fact performs well in sentence classification.</p>
    <p class="normal">It can be useful to know how the problem we just solved can be useful in the real world. Assume that you have a large document about the history of Rome in your hand, and you want to find out about Julius Caesar without reading the whole document. In this situation, the sentence classifier we just implemented can be used as a handy tool to summarize the sentences that only correspond to a person, so you don’t have to read the whole document.</p>
    <p class="normal">Sentence classification can be used for many other tasks as well; one common use of this is classifying movie reviews as positive or negative, which is useful for automating the computation of movie ratings. Another important application of sentence classification can be seen in the medical domain, where it is used to extract clinically useful sentences from large documents containing large amounts of text.</p>
    <h1 id="_idParaDest-131" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we discussed CNNs and their various applications. First, we went through a detailed explanation of what CNNs are and their ability to excel at machine learning tasks. Next we decomposed the CNN into several components, such as convolution and pooling layers, and discussed in detail how these operators work. Furthermore, we discussed several hyperparameters that are related to these operators such as filter size, stride, and padding.</p>
    <p class="normal">Then, to illustrate the functionality of CNNs, we walked through a simple example of classifying images of garments. We also did a bit of analysis to see why the CNN fails to recognize some images correctly.</p>
    <p class="normal">Finally, we started talking about how CNNs are applied for NLP tasks. Concretely, we discussed an altered architecture of CNNs that can be used to classify sentences. We then implemented this particular CNN architecture and tested it on an actual sentence classification task.</p>
    <p class="normal">In the next chapter, we will move on to one of the most popular types of neural networks used for many NLP tasks – <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>).</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"> <img src="../Images/QR_Code5143653472357468031.png" alt=""/></figure>
  </div>
</body></html>