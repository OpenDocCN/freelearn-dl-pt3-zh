<html><head></head><body>
  <div id="_idContainer019" class="Basic-Text-Frame">
    <h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-13" class="chapterTitle">Introduction to Natural Language Processing</h1>
    <p class="normal"><strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>) offers a much-needed set of tools and algorithms for understanding and processing the large volume of unstructured data in today’s world. Recently, deep learning has been widely adopted for many NLP tasks because of the remarkable performance deep learning algorithms have shown in a plethora of challenging tasks, such as image classification, speech recognition, and realistic text generation. TensorFlow is one of the most intuitive and efficient deep learning frameworks currently in existence that enables such amazing feats. This book will enable aspiring deep learning developers to handle massive amounts of data using NLP and TensorFlow. This chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">What is Natural Language Processing?</li>
      <li class="bulletList">Tasks of Natural Language Processing</li>
      <li class="bulletList">The traditional approach to Natural Language Processing</li>
      <li class="bulletList">The deep learning approach to Natural Language Processing</li>
      <li class="bulletList">Introduction to the technical tools</li>
    </ul>
    <p class="normal">In this chapter, we will provide an introduction to NLP and to the rest of the book. We will answer the question, “What is Natural Language Processing?”. Also, we’ll look at some of its most important use cases. We will also consider the traditional approaches and the more recent deep learning-based approaches to NLP, including a <strong class="keyWord">Fully Connected Neural Network </strong>(<strong class="keyWord">FCNN</strong>). Finally, we will conclude with an overview of the rest of the book and the technical tools we will be using.</p>
    <h1 id="_idParaDest-14" class="heading-1">What is Natural Language Processing?</h1>
    <p class="normal">According to DOMO (<a href="https://www.domo.com/"><span class="url">https://www.domo.com/</span></a>), an analytics company, there<a id="_idIndexMarker000"/> were 1.7MB for every person<a id="_idIndexMarker001"/> on earth every second by 2020, with a staggering 4.6 billion active users on the internet. This includes roughly 500,000 tweets sent and 306 billion emails circulated every day. These figures are only going in one direction as this book is being written, and that is up! Of all this data, a large fraction is unstructured text and speech as there are billions of emails and social media content created and phone calls made every day.</p>
    <p class="normal">These statistics provide a good basis for us to define what NLP is. Simply put, the goal of NLP is to make machines understand our spoken and written languages. Moreover, NLP is ubiquitous<a id="_idIndexMarker002"/> and is already a large part of human life. <strong class="keyWord">Virtual Assistants </strong>(<strong class="keyWord">VAs</strong>), such as Google Assistant, Cortana, Alexa, and Apple Siri, are largely NLP systems. Numerous NLP tasks take place when one asks a VA, “<em class="italic">Can you show me a good Italian restaurant nearby?</em>” First, the VA needs to convert the utterance to text (that is, speech-to-text). Next, it must understand the semantics of the request (for example, identify the most important keywords like restaurant and Italian) and formulate a structured request (for example, cuisine = Italian, rating = 3–5, distance &lt; 10 km). Then, the VA must search for restaurants filtering by the location and cuisine, and then, rank the restaurants by the ratings received. To calculate an overall rating for a restaurant, a good NLP system may look at both the rating and text description provided by each user. Finally, once the user is at the restaurant, the VA might assist the user by translating various menu items from Italian to English. This example shows that NLP has become an integral part of human life.</p>
    <p class="normal">It should be understood that NLP is an extremely challenging field of research as words and semantics have a highly complex nonlinear relationship, and it is even more difficult to capture this information as a robust numerical representation. To make matters worse, each language has its own grammar, syntax, and vocabulary. Therefore, processing textual data involves various complex tasks such as text parsing (for example, tokenization and stemming), morphological analysis, word sense disambiguation, and understanding the underlying grammatical structure of a language. For example, in these two sentences, <em class="italic">I went to the bank</em> and <em class="italic">I walked along the river bank</em>, the word <em class="italic">bank</em> has two entirely different meanings, due to the context it’s used in. To distinguish or (disambiguate) the word <em class="italic">bank</em>, we need to understand the context in which the word is being used. Machine learning has become a key enabler for NLP, helping to accomplish the aforementioned<a id="_idIndexMarker003"/> tasks through machines. Below we discuss some of the important tasks that fall under NLP.</p>
    <h1 id="_idParaDest-15" class="heading-1">Tasks of Natural Language Processing</h1>
    <p class="normal">NLP has a multitude of real-world applications. A good NLP system is one that performs many NLP tasks. When you search for today’s weather on Google or use Google Translate to find out how to say, “<em class="italic">How are you?</em>” in French, you rely on a subset of such tasks in NLP. We will list some of the most ubiquitous tasks here, and this book covers most of these tasks:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Tokenization</strong>: Tokenization is the task<a id="_idIndexMarker004"/> of separating a text corpus<a id="_idIndexMarker005"/> into atomic units (for example, words or characters). Although it may seem trivial for a language like English, tokenization is an important task. For example, in the Japanese language, words are not delimited by spaces or punctuation marks.</li>
      <li class="bulletList"><strong class="keyWord">Word-Sense Disambiguation</strong> (<strong class="keyWord">WSD</strong>): WSD is the task of identifying<a id="_idIndexMarker006"/> the correct meaning <a id="_idIndexMarker007"/>of a word. For example, in the sentences, <em class="italic">The dog barked at the mailman</em> and <em class="italic">Tree bark is sometimes used as a medicine</em>, the word <em class="italic">bark</em> has two different meanings. WSD is critical for tasks such as question answering.</li>
      <li class="bulletList"><strong class="keyWord">Named Entity Recognition</strong> (<strong class="keyWord">NER</strong>): NER attempts to extract<a id="_idIndexMarker008"/> entities (for example, person, location, and organization) from a given body<a id="_idIndexMarker009"/> of text or a text corpus. For example, the sentence, <em class="italic">John gave Mary two apples at school on Monday</em> will be transformed to <em class="italic">[John]name gave [Mary]name [two]number apples at [school]organization on [Monday]time</em>. NER is an imperative topic in fields such as information retrieval and knowledge representation.</li>
      <li class="bulletList"><strong class="keyWord">Part-of-Speech</strong> (<strong class="keyWord">PoS</strong>) <strong class="keyWord">tagging</strong>: PoS tagging is the task of assigning<a id="_idIndexMarker010"/> words to their respective<a id="_idIndexMarker011"/> parts of speech. It can either be basic tags such as noun, verb, adjective, adverb, and preposition, or it can be granular such as proper noun, common noun, phrasal verb, verb, and so on. The Penn Treebank project, a popular project focusing PoS, defines a comprehensive list<a id="_idIndexMarker012"/> of PoS tags at <a href="https://www.ling.upenn.edu/courses/ling001/penn_treebank_pos.html"><span class="url">https://www.ling.upenn.edu/courses/ling001/penn_treebank_pos.html</span></a>.</li>
      <li class="bulletList"><strong class="keyWord">Sentence</strong>/<strong class="keyWord">synopsis classification</strong>: Sentence or synopsis (for example, movie reviews) classification<a id="_idIndexMarker013"/> has many use cases<a id="_idIndexMarker014"/> such as spam detection, news article classification (for example, political, technology, and sport), and product review ratings (that is, positive or negative). This is achieved by training a classification model with labeled data (that is, reviews annotated by humans, with either a positive or negative label).</li>
      <li class="bulletList"><strong class="keyWord">Text generation</strong>: In text generation, a learning model (for example, a neural network) is trained with text<a id="_idIndexMarker015"/> corpora (a large collection of textual documents), and it then predicts new text<a id="_idIndexMarker016"/> that follows. For example, language modeling can output an entirely new science fiction story by using existing science fiction stories for training. </li>
    </ul>
    <p class="bulletList">Recently, OpenAI released a language model known as OpenAI-GPT-2, which can generate<a id="_idIndexMarker017"/> incredibly realistic text. Furthermore, this task plays a very important role in understanding language, which helps a downstream decision-support model get off the ground quickly.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Question Answering</strong> (<strong class="keyWord">QA</strong>): QA techniques possess a high commercial value, and such techniques<a id="_idIndexMarker018"/> are found at the foundation of chatbots<a id="_idIndexMarker019"/> and VA (for example, Google Assistant and Apple Siri). Chatbots have been adopted by many companies for customer support. Chatbots can be used to answer and resolve straightforward customer concerns (for example, changing a customer’s monthly mobile plan), which can be solved without human intervention. QA touches upon many other aspects of NLP such as information retrieval and knowledge representation. Consequently, all this makes developing a QA system very difficult.</li>
      <li class="bulletList"><strong class="keyWord">Machine Translation</strong> (<strong class="keyWord">MT</strong>): MT is the task of transforming<a id="_idIndexMarker020"/> a sentence/phrase<a id="_idIndexMarker021"/> from a source language (for example, German) to a target language (for example, English). This is a very challenging task, as different languages have different syntactical structures, which means that it is not a one-to-one transformation. Furthermore, word-to-word relationships<a id="_idIndexMarker022"/> between languages<a id="_idIndexMarker023"/> can be one-to-many, one-to-one, many-to-one, or many-to-many. This<a id="_idIndexMarker024"/> is known as the <strong class="keyWord">word alignment problem</strong> in MT literature.</li>
    </ul>
    <p class="normal">Finally, to develop a system that can assist a human in day-to-day tasks (for example, VA or a chatbot) many of these tasks need to be orchestrated in a seamless manner. As we saw in the previous example where the user asks, “<em class="italic">Can you show me a good Italian restaurant nearby?</em>” several different NLP tasks, such as speech-to-text conversion, semantic and sentiment analyses, question answering, and machine translation, need to be completed. In <em class="italic">Figure 1.1</em>, we provide a hierarchical taxonomy of different NLP tasks categorized into several different types. It is a difficult task to attribute an NLP task to a single classification. Therefore, you can see some tasks spanning multiple categories. We will split the categories into two main types: language-based (light-colored with black text) and problem formulation-based (dark-colored with white text). The linguistic breakdown has two categories: syntactic (structure-based) and semantic (meaning-based). The problem formulation-based breakdown has three categories: preprocessing tasks (tasks that are performed on text data before feeding to a model), discriminative tasks (tasks where we attempt to assign an input text to one or more categories<a id="_idIndexMarker025"/> from a set of predefined categories) and generative tasks (tasks where we attempt to generate a new textual output). Of course, this is one classification among many. But it will show how difficult it is to assign a specific NLP task to a specific category.</p>
    <figure class="mediaobject"> <img src="../Images/B14070_01_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.1: A taxonomy of the popular tasks of NLP categorized under broader categories</p>
    <p class="normal">Having understood the various tasks in NLP, let us now move on to understand how we can solve these tasks with the help of machines. We will discuss both the traditional method and the deep- learning-based approach.</p>
    <h1 id="_idParaDest-16" class="heading-1">The traditional approach to Natural Language Processing</h1>
    <p class="normal">The traditional or classical approach<a id="_idIndexMarker026"/> to solving NLP is a sequential flow<a id="_idIndexMarker027"/> of several key steps, and it is a statistical approach. When we take a closer look at a traditional NLP learning model, we will be able to see a set of distinct tasks taking place, such as preprocessing data by removing unwanted data, feature engineering to get good numerical representations of textual data, learning to use machine learning algorithms with the aid of training data, and predicting outputs for novel, unseen data. Of these, feature engineering was the most time-consuming and crucial step for obtaining good performance on a given NLP task.</p>
    <h2 id="_idParaDest-17" class="heading-2">Understanding the traditional approach</h2>
    <p class="normal">The traditional approach to solving NLP tasks involves a collection of distinct subtasks. First, the text corpora<a id="_idIndexMarker028"/> need to be preprocessed, focusing on reducing the vocabulary and <em class="italic">distractions</em>. </p>
    <p class="normal">By <em class="italic">distractions</em>, I refer to the things that distract the algorithm (for example, punctuation marks and stop word removal) from capturing the vital linguistic information required for the task.</p>
    <p class="normal">Next come several feature engineering<a id="_idIndexMarker029"/> steps. The main objective of feature engineering<a id="_idIndexMarker030"/> is to make learning easier for the algorithms. Often the features are hand-engineered and biased toward the human understanding of a language. Feature engineering was of the utmost importance for classical NLP algorithms, and consequently, the best-performing systems often had the best-engineered features. For example, for a sentiment classification task, you can represent a sentence with a parse tree and assign positive, negative, or neutral labels to each node/subtree in the tree to classify that sentence as positive or negative. Additionally, the feature engineering phase can use external resources such as WordNet (a lexical database that can provide insights into how different words are related to each other – e.g. synonyms) to develop better features. We will soon look<a id="_idIndexMarker031"/> at a simple feature engineering technique known as <em class="italic">bag-of-words</em>.</p>
    <p class="normal">Next, the learning algorithm<a id="_idIndexMarker032"/> learns to perform well at the given<a id="_idIndexMarker033"/> task using the obtained features and, optionally, the external resources. For example, for a text summarization task, a parallel corpus containing common phrases and succinct paraphrases would be a good external resource. Finally, prediction<a id="_idIndexMarker034"/> occurs. Prediction is straightforward, where you will feed a new input and obtain the predicted label by forwarding the input through the learning model. The entire process of the traditional approach is depicted in <em class="italic">Figure 1.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_01_02.png" alt="C:\Users\gauravg\Desktop\14070\CH01\B08681_01_02.png"/></figure>
    <p class="packt_figref">Figure 1.2: The general approach of classical NLP</p>
    <p class="normal">Next, let’s discuss a use case where we use NLP to generate football game summaries.</p>
    <h3 id="_idParaDest-18" class="heading-3">Example – generating football game summaries</h3>
    <p class="normal">To gain an in-depth understanding<a id="_idIndexMarker035"/> of the traditional approach to NLP, let’s consider a task of automatic text generation from the statistics of a game of football. We have several sets of game statistics (for example, the score, penalties, and yellow cards) and the corresponding articles generated for that game by a journalist, as the training data. Let’s also assume that for a given game, we have a mapping from each statistical parameter to the most relevant phrase of the summary for that parameter. Our task here is that, given a new game, we need to generate a natural-looking summary of the game. Of course, this can be as simple as finding the best-matching statistics for the new game from the training data and retrieving the corresponding summary. However, there are more sophisticated and elegant ways of generating text.</p>
    <p class="normal">If we were to incorporate machine learning to generate natural language, a sequence of operations, such as preprocessing the text, feature engineering, learning, and prediction, is likely to be performed.</p>
    <p class="normal"><strong class="keyWord">Preprocessing</strong>: The text involves<a id="_idIndexMarker036"/> operations, such as tokenization (for example, splitting “<em class="italic">I went home</em>” into “<em class="italic">I</em>”, “<em class="italic">went</em>”, “<em class="italic">home</em>”), stemming (for example, converting <em class="italic">listened</em> to <em class="italic">listen</em>), and removing punctuation (for example, ! and ;), in order to reduce the vocabulary (that is, the features), thus reducing the dimensionality of the data. Tokenization<a id="_idIndexMarker037"/> might appear<a id="_idIndexMarker038"/> trivial for a language such as English, as the words are isolated; however, this is not the case for certain languages such as Thai, Japanese, and Chinese, as these languages are not consistently delimited. Next, it is important to understand that stemming is not a trivial operation either. It might appear that stemming is a simple operation that relies on a simple set of rules such as removing <em class="italic">ed</em> from a verb (for example, the stemmed result of <em class="italic">listened</em> is <em class="italic">listen</em>); however, it requires more than a simple rule base to develop a good stemming algorithm, as stemming certain words can be tricky (for example, using rule-based stemming, the stemmed result of <em class="italic">argued</em> is <em class="italic">argu</em>). In addition, the effort required for proper stemming can vary in complexity for other languages.</p>
    <p class="normal"><strong class="keyWord">Feature engineering </strong>is used to transform raw text data into an appealing numerical representation so that a model<a id="_idIndexMarker039"/> can be trained on that data, for example, converting text into a bag-of-words representation or using n-gram representation, which we will discuss later. However, remember that state-of-the-art classical models rely on much more sophisticated feature engineering techniques.</p>
    <p class="normal">The following are some of the feature engineering techniques:</p>
    <p class="normal"><strong class="keyWord">Bag-of-words</strong>: This is a feature engineering technique<a id="_idIndexMarker040"/> that creates feature representations based on the word occurrence frequency. For example, let’s consider the following sentences:</p>
    <ul>
      <li class="bulletList"><em class="italic">Bob went to the market to buy some flowers</em></li>
      <li class="bulletList"><em class="italic">Bob bought the flowers to give to Mary</em></li>
    </ul>
    <p class="normal">The vocabulary for these two sentences would be:</p>
    <p class="normal">[“Bob”, “went”, “to”, “the”, “market”, “buy”, “some”, “flowers”, “bought”, “give”, “Mary”]</p>
    <p class="normal">Next, we will create a feature vector of size <em class="italic">V</em> (vocabulary size) for each sentence, showing how many times each word in the vocabulary appears in the sentence. In this example, the feature vectors for the sentences would respectively be as follows:</p>
    <p class="normal">[1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0]</p>
    <p class="normal">[1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 1]</p>
    <p class="normal">A crucial limitation of the bag-of-words method<a id="_idIndexMarker041"/> is that it loses contextual information as the order of words is no longer preserved.</p>
    <p class="normal"><strong class="keyWord">n-gram</strong>: This is another feature engineering technique<a id="_idIndexMarker042"/> that breaks down text into smaller components<a id="_idIndexMarker043"/> consisting of <em class="italic">n </em>letters (or words). For example, 2-gram would break the text into two-letter (or two-word) entities. For example, consider this sentence:</p>
    <p class="normal"><em class="italic">Bob went to the market to buy some flowers</em></p>
    <p class="normal">The letter level n-gram decomposition for this sentence is as follows:</p>
    <p class="normal">[“Bo”, “ob”, “b “, “ w”, “we”, “en”, ..., “me”, “e “,” f”, “fl”, “lo”, “ow”, “we”, “er”, “rs”] </p>
    <p class="normal">The word-based n-gram decomposition is this:</p>
    <p class="normal">[“Bob went”, “went to”, “to the”, “the market”, ..., “to buy”, “buy some”, “some flowers”]</p>
    <p class="normal">The advantage in this representation (letter level) is that the vocabulary will be significantly smaller than if we were to use words as features for large corpora.</p>
    <p class="normal">Next, we need to structure our data to be able to feed it into a learning model. For example, we will have data tuples of the form (<em class="italic">a statistic, a phrase explaining the statistic</em>) as follows:</p>
    <p class="normal">Total goals = 4, “The game was tied with 2 goals for each team at the end of the first half”</p>
    <p class="normal">Team 1 = Manchester United, “The game was between Manchester United and Barcelona”</p>
    <p class="normal">Team 1 goals = 5, “Manchester United managed to get 5 goals”</p>
    <p class="normal"><strong class="keyWord">The learning process </strong>may comprise three sub-modules: a <strong class="keyWord">Hidden Markov Model </strong>(<strong class="keyWord">HMM</strong>), a sentence planner, and a discourse<a id="_idIndexMarker044"/> planner. An HMM<a id="_idIndexMarker045"/> is a recurrent model that can be used to solve time-series problems. For example, generating text is a time-series problem as the order of generated words matters. In our example, an HMM might learn to model language (i.e. generate meaningful text) by training on a corpus of statistics and related phrases. We will train the HMM so that it produces a relevant sequence of text, given the statistics as the starting input. Once trained, the HMM can be used for inference in a recursive manner, where we start with a seed (e.g. a statistic) and predict the first word of the description, then use the predicted word to generate the next word, and so on.</p>
    <p class="normal">Next, we can have a sentence planner that corrects any syntactical or grammatical errors, that might have been introduced by the model. For example, a sentence planner might take in the phrase, <em class="italic">I go house </em>and output <em class="italic">I go home</em>. For this, it can use a database of rules, which contains<a id="_idIndexMarker046"/> the correct way of conveying meanings, such as the need for a preposition between a verb and the word <em class="italic">house</em>.</p>
    <p class="normal">Using the HMM<a id="_idIndexMarker047"/> and the sentence planner, we will have syntactically grammatically correct sentences. Then, we need to collate these phrases in such a way that the essay made from the collection of phrases is human readable and flows well. For example, consider the three phrases, <em class="italic">Player 10 of the Barcelona team scored a goal in the second half, Barcelona played against Manchester United, and Player 3 from Manchester United got a yellow card in the first half</em>; having these sentences in this order does not make much sense. We like to have them in this order: <em class="italic">Barcelona played against Manchester United, Player 3 from Manchester United got a yellow card in the first half, and Player 10 of the Barcelona team scored a goal in the second half</em>. To do this, we use a discourse planner; discourse planners can organize a set of messages so that the meaning of them is conveyed properly.</p>
    <p class="normal">Now, we can get a set of arbitrary test statistics and obtain an essay explaining the statistics by following the preceding workflow, which is depicted in <em class="italic">Figure 1.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_01_03.png" alt="C:\Users\gauravg\Desktop\14070\CH01\B08681_01_03.png"/></figure>
    <p class="packt_figref">Figure 1.3: The classical approach to solving a language modeling task</p>
    <p class="normal">Here, it is important to note that this is a very high-level explanation that only covers the main general-purpose components<a id="_idIndexMarker048"/> that are most likely to be included in traditional NLP. The details can largely vary according to the particular application we are interested in solving. For example, additional application-specific crucial components might be needed for certain tasks (a rule base and an alignment model in machine translation). However, in this book, we do not stress about such details as the main objective here is to discuss more modern ways of Natural Language Processing.</p>
    <h2 id="_idParaDest-19" class="heading-2">Drawbacks of the traditional approach</h2>
    <p class="normal">Let’s list several key drawbacks<a id="_idIndexMarker049"/> of the traditional approach as this would lay a good foundation for discussing the motivation for deep learning:</p>
    <ul>
      <li class="bulletList">The preprocessing steps used in traditional NLP forces a trade-off of potentially useful information embedded in the text (for example, punctuation and tense information) in order to make the learning feasible by reducing the vocabulary. Though preprocessing is still used in modern deep-learning-based solutions, it is not as crucial for them as it is for the traditional NLP workflow due to the large representational capacity of deep networks and their ability to optimize high-end hardware like GPUs.</li>
      <li class="bulletList">Feature engineering is very labor-intensive. In order to design a reliable system, good features need to be devised. This process can be very tedious as different feature spaces need to be extensively explored and evaluated. Additionally, in order to effectively explore robust features, domain expertise is required, which can be scarce and expensive for certain NLP tasks.</li>
      <li class="bulletList">Various external resources <a id="_idIndexMarker050"/>are needed for it to perform well, and there are not many freely available ones. Such external resources often consist of manually created information stored in large databases. Creating one for a particular task can take several years, depending on the severity of the task (for example, a machine translation rule base).</li>
    </ul>
    <p class="normal">Now, let’s discuss how deep learning can help to solve NLP problems.</p>
    <h1 id="_idParaDest-20" class="heading-1">The deep learning approach to Natural Language Processing</h1>
    <p class="normal">I think it is safe to say that deep learning<a id="_idIndexMarker051"/> revolutionized machine learning, especially in fields such as computer <a id="_idIndexMarker052"/>vision, speech recognition, and of course, NLP. Deep models created a wave of paradigm shifts in many of the fields in machine learning, as deep models learned rich features from raw data instead of using limited human-engineered features. This consequentially caused the pesky and expensive feature engineering to be obsolete. With this, deep models made the traditional workflow more efficient, as deep models perform feature learning and task learning, simultaneously. Moreover, due to the massive number of parameters (that is, weights) in a deep model, it can encompass significantly more features than a human could’ve engineered. However, deep models are considered a black box due to the poor interpretability of the model. For example, understanding the “how” and “what” features learned by deep models for a given problem is still an active area of research. But it is important to understand that there is a lot more research focusing on “model interpretability of deep learning models”. </p>
    <p class="normal">A deep neural network<a id="_idIndexMarker053"/> is essentially an artificial neural network<a id="_idIndexMarker054"/> that has an input layer, many interconnected hidden layers in the middle, and finally, an output layer (for example, a classifier or a regressor). As you can see, this forms an end-to-end model from raw data to predictions. These hidden layers in the middle give the power to deep models as they are responsible for learning the good features from raw data, eventually succeeding at the task at hand. Let’s now understand the history of deep learning briefly.</p>
    <h2 id="_idParaDest-21" class="heading-2">History of deep learning</h2>
    <p class="normal">Let’s briefly discuss the roots<a id="_idIndexMarker055"/> of deep learning and how the field evolved to be a very promising technique for machine learning. In 1960, Hubel and Weisel performed an interesting experiment and discovered that a cat’s visual cortex is made of simple and complex cells, and that these cells are organized in a hierarchical form. Also, these cells react differently to different stimuli. For example, simple cells are activated by variously oriented edges while complex cells are insensitive to spatial variations (for example, the orientation of the edge). This kindled the motivation for replicating a similar behavior in machines, giving rise to the concept of artificial neural networks.</p>
    <p class="normal">In the years that followed, neural networks gained the attention of many researchers. In 1965, a neural network<a id="_idIndexMarker056"/> trained by a method known as the <strong class="keyWord">Group Method of Data Handling</strong> (<strong class="keyWord">GMDH</strong>) and based on the famous <em class="italic">Perceptron</em> by Rosenblatt, was introduced by Ivakhnenko and others. Later, in 1979, Fukushima introduced the <em class="italic">Neocognitron</em>, which planted the seeds for one<a id="_idIndexMarker057"/> of the most famous variants of deep models—Convolutional Neural Networks (CNNs). Unlike the perceptrons, which always took in a 1D input, a Neocognitron was able to process 2D inputs using convolution operations.</p>
    <p class="normal">Artificial neural networks used to backpropagate the error signal to optimize the network parameters by computing the gradients of the weights of a given layer with regards to the loss. Then, the weights are updated by pushing them in the opposite direction of the gradient, in order to minimize the loss. For a layer further away from the output layer (i.e. where the loss is computed), the algorithm uses the chain rule to compute gradients. The chain rule used with many layers<a id="_idIndexMarker058"/> led to a practical problem known as the vanishing gradients problem, strictly limiting the potential number of layers (depth) of the neural network. The gradients of layers closer to the inputs (i.e. further away from the output layer), being very small, cause the model training to stop prematurely, leading to an underfitted<a id="_idIndexMarker059"/> model. This is known as the <strong class="keyWord">vanishing gradients phenomenon</strong>. </p>
    <p class="normal">Then, in 2006, it was found that <em class="italic">pretraining</em> a deep neural network by minimizing the <em class="italic">reconstruction error</em> (obtained by trying to compress the input to a lower dimensionality and then reconstructing it back<a id="_idIndexMarker060"/> into the original dimensionality) for each layer of the network provides a good initial starting point for the weight of the neural network; this allows a consistent flow of gradients from the output layer to the input layer. This essentially allowed neural network models to have more layers without the ill effects of the vanishing gradient. Also, these deeper models were able to surpass traditional machine learning models in many tasks, mostly in computer vision (for example, test accuracy for the MNIST handwritten digit dataset). With this breakthrough, deep learning became the buzzword in the machine learning community.</p>
    <p class="normal">Things started gaining progressive momentum when, in 2012, AlexNet (a deep convolutional neural network<a id="_idIndexMarker061"/> created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton) won the <strong class="keyWord">Large Scale Visual Recognition Challenge </strong>(<strong class="keyWord">LSVRC</strong>) 2012 with an error decrease of 10% from the previous best. During this time, advances were made in speech recognition, wherein state-of-the-art speech recognition accuracies were reported using<a id="_idIndexMarker062"/> deep neural networks. Furthermore, people began to realize that <strong class="keyWord">Graphical Processing Units </strong>(<strong class="keyWord">GPUs</strong>) enable more parallelism, which allows for faster training of larger<a id="_idIndexMarker063"/> and deeper networks compared with <strong class="keyWord">Central Processing Units</strong> (<strong class="keyWord">CPUs</strong>).</p>
    <p class="normal">Deep models were further improved with better model initialization techniques (for example, Xavier initialization), making the time-consuming pretraining redundant. Also, better <a id="_idIndexMarker064"/>nonlinear activation functions, such as <strong class="keyWord">Rectified Linear Units </strong>(<strong class="keyWord">ReLUs</strong>), were introduced, which alleviated the adversities of the vanishing gradient in deeper models. Better optimization (or learning) techniques, such as the Adam optimizer, automatically tweaked<a id="_idIndexMarker065"/> individual learning rates of each parameter among the millions of parameters that we have in the neural network model, which rewrote the state-of-the-art performance in many different fields of machine learning, such as object classification and speech recognition. These advancements also allowed neural network models to have large numbers of hidden layers. The ability to increase the number of hidden layers (that is, to make the neural networks deep) is one of the primary contributors to the significantly better performance of neural network models compared with other machine learning models. Furthermore, better intermediate regularizers, such as batch normalization layers, have improved the performance of deep nets for many tasks.</p>
    <p class="normal">Later, even deeper models such as ResNets, Highway Nets, and Ladder Nets were introduced, which had hundreds of layers and billions of parameters. It was possible to have such an enormous<a id="_idIndexMarker066"/> number of layers with the help of various empirically and theoretically inspired techniques. For example, ResNets use shortcut connections or skip connections to connect layers that are far apart, which minimizes the diminishing of gradients layer to layer, as discussed earlier.</p>
    <h2 id="_idParaDest-22" class="heading-2">The current state of deep learning and NLP</h2>
    <p class="normal">Many different deep models<a id="_idIndexMarker067"/> have seen the light since their inception in early 2000. Even though<a id="_idIndexMarker068"/> they share a resemblance, such as all of them using nonlinear transformation of the inputs and parameters, the details<a id="_idIndexMarker069"/> can vary vastly. For example, a <strong class="keyWord">CNN</strong> can learn from two-dimensional data (for example, RGB images) as it is, while a multilayer perceptron model requires the input to be unwrapped to a one-dimensional vector, causing the loss of important spatial information.</p>
    <p class="normal">When processing text, as one of the most intuitive interpretations of text is to perceive it as a sequence of characters, the learning model should be able to do time-series modeling, thus requiring the <em class="italic">memory</em> of the past. To understand this, think of a language modeling task; the next word for the word <em class="italic">cat </em>should be different from the next word for the word <em class="italic">climbed</em>. One such popular model<a id="_idIndexMarker070"/> that encompasses this ability is known as a <strong class="keyWord">Recurrent Neural Network</strong> (<strong class="keyWord">RNN</strong>). We will see in <em class="chapterRef">Chapter 6</em>, <em class="italic">Recurrent Neural Networks</em>,<em class="italic"> </em>how exactly RNNs achieve this by going through interactive exercises.</p>
    <p class="normal">It should be noted that <em class="italic">memory</em> is not a trivial operation that is inherent to a learning model. Conversely, ways of persisting memory should be carefully designed.</p>
    <p class="normal">Also, the term <em class="italic">memory</em> should not be confused with the learned weights of a non-sequential deep network that only looks at the current input, where a sequential model (for example, an RNN) will look at both the learned weights and the previous element of the sequence to predict the next output.</p>
    <p class="normal">One prominent drawback of RNNs is that they cannot remember more than a few (approximately seven) time steps, thus lacking long-term memory. <strong class="keyWord">Long Short-Term Memory </strong>(<strong class="keyWord">LSTM</strong>) networks are an extension of RNNs<a id="_idIndexMarker071"/> that encapsulate long-term memory. Therefore, often LSTMs are preferred over standard RNNs, nowadays. We will peek under the hood in <em class="chapterRef">Chapter 7, Understanding Long Short-Term Memory Networks</em>, to understand them better.</p>
    <p class="normal">Finally, a model known as a <strong class="keyWord">Transformer</strong> has been introduced by Google fairly<a id="_idIndexMarker072"/> recently, which has outperformed <a id="_idIndexMarker073"/>many of the previous state-of-the-art models such as LSTMs<a id="_idIndexMarker074"/> on a plethora of NLP tasks. Previously, both recurrent models (e.g. LSTMs) and convolutional models (e.g. CNNs) dominated the NLP domain. For example, CNNs have been used for sentence classification, machine translation, and sequence-to-sequence learning tasks. However, Transformers use an entirely different approach where they use neither recurrence nor convolution, but an attention mechanism. The attention mechanism allows the model to look at the entire sequence at once, to produce a single output. For example, consider the sentence “<em class="italic">The animal didn’t cross the road because it was tired</em>.” While generating intermediate representations for the word “<em class="italic">it</em>,” it would be useful for the model to learn that “<em class="italic">it</em>” refers to the “<em class="italic">animal</em>”. The attention mechanism allows the Transformer model to learn such relationships. This capability cannot be replicated with standard recurrent models or convolutional models. We will investigate these models further in <em class="chapterRef">Chapter 10, Transformers </em>and<em class="chapterRef"> Chapter 11, Image Captioning with Transformers.</em></p>
    <p class="normal">In summary, we can mainly separate deep networks into three categories: the non-sequential models<a id="_idIndexMarker075"/> that deal with only a single input at a time for both training and prediction (for example, image classification), the sequential models<a id="_idIndexMarker076"/> that cope with sequences of inputs of arbitrary length (for example, text generation where a single word is a single input), and finally, attention-based models<a id="_idIndexMarker077"/> that look at the sequence at once such as the Transformer, BERT, and XLNet, which are pretrained models based on the Transformer architecture. We can categorize non-sequential (also called feed-forward) models<a id="_idIndexMarker078"/> into deep (approximately less than 20 layers) and very deep networks (can be greater than hundreds of layers). The sequential models are categorized into short-term memory models (for example, RNNs), which can only memorize short-term patterns, and long-term memory models, which can memorize longer patterns. In <em class="italic">Figure 1.4</em>, we outline the discussed taxonomy. You don’t have to understand these different deep learning models fully at this point, but it illustrates the diversity of the deep learning models:</p>
    <figure class="mediaobject"><img src="../Images/B14070_01_04.png" alt="Diagram  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 1.4: A general taxonomy of the most commonly used deep learning methods, categorized into several classes</p>
    <p class="normal">Now, let’s take our first steps toward understanding the inner workings of a neural network.</p>
    <h2 id="_idParaDest-23" class="heading-2">Understanding a simple deep model – a fully connected neural network</h2>
    <p class="normal">Now, let’s have a closer look at a deep neural network<a id="_idIndexMarker079"/> in order to gain a better understanding. Although there are numerous different<a id="_idIndexMarker080"/> variants of deep models, let’s look at one of the earliest<a id="_idIndexMarker081"/> models (dating back to 1950–60), known as a <strong class="keyWord">fully connected neural network</strong> (<strong class="keyWord">FCNN</strong>), sometimes called a multilayer perceptron. <em class="italic">Figure 1.5</em> depicts a standard three-layered FCNN.</p>
    <p class="normal">The goal of an FCNN is to map an input (for example, an image or a sentence) to a certain label or annotation (for example, the object category for images). This is achieved by using an input <em class="italic">x</em> to compute <em class="italic">h</em> – a hidden representation of <em class="italic">x</em> – using a transformation such as <em class="italic"><img src="../Images/B14070_01_011.png" alt="" style="height: 1em !important;"/></em>; here, <em class="italic">W</em> and <em class="italic">b</em> are the weights and bias of the FCNN, respectively, and <em class="italic"><img src="../Images/B14070_01_021.png" alt="" style="height: 1em !important;"/></em> is the sigmoid activation function. Neural networks use non-linear activation functions<a id="_idIndexMarker082"/> at every layer. Sigmoid activation<a id="_idIndexMarker083"/> is one such activation. It is an element-wise transformation applied to the output of a layer, where the sigmoidal output of <em class="italic">x</em> is given by, <em class="italic"><img src="../Images/B14070_01_031.png" alt="" style="height: 1em !important;"/></em>. Next, a classifier is placed on top of the FCNN that gives the ability to leverage the learned features in hidden layers to classify inputs. The classifier is a part of the FCNN and yet another hidden layer with some weights, <em class="italic">W</em><sub class="subscript">s</sub> and a bias, <em class="italic">b</em><sub class="subscript">s</sub>. Also, we can calculate the final output of the FCNN as <em class="italic"><img src="../Images/B14070_01_041.png" alt="" style="height: 1.2em !important;"/></em>. For example, a softmax classifier can be used for multi-label classification problems. It provides a normalized representation of the scores output by the classifier layer. That is, it will produce a valid probability distribution over the classes in the classifier layer. The label is considered to be the output node with the highest softmax value. Then, with this, we can define a classification loss that is calculated as the difference between the predicted output label and the actual output label. An example of such a loss function is the mean squared loss. You don’t have to worry if you don’t understand the actual intricacies of the loss function. We will discuss quite a few of them in later chapters. Next, the neural network parameters, <em class="italic">W</em>, <em class="italic">b</em>, <em class="italic">W</em><sub class="subscript">s</sub>, and <em class="italic">b</em><sub class="subscript">s</sub>, are optimized using a standard stochastic optimizer (for example, the stochastic gradient descent) to reduce the classification loss of all the inputs. <em class="italic">Figure 1.5</em> depicts the process explained in this paragraph for a three-layer FCNN. We will walk through the details on how to use such a model for NLP tasks, step by step, in <em class="chapterRef">Chapter 3</em>, <em class="italic">Word2vec – Learning Word Embeddings</em>.</p>
    <figure class="mediaobject"><img src="../Images/B14070_01_05.png" alt="C:\Users\gauravg\Desktop\14070\CH01\B08681_01_05.png"/></figure>
    <p class="packt_figref">Figure 1.5: An example of a fully connected neural network (FCNN)</p>
    <p class="normal">Let’s look at an example of how to use a neural network for a sentiment analysis task. Consider that we have a dataset<a id="_idIndexMarker084"/> where the input is a sentence<a id="_idIndexMarker085"/> expressing a positive or negative opinion about a movie and a corresponding label saying if the sentence is actually positive (<strong class="keyWord">1</strong>) or negative (<strong class="keyWord">0</strong>). Then, we are given a test dataset, where we have single-sentence movie reviews, and our task is to classify these new sentences as positive or negative.</p>
    <p class="normal">It is possible to use a neural network (which can be deep or shallow, depending on the difficulty of the task) for this task by adhering to the following workflow:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Tokenize the sentence by words.</li>
      <li class="numberedList">Convert the sentences into a fixed sized numerical representation (for example, Bag-of-Words representation). A fixed sized representation is needed as fully connected neural networks require a fixed sized input.</li>
      <li class="numberedList">Feed the numerical inputs to the neural network, predict the output (positive or negative), and compare that with the true target.</li>
      <li class="numberedList">Optimize the neural network using a desired loss function.</li>
    </ol>
    <p class="normal">In this section we looked at deep learning in more detail. We looked at the history and the current state of NLP. Finally, we looked at a fully connected neural network (a type of deep learning model) in more detail. </p>
    <p class="normal">Now that we’ve introduced NLP, its tasks, and how approaches to it have evolved over the years, let’s take a moment to look the technical tools required for the rest of this book.</p>
    <h1 id="_idParaDest-24" class="heading-1">Introduction to the technical tools</h1>
    <p class="normal">In this section, you will be introduced to the technical tools<a id="_idIndexMarker086"/> that will be used in the exercises of the following chapters. First, we will present a brief introduction to the main tools provided. Next, we will present a rough guide on how to install each tool along with hyperlinks to detailed guides provided by the official websites. Additionally, we will share tips on how to make sure that the tools were installed properly.</p>
    <h2 id="_idParaDest-25" class="heading-2">Description of the tools</h2>
    <p class="normal">We will use Python<a id="_idIndexMarker087"/> as the coding/scripting language. Python is a very versatile, easy-to-set-up coding language that is heavily used by the scientific and machine learning communities.</p>
    <p class="normal">Additionally, there are numerous scientific libraries built for Python, catering to areas ranging from deep learning to probabilistic inference to data visualization. TensorFlow is one such library that is well known among the deep learning community, providing many basic and advanced operations that are useful for deep learning. Next, we will use Jupyter Notebook in all our exercises as it provides a rich and interactive environment for coding compared to using Python scripts. We will also use pandas, NumPy and scikit-learn — three popular — two popular libraries for Python—for various miscellaneous purposes such as data preprocessing. Another library we will be using for various text-related operations is NLTK—the Python Natural Language Toolkit. Finally, we will use Matplotlib for data visualization.</p>
    <h2 id="_idParaDest-26" class="heading-2">Installing Anaconda and Python</h2>
    <p class="normal">Python is hassle-free to install<a id="_idIndexMarker088"/> in any of the commonly<a id="_idIndexMarker089"/> used operating systems, such as Windows, macOS, or Linux. We will use Anaconda to set up Python, as it does all the laborious work for setting up Python as well as the essential libraries.</p>
    <p class="normal">To install Anaconda, follow these steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Download<a id="_idIndexMarker090"/> Anaconda from <a href="https://www.continuum.io/downloads"><span class="url">https://www.continuum.io/downloads</span></a></li>
      <li class="numberedList">Select the appropriate OS and download Python 3.7</li>
      <li class="numberedList">Install Anaconda<a id="_idIndexMarker091"/> by following the instructions at <a href="https://docs.continuum.io/anaconda/install/"><span class="url">https://docs.continuum.io/anaconda/install/</span></a></li>
    </ol>
    <p class="normal">To check whether Anaconda was properly installed, open a Terminal window (Command Prompt in Windows), and then run the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">conda --version
</code></pre>
    <p class="normal">If installed properly, the version of the current Anaconda distribution should be shown in the Terminal.</p>
    <h3 id="_idParaDest-27" class="heading-3">Creating a Conda environment</h3>
    <p class="normal">One of the attractive features<a id="_idIndexMarker092"/> of Anaconda is that it allows you to create multiple Conda, or virtual, environments. Each Conda environment can have its own environment variables and Python libraries. For example, one Conda environment can be created to run TensorFlow 1.x, whereas another can run TensorFlow 2.x. This is great because it allows you to separate your development environments from any changes taking place in the host’s Python installation. Then, you can activate or deactivate Conda environments depending on which environment you want to use.</p>
    <p class="normal">To create a Conda environment, follow these instructions:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Run Conda and create <code class="inlineCode">-n packt.nlp.2 python=3.7</code> in the terminal window using the command <code class="inlineCode">conda create -n packt.nlp.2 python=3.7</code>.</li>
      <li class="numberedList">Change directory (<code class="inlineCode">cd</code>) to the project directory.</li>
      <li class="numberedList">Activate the new Conda environment by entering <code class="inlineCode">activate</code> <code class="inlineCode">packt.nlp.2</code> in the terminal. If successfully activated, you should see <code class="inlineCode">(packt.nlp.2)</code> appearing before the user prompt in the terminal.</li>
      <li class="numberedList">Install the required libraries using one of the following options.</li>
      <li class="numberedList">If you <strong class="keyWord">have a GPU</strong>, use <code class="inlineCode">pip install -r requirements-base.txt -r requirements-tf-gpu.txt</code></li>
      <li class="numberedList">If you <strong class="keyWord">do not have a GPU</strong>, use <code class="inlineCode">pip install -r requirements-base.txt -r requirements-tf.txt</code></li>
    </ol>
    <p class="normal">Next, we’ll discuss some prerequisites for GPU support for TensorFlow.</p>
    <h2 id="_idParaDest-28" class="heading-2">TensorFlow (GPU) software requirements</h2>
    <p class="normal">If you are using<a id="_idIndexMarker093"/> the TensorFlow GPU version, you will need to satisfy certain software requirements such as installing CUDA 11.0. An exhaustive list is available at <a href="https://www.tensorflow.org/install/gpu#software_requirements"><span class="url">https://www.tensorflow.org/install/gpu#software_requirements</span></a>.</p>
    <h2 id="_idParaDest-29" class="heading-2">Accessing Jupyter Notebook</h2>
    <p class="normal">After running the <code class="inlineCode">pip install</code> command, you should have Jupyter Notebook<a id="_idIndexMarker094"/> available in the Conda environment. To check whether Jupyter Notebook is properly installed and can be accessed, follow these steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Open a Terminal window.</li>
      <li class="numberedList">Activate the <code class="inlineCode">packt.nlp.2</code> Conda environment if it is not already by running <code class="inlineCode">activate packt.nlp.2</code></li>
      <li class="numberedList">Run the command: <code class="inlineCode">jupyter notebook</code></li>
    </ol>
    <p class="normal">You should be presented<a id="_idIndexMarker095"/> with a new browser window that looks like <em class="italic">Figure 1.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_01_06.png" alt="C:\Users\gauravg\Desktop\14070\CH01\B08681_01_06.png"/></figure>
    <p class="packt_figref">Figure 1.6: Jupyter Notebook installed successfully</p>
    <h2 id="_idParaDest-30" class="heading-2">Verifying the TensorFlow installation</h2>
    <p class="normal">In this book, we are using TensorFlow 2.7.0. It is important<a id="_idIndexMarker096"/> that you install the exact version used in the book as TensorFlow can undergo many changes while migrating from one version to the other. TensorFlow should be installed in the <code class="inlineCode">packt.nlp.2</code> Conda environment if everything went well. If you are having trouble installing TensorFlow,<a id="_idIndexMarker097"/> you can find guides and troubleshooting instructions at <a href="https://www.tensorflow.org/install"><span class="url">https://www.tensorflow.org/install</span></a>.</p>
    <p class="normal">To check whether TensorFlow installed properly, follow these steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Open Command Prompt in Windows or Terminal in Linux or macOS.</li>
      <li class="numberedList">Activate the <code class="inlineCode">packt.nlp.2</code> Conda environment.</li>
      <li class="numberedList">Type <code class="inlineCode">python</code> to enter the Python prompt. You should now see the Python version right below. Make sure that you are using Python 3.</li>
      <li class="numberedList">Next, enter the following commands:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf 
<span class="hljs-built_in">print</span>(tf. version )
</code></pre>
      </li>
    </ol>
    <p class="normal">If all went well, you should not have any errors (there might be warnings if your computer does not have a dedicated GPU, but you can ignore them) and TensorFlow<a id="_idIndexMarker098"/> version 2.7.0 should be shown.</p>
    <div class="note">
      <p class="normal">Many cloud-based computational platforms are also available, where you can set up your own machine with various customization (operating system, GPU card type, number of GPU cards, and so on). Many are migrating to such cloud-based services<a id="_idIndexMarker099"/> due to the following benefits: </p>
      <ul>
        <li class="bulletList">More customization options </li>
        <li class="bulletList">Less maintenance effort </li>
        <li class="bulletList">No infrastructure requirements</li>
      </ul>
    </div>
    <p class="normal">Several popular<a id="_idIndexMarker100"/> cloud-based<a id="_idIndexMarker101"/> computational platforms<a id="_idIndexMarker102"/> are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Google Colab</strong>: <a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>): <a href="https://cloud.google.com/"><span class="url">https://cloud.google.com/</span></a></li>
      <li class="bulletList"><strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>): <a href="https://aws.amazon.com/"><span class="url">https://aws.amazon.com/</span></a></li>
    </ul>
    <p class="normal">Google Colab is a great cloud-based platform that allows you to write TensorFlow code and execute it on CPU/GPU hardware for free. </p>
    <h1 id="_idParaDest-31" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we broadly explored NLP to get an impression of the kind of tasks involved in building a good NLP-based system. First, we explained why we need NLP and then discussed various tasks of NLP to generally understand the objective of each task and how difficult it is to succeed at them.</p>
    <p class="normal">After that, we looked at the classical approach of solving NLP and went into the details of the workflow using an example of generating sport summaries for football games. We saw that the traditional approach usually involves cumbersome and tedious feature engineering. For example, in order to check the correctness of a generated phrase, we might need to generate a parse tree for that phrase. Then, we discussed the paradigm shift that transpired with deep learning and saw how deep learning made the feature engineering step obsolete. We started with a bit of time-traveling to go back to the inception of deep learning and artificial neural networks and worked our way through to the massive modern networks with hundreds of hidden layers. Afterward, we walked through a simple example illustrating a deep model—a multilayer perceptron model—to understand the mathematical wizardry taking place in such a model (on the surface of course!).</p>
    <p class="normal">With a foundation in both the traditional and modern ways of approaching NLP, we then discussed the roadmap to understand the topics we will be covering in the book, from learning word embeddings to mighty LSTMs, and to state-of-the-art Transformers! Finally, we set up our virtual Conda environment by installing Python, scikit-learn, Jupyter Notebook, and TensorFlow.</p>
    <p class="normal">In the next chapter, you will learn the basics of TensorFlow. By the end of the chapter, you should be comfortable with writing a simple algorithm that can take some input, transform the input through a defined function and output the result.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <p class="center"><span class="url"><img src="../Images/QR_Code5143653472357468031.png" alt=""/></span></p>
    <figure class="mediaobject"> </figure>
  </div>
</body></html>