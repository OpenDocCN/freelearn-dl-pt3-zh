- en: '*Chapter 12*: Boosting Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More often than not, the leap between good and great doesn't involve drastic
    changes, but instead subtle tweaks and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: It is often said that 20% of the effort can get you 80% of the results (this
    is known as the **Pareto principle**). But what about that gap between 80% and
    100%? What do we need to do to exceed expectations, to improve our solutions,
    to squeeze as much performance out of our computer vision algorithms as possible?
  prefs: []
  type: TYPE_NORMAL
- en: Well, as with all things deep learning, the answer is a mixture of art and science.
    The good news is that in this chapter, we'll focus on simple tools you can use
    to boost the performance of your neural networks!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using convolutional neural network ensembles to improve accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using test time augmentation to improve accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using rank-N accuracy to evaluate performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using label smoothing to increase performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpointing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the training process using `tf.GradientTape`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing class activation maps to better understand your network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As usual, you''ll get the most out of these recipes if you can access a GPU,
    given that some of the examples in this chapter are quite resource-intensive.
    Also, if there are any preparatory steps you''ll need to perform in order to complete
    a recipe, you''ll find them in the *Getting ready* sections provided. As a last
    remark, the code for this chapter is available in the companion repository on
    GitHub: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/2Ko3H3K](https://bit.ly/2Ko3H3K).'
  prefs: []
  type: TYPE_NORMAL
- en: Using convolutional neural network ensembles to improve accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, one of the most robust classifiers is, in fact, a meta-classifier,
    known as an ensemble. An ensemble is comprised of what's known as weak classifiers,
    predictive models just a tad better than random guessing. However, when combined,
    they result in a rather robust algorithm, especially against high variance (overfitting).
    Some of the most famous examples of ensembles we may encounter include Random
    Forest and Gradient Boosting Machines.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we can leverage the same principle when it comes to neural
    networks, thus creating a whole that's more than the sum of its parts. Do you
    want to learn how? Keep reading!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe depends on `Pillow` and `tensorflow_docs`, which can be easily
    installed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also be using the famous `Caltech 101` dataset, available here: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and decompress `101_ObjectCategories.tar.gz` to your preferred location.
    For the purposes of this recipe, we''ll place it in `~/.keras/datasets/101_ObjectCategories`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Caltech 101 sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Caltech 101 sample images
  prefs: []
  type: TYPE_NORMAL
- en: Let's start this recipe, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create an ensemble of **Convolutional Neural Networks**
    (**CNNs**):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `load_images_and_labels()` function, which reads the images and
    categories of the `Caltech 101` dataset and returns them as NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_model()` function, which is in charge of building a VGG-like
    convolutional neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build the fully connected part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `plot_model_history()` function, which we''ll use to plot the training
    and validation curves of the networks in the ensemble:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To enhance reproducibility, set a random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the paths to the images of `Caltech 101`, as well as the classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reserve 20% of the data for test purposes and use the rest to train the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size, the number of epochs, and the number of batches per
    epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll use data augmentation here to perform a series of random transformations,
    such as horizontal flipping, rotations, and zooming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our ensemble will be comprised of `5` models. We''ll save the predictions of
    each network in the ensemble in the `ensemble_preds` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll train each model in a similar fashion. We''ll start by creating and
    compiling the network itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll fit the model using data augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the accuracy of the model on the test set, plot its training and validation
    accuracy curves, and store its predictions in `ensemble_preds`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last step consists of averaging the predictions of each member of the ensemble,
    effectively producing a joint prediction for the whole meta-classifier, and then
    computing the accuracy on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because we are training five networks, this program can take a while to complete.
    When it does, you should see accuracies similar to the following for each member
    of the ensemble:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can observe the accuracy ranges between 65% and 67.5%. The following
    figure shows the training and validation curves for models 1 to 5 (from left to
    right, models 1, 2, and 3 on the top row; models 4 and 5 on the bottom row):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Curves for the training and validation accuracy for the five
    models in the ensemble'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Curves for the training and validation accuracy for the five models
    in the ensemble
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the most interesting result is the accuracy of the ensemble, which
    is the result of averaging the predictions of each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Truly impressive! Just by combining the predictions of the five networks, we
    bumped our accuracy all the way to 72.2%, on a very challenging dataset – `Caltech
    101`! Let's discuss this a bit further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we leveraged the power of ensembles by training five neural
    networks on the challenging `Caltech 101` dataset. It must be noted that our process
    was pretty straightforward and unremarkable. We started by loading and shaping
    the data in a format suitable for training and then using the same template to
    train several copies of a VGG-inspired architecture.
  prefs: []
  type: TYPE_NORMAL
- en: To create more robust classifiers, we used data augmentation and trained each
    network for 40 epochs. Besides these details, we didn't change the architecture
    of the networks, nor did we tweak each particular member. The result is that each
    model was between 65% and 67% accurate on the test set. However, when combined,
    they reached a decent 72%!
  prefs: []
  type: TYPE_NORMAL
- en: Why did this happen, though? The rationale behind ensemble learning is that
    each model develops its own biases during the training process, which is a consequence
    of the stochastic nature of deep learning. However, when combining their decisions
    through a voting process (which is basically what averaging their predictions
    does), these differences smooth out and give far more robust results.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, training several models is a resource-intensive task, and depending
    on the size and complexity of the problem, it might be outright impossible to
    do so. Nevertheless, it's a very useful tool that can boost your predicting power
    just by creating and combining multiple copies of the same network.
  prefs: []
  type: TYPE_NORMAL
- en: Not bad, huh?
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to understand the mathematical basis behind ensembles, read this
    article about **Jensen''s Inequality**: [https://en.wikipedia.org/wiki/Jensen%27s_inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).'
  prefs: []
  type: TYPE_NORMAL
- en: Using test time augmentation to improve accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, when we're testing the predictive power of a network, we use
    a test set to do so. This test set is comprised of images the model has never
    seen. Then, we present them to the model and ask it what class each belongs to.
    The thing is… we do it *once*.
  prefs: []
  type: TYPE_NORMAL
- en: What if we were more forgiving and gave the model multiple chances to do this?
    Would its accuracy improve? Well, more often than not, it does!
  prefs: []
  type: TYPE_NORMAL
- en: This technique is known as **Test Time Augmentation** (**TTA**), and it's the
    focus of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to load the images in the dataset, we need `Pillow`. Install it using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, download the `Caltech 101` dataset, which is available here: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and decompress `101_ObjectCategories.tar.gz` to a location of your choosing.
    For the rest of this recipe, we''ll work under the assumption that the dataset
    is in `~/.keras/datasets/101_ObjectCategories`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sample of what you can find inside `Caltech 101`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Caltech 101 sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Caltech 101 sample images
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn the benefits of TTA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dependencies we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `load_images_and_labels()` function in order to read the data from
    `Caltech 101` (in NumPy format):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_model()` function, which returns a network based on the famous
    **VGG** architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build the fully connected part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `flip_augment()` function is the basis of our **TTA** scheme. It takes
    an image and produces copies of it that can be randomly flipped (horizontally)
    with a 50% probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To ensure reproducibility, set a random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the paths to the images of `Caltech 101`, as well as its classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use 20% of the data for test purposes and leave the rest to train the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size and the number of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll randomly horizontally flip the images in the train set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build and compile the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions on the test set and use them to compute the accuracy of the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll use **TTA** on the test set. We''ll store the predictions for each
    copy of an image in the test set in the predictions list. We''ll create 10 copies
    of each image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will iterate over each image of the test set, creating a batch of
    copies of it and passing it through the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final prediction of each image will be the most predicted class in the
    batch of copies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will compute the accuracy on the predictions made by the model
    using TTA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a while, we''ll see results similar to these:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The network achieves an accuracy of 64.4% without TTA, while it increases to
    65.3% if we give the model more chances to generate correct predictions. Cool,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned that **test time augmentation** is a simple technique
    that entails only a few changes once the network has been trained. The reasoning
    behind this is that if we present the network with copies of images in the test
    set that have been altered in a similar way to the ones it saw during training,
    the network should do better.
  prefs: []
  type: TYPE_NORMAL
- en: However, the key is that these transformations, which are done during the evaluation
    phase, should match the ones that were done during the training period; otherwise,
    we would be feeding the model incongruent data!
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s a caveat, though: TTA is really, really slow! After all, we are multiplying
    the size of the test set by the augmentation factor, which in our case was 10\.
    This means that instead of evaluating one image at a time, the network must process
    10 instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, TTA is not suitable for real-time or speed-constrained applications,
    but it can be useful when time or speed are not an issue.
  prefs: []
  type: TYPE_NORMAL
- en: Using rank-N accuracy to evaluate performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, when we're training deep learning-based image classifiers,
    we care about the accuracy, which is a binary measure of a model's performance,
    based on a one-on-one comparison between its predictions and the ground-truth
    labels. When the model says there's a *leopard* in a photo, is there actually
    a *leopard* there? In other words, we measure how *precise* the model is.
  prefs: []
  type: TYPE_NORMAL
- en: However, for more complex datasets, this way of assessing a network's learning
    might be counterproductive and even unfair, because it's too restrictive. What
    if the model didn't classify the feline in the picture as a *leopard* but as a
    *tiger*? Moreover, what if the second most probable class was, indeed, a *leopard*?
    This means the model has some more learning to do, but it's getting there! That's
    valuable!
  prefs: []
  type: TYPE_NORMAL
- en: This is the reasoning behind **rank-N accuracy**, a more lenient and fairer
    way of measuring a predictive model's performance, which counts a prediction as
    correct if the ground-truth label is in the top-N most probable classes output
    by the model. In this recipe, we'll learn how to implement it and use it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install `Pillow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, download and unzip the `Caltech 101` dataset, which is available here:
    [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Make sure to click on the `101_ObjectCategories.tar.gz` file. Once downloaded,
    place it in a location of your choosing. For the rest of this recipe, we''ll work
    under the assumption that the dataset is in `~/.keras/datasets/101_ObjectCategories`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sample of `Caltech 101`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Caltech 101 sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Caltech 101 sample images
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement this recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement and use **rank-N accuracy**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `load_images_and_labels()` function in order to read the data from
    `Caltech 101`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_model()` function to create a **VGG**-inspired network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build the fully connected part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `rank_n()` function, which computes the **rank-N accuracy** based
    on the predictions and ground-truth labels. Notice that it produces a value between
    0 and 1, where a "hit" or correct prediction is accounted for when the ground-truth
    label is in the N most probable categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the sake of reproducibility, set a random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the paths to the images of `Caltech 101`, as well as its classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use 20% of the data for test purposes and leave the rest to train the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size and the number of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an `ImageDataGenerator()` to augment the images in the training set
    with random flips, rotations, and other transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build and compile the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute rank-1 (regular accuracy), rank-3, rank-5, and rank-10 accuracies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can observe that 64.29% of the time, the network produces an exact
    match. However, 78.05% of the time, the correct prediction is in the top 3, 83.01%
    of the time it's in the top 5, and almost 90% of the time it's in the top 10\.
    These are pretty interesting and encouraging results, considering our dataset
    is comprised of 101 classes that are very different from each other.
  prefs: []
  type: TYPE_NORMAL
- en: We'll dig deeper in the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned about the existence and utility of rank-N accuracy.
    We also implemented it with a simple function, `rank_n()`, which we then tested
    on a network that had been trained on the challenging `Caltech-101` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Rank-N, particularly the rank-1 and rank-5 accuracies, are common in the literature
    of networks that have been trained on massive, challenging datasets, such as COCO
    or ImageNet, where even humans have a hard time discerning between categories.
    It is particularly useful when we have fine-grained classes that share a common
    parent or ancestor, such as *Pug* and *Golden Retriever*, both being *Dog* breeds.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why rank-N is meaningful is a well-trained model that has truly learned
    to generalize will produce contextually similar classes in its top-N predictions
    (typically, the top 5).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we can take rank-N accuracy too far, to the point where it loses
    its meaning and utility. For instance, a rank-5 accuracy on `MNIST`, a dataset
    comprised of 10 categories, would be almost useless.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Want to see rank-N being used in the wild? Take a look at the results section
    of this paper: [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Using label smoothing to increase performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the constant battles we have to fight against in machine learning is
    overfitting. There are many techniques we can use to prevent a model from losing
    generalization power, such as dropout, L1 and L2 regularization, and even data
    augmentation. A recent addition to this group is **label smoothing**, a more forgiving
    alternative to one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas in one-hot encoding we represent each category as a binary vector where
    the only non-zero element corresponds to the class that's been encoded, with **label
    smoothing**, we represent each label as a probability distribution where all the
    elements have a non-zero probability. The one with the highest probability, of
    course, is the one that corresponds to the encoded class.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a smoothed version of the *[0, 1, 0]* vector would be *[0.01,
    0.98, 0.01]*.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to use **label smoothing**. Keep reading!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install `Pillow`, which we''ll need to manipulate the images in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Head to the `Caltech 101` website: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and unzip the file named `101_ObjectCategories.tar.gz` in a location
    of your preference. From now on, we''ll assume the data is in `~/.keras/datasets/101_ObjectCategories`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sample from `Caltech 101`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Caltech 101 sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Caltech 101 sample images
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `load_images_and_labels()` function in order to read the data from
    `Caltech 101`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `build_model()` function to create a **VGG**-based network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build the fully connected part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a random seed to enhance reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the paths to the images of `Caltech 101`, as well as its classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use 20% of the data for test purposes and leave the rest to train the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size and the number of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an `ImageDataGenerator()` to augment the images in the training set
    with random flips, rotations, and other transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll train two models: one with and an other without **label smoothing**.
    This will allow us to compare their performance and assess whether **label smoothing**
    has an impact on performance. The logic is pretty much the same in both cases,
    starting with the model creation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If `with_label_smoothing` is `True`, then we''ll set the smoothing factor to
    0.1\. Otherwise, the factor will be 0, which implies we''ll use regular one-hot
    encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We apply `loss` function – in this case, `CategoricalCrossentropy()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions on the test set and compute the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The script will train two models: one without `loss` function. Here are the
    results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just by using **label smoothing**, we improved our test score by almost 0.7%,
    a non-negligible boost considering the size of our dataset and its complexity.
    We'll dive deeper in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned how to apply `CategoricalCrossentropy()` loss function,
    which is used to measure the network's learning.
  prefs: []
  type: TYPE_NORMAL
- en: Why does label smoothing work, though? Despite its widespread use in many areas
    of deep learning, including **Natural Language Processing** (**NLP**) and, of
    course, **computer vision**, **label smoothing** is still poorly understood. However,
    what many have observed (including ourselves, in this example) is that by softening
    the targets, the generalization and learning speed of a network often improves
    significantly, preventing it from becoming overconfident, thus shielding us against
    the harmful effects of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: For a very interesting insight into **label smoothing**, read the paper mentioned
    in the *See also* section.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper explores the reasons why **label smoothing** helps, as well as when
    it does not. It''s a worthy read! You can download it here: [https://arxiv.org/abs/1906.02629](https://arxiv.org/abs/1906.02629).'
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a deep neural network is an expensive process in terms of time, storage,
    and resources. Retraining a network each time we want to use it is preposterous
    and impractical. The good news is that we can use a mechanism to automatically
    save the best versions of a network during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll talk about such a mechanism, known as checkpointing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn about the different modalities of checkpointing
    you have at your disposal in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the modules we will be using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load `Fashion-MNIST` into `tf.data.Datasets`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use 20% of the training data to validate the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the train, test, and validation subsets into `tf.data.Datasets`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_network()` method, which, as its name suggests, creates the
    model we''ll train on `Fashion-MNIST`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build the fully connected part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `train_and_checkpoint()` function, which loads the dataset and then
    builds, compiles, and fits the network, saving the checkpoints according to the
    logic established by the `checkpointer` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size, the number of epochs to train the model for, and the
    buffer size of each subset of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first way to generate checkpoints is by just saving a different model after
    each iteration. To do this, we must pass `save_best_only=False` to `ModelCheckpoint()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we save all the checkpoints in the `save_all` folder, with the epoch,
    the loss, and the validation loss in the checkpointed model name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A more efficient way of checkpointing is to just save the best model so far.
    We can achieve this by setting `save_best_only` to `True` in `ModelCheckpoint()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll save the results in the `best_only` directory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A leaner way to generate checkpoints is to just save one, corresponding to
    the best model so far, instead of storing each incrementally improved model. To
    achieve this, we can remove any parameters from the checkpoint name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running these three experiments, we can examine each output folder to
    see how many checkpoints were generated. In the first experiment, we saved a model
    after each epoch, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Experiment 1 results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Experiment 1 results
  prefs: []
  type: TYPE_NORMAL
- en: 'The downside of this approach is that we end up with a lot of useless snapshots.
    The upside is that, if we want, we can resume training from any epoch by loading
    the corresponding epoch. A better approach is to save only the best model so far,
    which, as the following screenshot shows, produces fewer models. By inspecting
    the checkpoint names, we can see that each one has a validation loss that''s lower
    than the one before it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Experiment 2 results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Experiment 2 results
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can just save the best model, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Experiment 3 results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Experiment 3 results
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we learned how to checkpoint models, which saves us a huge amount
    of time as we don't need to retrain a model from scratch. Checkpointing is great
    because we can save the best model according to our own criteria, such as the
    validation loss, training accuracy, or any other measurement.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the `ModelCheckpoint()` callback, we can save a snapshot of the
    network after each completed epoch, thus keeping only the best model or a history
    of the best models produced during training.
  prefs: []
  type: TYPE_NORMAL
- en: Each strategy has its pros and cons. For instance, generating models after each
    epoch has the upside of allowing us to resume training from any epoch, but at
    the cost of occupying lots of space on disk, while saving the best model only
    preserves space but reduces our flexibility to experiment.
  prefs: []
  type: TYPE_NORMAL
- en: What strategy will you use in your next project?
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the training process using tf.GradientTape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the biggest competitors of TensorFlow is another well-known framework:
    PyTorch. What made PyTorch so attractive until the arrival of TensorFlow 2.x was
    the level of control it gives to its users, particularly when it comes to training
    neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: If we are working with somewhat traditional neural networks to solve common
    problems, such as image classification, we don't need that much control over how
    to train a model, and therefore can rely on TensorFlow's (or the Keras API's)
    built-in capabilities, loss functions, and optimizers without a problem.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we are researchers that are exploring new ways to do things, as
    well as new architectures and novel strategies to solve challenging problems?
    That's when, in the past, we had to resort to PyTorch, due to it being considerably
    easier to customize the training models than using TensorFlow 1.x, but not anymore!
    TensorFlow 2.x's `tf.GradientTape` allows us to create custom training loops for
    models implemented in Keras and low-level TensorFlow more easily, and in this
    recipe, we'll learn how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the modules we will be using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load and prepare `Fashion-MNIST`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_network()` method, which, as its name suggests, creates the
    model we''ll train on `Fashion-MNIST`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build the fully connected part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To demonstrate how to use `tf.GradientTape`, we''ll implement the `training_step()`
    function, which obtains the gradients for a batch of data and then backpropagates
    them using an optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size and the number of epochs to train the model for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the optimizer and the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll create our custom training loop. First, we''ll go over each epoch,
    measuring the time it takes to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll iterate over each batch of data and pass them, along with the network
    and the optimizer, to our `training_step()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll print the epoch''s elapsed time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, evaluate the network on the test set to make sure it learned without
    any problems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we learned how to create our own custom training loop. Although
    we didn''t do anything particularly interesting in this instance, we highlighted
    the components (or ingredients, if you will) to cook up a custom deep learning
    training loop with `tf.GradientTape`:'
  prefs: []
  type: TYPE_NORMAL
- en: The network architecture itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function used to compute the model loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimizer used to update the model weights based on the gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The step function, which implements a forward pass (compute the gradients) and
    a backward pass (apply the gradients through the optimizers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to study more realistic and appealing uses of `tf.GradientTape`,
    you can refer to [*Chapter 6*](B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214),
    *Generative Models and Adversarial Attacks*; [*Chapter 7*](B14768_07_Final_JM_ePub.xhtml#_idTextAnchor248),
    *Captioning Images with CNNs and RNNs*; and [*Chapter 8*](B14768_08_Final_JM_ePub.xhtml#_idTextAnchor270),
    *Fine-Grained Understanding of Images through Segmentation*. However, you can
    just read the next recipe, where we'll learn how to visualize class activation
    maps in order to debug deep neural networks!
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing class activation maps to better understand your network
  prefs: []
  type: TYPE_NORMAL
- en: Despite their incontestable power and usefulness, one of the biggest gripes
    about deep neural networks is their mysterious nature. Most of the time, we use
    them as black boxes, where we know they work but not why they do.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, it's truly challenging to say why a network arrived at a particular
    result, which neurons were activated and why, or where the network is looking
    at to figure out the class or nature of an object in an image.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, how can we trust something we don't understand? How can we improve
    it or fix it if it breaks?
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, in this recipe, we'll study a novel method to shine some light
    on these topics, known as **Gradient Weighted Class Activation Mapping**, or **Grad-CAM**
    for short.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's get going!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need `OpenCV`, `Pillow`, and `imutils`. You can install
    them in one go like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to implement this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the modules we will be using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `GradCAM` class, which will encapsulate the **Grad-CAM** algorithm,
    allowing us to produce a heatmap of the activation maps of a given layer. Let''s
    start by defining the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we are receiving the `class_index` of a class we want to inspect, and
    the `layer_name` of a layer whose activations we want to visualize. If we don''t
    receive a `layer_name`, we''ll take the outermost output layer of our `model`
    by default. Finally, we create `grad_model` by relying on the `_create_grad_model()`
    method, as defined here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This model takes the same inputs as `model`, but outputs both the activations
    of the layer of interest, and the predictions of `model` itself.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we must define the `compute_heatmap()` method. First, we must pass the
    input image to `grad_model`, obtaining both the activation map of the layer of
    interest and the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can calculate the gradients based on the loss corresponding to the `class_index`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can compute guided gradients by, basically, finding positive values in both
    `float_conv_outputs` and `float_grads`, and multiplying those by the gradients,
    which will enable us to visualize what neurons are activating:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can compute the gradient weights by averaging the guided gradients,
    and then use those weights to add the pondered maps to our **Grad-CAM** visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we take the **Grad-CAM** visualization, resize it to the dimensions of
    the input image, and min-max normalize it before returning it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last method of the `GradCAM` class overlays a heatmap onto the original
    image. This lets us get a better sense of the visual cues the network is looking
    at when making predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s instantiate a **ResNet50** trained on ImageNet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the input image, resize it to the dimensions expected by ResNet50, turn
    it into a NumPy array, and preprocess it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the image through the model and extract the index of the most probable
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a **GradCAM** object and calculate the heatmap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Overlay the heatmap on top of the original image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Decode the predictions to make it human-readable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Label the overlaid heatmap with the class and its associated probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, merge the original image, the heatmap, and the labeled overlay into
    a single image and save it to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Visualization of Grad-CAM'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_12_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Visualization of Grad-CAM
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the network classified my dog as a Pug, which is correct, with
    a confidence of 85.03%. Moreover, the heatmap reveals the network activates around
    the nose and eyes of my dog's face, which means these are important features and
    the model is behaving as expected.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned and implemented **Grad-CAM**, a very useful algorithm
    for visually inspecting the activations of a neural network. This can be an effective
    way of debugging its behavior as it ensures it's looking at the right parts of
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very important tool because the high accuracy or performance of our
    model may have less to do with the actual learning, and more to do with factors
    that have been unaccounted for. For instance, if we are working on a pet classifier
    to distinguish between dogs and cats, we should use **Grad-CAM** to verify that
    the network looks at features inherent to these animals in order to properly classify
    them, and not at the surroundings, background noise, or less important elements
    in the images.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can expand your knowledge of **Grad-CAM** by reading the following paper:
    [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).'
  prefs: []
  type: TYPE_NORMAL
