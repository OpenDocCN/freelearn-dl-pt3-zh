["```\n    import pandas as pd\n    ```", "```\n    import numpy as np\n    ```", "```\n    import matplotlib.pyplot as plt\n    ```", "```\n    df = pd.read_csv('/content/sales_data.csv')\n    ```", "```\n    df.head()\n    ```", "```\n    # Check data types\n    ```", "```\n    print(df.dtypes)\n    ```", "```\n    # Summary statistics\n    ```", "```\n    print(df.describe())\n    ```", "```\n    #Sales data plot\n    ```", "```\n    df.set_index('Date').plot()\n    ```", "```\n    plt.ylabel('Sales')\n    ```", "```\n    plt.title('Sales Over Time')\n    ```", "```\n    plt.xticks(rotation=90)\n    ```", "```\n    plt.show()\n    ```", "```\n# Split data into training and validation sets\nsplit_time = int(len(df) * 0.8)\ntrain_df = df.iloc[:split_time]\nvalid_df = df.iloc[split_time:]\n```", "```\nplt.figure(figsize=(12, 9))\n# Plotting the training data in green\nplt.plot(train_df['Date'], train_df['Sales'], 'green',\n    label = 'Training Data')\nplt.plot(valid_df['Date'], valid_df['Sales'], 'blue',\n    label = 'Validation Data')\nplt.title('Fixed Partitioning')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nall_dates = np.concatenate([train_df['Date'],\n    valid_df['Date']])\nplt.xticks(all_dates[::180], rotation=90)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```", "```\n    # Apply naive forecast\n    ```", "```\n    df['Naive_Forecast'] = df['Sales'].shift(1)\n    ```", "```\n    df.head()\n    ```", "```\n    def plot_forecast(validation_df, forecast_df,\n    ```", "```\n        start_date=None, end_date=None,\n    ```", "```\n        plot_title='Naive Forecasting',\n    ```", "```\n        forecast_label='Naive Forecast'):\n    ```", "```\n            if start_date:\n    ```", "```\n                validation_df = validation_df[\n    ```", "```\n                    validation_df['Date'] >= start_date]\n    ```", "```\n                forecast_df = forecast_df[forecast_df[\n    ```", "```\n                    'Date'] >= start_date]\n    ```", "```\n            if end_date:\n    ```", "```\n                validation_df = validation_df[\n    ```", "```\n                    validation_df['Date'] <= end_date]\n    ```", "```\n                forecast_df = forecast_df[forecast_df[\n    ```", "```\n                    'Date'] <= end_date]\n    ```", "```\n        # Extract the dates in the selected range\n    ```", "```\n        all_dates = validation_df['Date']\n    ```", "```\n        plt.figure(figsize=(12, 9))\n    ```", "```\n        plt.plot(validation_df['Date'],\n    ```", "```\n            validation_df['Sales'],\n    ```", "```\n            label='Validation Data')\n    ```", "```\n        plt.plot(forecast_df['Date'],\n    ```", "```\n            forecast_df['Naive_Forecast'],\n    ```", "```\n            label=forecast_label)\n    ```", "```\n        plt.legend(loc='best')\n    ```", "```\n        plt.title(plot_title)\n    ```", "```\n        plt.xlabel('Date')\n    ```", "```\n        plt.ylabel('Sales')\n    ```", "```\n        # Set x-ticks to every 90th date in the selected range\n    ```", "```\n        plt.xticks(all_dates[::90], rotation=90)\n    ```", "```\n        plt.legend()\n    ```", "```\n        plt.tight_layout()\n    ```", "```\n        plt.show()\n    ```", "```\n    plot_forecast(valid_df, valid_df,\n    ```", "```\n        plot_title='Naive Forecasting',\n    ```", "```\n        forecast_label='Naive Forecast')\n    ```", "```\n    plot_forecast(valid_df, valid_df,\n    ```", "```\n        start_date='2022-01-01', end_date='2022-06-30')\n    ```", "```\n    # Compute and print mean squared error\n    ```", "```\n    mse = tf.keras.metrics.mean_squared_error(\n    ```", "```\n        valid_df['Sales'], valid_df\n    ```", "```\n            ['Naive_Forecast']).numpy()\n    ```", "```\n    print('Mean Squared Error:', mse)\n    ```", "```\n    # Compute and print mean absolute error\n    ```", "```\n    mae = tf.keras.metrics.mean_absolute_error(\n    ```", "```\n        valid_df['Sales'],\n    ```", "```\n        valid_df['Naive_Forecast']).numpy()\n    ```", "```\n    print('Mean Absolute Error:', mae)\n    ```", "```\n# Calculate moving average over a 30-day window\nwindow =30\ndf['Moving_Average_Forecast'] = df['Sales'].rolling(\n    window=window).mean().shift(1)\n```", "```\n# Compute and print mean squared error\nmse = tf.keras.metrics.mean_squared_error(\n    valid_df['Sales'],\n    df.loc[valid_df.index,\n    'Moving_Average_Forecast']).numpy()\nprint('Mean Squared Error:', mse)\n# Compute and print mean absolute error\nmae = tf.keras.metrics.mean_absolute_error(\n    valid_df['Sales'],\n    df.loc[valid_df.index,\n    'Moving_Average_Forecast']).numpy()\nprint('Mean Absolute Error:', mae)\nprint('Mean Squared Error for moving average forecast:',\n    mse)\n```", "```\n# Perform seasonal differencing\ndf['Differenced_Sales'] = df['Sales'].diff(365)\n# Plotting the differenced sales data\nplt.figure(figsize=(12, 9))\n# Plotting the differenced data\nplt.plot(df['Date'], df['Differenced_Sales'],\n    label = 'Differenced Sales')\nplt.title('Seasonally Differenced Sales Data')\nplt.xlabel('Date')\nplt.ylabel('Differenced Sales')\n# Select dates to be displayed on x-axis\nall_dates = df['Date']\nplt.xticks(all_dates[::90], rotation=90)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```", "```\nwindow=7\n# Restore trend and seasonality\ndf['Restored_Sales'] = df['Sales'].shift(\n    365) + df['Differenced_Sales']\n# Compute moving average on the restored data\ndf['Restored_Moving_Average_Forecast'] = df[\n    'Restored_Sales'].rolling(\n    window=window).mean().shift(1)\n# Split into training and validation again\ntrain_df = df.iloc[:split_time]\nvalid_df = df.iloc[split_time:]\n# Get forecast and true values on validation set\nforecast = valid_df['Restored_Moving_Average_Forecast']\ntrue_values = valid_df['Sales']\n```", "```\n    import tensorflow as tf\n    ```", "```\n    import numpy as np\n    ```", "```\n    # Create an array of temperatures for 2 weeks\n    ```", "```\n    temperature = np.arange(1, 15)\n    ```", "```\n    print(temperature)\n    ```", "```\n    [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    ```", "```\n    window_size = 3\n    ```", "```\n    batch_size = 2\n    ```", "```\n    shuffle_buffer = 10\n    ```", "```\n    dataset = tf.data.Dataset.from_tensor_slices(temperature)\n    ```", "```\n    for element in dataset:\n    ```", "```\n        print(element.numpy())\n    ```", "```\n    dataset = dataset.window(window_size + 1, shift=1, \n    ```", "```\n        drop_remainder=True)\n    ```", "```\n    for window in dataset:\n    ```", "```\n        window_data = ' '.join([str(\n    ```", "```\n            element.numpy()) for element in window])\n    ```", "```\n        print(window_data)\n    ```", "```\n    After window:\n    ```", "```\n    1 2 3 4\n    ```", "```\n    2 3 4 5\n    ```", "```\n    3 4 5 6\n    ```", "```\n    4 5 6 7\n    ```", "```\n    5 6 7 8\n    ```", "```\n    6 7 8 9\n    ```", "```\n    7 8 9 10\n    ```", "```\n    8 9 10 11\n    ```", "```\n    9 10 11 12\n    ```", "```\n    10 11 12 13\n    ```", "```\n    11 12 13 14\n    ```", "```\n    dataset = dataset.flat_map(\n    ```", "```\n        lambda window: window.batch(window_size + 1))\n    ```", "```\n    for element in dataset:\n    ```", "```\n        print(element.numpy())\n    ```", "```\n    After flat_map:\n    ```", "```\n    [1 2 3 4]\n    ```", "```\n    [2 3 4 5]\n    ```", "```\n    [3 4 5 6]\n    ```", "```\n    [4 5 6 7]\n    ```", "```\n    [5 6 7 8]\n    ```", "```\n    [6 7 8 9]\n    ```", "```\n    [ 7  8  9 10]\n    ```", "```\n    [ 8  9 10 11]\n    ```", "```\n    [ 9 10 11 12]\n    ```", "```\n    [10 11 12 13]\n    ```", "```\n    [11 12 13 14]\n    ```", "```\n    dataset = dataset.shuffle(shuffle_buffer)\n    ```", "```\n    print(\"\\nAfter shuffle:\")\n    ```", "```\n    for element in dataset:\n    ```", "```\n        print(element.numpy())\n    ```", "```\n    After shuffle:\n    ```", "```\n    [5 6 7 8]\n    ```", "```\n    [4 5 6 7]\n    ```", "```\n    [1 2 3 4]\n    ```", "```\n    [11 12 13 14]\n    ```", "```\n    [ 7  8  9 10]\n    ```", "```\n    [ 8  9 10 11]\n    ```", "```\n    [10 11 12 13]\n    ```", "```\n    [ 9 10 11 12]\n    ```", "```\n    [6 7 8 9]\n    ```", "```\n    [2 3 4 5]\n    ```", "```\n    [3 4 5 6]\n    ```", "```\n    dataset = dataset.map(lambda window: (window[:-1],\n    ```", "```\n        window[-1]))\n    ```", "```\n    print(\"\\nAfter map:\")\n    ```", "```\n    for x,y in dataset:\n    ```", "```\n        print(\"x =\", x.numpy(), \"y =\", y.numpy())\n    ```", "```\n    After map:\n    ```", "```\n    features = [3 4 5] label = 6\n    ```", "```\n    features = [1 2 3] label = 4\n    ```", "```\n    features = [10 11 12] label = 13\n    ```", "```\n    features = [ 8  9 10] label = 11\n    ```", "```\n    features = [4 5 6] label = 7\n    ```", "```\n    features = [7 8 9] label = 10\n    ```", "```\n    features = [11 12 13] label = 14\n    ```", "```\n    features = [5 6 7] label = 8\n    ```", "```\n    features = [2 3 4] label = 5\n    ```", "```\n    features = [6 7 8] label = 9\n    ```", "```\n    features = [ 9 10 11] label = 12\n    ```", "```\n    dataset = dataset.batch(batch_size).prefetch(1)\n    ```", "```\n    print(\"\\nAfter batch and prefetch:\")\n    ```", "```\n    for batch in dataset:\n    ```", "```\n        print(batch)\n    ```", "```\n    After batch and prefetch:\n    ```", "```\n    (<tf.Tensor: shape=(2, 3), dtype=int64,\n    ```", "```\n        numpy=array([[10, 11, 12],\n    ```", "```\n            [ 3,  4,  5]])>, <tf.Tensor: shape=(2,),\n    ```", "```\n            dtype=int64, numpy=array([13,  6])>)\n    ```", "```\n    (<tf.Tensor: shape=(2, 3), dtype=int64,\n    ```", "```\n        numpy=array([[ 9, 10, 11],\n    ```", "```\n            [11, 12, 13]])>, <tf.Tensor: shape=(2,),\n    ```", "```\n            dtype=int64, numpy=array([12, 14])>)\n    ```", "```\n    (<tf.Tensor: shape=(2, 3), dtype=int64,\n    ```", "```\n        numpy=array([[6, 7, 8],\n    ```", "```\n            [7, 8, 9]])>, <tf.Tensor: shape=(2,),\n    ```", "```\n            dtype=int64, numpy=array([ 9, 10])>)\n    ```", "```\n    (<tf.Tensor: shape=(2, 3), dtype=int64,\n    ```", "```\n        numpy=array([[1, 2, 3],\n    ```", "```\n            [4, 5, 6]])>, <tf.Tensor: shape=(2,),\n    ```", "```\n            dtype=int64, numpy=array([4, 7])>)\n    ```", "```\n    (<tf.Tensor: shape=(2, 3), dtype=int64,\n    ```", "```\n        numpy=array([[2, 3, 4],\n    ```", "```\n            [5, 6, 7]])>, <tf.Tensor: shape=(2,),\n    ```", "```\n            dtype=int64, numpy=array([5, 8])>)\n    ```", "```\n    (<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array(\n    ```", "```\n        [[ 8,  9, 10]])>, <tf.Tensor: shape=(1,),\n    ```", "```\n        dtype=int64, numpy=array([11])>)\n    ```", "```\n    time = pd.to_datetime(df['Date'])\n    ```", "```\n    sales = df['Sales'].values\n    ```", "```\n    split_time = int(len(df) * 0.8)\n    ```", "```\n    time_train = time[:split_time]\n    ```", "```\n    x_train = sales[:split_time]\n    ```", "```\n    time_valid = time[split_time:]\n    ```", "```\n    x_valid = sales[split_time:]\n    ```", "```\n    def windowed_dataset(\n    ```", "```\n        series, window_size, batch_size, shuffle_buffer):\n    ```", "```\n            dataset = tf.data.Dataset.from_tensor_slices(\n    ```", "```\n                series)\n    ```", "```\n            dataset = dataset.window(window_size + 1,\n    ```", "```\n                shift=1, drop_remainder=True)\n    ```", "```\n            dataset = dataset.flat_map(lambda window:\n    ```", "```\n                window.batch(window_size + 1))\n    ```", "```\n            dataset = dataset.shuffle(shuffle_buffer).map(\n    ```", "```\n                lambda window: (window[:-1], window[-1]))\n    ```", "```\n            dataset = dataset.batch(\n    ```", "```\n                batch_size).prefetch(1)\n    ```", "```\n        return dataset\n    ```", "```\n    dataset = windowed_dataset(x_train, window_size,\n    ```", "```\n        batch_size, shuffle_buffer_size)\n    ```", "```\n    model = tf.keras.models.Sequential([\n    ```", "```\n        tf.keras.layers.Dense(10,\n    ```", "```\n            input_shape=[window_size], activation=\"relu\"),\n    ```", "```\n        tf.keras.layers.Dense(10, activation=\"relu\"),\n    ```", "```\n        tf.keras.layers.Dense(1)\n    ```", "```\n        ])\n    ```", "```\n    model.compile(loss=\"mse\",\n    ```", "```\n        optimizer=tf.keras.optimizers.SGD(\n    ```", "```\n            learning_rate=1e-6, momentum=0.9))\n    ```", "```\n    model.fit(dataset, epochs=100, verbose=0)\n    ```", "```\n    input_batches = [sales[time:time + window_size][\n    ```", "```\n        np.newaxis] for time in range(len(\n    ```", "```\n            sales) - window_size)]\n    ```", "```\n    inputs = np.concatenate(input_batches, axis=0)\n    ```", "```\n    forecast = model.predict(inputs)\n    ```", "```\n    results = forecast[split_time-window_size:, 0]\n    ```", "```\n    print(tf.keras.metrics.mean_squared_error(x_valid,\n    ```", "```\n        results).numpy())\n    ```", "```\n    print(tf.keras.metrics.mean_absolute_error(x_valid,\n    ```", "```\n        results).numpy())\n    ```"]