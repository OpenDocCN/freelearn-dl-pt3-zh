<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Language Modeling</h1>
                </header>
            
            <article>
                
<p>This chapter is the first of several in which we'll discuss different neural network algorithms in the context of <strong>natural language processing</strong> (<strong>NLP</strong>). NLP teaches computers to process and analyze natural language data in order to perform tasks such as machine translation, sentiment analysis, natural language generation, and so on. But to successfully solve such complex problems, we have to represent the natural language in a way that the computer can understand, and this is not a trivial task.</p>
<p>To understand why, let's go back to image recognition. The neural network input is fairly intuitive—a 2D tensor with preprocessed pixel intensities, which preserves the spatial features of the image. Let's take a 28 x 28 MNIST image, which contains 784 pixels. All the information about the digit in the image is contained within these pixels only and we don't need any external information to classify the image. We can also safely assume that each pixel (perhaps excluding the ones near the image borders) carries the same information weight. Therefore, we feed them all to the network to do its magic and we let the results speak for themselves.</p>
<p>Now, let's focus on text data. <span>Unlike an image, we have 1D (as opposed to 2D) data—a single lon</span><span>g sequence of words.</span> A general rule of thumb is that a single-spaced A4 page contains 500 words. To feed a network (or any ML algorithm) the informational equivalent of a single MNIST image, we need 1.5 pages of text. The text structure has several hierarchical levels; starting from characters, then words, sentences, and paragraphs, all of which can fit within 1.5 pages of text. A<span>ll the pixels of the image relate to one digit; however, we don't know whether all the words relate to the same subject.</span> To avoid this complexity, NLP algorithms usually work with shorter sequences. Even though some algorithms use <strong>recurrent neural networks</strong> (<strong>RNNs</strong>), which take into account all previous inputs, in practice, they are still limited to a relatively short window of the immediately preceding words. Therefore, an NLP algorithm has to do more (perform well) with less (a smaller amount of input information).</p>
<p>To help us with this, we'll use a special type of <span>vector </span>word representation (language model). The language models we'll discuss use the context of a word (its surrounding words) to create a unique embedding vector associated with that word. These vectors carry more information about the word, compared to, say, one-hot encoding. They serve as a base for various NLP tasks.</p>
<p>In this chapter, we will cover the following topics: </p>
<ul>
<li>Understanding <em>n</em>-grams</li>
<li>Introducing neural language models:
<ul>
<li>Neural probabilistic language model </li>
<li>Word2Vec and fastText</li>
<li><span>Global</span> Vectors for Word Representation</li>
</ul>
</li>
</ul>
<ul>
<li>Implementing language models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding n-grams</h1>
                </header>
            
            <article>
                
<p>A word-based language model defines a<span> </span>probability<span> </span>distribution over sequences of words. Given a sequence of words of length<span> </span><em>m</em> (for example, a sentence), it assigns a probability <em>P</em>(<em>w1, ... , w<sub>m</sub></em>)<span> </span>to the full sequence of words. We can use these probabilities as follows:</p>
<ul>
<li>To estimate the likelihood of different phrases in NLP applications.</li>
<li>As a generative model to create new text. <span>A word-based language model can compute the likelihood of a given word following a sequence of words.</span></li>
</ul>
<p>The inference of the probability of a<span> </span>long<span> </span>sequence, say<span> </span><em>w<sub>1</sub>, ..., w<sub>m</sub></em>, is typically infeasible. We can calculate the joint probability of<span> </span><em>P</em>(<em>w<sub>1</sub>, ... , w<sub>m</sub></em>)<span> </span>with the chain rule of joint probability (<a href="b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml">Chapter 1</a>, <em>The Nuts and Bolts of Neural Networks</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d57f7d17-249c-4cbf-9346-c37d15d16f32.png" style="width:31.58em;height:1.17em;"/></p>
<p>The probability of the later words given the earlier words would be especially difficult to estimate from the data. That's why this joint probability is typically approximated by an independence assumption that the<span> </span><em>i-</em>th word is only dependent on the<span> </span><em>n-1</em><span> </span>previous words. We'll only model the joint probabilities of combinations of<span> </span><em>n</em><span> </span>sequential words, called <em>n</em>-grams. <span>For example, in the phrase</span><span> </span><em>the quick brown fox</em><span>, we have the following <em>n</em>-grams:</span></p>
<ul>
<li><strong>1-gram</strong>: <em>The</em>, <em>quick</em>, <em>brown</em>, and <em>fox</em> (also known as a unigram).</li>
<li><strong>2-grams</strong>: <em>The quick</em>, <em>quick brown</em>, and <em>brown fox</em> (also known as a bigram).</li>
<li><strong>3-grams</strong>: <em>The quick brown</em> and <em>quick brown fox</em> (also known as a trigram).</li>
<li><strong>4-grams</strong>: <em>The quick brown fox</em>.</li>
</ul>
<p><span>The inference of the joint distribution is approximated with the help of <em>n</em>-gram models that split the joint distribution into multiple independent parts. </span></p>
<div class="packt_tip"><span>The term <em>n</em></span>-grams <span>can refer to other types of sequences of length</span><span> </span><em>n</em><span>, such as</span><span> </span><em>n</em><span> </span><span>characters.</span></div>
<p>If we have a large corpus of text, we can find all the <em>n</em>-grams up until a certain<span> </span><em>n</em><span> </span>(typically 2 to 4) and count the occurrence of each <em>n</em>-gram in that corpus. From these counts, we can estimate the probabilities of the last word of each <em>n</em>-gram, given the previous<span> </span><em>n-1</em><span> </span>words:</p>
<ul>
<li><strong>1-gram</strong>:<sub><img class="fm-editor-equation" src="assets/fb25070e-7589-4481-89ba-4ae57914aea4.png" style="width:22.67em;height:2.67em;"/></sub></li>
</ul>
<p> </p>
<ul>
<li><strong>2-gram</strong>:<span> </span><sub><img class="fm-editor-equation" src="assets/25b8576c-7d29-43b9-b271-9f93061e8f1a.png" style="width:15.75em;height:2.67em;"/></sub></li>
</ul>
<p> </p>
<ul>
<li><strong>N-gram</strong>:<span> </span><sub><img class="fm-editor-equation" src="assets/25f22551-a6df-486d-92b9-7f1cfdd37d1f.png" style="width:27.75em;height:2.67em;"/></sub></li>
</ul>
<p>The independent assumption that the<span> </span><em>i-</em>th word is only dependent on the previous <em>n-1</em> words can now be used to approximate the joint distribution.</p>
<p>For example, for a unigram, we can approximate the joint distribution by using the following formula:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5a638abc-d9c6-43ab-98f0-d0800f5b8579.png" style="width:24.25em;height:1.33em;"/></div>
<p>For a trigram, we can approximate the joint distribution by using the following formula:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6de5cc70-b831-454a-b92a-d7d6b822a585.png" style="width:35.92em;height:1.33em;"/></div>
<p>We can see that, based on the vocabulary size, the number of <em>n</em>-grams grows exponentially with<span> </span><em>n</em>. For example, if a small vocabulary contains 100 words, then the number of possible 5-grams would be<span> </span><em>100<sup>5</sup><span> </span>= 10,000,000,000</em><span> </span>different 5-grams. In comparison, the entire works of Shakespeare contain around 30,000 different words, illustrating the infeasibility of using <em>n</em>-grams with a large<span> </span><em>n</em>. Not only is there the issue of storing all the probabilities, but we would also need a very large text corpus to create decent <em>n</em>-gram probability estimations for larger values of<span> </span><em>n</em>.</p>
<p>This problem is known as the curse of dimensionality. When the number of possible input variables (words) increases, the number of different combinations of these input values increases exponentially. The curse of dimensionality arises when the learning algorithm needs at least one example per relevant combination of values, which is the case in <em>n</em>-gram modeling. The larger our<span> </span><em>n</em>, the better we can approximate the original distribution and the more data we would need to make good estimations of the <em>n</em>-gram probabilities.</p>
<p>Now that we are familiar with the <em>n</em>-gram model and the curse of dimensionality, let's discuss how to solve it with the help of neural language models. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing neural language models</h1>
                </header>
            
            <article>
                
<p>One way to overcome the curse of <span>dimensionality</span><span> </span>is by learning a lower-dimensional, distributed<span> </span>representation<span> </span>of the words (<em>A Neural Probabilistic Language Model</em>, <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>). This distributed representation is created by learning an embedding function that transforms the space of words into a lower-dimensional space of word embeddings as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1433 image-border" src="assets/651755db-3fcd-4cb6-b39d-91f34cb191a7.png" style="width:45.75em;height:11.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Words -&gt; one-hot encoding -&gt; word embedding vectors</div>
<p>Words from the vocabulary with size<span> </span><em>V</em><span> </span>are transformed into one-hot encoding vectors of size<span> </span><em>V</em><span> </span>(each word is encoded uniquely). Then, the embedding function transforms this <em>V</em>-dimensional space into a distributed representation of size<span> </span><em>D</em><span> </span>(here, <em>D</em>=4).</p>
<p>The idea is that the embedding function learns semantic information about the words. It associates each word in the vocabulary with a continuous-valued vector representation, that is, the word embedding. Each word corresponds to a point in this embedding space, and different dimensions correspond to the grammatical or semantic properties of these words.</p>
<p>The goal is to ensure that the words close to each other in the embedding space have similar meanings. In this way, the information that some words are semantically similar can be exploited by the language model. For example, it might learn that <em>fox</em> and <em>cat</em> are semantically related and that both <em>the quick brown fox</em> and <em>the quick brown cat</em> are valid phrases. A sequence of words can then be replaced with a sequence of embedding vectors that capture the characteristics of these words. We can use this sequence as a base for various NLP tasks. <span>For example, a classifier trying to classify the sentiment of an article might be trained on using previously learned word embeddings, instead of one-hot encoding vectors. In this way, the semantic information of the words becomes readily available for the sentiment classifier.</span></p>
<p><span>Word embeddings are one of the central paradigms when solving NLP tasks. We can use them to improve the performance of other tasks where there might not be a lot of labeled data available. Next, we'll discuss the first neural language model that was introduced in 2001 (which serves as an example that many of the concepts in deep learning are not new). </span></p>
<div class="packt_tip">We usually denote vectors with bold non-italic lowercase letters, such as <strong>w</strong>. But the convention in neural language models is to use italic lowercase, such as <em>w.</em> In this chapter, we'll use this convention.</div>
<p>In the next section, we will take a look at the <strong>neural probabilistic language model</strong> (<strong>NPLM</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural probabilistic language model</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>It is possible to learn the language model and, implicitly, the embedding function via a feedforward fully connected network. Given a sequence of</span><span> </span><em>n-1</em><span> </span><span>words (</span><em>w<sub>t-n+1</sub><span> </span>, ..., w<sub>t-1</sub></em><span>), it tries to output the probability distribution of the next word, </span><em>w<sub>t</sub></em><span> </span>(the following diagram is based on <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>)<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1434 image-border" src="assets/4ddf599f-8890-4f57-86bf-872e19c666f9.png" style="width:29.67em;height:23.83em;"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">A neural network language model that outputs the probability distribution of the word w<sub>t</sub>, given the words <em>w<sub>t-n+1</sub></em><span> ... </span><em>w<sub>t-1</sub></em>.<span> </span><em>C</em><span> </span>is the embedding matrix</div>
<p>The network layers play different roles, such as the following:</p>
<ol>
<li>The embedding layer takes the one-hot representation of the word<span> </span><em>w<sub>i</sub></em><span> </span>and transforms it into the word's embedding vector by multiplying it with the embedding matrix,<span> </span><strong>C</strong>. This computation can be efficiently implemented with table lookup. The embedding matrix,<span> </span><strong>C</strong><span>, </span>is shared between the words, so all words use the same embedding function.<span> </span><strong>C</strong><span> </span>is a<span> </span><em>V * D</em><span> </span>matrix, where<span> </span><em>V</em><span> </span>is the size of the vocabulary and<span> </span><em>D</em><span> is </span>the size of the embedding. In other words, the matrix, <strong>C</strong>, represents the network weights of the hidden <em>tanh</em> layer.</li>
</ol>
<ol start="2">
<li>The resulting embeddings are concatenated and serve as an input to the hidden layer, which uses<span> </span><em>tanh</em><span> </span>activation. The output of the hidden layer is thus represented by the <sub><img class="fm-editor-equation" src="assets/75975090-e212-41c9-b67a-286040eb447a.png" style="width:23.50em;height:1.17em;"/> </sub>function, where <strong>H</strong> represents the embedding to hidden layer weights and<span> </span><em>d</em><span> </span>represents the hidden biases.</li>
<li>Finally, we have the output layer with weights,<span> </span><strong>U</strong>, bias,<span> </span><em>b</em>, and softmax activation, which map the hidden layer to the word space probability distribution: <sub><img class="fm-editor-equation" src="assets/1b6b697c-5fc7-4038-9f98-f897be806696.png" style="width:10.83em;height:1.33em;"/></sub><em>.</em></li>
</ol>
<p><span>This model simultaneously learns an embedding of all the words in</span> the vocabulary (em<span>bedding layer)</span><span> </span><span>and a model of the probability function for sequences of words (network output). I</span><span>t is able to generalize this probability function to sequences of words that were not seen during training. A specific combination of words in the test set might not be seen in the training set, but a sequence with similar embedding features is much more likely to be seen during training. </span>Since<span> </span>we can construct the training data and labels based on the positions of the words (which already exist in the text), training this model is an unsupervised learning task. Next, we'll discuss the word2vec language model, which was introduced in 2013 and sparked an interest in the field of NLP in the context of neural networks. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word2Vec</h1>
                </header>
            
            <article>
                
<p><span>A lot of research has gone into creating better word embedding models, in particular by omitting learning the probability function over sequences of words. One of the most popular ways to do this is with </span><span>word2</span>vec (<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a> and<span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a></span><span>, <a href="https://arxiv.org/abs/1310.4546">https://arxiv.org/abs/1310.4546</a>). Similar to NPLM, word2vec creates embedding vectors based on the context (surrounding words) of the word in focus. It</span><span> comes in two flavors: <strong>continuous bag of words</strong> (<strong>CBOW</strong>) and <strong>Skip-gram</strong>. We'll start with CBOW and then we'll discuss Skip-gram.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CBOW</h1>
                </header>
            
            <article>
                
<p>CBOW predicts the most likely word given its context (surrounding words). <span>For example, given the sequence <em>The quick</em> _____ <em>fox jumps</em>, the model will predict <em>brown</em>. The context is the <em>n</em> preceding and the <em>n</em> following words of the word in focus (unlike NPLM, where only the preceding words participate). The following screenshot shows the context window as it slides across the text:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1435 image-border" src="assets/cf31bab8-c35a-4cfd-817f-49f3bd0aa8d6.png" style="width:23.67em;height:8.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A word2vec sliding context window with <em>n = 2</em>. The same type of context window applies to both CBOW and Skip-gram</div>
<p><span>CBOW takes all words within the context with equal weights and doesn't consider their order (</span><span>hence the <em>bag</em> in the name). </span><span>It is somewhat similar to NPLM, but because it learns only the embedding vectors,</span> we'll train the model with the help of the following simple neural network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1436 image-border" src="assets/7bc7f5b4-39f6-4097-9b72-a0f4a0c64f8a.png" style="width:55.33em;height:23.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A CBOW model network</div>
<p>Here's how it works:</p>
<ul>
<li>The network has input, hidden, and output layers.</li>
<li>The input is the one-hot encoded word representations. The one-hot encoded vector size of each word is equal to the size of the vocabulary, <em>V</em>.</li>
<li>The embedding vectors are represented by the input-to-hidden weights, <strong>W</strong><em><sub>V<span>×</span>D</sub></em><span>, of the network. They are</span> <em>V × D-</em><span>shaped matrix, where</span><span> </span><em>D</em><span> </span><span>is the length of the embedding vector (which is the same as the number of units in the hidden layer). As in NPLM, we can think of the weights as a lookup table, where each row represents one word embedding vector. Because each input word is one-hot encoded, it will always activate a single row of the weights. That is, for each input sample (word), only the word's own embedding vector will participate.</span></li>
<li>The embedding vectors of all context words are averaged to produce the output of the hidden network layer (there is no activation function).</li>
<li>The hidden activations serve as input to the output softmax layer of size <em>V</em> (with the weight vector <strong>W<sup>'</sup></strong><em><sub>D<span>×</span>V</sub></em>), which predicts the most likely word to be found in the context (proximity) of the input words. The index with the highest activation represents the one-hot encoded related word. </li>
</ul>
<p><span>We'll train the network with gradient descent and backpropagation. The training set consists of (context and label) one-hot encoded pairs of words, appearing in close proximity to each other in the text. For example, if part of the text is the sequence <kbd>[the, quick, brown, fox, jumps]</kbd> and <em>n = 2</em>, the training tuples will inclu</span>de <kbd>([quick, brown], the)</kbd>, <kbd>([the, brown, fox], quick)</kbd>, <kbd>([the, quick, fox jumps], brown)</kbd>, an<span>d so on. Since we are only interested in the embeddings, <strong>W</strong><em><sub>V×D</sub></em>, we'll discard the rest of the network weights, <strong>W<sup>'</sup></strong><em><sub>V×D</sub></em>, when the training is finished.</span></p>
<p><span>CBOW will tell us which word is most likely to appear in a given context. This could be a problem for rare words. For example, given the context <em>The weather today is really</em> ____, the model will predict the word <em>beautiful</em> rather than <em>fabulous</em> (hey, it's just an example). CBOW is </span><span>several times faster to train than the Skip-gram and achieves slightly better accuracy for frequent words.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Skip-gram</h1>
                </header>
            
            <article>
                
<p>Given an input word, the Skip-gram model can predict its context (the opposite of CBOW). For example, the word <em>brown</em> will predict the words <span><em>The quick fox jumps</em>. Unlike CBOW, the input is a single one-hot word. But how do we represent the context words in the output? Instead of trying to predict the whole context (all surrounding words) simultaneously, Skip-gram transforms the context into multiple training pairs such as </span><kbd>(fox, the)</kbd>, <kbd>(fox, quick)</kbd>, <kbd>(fox, brown)</kbd>, and <kbd>(fox, jumps)</kbd>. Once again, we can train the model with a simple one-layer network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1437 image-border" src="assets/463ba7aa-d28f-4a91-bfb9-a43718ae33d5.png" style="width:27.17em;height:19.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">A Skip-gram model network</div>
<p>As with CBOW, the output is a softmax, which represents the one-hot encoded most probable context word. The input-to-hidden weights, <strong>W</strong><em><sub>V<span>×</span>D</sub></em>, represent the word embeddings lookup table and the hidden-to-output weights, <strong>W<sup>'</sup></strong><em><sub>D<span>×</span>V</sub></em>, are only relevant during training. The hidden layer doesn't have an activation function (that is, it uses linear activation).</p>
<p>We'll train the model with backpropagation (no surprises here). Given a sequence of words, <em>w<sub>1</sub>, ..., w<sub>M</sub></em>, the objective of the Skip-gram model is to maximize the average log probability w<span>here </span><em>n</em><span> is the window size</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/57a3d779-efa4-4e17-b47f-9ea3e2ac7cba.png" style="width:18.00em;height:4.08em;"/></p>
<p>The model defines the probability, <sub><img class="fm-editor-equation" src="assets/cfd1f18b-16bd-4da0-b9ca-6f037633c6fb.png" style="width:5.25em;height:1.08em;"/>,</sub> as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a1bf9184-f2d2-4ed3-87f9-def2a0b5f267.png" style="width:18.58em;height:3.83em;"/></p>
<p>In this example, <em>w<sub>I</sub></em> and <em>w<sub>O</sub></em> are the input and output words and <strong>v</strong><em><sub>w</sub></em> and <strong>v</strong><em><sup>'</sup><sub>w</sub></em> are the corresponding word vectors in the input and output weights <strong>W</strong><em><sub>V<span>×</span>D</sub></em> and <strong>W<sup>'</sup></strong><em><sub>D<span>×</span>V</sub></em>, respectively (we keep the original notation of the paper). Since the net doesn't have a hidden activation function, its output value for one input/output word pair is simply the multiplication of the input word vector, <sub><img class="fm-editor-equation" src="assets/198b51f8-3363-4fe6-b6be-69cea1dd7fdc.png" style="width:2.50em;height:1.33em;"/>,</sub> and the output word vector, <sub><img class="fm-editor-equation" src="assets/408cc438-39b1-44e4-8904-02094aa20d34.png" style="width:2.00em;height:1.33em;"/></sub> <span>(hence the transpose operation). </span></p>
<p>The authors of the word2vec paper note that word representations cannot represent idiomatic phrases that are not compositions of the individual words. For example, <em>New York Times</em> is a newspaper, and not just a natural combination of the meanings of <span><em>New</em>,</span> <em><span>York</span>, </em>and <em>Times</em>. To overcome this, the model can be extended to include whole phrases. However, this significantly increases the vocabulary size. And, as we can see from the preceding formula, the softmax denominator needs to compute the output vectors for all words of the vocabulary. Additionally, every weight of the <strong>W<sup>'</sup></strong><em><sub>D<span>×</span>V</sub></em> matrix is updated on every training step, which slows the training.</p>
<p>To solve this, we can replace the softmax with the so-called <strong>negative sampling</strong> (<strong>NEG</strong>). For each training sample, we'll take the positive training pair (for example, <kbd>(fox, brown)</kbd>), as well as <em>k</em> additional negative pairs (for example, <kbd>(fox, puzzle)</kbd>), where <em>k</em> is usually in the range of [5,20]. I<span>nstead of predicting the word that best matches the input word (softmax), we'll simply predict whether the current pair of words is true or not.</span> In effect, we convert the multinomial classification problem (classify as one of many classes) to a binary logistic regression (or binary classification) problem. By learning the distinction between positive and negative pairs, the classifier will eventually learn the word vectors in the same way, as with multinomial classification. In word2vec, the words for the negative pairs are drawn from a special distribution, which draws less frequent words more often, compared to more frequent ones.</p>
<p>Some of the most frequent words to occur carry less information value compared to the rare words. Examples of such words are the definite and indefinite articles <em>a</em>, <em>an</em>, and <em>the</em>. The model will benefit more from observing the pairs <em>London</em> and <em>city</em> compared to <em>the</em> and <em>city</em> because almost all words co-occur frequently with <em>the</em>. The opposite is also true—the vector representations of frequent words do not change significantly after training on a large number of examples. To counter the imbalance between the rare and frequent words, the authors of the paper propose a subsampling approach, where each word, <em>w<sub>i</sub></em>, of the training set is discarded with some probability, computed by the heuristic formula where <em>f(w<sub>i</sub>)</em><span> is the frequency of word </span><em>w<sub>i</sub></em><span> and </span><em>t</em><span> is a threshold (usually around 10</span><sup>-5</sup><span>)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/95e5cdee-8daa-4410-8b6b-e56d7b5653ce.png" style="width:9.75em;height:3.33em;"/></p>
<p>It aggressively subsamples words with a frequency of greater than <em>t</em>, but also preserves the ranking of the frequencies.</p>
<p>In conclusion, we can say that, in general, Skip-gram performs better on rare words compared to CBOW, but it takes longer to train.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">fastText</h1>
                </header>
            
            <article>
                
<p>fastText (<a href="https://fasttext.cc/">https://fasttext.cc/</a>) is a library for learning word embeddings and text classification created by the <strong>Facebook AI Research</strong> (<strong>FAIR</strong>) group. <span>Word2Vec treats each word in the corpus as an atomic entity and generates a vector for each word, but this approach ignores the internal structure of the words. In contrast, fastText decomposes each word, <em>w</em>, to a bag of character <em>n</em>-grams. For example, if <em>n = 3</em>, we can decompose the word <em>there</em> to the character 3-grams and the special sequence <em>&lt;there&gt;</em> for the whole word:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>&lt;th</em>, <em>the</em>, <em>her</em>, <em>ere</em>, <em>re&gt;</em></p>
<p>Note the use of the special characters <em>&lt;</em> and <em>&gt;</em> to indicate the start and the end of the word. This is necessary to avoid mismatching between <em>n</em>-grams from different words. For example, the word <em>her</em> will be represented as <em>&lt;her&gt;</em> and it will not be mistaken for the <em>n</em>-gram <em>her</em> from the word <em>there</em>. The authors of fastText suggest <em><strong>3 ≤ n </strong></em><span><strong><em>≤ 6</em></strong>.</span></p>
<p>Recall the softmax formula we introduced in the <em>Skip-gram</em> section. Let's generalize it by replacing the vector multiplication operation of the word2vec network with a generic scoring function, <strong><em>s</em></strong>,<strong><em> </em></strong><span>where </span><em>w<sub>t</sub></em><span> is the input word and </span><em>w<sub>c</sub></em><span> is the context word</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5e4e75aa-b63f-41af-bd4c-9fb77323debb.png" style="width:19.25em;height:3.75em;"/></p>
<p>In the case of fastText, we'll represent a word with the sum of the vector representations of its <em>n</em>-grams. Let's denote the set of <em>n</em>-grams that appear in word <em>w</em> with <em>G<sub>w</sub> = {1 ... G}</em>, the vector representation of an <em>n</em>-gram, <em>g</em>, with <strong>v<em><sub>g</sub></em></strong>, and the potential vector of the context word, <em>c</em>, with <strong>v</strong><em>'<strong><sub>c</sub></strong></em>. Then, the scoring function defined by fastText becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6470be2a-59c7-461e-869d-1e98f2cd83d3.png" style="width:10.92em;height:3.25em;"/></p>
<p>In effect, we train the fastText model with Skip-gram type word pairs, but the input word is represented as a bag of <em>n</em>-grams. </p>
<p>Using character <em>n</em>-grams has several advantages over the traditional word2vec model:</p>
<ul>
<li>It can classify unknown or misspelled words if they share <em>n</em>-grams with other words familiar to the model.</li>
<li>It can generate better word embeddings for rare words. Even if a word is rare, its character <em>n</em>-grams are still shared with other words, so the embeddings can still be good.</li>
</ul>
<p>Now that we are familiar with word2vec, we'll introduce the Global Vectors for Word Representation language model, which improves some word2vec deficiencies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Global Vectors for Word Representation model</h1>
                </header>
            
            <article>
                
<p>One disadvantage of word2vec is that it only uses the local context of words and doesn't consider their global co-occurrences. In this way, the model loses a readily available, valuable source of information. As the name suggests, the <strong>Global Vectors for Word Representation</strong> (<strong>GloVe</strong>) model tries to solve this (<a href="https://nlp.stanford.edu/pubs/glove.pdf">https://nlp.stanford.edu/pubs/glove.pdf</a>). </p>
<p class="CDPAlignLeft CDPAlign"><span>The algorithm starts with the global word-word co-occurrence matrix,</span> <strong>X</strong><span>. A cell,</span> <em>X<sub>ij</sub></em>, <span>indicates how often the word</span> <em>j</em> <span>appears in the context of word</span> <em>i</em><span>. The following table shows the co-occurrence matrix for a window with size <em>n </em>= 2 of the sequence <em>I like DL. I like NLP. I enjoy cycling</em>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1439 image-border" src="assets/ec13a661-4de4-403e-93dc-99355e343fcd.png" style="width:28.42em;height:12.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A co-occurrence matrix of the sequence I like DL. I like NLP. I enjoy cycling</div>
<p>Let's denote<span> the number of times any word appears </span><span>in the context of word <em>i</em> with</span> <img class="fm-editor-equation" src="assets/89690123-8c01-410a-a982-61a58a6ea93b.png" style="width:5.67em;height:1.17em;"/> and <span>the probability that word <em>j</em> appears in the context of word <em>i</em> with <img class="fm-editor-equation" src="assets/760aa31f-aec2-40d3-9a1a-a0597141079e.png" style="width:8.83em;height:1.08em;"/>. To better understand how this can help us, we'll use an example that shows the co-occurrence probabilities for the target words <em>ice</em> and <em>steam</em> with selected context words from a 6 billion token corpus:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1440 image-border" src="assets/e01e3c90-35c6-49cf-9cb9-408a1cc6ff0f.png" style="width:34.25em;height:7.00em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Co-occurrence probabilities for the target words ice and steam with selected context words from a 6 billion token corpus: source: https://nlp.stanford.edu/pubs/glove.pdf</div>
<p>The bottom row shows the ratio of the probabilities. The word <strong>solid</strong> (the first column) is related to <strong>ice</strong>, but less related to <strong>steam</strong>, so the ratio between their probabilities is large. Conversely, <strong>gas</strong> is more related to <strong>steam</strong> than to <strong>ice</strong> and the ratio between their probabilities is very small. The words <strong>water</strong> and <strong>fashion</strong> are equally related to both target words, hence the ratio of the probabilities is close to one. The ratio is better in distinguishing relevant words <em>(</em><strong>solid</strong> and <strong>gas</strong><em>)</em> from irrelevant words <em>(</em><strong>water</strong> and <strong>fashion</strong><em>),</em> compared to the raw probabilities<em>.</em> Additionally, it is better at discriminating between the two relevant words.</p>
<p>With the previous argument, the authors of GloVe propose starting the word vector learning with the ratios of co-occurrence probabilities, rather than the probabilities themselves. With that starting point, and keeping in mind that the ratio <sub><img class="fm-editor-equation" src="assets/a59c08e9-51d4-4beb-8523-4ef9f0ca0ca9.png" style="width:3.25em;height:1.17em;"/></sub> depends on three words—<em>i</em>, <em>j</em>, and <em>k—</em>we can define the most general form of the GloVe model as follows, <span>where </span><sub><img class="fm-editor-equation" src="assets/a4e0ad61-266b-42b5-97eb-67e1b19d4fa8.png" style="width:3.58em;height:1.17em;"/> </sub><span>are the word vectors and </span><sub><img class="fm-editor-equation" src="assets/f2464866-daa8-4cb7-99e4-60f99a9a8536.png" style="width:3.42em;height:1.08em;"/></sub><span> is a special context vector, which we'll discuss later (</span><sub><img class="fm-editor-equation" src="assets/b5075698-73c1-4bbd-99eb-4ff0d272b288.png" style="width:1.42em;height:1.08em;"/></sub><span> is the </span><em>D</em><span>-dimensional vector space of real numbers)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6d2c29e4-9762-486b-a188-e70f17501842.png" style="width:10.08em;height:2.83em;"/></p>
<p>In other words, <em>F</em> is such a function that, when computed with these three specific vectors (we assume that we already know them), will output the ratio of probabilities. Furthermore, <em>F</em> should encode the information of the probabilities ratio because we've already identified its importance. Since vector spaces are inherently linear, one way to encode this information is with the vector difference of the two target words. Therefore, the function becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1a938fcb-379e-4210-a6a3-3a3ade67f6d9.png" style="width:12.67em;height:3.25em;"/></p>
<p>Next, let's note that the function arguments are vectors, but the ratio of probabilities is scalar. To solve this issue, we can take the dot product of the arguments:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a7494734-0fd3-4297-a393-de1ec3c20c1c.png" style="width:13.50em;height:3.17em;"/></p>
<p>Then, let's observe that the distinction between a word and its context word is arbitrary and we can freely exchange the two roles. Therefore, we should have <sub><img class="fm-editor-equation" src="assets/f11ecfab-81ef-4534-ac16-afd696d1af6d.png" style="width:14.50em;height:1.33em;"/></sub>, but the preceding equation doesn't satisfy this condition. Long story short (there is a more detailed explanation in the paper), to satisfy this condition, we need to introduce another restriction in the form of the following equation <span>where,</span> <sub><img class="fm-editor-equation" src="assets/8edde8d5-a0e9-40e3-ba81-3e61184d7bbd.png" style="width:0.83em;height:1.17em;"/></sub> <span>and</span> <sub><img class="fm-editor-equation" src="assets/e189ae1c-eb9b-4faf-8ceb-6f7faaddb01b.png" style="width:0.92em;height:1.17em;"/></sub> <span>are bias scalar values</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/109aeb0d-0d5a-421b-a793-02edb89b4361.png" style="width:15.25em;height:1.83em;"/></p>
<p>One issue with this formula is that <em>log(0)</em> is undefined, but the majority of the <em>X<sub>ik</sub></em> entries will be <em>0</em>. Additionally, it takes all co-occurrences with the same weight, but rare co-occurrences are noisy and carry less information than the more frequent ones. To solve all these issues, the authors propose a least squares regression model with a weighting function, <em>f(X<sub>ij</sub>)</em>, for each co-occurrence. The model has the following cost function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/75a06b7d-7aaf-486b-8759-a77da7a76184.png" style="width:24.75em;height:4.33em;"/></p>
<p>Finally, the weighting function, <em>f</em>, should satisfy several properties. First, <em>f(0) = 0</em>. Then, <em>f(x)</em> should be non-decreasing so that rare co-occurrences are not overweighted. And finally, <em>f(x)</em> should be relatively small for large values of <em>x</em>, so that frequent co-occurrences are not overweighted. Based on these properties and their experiments, the authors propose the following function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c7a7a719-5075-4504-b703-0ac6c60ff2e8.png" style="width:17.92em;height:3.25em;"/></p>
<p>The following graph shows <em>f(x)</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1441 image-border" src="assets/49d5223a-7ba8-42a4-b3ea-5bc5c794964c.png" style="width:26.33em;height:13.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Weighting function <em>f(X<sub>ij</sub>)</em> with a cut-off value of <em>x<sub>max</sub></em>= 100 and<span> </span><em>α = 3/4.</em> The authors' experiments show that these parameters work best; source: https://nlp.stanford.edu/pubs/glove.pdf</div>
<p>The model generates two sets of word vectors: <em>W</em> and <img class="fm-editor-equation" src="assets/e95f0f4d-b4cb-40e3-bb19-e5df3bbcfd59.png" style="width:1.08em;height:1.08em;"/>. When <em>X</em> is symmetric, <em>W</em> and <img src="assets/f0df870d-e98c-411b-b409-ce351e2fbb86.png" style="width:1.08em;height:1.08em;"/> are equivalent and differ only as a result of their random initializations. But the authors note that training an ensemble of networks and averaging their results usually helps to prevent overfitting. To mimic this behavior, they choose to use the sum <sub><span><img class="fm-editor-equation" src="assets/3b86fd4d-d51e-4d81-87d3-073d63bd0b21.png" style="width:3.33em;height:1.08em;"/></span></sub> as the final word vectors, observing a small increase in the performance.</p>
<p>This concludes our discussion about neural language models. In the next section, we'll see how to train and visualize a word2vec model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing language models</h1>
                </header>
            
            <article>
                
<p><span>In this section, we'll implement a short pipeline for preprocessing text sequences and training a word2vec model with the processed data. We'll also implement another example to visualize embedding vectors and check some of their interesting properties.</span></p>
<p><span>The code in this section requires the following Python packages:</span></p>
<ul>
<li><span><strong>Gensim</strong> (version 3.80, <a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a>) is an open source Python library for unsupervised topic modeling and NLP. It supports all three models that we have discussed so far (word2vec, GloVe, and fastText). </span></li>
<li>The <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>, <a href="https://www.nltk.org/">https://www.nltk.org/</a>, <span>ver 3.4.4</span>) is a Python suite of libraries and programs for symbolic and statistical NLP.</li>
<li>Scikit-learn<span> (ver 0.19.1, <a href="https://scikit-learn.org/">https://scikit-learn.org/</a>) is an open source Python ML library with various classification, regression, and clustering algorithms. More specifically, we'll use</span> <span><strong>t-Distributed Stochastic Neighbor Embedding</strong> (<strong>t-SNE</strong>, <a href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a>) to visualize high-dimensional embedding vectors (more on that later). </span></li>
</ul>
<p>With this introduction, let's continue with training the language model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the embedding model</h1>
                </header>
            
            <article>
                
<p>In the first example, we'll train a word2vec model on the classic novel <em>War and Peace</em> by Leo Tolstoy. The novel is stored as a regular text file in the code repository. Let's start:</p>
<ol>
<li>As the tradition goes, we'll do the imports:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>logging<br/><span>import </span>pprint  # beautify prints<br/><br/><span>import </span>gensim<br/><span>import </span>nltk</pre>
<ol start="2">
<li>Then, we'll set the logging level to <kbd>INFO</kbd> so we can track the training progress:</li>
</ol>
<pre style="padding-left: 60px">logging.basicConfig(<span>level</span>=logging.INFO)</pre>
<ol start="3">
<li>Next, we'll implement the text tokenization pipeline. <span>Tokenization refers to the breaking up of a text sequence into pieces (or <strong>tokens</strong>) such as words, keywords, phrases, symbols, and other elements. Tokens can be individual words, phrases, or even whole sentences. We'll implement two-level tokenization; first, we'll split the text into sentences and then we'll split each sentence into individual words:</span></li>
</ol>
<pre style="padding-left: 60px"><span>class </span>TokenizedSentences:<br/>    <span>"""Split text to sentences and tokenize them"""<br/></span><span><br/></span><span>    </span><span>def </span><span>__init__</span>(<span>self</span><span>, </span>filename: <span>str</span>):<br/>        <span>self</span>.filename = filename<br/><br/>    <span>def </span><span>__iter__</span>(<span>self</span>):<br/>        <span>with </span><span>open</span>(<span>self</span>.filename) <span>as </span>f:<br/>            corpus = f.read()<br/><br/>        raw_sentences = nltk.tokenize.sent_tokenize(corpus)<br/>        <span>for </span>sentence <span>in </span>raw_sentences:<br/>            <span>if </span><span>len</span>(sentence) &gt; <span>0</span>:<br/>                <span>yield </span>gensim.utils.simple_preprocess(sentence<span>, </span><span>min_len</span>=<span>2</span><span>, </span><span>max_len</span>=<span>15</span>)</pre>
<p style="padding-left: 60px">The <kbd>TokenizedSentences</kbd> iterator takes as an argument the text filename, where the novel is located. Here's how the rest of it works:</p>
<ol>
<li style="padding-left: 60px">The iteration starts by reading the full contents of the file in the <kbd>corpus</kbd> variable.</li>
<li style="padding-left: 60px">The raw text is split into a list of sentences (the <kbd>raw_sentences</kbd> variable) with the help of NLTK's <kbd>nltk.tokenize.sent_tokenize(corpus)</kbd> function. For example, it will return a <kbd>['I like DL.', 'I like NLP.'</kbd>, <kbd>'I enjoy cycling.']</kbd> for input list <kbd>'I like DL. I like NLP. I enjoy cycling.'</kbd>.</li>
<li style="padding-left: 60px">Next, each <kbd>sentence</kbd> is preprocessed with the <kbd>gensim.utils.simple_preprocess(sentence, min_len=2, max_len=15)</kbd> function. It converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long. For example, the <kbd>'I like DL'</kbd> sentence will be tokenized to the <kbd>['like', 'dl']</kbd> list. The punctuation characters are also removed. <span>The tokenized sentence is yielded as the final result.</span></li>
<li>Then, we'll instantiate <span><kbd>TokenizedSentences</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px">sentences = TokenizedSentences(<span>'war_and_peace.txt'</span>)</pre>
<ol start="5">
<li>Next, we'll instantiate Gensim's word2vec training model:</li>
</ol>
<pre style="padding-left: 60px">model = gensim.models.word2vec. \<br/>    Word2Vec(<span>sentences</span>=sentences<span>,<br/></span><span>             </span><span>sg</span>=<span>1</span><span>,  </span><span># 0 for CBOW and 1 for Skip-gram<br/></span><span>             </span><span>window</span>=<span>5</span><span>,  </span><span># the size of the context window<br/></span><span>             </span><span>negative</span>=<span>5</span><span>,  </span><span># negative sampling word count<br/></span><span>             </span><span>min_count</span>=<span>5</span><span>,  </span><span># minimal word occurrences to include<br/></span><span>             </span><span>iter</span>=<span>5</span><span>,  </span><span># number of epochs<br/></span><span>             </span>)</pre>
<p style="padding-left: 60px">The model takes <kbd>sentences</kbd> as a training dataset. <kbd>Word2Vec</kbd> supports all parameters and variants of the model that we've discussed in this chapter. For example, you can switch between CBOW or Skip-gram with the <kbd>sg</kbd> parameter. You can also set the context window size, negative sampling count, number of epochs, and other things. You can explore all parameters in the code itself.</p>
<div class="packt_tip">Alternatively, you can use the fastText model by replacing <kbd>gensim.models.word2vec.Word2Vec</kbd> with <kbd>gensim.models.fasttext.FastText</kbd> (it works with the same input parameters). </div>
<ol start="6">
<li>The <kbd>Word2Vec</kbd> constructor also initiates the training. After a short time (you don't need the GPU, as the training dataset is small), the generated embedding vectors are stored in the <kbd>model.wv</kbd> object. On one hand, it acts like a dictionary and you can access the vector for each word with <kbd>model.wv['WORD_GOES_HERE'],</kbd> however, it also supports some other interesting functionality. You can measure the similarity between different words based on the difference of their word vectors with the <kbd>model.wv.most_similar</kbd> method. First, it converts each word vector to a unit vector (a vector with a length of one). Then, it computes the dot product between the unit vector of the target word and the unit vectors of all other words. The higher the dot product between two vectors, the more similar they are. For example, <kbd>pprint.pprint(model.wv.most_similar(positive='mother', topn=5))</kbd> will output the five most similar words to the word <kbd>'mother'</kbd> and their dot products:</li>
</ol>
<pre style="padding-left: 60px">[('sister', 0.9024157524108887),<br/> ('daughter', 0.8976515531539917),<br/> ('brother', 0.8965438008308411),<br/> ('father', 0.8935455679893494),<br/> ('husband', 0.8779271245002747)]</pre>
<p style="padding-left: 60px">The result serves as a kind of proof that the word vectors correctly encode the meaning of the words. The word <kbd>'mother'</kbd> is indeed related by meaning to <kbd>'sister'</kbd>, <kbd>'daughter'</kbd>, and so on.</p>
<p style="padding-left: 60px">We can also find the most similar words to a combination of target words. For example, <kbd>model.wv.most_similar(positive=['woman', 'king'], topn=5)</kbd> will take the mean of the word vectors of <kbd>'woman'</kbd> and <kbd>'king'</kbd> and then it will find the words with the most similar vectors to the new mean value:</p>
<pre style="padding-left: 60px">[('heiress', 0.9176832437515259), ('admirable', 0.9104862213134766), ('honorable', 0.9047746658325195), ('creature', 0.9040032625198364), ('depraved', 0.9013445973396301)]</pre>
<p>We can see that some of the words are relevant (<kbd>'heiress'</kbd>), but most aren't (<kbd>'creature'</kbd>, <kbd>'admirable'</kbd>). Perhaps our training dataset is too small to capture more complex relations like these.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing embedding vectors</h1>
                </header>
            
            <article>
                
<p>To obtain better word vectors, compared to the ones in the <em>Training embedding model</em> section, we'll train another word2vec model. However, this time, we will use a larger corpus—the <kbd>text8</kbd> dataset, which consists of the first 100,000,000 bytes of plain text from Wikipedia. The dataset is included in Gensim and it's<span> tokenized as a single long list of words. With that, let's start:</span></p>
<ol>
<li>As usual, the imports are first. We'll also set the logging to <kbd>INFO</kbd> for good measure:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>logging<br/><span>import </span>pprint  <span># beautify prints<br/></span><span><br/></span><span>import </span>gensim.downloader <span>as </span>gensim_downloader<br/><span>import </span>matplotlib.pyplot <span>as </span>plt<br/><span>import </span>numpy <span>as </span>np<br/><span>from </span>gensim.models.word2vec <span>import </span>Word2Vec<br/><span>from </span>sklearn.manifold <span>import </span>TSNE<br/><br/>logging.basicConfig(<span>level</span>=logging.INFO)</pre>
<ol start="2">
<li>Next, we'll train the <kbd>Word2vec</kbd> model. This time, we'll use CBOW for faster training. We'll load the dataset with <kbd>gensim_downloader.load('text8')</kbd>:</li>
</ol>
<pre style="padding-left: 60px">model = Word2Vec(<br/>    <span>sentences</span>=gensim_downloader.load(<span>'text8'</span>)<span>,  </span><span># download and load the text8 dataset<br/></span><span>    </span><span>sg</span>=<span>0</span><span>, </span><span>size</span>=<span>100</span><span>, </span><span>window</span>=<span>5</span><span>, </span><span>negative</span>=<span>5</span><span>, </span><span>min_count</span>=<span>5</span><span>, </span><span>iter</span>=<span>5</span>)</pre>
<ol start="3">
<li>To see if this model is better, we can try to find the words most similar to <kbd>'woman'</kbd> and <kbd>'king'</kbd>, but most dissimilar to <kbd>'man'</kbd>. Ideally, one of the words would be <kbd>'queen'</kbd>. We can do this with the expression <kbd>pprint.pprint(model.wv.most_similar(positive=['woman', 'king'], negative=['man']))</kbd>. The output is as follows:</li>
</ol>
<pre style="padding-left: 60px">[('queen', 0.6532326936721802), ('prince', 0.6139929294586182), ('empress', 0.6126195192337036), ('princess', 0.6075714230537415), ('elizabeth', 0.588543176651001), ('throne', 0.5846244692802429), ('daughter', 0.5667101144790649), ('son', 0.5659586191177368), ('isabella', 0.5611927509307861), ('scots', 0.5606790781021118)]</pre>
<p style="padding-left: 60px">Indeed, the most similar word is <kbd>'queen'</kbd>, but the rest of the words are relevant as well.</p>
<ol start="4">
<li>Next, we'll display the words in a 2D plot with the help of <span>a t-SNE visualization model on the collected word vectors. The t-SNE </span><span>models each high-dimensional embedding vector on a two- or three-dimensional point in a way where similar objects are modeled on nearby points and dissimilar objects are modeled on distant points with a high probability.</span> We'll start with several <kbd>target_words</kbd> and then we'll collect clusters of the <em>n</em> most similar words (and their vectors) to each target word. The following is the code that does this:</li>
</ol>
<pre style="padding-left: 60px">target_words = [<span>'mother'</span><span>, </span><span>'car'</span><span>, </span><span>'tree'</span><span>, </span><span>'science'</span><span>, </span><span>'building'</span><span>, </span><span>'elephant'</span><span>, </span><span>'green'</span>]<br/>word_groups<span>, </span>embedding_groups = <span>list</span>()<span>, </span><span>list</span>()<br/><br/><span>for </span>word <span>in </span>target_words:<br/>    words = [w <span>for </span>w<span>, </span>_ <span>in </span>model.most_similar(word<span>, </span><span>topn</span>=<span>5</span>)]<br/>    word_groups.append(words)<br/><br/>    embedding_groups.append([model.wv[w] <span>for </span>w <span>in </span>words])</pre>
<ol start="5">
<li>Then, we'll train a t-SNE visualization model on the collected clusters with<span> the following parameters:</span>
<ul>
<li><span><kbd>perplexity</kbd> is loosely related to the number of nearest neighbors considered when matching the original and the reduced vectors for each point. In other words, it determines whether the algorithm will focus on the local or the global properties of the data.</span></li>
<li><span><kbd>n_components=2</kbd> specifies the number of output vector dimensions.</span></li>
<li><span><kbd>n_iter=5000</kbd> is the number of training iterations.</span></li>
<li><span><kbd>init='pca'</kbd> to use <strong>principal component analysis</strong> (<strong>PCA</strong>)-based initialization.</span></li>
</ul>
</li>
</ol>
<p style="padding-left: 60px"><span>The model takes the <kbd>embedding_groups</kbd> clusters as input and out</span>puts the <kbd>embeddings_2d</kbd> arra<span>y with 2D embedding vectors. The following is the implementation:</span></p>
<pre style="padding-left: 60px"><span># Train the t-SNE algorithm<br/></span>embedding_groups = np.array(embedding_groups)<br/>m<span>, </span>n<span>, </span>vector_size = embedding_groups.shape<br/>tsne_model = TSNE(<span>perplexity</span>=<span>8</span><span>, </span><span>n_components</span>=<span>2</span><span>, </span><span>init</span>=<span>'pca'</span><span>, </span><span>n_iter</span>=<span>5000</span>)<br/><br/><span># generate 2d embeddings from the original 100d ones<br/></span>embeddings_2d = tsne_model.fit_transform(embedding_groups.reshape(m * n<span>, </span>vector_size))<br/>embeddings_2d = np.array(embeddings_2d).reshape(m<span>, </span>n<span>, </span><span>2</span>)</pre>
<ol start="6">
<li>Next, we'll display the new 2D embeddings. To do this, we'll initialize the plot and some of its properties for better visibility:</li>
</ol>
<pre style="padding-left: 60px"><span># Plot the results<br/></span>plt.figure(<span>figsize</span>=(<span>16</span><span>, </span><span>9</span>))<br/><span># Different color and marker for each group of similar words<br/></span>color_map = plt.get_cmap(<span>'Dark2'</span>)(np.linspace(<span>0</span><span>, </span><span>1</span><span>, </span><span>len</span>(target_words)))<br/>markers = [<span>'o'</span><span>, </span><span>'v'</span><span>, </span><span>'s'</span><span>, </span><span>'x'</span><span>, </span><span>'D'</span><span>, </span><span>'*'</span><span>, </span><span>'+'</span>]</pre>
<ol start="7">
<li>Then, we'll iterate over e<span>ach <kbd>similar_words</kbd> cluster and we'll display its words on a scatter plot as points. We'll use a unique marker for each cluster. The points will be annotated with their corresponding words:</span></li>
</ol>
<pre style="padding-left: 60px"><span># Iterate over all groups<br/></span><span>for </span>label<span>, </span>similar_words<span>, </span>emb<span>, </span>color<span>, </span>marker <span>in </span>\<br/>        <span>zip</span>(target_words<span>, </span>word_groups<span>, </span>embeddings_2d<span>, </span>color_map<span>, </span>markers):<br/>    x<span>, </span>y = emb[:<span>, </span><span>0</span>]<span>, </span>emb[:<span>, </span><span>1</span>]<br/><br/>    <span># Plot the points of each word group<br/></span><span>    </span>plt.scatter(<span>x</span>=x<span>, </span><span>y</span>=y<span>, </span><span>c</span>=color<span>, </span><span>label</span>=label<span>, </span><span>marker</span>=marker)<br/><br/>    <span># Annotate each point with its corresponding caption<br/></span><span>    </span><span>for </span>word<span>, </span>w_x<span>, </span>w_y <span>in </span><span>zip</span>(similar_words<span>, </span>x<span>, </span>y):<br/>        plt.annotate(word<span>, </span><span>xy</span>=(w_x<span>, </span>w_y)<span>, </span><span>xytext</span>=(<span>0</span><span>, </span><span>15</span>)<span>,<br/></span><span>                     </span><span>textcoords</span>=<span>'offset points'</span><span>, </span><span>ha</span>=<span>'center'</span><span>, </span><span>va</span>=<span>'top'</span><span>, </span><span>size</span>=<span>10</span>)</pre>
<ol start="8">
<li>Finally, we'll display the plot:</li>
</ol>
<pre style="padding-left: 60px">plt.legend()<br/>plt.grid(<span>True</span>)<br/>plt.show()</pre>
<p>We can see how each cluster of related words is grouped in a close region of the 2D plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1442 image-border" src="assets/65557fb2-c974-4a23-a244-897b53e74984.png" style="width:81.75em;height:39.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">t-SNE visualization of the target words and their clusters of the most similar words</div>
<p><span>The graph, once again, proves that the obtained word vectors contain relevant information for the words. With the end of this example, we conclude the chapter as well.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This was the first chapter devoted to NLP. Appropriately, we started with the basic building blocks of most NLP algorithms today—the words and their context-based vector representations. We started with <em>n</em>-grams and the need to represent words as vectors. Then, we discussed the word2vec, fastText, and GloVe models. Finally, we implemented a simple pipeline to train an embedding model and we visualized word vectors with t-SNE.</p>
<p>In the next chapter, we'll discuss RNNs—a neural network architecture that naturally lends itself to NLP tasks. </p>


            </article>

            
        </section>
    </body></html>