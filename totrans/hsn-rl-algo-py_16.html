<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Practical Implementation for Resolving RL Challenges</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will wrap up some of the concepts behind <strong>deep reinforcement learning</strong> (<strong>deep RL</strong>) algorithms that we explained in the previous chapters to give you a broad view of their use and establish a general rule for choosing the most suitable one for a given problem. Moreover, we will propose some guidelines so that you can start the development of your own deep RL algorithm. This guideline shows the steps you need to take from the start of development so that you can <span>easily </span>experiment without losing too much time on debugging. In the same section, we also list the most important hyperparameters to tune and additional normalization processes to take care of.</p>
<p>Then, we'll address the main <span>challenges of this field by addressing issues such as stability, efficiency, and generalization. We'll use these three main problems as a pivotal point to transition to more advanced reinforcement learning techniques such as unsupervised RL and transfer learning. Unsupervised RL and transfer learning are of fundamental importance for deploying and solving demanding RL tasks. This is because they are techniques that address the three challenges we mentioned previously.</span></p>
<p><span>We will also look into how we can apply RL to real-world problems and how RL algorithms can be used for bridging the gap between simulation and the real world.</span></p>
<p>To conclude this chapter and this book as a whole, we'll discuss the future of reinforcement learning from both a technical and social perspective.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Best practices of deep RL</li>
<li>Challenges in deep RL</li>
<li>Advanced techniques</li>
<li>RL in the real world</li>
<li>Future of RL and its impact on society</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Best practices of deep RL</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we covered plenty of reinforcement learning algorithms, some of which are only upgrades (for example TD3, A2C, and so on), while others were fundamentally different from the others (such as TRPO and DPG) and propose an alternative way to reach the same objective. Moreover, we addressed non-RL optimization algorithms such as<span> imitation learning and evolution strategies </span>to solve sequential decision-making tasks. All of these alternatives may have created confusion and you may not know exactly which algorithm is best for a particular problem. If that is the case, don't worry, as we'll now go through some rules that you can use in order to decide which is the best algorithm to use for a given task.</p>
<p>Also, if you implemented some of the algorithms we went through in this book, you might find it hard to put all the pieces together to make the algorithm work properly. D<span>eep RL algorithms are notoriously difficult to debug and train, and the training time is very long. As a result, the whole training process is very slow and arduous. Luckily, there are a few strategies</span> <span>that you can adopt that will prevent some terrible headaches while developing deep RL algorithms. </span>But before looking at what these strategies are, let's deal with choosing the appropriate algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the appropriate algorithm</h1>
                </header>
            
            <article>
                
<p>The main driving force that differentiates the various types of RL algorithms is sample efficiency and training time.</p>
<div class="packt_infobox">We consider sample efficiency as the number of interactions with the environment that an agent has to make in order to learn the task. The numbers that we'll provide are an indication of the efficiency of the algorithm and are measured with respect to other algorithms on typical environments.</div>
<p>Clearly, there are other parameters that influence this choice, but usually, they have a minor impact and are of less importance. Just to give you an idea, the other parameters to be evaluated are the availability of CPUs and GPUs, the type of reward function, the scalability, and the complexity of the algorithm, as well as that of the environment.</p>
<p>For this comparison, we will take into consideration gradient-free black-box algorithms such as evolution strategies, model-based RL such as DAgger, and model-free RL. Of the latter, we will differentiate between policy gradient algorithms such as DDPG and TRPO and value-based algorithms such as DQN. </p>
<p>The following diagram shows the data efficiency of these four categories of algorithms <span>(note that the leftmost methods are less sample efficient than the rightmost methods)</span>. In particular, the efficiency of the algorithm increases as you move to the right of the diagram. So, you can see that gradient-free methods are those that require more data points from the environment, followed by policy gradient methods, value-based methods, <span>and finally model-based RL, which are the most sample efficient:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2026 image-border" src="assets/8c99a8da-3b3b-4316-8252-e88cb7fd277a.png" style="width:210.50em;height:20.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.1. Sample efficiency comparison between model-based RL methods, policy gradient algorithms, value-based algorithms, and gradient-free algorithms (the leftmost methods are less efficient than the rightmost methods)</div>
<p>Conversely, the training time of these algorithms is inversed related to their sample efficiency. This relationship is summarized in the following diagram <span>(note that the leftmost methods are slower to train than the rightmost methods)</span>. We can see that <span class="packt_screen">Model-based</span> algorithms are way slower to train than <span class="packt_screen">V<span>alue-based</span></span> <span>algorithms</span>, almost by a factor of 5, which in turn almost quintuples the time of <span>policy gradient algorithms, which are about</span> <strong>5x </strong>slower to train than gradient-free methods. </p>
<p>Be aware that these numbers are just to highlight the average case, and the training time is only related to the speed at which the algorithm is trained, and not to the time needed to acquire new transitions from the environment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2027 image-border" src="assets/6faa04da-a432-46dd-989d-ab505bd60a14.png" style="width:317.75em;height:32.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 13.2. Training time efficiency comparison between model-based RL methods, policy gradient algorithms, value-based algorithms, and gradient-free algorithms (the leftmost methods are slower to train than the rightmost methods)</span></div>
<p>We can see that the sample efficiency of an algorithm is complementary to its training time, meaning that an algorithm that is data efficient is slow to train and vice versa. <span>Thus, because the overall learning time of an agent takes into account both the training time and the speed of the environment, </span>you have to <span>find a trade-off between sample efficiency and training time that meet your needs</span>. In fact, the main purpose of model-based and more efficient model-free algorithms is to reduce the number of steps with the environment so that these algorithms are easier to deploy and train in the real world, where the interactions are slower than in simulators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From zero to one</h1>
                </header>
            
            <article>
                
<p>Once you have defined the algorithm that best fits your needs, whether that's one of the well-known algorithms or a new one, you have to develop it. As you saw throughout this book, reinforcement learning algorithms don't have much in common with supervised learning algorithms. For this reason, there are different aspects that are worth pointing out in order to facilitate the debugging, experimentation, and tuning of the algorithm:</p>
<ul>
<li><strong>Start with easy problems</strong>: <span>Initially, you would want to experiment with a workable version of the code as fast as possible. However, it would be advisable to gradually proceed with increasingly complex environments. This will greatly help to reduce the overall training and debugging time</span><span>.</span> Let me present an example. You can start with CartPole-v1 or RoboschoolInvertedPendulum-v1 if you need a discrete or continuous environment, respectively. Then, you can move to a medium-complexity environment such as RoboschoolHopper-v1, LunarLander-v2, or a related environment with RGB images. At this point, you should have a bug-free code that you can finally train and tune on your final task. Moreover<span>, y</span>ou should be as familiar as possible with the easier tasks so that you know what to look for if something is not working.</li>
</ul>
<ul>
<li><strong>Training is slow</strong>: Training deep reinforcement learning algorithms takes time and the learning curve can follow any kind of shape. As we saw in <span>the previous chapters,</span> the learning curves (that is, the cumulative reward of the trajectories with respect to the number of steps) can resemble a logarithm function, a hyperbolic tangent function, as shown in the following diagram, or a more complex function. The possible shapes depend on the reward function, its sparsity, and the complexity of the environment. If you are working on a new environment and you don't know what to expect, the suggestion here is to be patient and leave it running until you are sure that the progress has stopped. Also, don't get too involved with the plots while training.</li>
</ul>
<ul>
<li><strong>Develop some baselines</strong>: For new tasks, the suggestion is to develop at least two baselines so that you can compare your algorithm with them. One baseline could simply be a random agent, with the other being an algorithm such as REINFORCE or A2C. These baselines can then be used as a lower bound for performance and efficiency.</li>
<li><strong>Plots and histograms</strong>: To monitor the progress of the algorithm and to help during the debugging phase, an important factor is to plot and display histograms of key parameters such as the loss function, the cumulative reward, the actions (if possible), the length of the trajectories, the KL penalty, the entropy, and the value function. In addition to plotting the means, you can add the minimum and maximum values and the standard deviation. In this book, we primarily <span>used </span>TensorBoard to visualize this information, but you can use any tool you want.</li>
<li><strong>Use multiple seeds</strong>: Deep reinforcement learning embeds <span>stochasticity </span>both in the neural networks and in the environments, which often makes the results incoherent between different runs. So, to ensure consistency and stability, it's better to use multiple random seeds.</li>
<li><strong>Normalization:</strong> Depending on the design of the environment, it could be helpful to normalize the rewards, the advantage, and the observations. The advantage values (for example, in TRPO and PPO) can be normalized in a batch to have a mean of 0 and a standard deviation of 1. Additionally, the observations can be normalized using a set of initial random steps. Instead, the rewards can be normalized by a running estimate of the mean and standard deviation of the discounted or undiscounted reward.</li>
</ul>
<ul>
<li><strong>Hyperparameter tuning</strong>: Hyperparameters change a lot based on the class and type of algorithm. For example, value-based methods have multiple distinct hyperparameters compared to policy gradients, but also instances of these classes such as TRPO and PPO have many unique hyperparameters. That being said, for each algorithm that was introduced throughout this book, we specified the hyperparameters that were used and the most important ones to tune. Among them, there are at least two hyperparameters that are used by all the RL algorithms: learning rate and discount factor. The former is slightly less important than in supervised learning, but nevertheless, it remains one of the first hyperparameters to tune so that we have a working algorithm. The discount factor is unique to RL algorithms. The introduction of a discount factor may introduce bias as it modifies the objective function. However, in practice, it produces a better policy. Thus, to a certain degree, the shorter the horizon, the better it is, as it reduces instability:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2028 image-border" src="assets/6c0e587b-2b78-49e9-a61f-e3454508efbb.png" style="width:32.08em;height:20.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.3. Example of a logarithmic and hyperbolic tangent function</div>
<div class="packt_infobox"><span>For all the color references mentioned in the chapter, please refer to the color images bundle at </span><a href="http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf">http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf</a><span>.</span></div>
<p>Adopt these techniques and you'll be able to train, develop, and deploy your algorithms much more easily. Furthermore, you'll have algorithms that are more stable and robust.</p>
<p>Having a critical view and understanding of the drawbacks of deep reinforcement learning is a key factor when it comes to actually pushing the boundaries on what RL algorithms can do to design better state-of-the-art algorithms. In the following section, we'll present the main challenges of deep RL in a more concise view. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges in deep RL</h1>
                </header>
            
            <article>
                
<p>The efforts that have been put into the research of reinforcement learning algorithms in recent years has been huge. Especially since the introduction of the deep neural network as a function approximation, the advancement and results have been outstanding. Yet some major issues remain unsolved. These limit the applicability of RL algorithms to more extensive and interesting tasks. We are talking about the issues of stability, reproducibility, efficiency, and generalization, although scalability and the exploration problem could be added to this list.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stability and reproducibility</h1>
                </header>
            
            <article>
                
<p>Stability and reproducibility are somehow interconnected with each other as the goal is to design an algorithm that is capable of consistency across multiple runs and that is not too invariant to small tweaks. For example, the algorithm shouldn't be too sensitive to changes in the values of the hyperparameters.</p>
<p>The main factor that makes deep RL algorithms difficult to replicate is intrinsic to the nature of deep neural networks. This is mainly due to random initialization of the deep neural networks and the stochasticity of optimization. Moreover, this situation is exacerbated in RL, considering that the environments are stochastic. Combined, these factors are also to the detriment of the interpretability of results.</p>
<p>Stability is also put to the test by the high instability of RL algorithms, as we saw in Q-learning and REINFORCE. For example, in value-based algorithms, there isn't any guarantee of convergence and the algorithms suffer from high bias and instability. DQN uses many tricks to stabilize the learning process, such as an experienced replay and a delay in the update of the target network. Though these strategies can alleviate the instability problems, they don't go away. </p>
<p>To overcome any constraints that are intrinsic to the algorithm in terms of stability and reproducibility, we need to intervene outside of it. To this end, many different benchmarks and some rules of thumb can be employed to ensure a good level of reproducibility and consistency of results. These are as follows:</p>
<ul>
<li>Whenever possible, test the algorithms on multiple but similar environments. For example, test it on a suite of environments such as Roboschool or Atari Gym where the tasks are comparable to each other in terms of action and state spaces but have different goals.</li>
</ul>
<ul>
<li>Run many trials across different random seeds. The results may vary significantly by changing the seeds. As an example of this, the following diagram shows two runs of the exact same algorithm with the same hyperparameters, but with a different seed. You can see that the differences are large. So, depending on your goal, it could be helpful to use multiple random seeds, generally between three and five. For example, in academic papers, it is good practice to average all the results across five runs and take<span> the standard deviation</span> into account as well.</li>
<li>If the results are unsteady, consider using a more stable algorithm or employing some further strategies. Also, keep in mind that the effects in the changes of the hyperparameters can vary significantly across algorithms and environments:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2029 image-border" src="assets/a8b58095-2ae3-4764-b795-fde9589f58ab.png" style="width:38.08em;height:23.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.4. Performance of two trials of the same algorithm with different random seeds</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Efficiency</h1>
                </header>
            
            <article>
                
<p>In the previous section, <em>Choosing the appropriate algorithm</em>, we saw that the sample efficiency between the algorithms is highly variable. Moreover, from the previous chapters, we saw that more efficient methods, such as value-based learning, still require a substantial number of interactions with the environment to learn. Maybe only model-based RL can save itself from the hunger of data. Unfortunately, model-based methods have other downsides, such as a lower performance bound.</p>
<p>For this reason, hybrid model-based and model-free approaches have been built. However, these are difficult to engineer and are impractical for use in real-world problems. As you can see, the efficiency-related problem is very hard to solve but at the same time very important to address so that we can deploy RL methods in the real world.</p>
<p>There are two alternative ways to deal with very slow environments such as the physical world. One is to use a lower-fidelity simulator in the first place and then fine-tune the agent in the final environment. The other is to train the agent directly in the final environment, but transferring some prior related knowledge so as to avoid learning the task from scratch. It's like learning to drive when you've already trained your sensory system. In both cases, because we are transferring knowledge from one environment to another, we talk about a methodology called transfer learning. We'll elaborate on t<span>his methodology </span>very soon in the <em>Advanced techniques</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generalization</h1>
                </header>
            
            <article>
                
<p>The concept of generalization refers to two aspects<span> that are </span>different, but somehow <span>related</span>. In general terms, the concept of generalization in reinforcement learning refers to the capability of an algorithm to obtain good performance in a related environment. For example, if an agent has been trained to walk on dirty roads, we might expect that the same agent will perform well on paved roads. The better the generalization capabilities, the better the agent will perform in different environments. The second and lesser-used means of generalization refers to the property of the algorithm to achieve good performance in an environment where only limited data can be gathered.</p>
<p>In RL, the agent can choose the states to visit by itself and do so for as long as it wants so that it can also overfit on a certain problem space. However, if good generalization capabilities are required, a trade-off has to be found. This is only partially true if the agent is allowed to gather potentially infinite data for the environment as it will act as a sort of self-regularization method.</p>
<p>Nonetheless, to help with generalization across other environments, an agent must be capable of abstract reasoning to discern from the mere state-action mapping and interpret the task using multiple factors. Examples of abstract reasoning can be found in model-based reinforcement learning, transfer learning, and in the use of auxiliary tasks. We'll cover the latter topic later, but in brief, it is a technique that's used to improve generalization and sample efficiency by augmenting an RL agent with auxiliary tasks that were learned jointly with the main task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced techniques</h1>
                </header>
            
            <article>
                
<p>The challenges we listed previously have no simple solutions. However, there has been an effort in trying to overcome them and to come up with novel strategies to improve efficiency, generalization, and stability. Two of the most widespread and promising techniques that focus on efficiency and generalization are unsupervised reinforcement learning and transfer learning. In most cases, these strategies work in symbiosis with the deep reinforcement learning algorithms that we developed in the previous chapters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised RL</h1>
                </header>
            
            <article>
                
<p>Unsupervised RL is related to the usual unsupervised learning in how both methods don't use any source of supervision. While in unsupervised learning the data isn't labeled, in the reinforced counterpart, the reward is not given. That is, given an action, the environment returns only the next state. Both the reward and the <strong>done</strong> status are removed.</p>
<p>Unsupervised RL can be helpful in many occurrences, for example, when the annotation of the environment with hand-designed rewards is not scalable, or when an environment can serve multiple tasks. In the latter case, unsupervised learning can be employed so that we can learn about the dynamics of the environment. Methods that are able to learn from unsupervised sources can also be used <span>as an additional source of information</span> in environments with very sparse rewards. </p>
<p>How can we design an algorithm that can learn about the environment without any source of supervision? Can't we just employ model-based learning? Well, model-based RL still needs the reward signal to plan or infer the next actions. Therefore, a different solution is required. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intrinsic reward</h1>
                </header>
            
            <article>
                
<p>A potential fair alternative is to develop a reward function that is intrinsic to the agent, meaning that it's controlled exclusively by the belief of the agent. This method comes close to the approach that's used by newborns to learn. In fact, they employ a pure explorative paradigm to navigate the world without an immediate benefit. Nonetheless, the knowledge that's acquired may be useful later in life.</p>
<p>The intrinsic reward is a sort of <span>exploration bonus</span> based on the estimation of the novelty of a state. The more unfamiliar a state is, the higher the intrinsic reward. Thus, with it, the agent is incentivized to explore new spaces of the environment. It may have become clear by now that the intrinsic reward can be used as an alternative exploration strategy. In fact, many algorithms use it in combination with the extrinsic reward (that is the usual reward that's returned by the environment) to boost the exploration in very sparse environments such as Montezuma's revenge. However, though the methods to estimate the intrinsic reward are very similar to those we studied in <a href="800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml">Chapter 12</a>, <em>Developing ESBAS Algorithm</em>, to incentivize policy exploration (these exploration strategies were still related to the extrinsic reward), here, we are only concentrating on pure unsupervised exploration methods.</p>
<p>Two primary curiosity-driven strategies that provide rewards on unfamiliar states and explore <span>the environment </span>efficiently are count-based and dynamics-based:</p>
<ul>
<li>Count-based strategies (also known as <strong>visitation counts</strong> strategies) aim to count or estimate the visitation count of each state and encourage the exploration of those<span> states with </span>low visitation, assigning a<span> high intrinsic reward </span>to them.</li>
<li>Dynamics-based strategies train a dynamic model of the environment, along with the agent's policy, and compute the intrinsic reward either on the prediction error, on the prediction uncertainty, or on the prediction improvement. The underlying idea is that by fitting a model on the states visited, the new and unfamiliar states will have a higher uncertainty or estimation error. These values are then used to compute the intrinsic reward and incentivize the exploration of unknown states.</li>
</ul>
<p>What happens if we apply only curiosity-driven approaches to the usual environments? The paper <em>Large-scale study of curiosity-driven learning</em> addressed this question and found that, on Atari games, pure curiosity-driven agents can learn and master the tasks without any external reward. Furthermore, they noted that, on Roboschool, walking behavior emerged purely out of these unsupervised algorithms based on intrinsic reward. The authors of the paper also suggested that these findings were due to the way in which the environments have been designed. Indeed, in <span>human-designed </span><span>environments (such as games), </span>the extrinsic reward is often aligned with the objective of seeking novelty. Nonetheless, in environments that are <span>not gamified</span>, pure curiosity-driven unsupervised approaches are able to explore and learn about the environment exclusively by themselves without any need for supervision whatsoever. Alternatively, RL algorithms can also benefit from a huge boost in exploration and consequently in performance <span>by combining the intrinsic with the extrinsic reward</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning</h1>
                </header>
            
            <article>
                
<p>Transferring knowledge between two environments, especially if these environments are similar to each other, is a hard task. Transfer learning strategies propose to bridge the knowledge gap so that the transition from an initial environment to a new one is as easy and smooth as possible. Specifically, transfer learning is the task of efficiently transferring knowledge from a source environment <span>(or multiple environments) to</span> a target environment. Thus, the more experience that has been acquired from a set of source tasks and transferred to a new target task, the faster the agent will learn and the better it will perform on the target task.</p>
<p>Generally speaking, when you think about an agent that hasn't been trained yet, you have to imagine a system that does not have any kind of information in it. Instead, when you play a game, you use a lot of prior knowledge. For example, you may guess the meaning of the <span><span>enemies </span></span>from their shapes and colors, as well as their dynamics. This implies that you are able to recognize the enemies when they shoot you, like in <span>the Space Invaders game that's shown in the following diagram</span>. Also, you can easily guess the general dynamics of the game. Instead, at the start of the training, an RL agent won't know anything. This comparison is important because it provides valuable insight into the importance of transferring knowledge between multiple environments. An agent that has the ability to use the experience that was acquired from a source task can learn exponentially faster on the target environment. For example, if the source environment is Pong and the target environment is Breakout, then many of the visual components could be reused, saving a lot of time for computation. T<span>o have an accurate understanding of its overall importance, i</span>magine the efficiency that's gained in much more complex environments:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2030 image-border" src="assets/89e22bd8-eb9f-445b-8bb4-2ff4300b3183.png" style="width:16.50em;height:23.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.5. A screenshot of Space Invaders. Are you able to infer the role of the sprites?</div>
<p>When speaking about transfer learning, we refer to 0-shot learning, 1-shot learning, and so on, as the number of attempts required in the target domain. For example, 0-shot learning means that the policy that has been trained on a source domain is directly employed on the target domain without further training. In this case, the agent must develop strong generalization capabilities to adjust itself to the new task. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of transfer learning</h1>
                </header>
            
            <article>
                
<p>Many types of transfer learning exist, and their usage depends on the specific case and needs. One of the distinctions is related to the number of source environments. Obviously, the more source environments you are training the agent on, the more diversity it has, and the more experience can be used in the target domain. Transfer learning from multiple source domains is called multi-task learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">1-task learning</h1>
                </header>
            
            <article>
                
<p>1-task learning or simply transfer learning is the task of training the policy on one domain and transferring it onto a new one. Three major techniques can be employed to do that. These are as follows:</p>
<ul>
<li><strong>Fine-tuning</strong>: This involves the refinement of the learned model on the target task. If you get involved in machine learning, and especially in computer vision or natural language processing, you have probably used this technique already. Unfortunately, in reinforcement learning, fine-tuning is not as easy as it is in the aforementioned fields, as it requires more careful engineering and <span>generally </span>has lower benefits. The reason for this is that, in general, the gap between the two RL tasks is bigger than the gap between two different image domains. For example, the differences between the classification of a cat and a dog are minor compared to the differences between Pong and Breakout. Nonetheless, fine-tuning can also be used in RL and tuning just the last few layers (or substituting them if the action space is totally different) could give better generalization properties.</li>
<li><strong>Domain randomization</strong>: This is based on the idea that the diversification of the dynamics on a source domain increases the robustness of the policy on a new environment. Domain randomization works by manipulating the source domain, for example, by varying the physics of the simulator, so that the policy that has been trained on multiple randomly modified source domains is robust enough to perform well on a target domain. This strategy is more effective for training agents that need to be employed in the real world. In such circumstances, the policy is more robust and the simulation doesn't have to be exactly the same as the physical world to provide the required levels of performance. </li>
</ul>
<ul>
<li><strong>Domain Adaptation</strong>: This is another process that's used, especially to map a policy from a simulation-based source domain to a target physical world. Domain adaptation consists of changing the data distribution of the source domain to match that of the target. It is mainly used in image-based tasks, and the models usually make use of <strong>generative adversarial networks</strong> (<strong>GANs</strong>) to turn synthetic images into realistic ones.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-task learning</h1>
                </header>
            
            <article>
                
<p>In multi-task learning, the higher the number of environments the agent has been trained on, the more diversity and the better performance the agent will achieve on the target environment. The multiple source tasks can either be learned by one or multiple agents. If only one agent has been trained, then its deployment on the target task is easy. Otherwise, if multiple agents learned separate tasks, then the resulting policies can <span>either </span>be used as an ensemble, and the predictions on the target task averaged, or an intermediate step called distillation is employed to merge the policies into one. Specifically, the process of distillation compresses the knowledge of an ensemble of models into a single one that is easier to deploy and that infers faster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RL in the real world</h1>
                </header>
            
            <article>
                
<p>So far, in this chapter, we went through the best practices when developing deep RL algorithms and the challenges behind RL. We also saw how unsupervised RL and meta-learning can alleviate the problem of low efficiency and bad generalization. Now, we want to show you the problems that need to be addressed when employing an RL agent in the real world, and how the gap within a simulated environment can be bridged.</p>
<p><span>Designing an agent that is capable of performing actions in the real world is demanding</span>. But most reinforcement learning <span>applications</span> need to be deployed in the world. Thus, we have to understand the main challenges that we face when dealing with the complexity of the physical world and consider some useful techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Facing real-world challenges</h1>
                </header>
            
            <article>
                
<p>Besides the big problems of sample-efficiency and generalization, when dealing with the real world, we need to face problems such as safety and domain constraints. In fact, the agent is often not free to interact with the world due to safety and cost constraints. A solution may come from the use of constraint algorithms such as TRPO and PPO, which are embedded into the system mechanisms to limit the change of actions while training. This could prevent the agent from a drastic change in its behavior. Unfortunately, in highly sensitive domains, this is not enough. For example, nowadays, you cannot start training a self-driving car on the road straight away. The policy may take hundreds or thousands of <span><span>cycles </span></span>to understand that falling off a cliff leads to a bad conclusion and learn to avoid it. The alternative option of training the policy in a simulation first is a viable option. Nevertheless, when employed in cities, more safety-related decisions have to be made.</p>
<p>As we just hinted at, a simulation-first solution is a feasible approach and depending on the complexity of the real task, it may lead to good performance. However, the simulator has to mimic the real-world environment<span> as closely as possible</span>. For example, the simulator on the left-hand side of the following image cannot be used if the world resembles the right-hand side of the same image. This gap between the real and the simulated world is known as the reality gap:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2079 image-border" src="assets/0687367a-cd9f-4f80-b4ab-570182383d94.png" style="width:51.33em;height:19.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 13.6. Comparison between an artificial world and the physical world</span></div>
<p>On the other hand, using a highly accurate and realistic environment may not be feasible either. The bottleneck is now the computation power that's required by the simulator. This limitation can be partially overcome by starting with a faster and less accurate simulator, and then progressively increasing the fidelity so as to decrease the reality gap. Eventually, this is to the<span> detriment of the speed, </span>but <span>at this point, </span>the agent should have already learned most of the tasks and may need only a few iterations to fine-tune itself. However, it is very difficult to develop highly accurate simulators that mimic the physical world. Thus, in practice, the reality gap will remain and techniques that improve generalization will have the responsibility to handle the situation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bridging the gap between simulation and the real world</h1>
                </header>
            
            <article>
                
<p>To seamlessly transition from the simulation to the real world and thus overtake the reality gap, some <span>generalization techniques that we presented earlier, such as domain adaptation and domain randomization, </span>could be used. For example, in the paper <em>Learning Dexterous In-Hand Manipulation</em>, the authors trained a human-like robot to manipulate physical objects with incredible dexterity using domain randomization. The policy learned from many different parallel simulations that were designed to provide a variety of experiences with random physical and visual attributes. This mechanism that prefers generalization over realism overall has been key, considering that the system, when deployed, showed a rich set of in-hand dexterous manipulation strategies, many of which are used by humans as well. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating your own environment</h1>
                </header>
            
            <article>
                
<p><span>For educational purposes, in this book, we have predominantly used fast and small-scale tasks that could best fit our needs. However, there are plenty of simulators in existence for locomotion tasks (such as Gazebo, Roboschool, and Mujoco), mechanical engineering, transportation, self-driving cars, security, and many more. </span>These existing environments are diverse, but there isn't one for every possible application. Thus, in some situations, you may find yourself in charge of creating your own.</p>
<p>The reward function by itself is difficult to design, but it is a key part of RL. With the wrong reward function, the environment can be impossible to solve and the agent may learn the wrong behaviors. In <a href="0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml">Chapter 1</a>, <em>The Landscape of Reinforcement Learning</em>, we gave the example of the boat-racing game, in which the boat <span>maximized the reward by driving in a circle to capture repopulating targets</span> instead of running toward the end of the trajectory as fast as possible. These are the kinds of behaviors to avoid while designing the reward function. </p>
<p>The general advice for designing the reward function (that can be applied in any environment) is to use positive rewards to incentive exploration and discourage the terminal states or negative rewards if the goal is to reach a terminal state as quickly as possible. The shape of the reward function is important to consider. Throughout this book, we have warned against sparse rewards. An optimal reward function should offer a smooth and dense function. </p>
<p>If, for some reason, the reward function is very difficult to put into formulas, there are two additional ways in which a supervision signal can be provided:</p>
<ul>
<li>G<span>ive a demonstration of the task </span>using imitation learning or inverse reinforcement learning.</li>
<li>Use human preferences to provide feedback about the agent's behavior. </li>
</ul>
<div class="packt_tip">The latter point is still a novel approach and if you are interested in it, you may find the paper <em>Deep Reinforcement Learning from Policy-Dependent Human Feedback</em> an interesting read (<a href="https://arxiv.org/abs/1902.04257">https://arxiv.org/abs/1902.04257</a>).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Future of RL and its impact on society</h1>
                </header>
            
            <article>
                
<p>The first foundations of AI were built more than 50 years ago, but only in the last few years has the innovation brought by AI spread through the world as a mainstream technology. T<span>his new wave of innovation i</span>s mainly due to the evolution of deep neural networks in supervised learning systems. However, the most recent breakthrough in artificial intelligence involves reinforcement learning, and most notably, deep reinforcement learning. Results like the ones that were obtained in the game of Go and Dota highlight the impressive quality of RL algorithms that are able to show long-term planning, ability in teamwork, and discover new game strategies that are difficult to comprehend even for humans. </p>
<p>The remarkable results that were obtained in the simulated environments started a new wave of applications of reinforcement learning in the physical world. We are only at the beginning, but many areas are and will be impacted, bringing with it profound transformations. RL agents that are e<span>mbedded </span>in our everyday life can enhance the quality of life by automating tedious work, addressing world-level challenges, and discovering new drugs <span>– </span>just to name a few possibilities. However, these systems, which will populate both our world and our lives, need to be safe and reliable. We aren't at this point yet, but we are on the right track.</p>
<p>The ethical use of AI has become a broad concern, such as in the employment of autonomous weapons. With this rapid technological progress, it is hard for the policymakers and the population to be at the forefront of creating open discussions about these issues. Many influential and reputable people also suggest that AI is a potential threat to humanity. But the future is impossible to predict, and the technology has a long way to go before developing agents that can actually show abilities that are <span>comparable </span>to those of humans. We have creativity, emotions, and adaptability that, for now, cannot be emulated by RL.</p>
<p>With <span>careful attention,</span> the near-term benefits brought by RL can dramatically outweigh the negative side. But to embed sophisticated RL agents in the physical environment, we need to work on the RL challenges we outlined previously. These are solvable and, once addressed, reinforcement learning has the potential to decrease social inequalities, improve the quality of our life, and the quality of our planet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">Throughout this book, we learned and implemented many reinforcement learning algorithms, but all this variety can be quite confusing when it comes to choosing one. For this reason, in this final chapter, we provided a rule of thumb that can be used to pick the class of RL algorithms that best fits your problem. It mainly considers the computational time and the sample efficiency of the algorithm. Furthermore, we provided some tips and tricks so that you can train and debug deep reinforcement learning algorithms better so as to make the process easier. </p>
<p>We also discussed the hidden challenges of reinforcement learning: stability and reproducibility, efficiency, and generalization. These are the main issues that have to be overcome in order to employ RL agents in the physical world. In fact, we detailed unsupervised reinforcement learning and transfer learning, two strategies that can be used to greatly improve generalization and sample efficiency. </p>
<p>Additionally, we detailed the most critical open problems and the cultural and technological impacts that reinforcement learning may have on our lives.</p>
<p>We hope that this book has provided you with a comprehensive understanding of reinforcement learning and piqued your interest in this fascinating field.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>How would you rank DQN, A2C, and ES based on their sample efficiency?</li>
<li><span>What would their rank be if they were rated on the training time and 100 CPUs were available?</span></li>
<li><span>Would you start debugging an RL algorithm on </span><span>CartPole or MontezumaRevenge?</span></li>
<li><span>Why is it better to use multiple seeds when comparing multiple deep RL algorithms?</span></li>
</ol>
<ol start="5">
<li>Does the in<span>trinsic reward help with the exploration of an environment?</span></li>
<li><span>What's transfer learning?</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><span>For an approach that uses a pure curiosity-driven approach in the Atari games, read the paper<span> </span><em>Large-scale study of curiosity-driven learning </em>(<a href="https://arxiv.org/pdf/1808.04355.pdf">https://arxiv.org/pdf/1808.04355.pdf</a>).</span></li>
<li><span>For practical use of domain randomization for learning dexterous in-hand manipulation, read the paper<span> </span><em>Learning Dexterous In-Hand Manipulation</em> (<a href="https://arxiv.org/pdf/1808.00177.pdf">https://arxiv.org/pdf/1808.00177.pdf</a>).</span></li>
<li><span>For some work that shows how human feedback can be applied as an alternative to the reward function, read the paper <em>Deep Reinforcement Learning from Policy-Dependent Human Feedback</em> (<a href="https://arxiv.org/pdf/1902.04257.pdf">https://arxiv.org/pdf/1902.04257.pdf</a>).</span></li>
</ul>


            </article>

            
        </section>
    </body></html>