<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer038">&#13;
			<p id="_idParaDest-14" class="chapter-number"><a id="_idTextAnchor013"/>Chapter 1:</p>&#13;
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Overview of TensorFlow Enterprise </h1>&#13;
			<p><a id="_idTextAnchor015"/>In this introductory chapter, you will learn how to set up and run TensorFlow Enterprise in a <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) environment. This will enable you to get some initial hands-on experience of how TensorFlow Enterprise integrates with other services in GCP. One of the most important improvements in TensorFlow Enterprise is the integration with the data storage options in Google Cloud, such as Google Cloud Storage and BigQuery. </p>&#13;
			<p>This chapter starts by covering how to complete a one-time setup for the cloud environment and enable the necessary cloud service APIs. Then we will see how easy it is to work with these data storage systems at scale. </p>&#13;
			<p>In this chapter, we'll cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Understanding TensorFlow Enterprise</li>&#13;
				<li>Configuring cloud environments for TensorFlow Enterprise</li>&#13;
				<li>Accessing the data sources</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>Understanding TensorFlow Enterprise</h1>&#13;
			<p><strong class="bold">TensorFlow</strong> has become an <a id="_idIndexMarker000"/>ecosystem consisting of many valuable assets. At the core of its popularity and versatility is a comprehensive machine learning library and model templates that evolve quickly with new features and capabilities. This popularity comes at a cost, and that cost is expressed as complexity, intricate dependencies, and API updates or deprecation timelines that can easily break the models and workflow that were laboriously built not too long ago. It is one thing to learn and use the latest improvement in your code as you build a model to experiment with your ideas and hypotheses, but it is quite another if your job is to build a model for long-term production use, maintenance, and support.</p>&#13;
			<p>Another problem associated with early TensorFlow in general concerned its code debugging process. In TensorFlow 1, lazy execution makes it rather tricky to test or debug your code because the code is not executed unless it is wrapped in a <em class="italic">session</em>, AKA a graph. Starting with TensorFlow 2, eager execution finally becomes a first-class citizen. Also, another welcome addition to TensorFlow 2 is the adoption of the Keras high-level API. This makes it much easier to code, experiment with, and maintain your model. It also improves the readability of your code and its training flow. </p>&#13;
			<p>For enterprise adoption, there <a id="_idIndexMarker001"/>are typically these three major challenges that are of concern for stakeholders:</p>&#13;
			<ul>&#13;
				<li>The first challenge is <strong class="bold">scale</strong>. A production-grade model has to be trained with large amounts of data, and often it is not practical or possible to fit into a single-node computer's memory. This also can be thought of as another problem: how do you pass training data to the model? It seems the natural and instinctive way is to declare and involve the entire dataset as a Pythonic structure such as a <strong class="bold">NumPy array</strong> or a <strong class="bold">pandas DataFrame</strong>, as we have seen in so many open source examples. But if the data is too large, then it seems reasonable to use another way of <a id="_idIndexMarker002"/>passing data into a model instance, similar to the Python iterator. In fact, <strong class="bold">TensorFlow.io</strong> and <strong class="bold">TensorFlow dataset libraries</strong> are specifically provided to address this issue. We will see how they ingest data in batches to a model training process in the subsequent chapters.</li>&#13;
				<li>The second challenge that typically arises in consideration of enterprise adoption of TensorFlow is the <strong class="bold">manageability</strong> of the development environment. Backward compatibility is not a strength of TensorFlow, because there are historically very quick updates to and new releases of APIs that replace or deprecate old ones. This includes but is not limited to library version, API signature, and usage style deprecation. As you can imagine by now, this is a deal-breaker for development, debugging, and maintenance of the codebase; it also doesn't help with managing the stability and reproducibility of a production environment and its scoring results. It can easily become a nightmare for someone who manages and controls a machine learning development infrastructure and the standard practices in an enterprise project. </li>&#13;
				<li>The third challenge is the efforts for API improvements, patch releases, and bug fixes. To address this, TensorFlow rolls these efforts into <strong class="bold">long-term support</strong>. Typically, for any TensorFlow release, Google's TensorFlow team is committed to providing these fixes for up to a year only. However, for an enterprise, this is too short for them to <a id="_idIndexMarker003"/>get a proper return on investment from the development cycle. Therefore, for enterprises' mission-critical performance, a longer commitment to TensorFlow releases is essential.</li>&#13;
			</ul>&#13;
			<p><strong class="bold">TensorFlow Enterprise</strong> was created <a id="_idIndexMarker004"/>to address these challenges. TensorFlow Enterprise is a special distribution of TensorFlow that is exclusively available through Google Cloud's various services. TensorFlow Enterprise is available through the following:</p>&#13;
			<ul>&#13;
				<li>Google Cloud AI Notebooks</li>&#13;
				<li>Google Cloud AI Deep Learning VMs</li>&#13;
				<li>Google Cloud AI Deep Learning Containers</li>&#13;
				<li>Partially available on Google Cloud AI Training</li>&#13;
			</ul>&#13;
			<p>The dependencies such as drivers and library version compatibility are managed by Google Cloud. It also provides optimized connectivity with other Google Cloud services, such as Cloud Storage and the data warehouse (<strong class="bold">BigQuery</strong>). Currently, TensorFlow Enterprise supports versions 1.15, 2.1, and 2.3 of Google Cloud, and the GCP and TensorFlow teams will provide long-term support for up to three years, including bug fixes and updates. </p>&#13;
			<p>In addition to these exclusive services and managed features, the TensorFlow team also takes enterprise <a id="_idIndexMarker005"/>support to another level by offering a <strong class="bold">white-glove service</strong>. This is a <a id="_idIndexMarker006"/>separate service from Google Cloud Support. In this case, TensorFlow engineers in Google will work with qualified enterprise customers to solve problems or provide bug fixes in cutting edge AI applicat<a id="_idTextAnchor017"/>ions. </p>&#13;
			<h2 id="_idParaDest-17"><a id="_idTextAnchor018"/>TensorFlow Enterprise packages</h2>&#13;
			<p>At the time of writing <a id="_idIndexMarker007"/>this book, TensorFlow Enterprise includes the following packages:</p>&#13;
			<div>&#13;
				<div id="_idContainer007" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.1.jpg" alt="Figure 1.1 – TensorFlow packages&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.1 – TensorFlow packages</p>&#13;
			<p>We will have more to say about how to launch JupyterLab in Google AI Platform in <a href="B16070_02_Final_JM_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 2</em></a>, <em class="italic">Running TensorFlow Enterprise in Google AI Platform</em>, but for now, as a demonstration, the following command can be executed as a CLI command in a <strong class="bold">JupyterLab</strong> cell. It w<a id="_idTextAnchor019"/>ill <a id="_idIndexMarker008"/>provide the version for each package in your instance so that you can be sure of version consistency:</p>&#13;
			<p class="source-code">!pip list | grep tensorflow</p>&#13;
			<p>Here's the output:</p>&#13;
			<div>&#13;
				<div id="_idContainer008" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.2.jpg" alt="Figure 1.2 – Google Cloud AI Platform JupyterLab environment&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.2 – Google Cloud AI Platform JupyterLab environment</p>&#13;
			<p>We confirmed the <a id="_idIndexMarker009"/>environment is running a TensorFlow Enterprise distribution and all the library versions. Knowing this would help in future debugging and collaboration efforts.</p>&#13;
			<h1 id="_idParaDest-18"><a id="_idTextAnchor020"/>Configuring cloud environments for TensorFlow Enterprise</h1>&#13;
			<p>Assuming you have a Google Cloud <a id="_idIndexMarker010"/>account already set up with a billing method, before you can start using TensorFlow Enterprise, there <a id="_idIndexMarker011"/>are some one-time setup steps that you must complete in Google Cloud. This setup consists of the following steps: </p>&#13;
			<ol>&#13;
				<li>Create a cloud project and enable billing.</li>&#13;
				<li>Create a Google Cloud Storage bucket.</li>&#13;
				<li>Enable the necessary APIs.</li>&#13;
			</ol>&#13;
			<p>The following are some quick instructions for the<a id="_idTextAnchor021"/>se steps.</p>&#13;
			<h2 id="_idParaDest-19"><a id="_idTextAnchor022"/>Setting up a cloud environment</h2>&#13;
			<p>Now we are going to take a look at what we need to set up in <strong class="bold">Google Cloud</strong> before we can start using TensorFlow Enterprise. These setups are needed so that essential Google Cloud services <a id="_idIndexMarker012"/>can integrate seamlessly into the user tenant. For example, the <strong class="bold">project ID</strong> is used to enable resource creation credentials and access for different services when working with data in the TensorFlow workflow. And by virtue of the project ID, you can read and write data into your Cloud Storage and data w<a id="_idTextAnchor023"/>arehouse. </p>&#13;
			<h3>Creating a project</h3>&#13;
			<p>This is the first step. It is <a id="_idIndexMarker013"/>needed in order to enable billing so you can use nearly all Google Cloud resources. Most resources will ask for a project ID. It also helps you organize and track your spending by knowing which services contribute to each workload. Let's get started:</p>&#13;
			<ol>&#13;
				<li value="1">The URL for the page to create a project ID is <a href="https://console.cloud.google.com/cloud-resource-manager">https://console.cloud.google.com/cloud-resource-manager</a>.<p>After you have signed into the GCP portal, you will see a panel similar to this:</p><div id="_idContainer009" class="IMG---Figure"><img src="Images/Figure_1.3.jpg" alt="Figure 1.3 – Google Cloud’s project creation panel&#13;&#10;"/></div><p class="figure-caption">Figure 1.3 – Google Cloud's project creation panel</p></li>&#13;
				<li>Click on <strong class="bold">CREATE PROJECT</strong>:<div id="_idContainer010" class="IMG---Figure"><img src="Images/Figure_1.4.jpg" alt="Figure 1.4 – Creating a new project&#13;&#10;"/></div><p class="figure-caption">Figure 1.4 – Creating a new project</p></li>&#13;
				<li>Then provide a <a id="_idIndexMarker014"/>project name, and the platform will instantly generate a project ID for you. You can either accept it or edit it. It may give you a warning regarding how many projects you can create if you already have a few active projects:<div id="_idContainer011" class="IMG---Figure"><img src="Images/Figure_1.5.jpg" alt="Figure 1.5 – Project name and project ID assignment&#13;&#10;"/></div><p class="figure-caption">Figure 1.5 – Project name and project ID assignment</p></li>&#13;
				<li>Make a note of the project name and project ID. Keep these handy for future use. Hit <strong class="bold">CREATE</strong> and soon you will see the platform dashboard for this project:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer012" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.6.jpg" alt="Figure 1.6 – The main project management panel&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.6 – The main project management panel</p>&#13;
			<p>The project ID will frequently be <a id="_idIndexMarker015"/>used when accessing data storage. It is also used to keep track of resource consumption and allocation in a clou<a id="_idTextAnchor024"/>d<a id="_idTextAnchor025"/> tenant. </p>&#13;
			<h2 id="_idParaDest-20"><a id="_idTextAnchor026"/>Creating a Google Cloud Storage bucket</h2>&#13;
			<p>A <strong class="bold">Google</strong> <strong class="bold">Cloud Storage bucket</strong> is a <a id="_idIndexMarker016"/>common way to store models and model assets from a <a id="_idIndexMarker017"/>model training job. Creating a storage bucket is very easy. Just look for <strong class="bold">Storage</strong> in the left panel and select <strong class="bold">Browser</strong>:</p>&#13;
			<div>&#13;
				<div id="_idContainer013" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.7.jpg" alt="Figure 1.7 – Google Cloud’s Storage options&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.7 – Google Cloud's Storage options</p>&#13;
			<p>Click <strong class="bold">CREATE BUCKET</strong>, and follow the instructions as indicated in the panel. In all cases, there are default options selected for you:</p>&#13;
			<ol>&#13;
				<li value="1"><strong class="bold">Choose where to store your data</strong>. This is a trade-off between cost and availability as <a id="_idIndexMarker018"/>measured by performance. The default is multi-region to ensure the highest availability.</li>&#13;
				<li><strong class="bold">Choose a default storage class for your data</strong>. This choice lets you decide on costs related to retrieval operations. The default is the standard level for frequently accessed data. </li>&#13;
				<li><strong class="bold">Choose how to control access to objects</strong>. This offers two different access levels for the bucket. The default is <strong class="bold">object-level permissions (ACLs)</strong> in addition to <strong class="bold">bucket level permission (IAM)</strong>.</li>&#13;
				<li><strong class="bold">Advanced settings (optional)</strong>. Here, you can choose the encryption type, bucket retention policy, and any bucket labels. The default is a Google-managed key and no <a id="_idIndexMarker019"/>retention policy nor labels:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer014" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.8.jpg" alt="Figure 1.8 – Storage bucket creation process and choices&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.8 – Storage bucket creation process a<a id="_idTextAnchor027"/>nd choices</p>&#13;
			<h2 id="_idParaDest-21"><a id="_idTextAnchor028"/>Enabling APIs</h2>&#13;
			<p>Now we have a <a id="_idIndexMarker020"/>project, but before we start consuming Google Cloud services, we need to enable some APIs. This process needs to be done only once, usually as the project ID is created:</p>&#13;
			<ol>&#13;
				<li value="1">For now, let's enable the Compute Engine API for the project of your choice: <div id="_idContainer015" class="IMG---Figure"><img src="Images/Figure_1.9.jpg" alt="Figure 1.9 – Google Cloud APIs and Services for the project&#13;&#10;"/></div><p class="figure-caption">Figure 1.9 – Google Cloud APIs and Services for the project</p><p>Optional: Then select <strong class="bold">ENABLE APIS AND SERVICES</strong>.</p><p>You may do it here now, or as you go through the exercises in this book. If you need to use a <a id="_idIndexMarker021"/>particular cloud service for the first time, you can enable the API as you go along:</p><div id="_idContainer016" class="IMG---Figure"><img src="Images/Figure_1.10.jpg" alt="Figure 1.10 – Enabling APIs and Services&#13;&#10;"/></div><p class="figure-caption">Figure 1.10 – Enabling APIs and Services</p></li>&#13;
				<li>In the search box, type <strong class="source-inline">Compute Engine API</strong>:<div id="_idContainer017" class="IMG---Figure"><img src="Images/Figure_1.11.jpg" alt="Figure 1.11 – Enabling the Compute Engine API&#13;&#10;"/></div><p class="figure-caption">Figure 1.11 – Enabling the Compute Engine API</p></li>&#13;
				<li>You will see the status of the <strong class="bold">Compute Engine API</strong> in your project as shown in the following screenshot. Enable it if it's not already enabled:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer018" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.12.jpg" alt="Figure 1.12 – Google Cloud Compute Engine API&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.12 – Google Cloud Compute Engine API</p>&#13;
			<p>For now, this is good enough. There are more APIs that you'll need as you go through the <a id="_idIndexMarker022"/>examples in this book; GCP will ask you to enable the API when relevant. You can do so at that time.</p>&#13;
			<p>If you wish, you may repeat the preceding procedure to enable several other APIs as well: specifically, the <em class="italic">BigQuery API</em>, <em class="italic">BigQuery Data Transfer API</em>, <em class="italic">BigQuery Connection API</em>, <em class="italic">Service Usage API</em>, <em class="italic">Cloud Storage</em>, and the <em class="italic">Storage Transfer API</em>.</p>&#13;
			<p>Next, let's take a look at how to move data in a storage bucket into a table inside a BigQuery data <a id="_idTextAnchor029"/>warehouse.</p>&#13;
			<h1 id="_idParaDest-22"><a id="_idTextAnchor030"/>Creating a data warehouse</h1>&#13;
			<p>We will use a simple <a id="_idIndexMarker023"/>example of putting data stored in a Google Cloud bucket into a table that can be queried by BigQuery. The easiest way to do so is to use the BigQuery UI. Make sure it is in the right project. We will use this example to create a dataset that contains one table.</p>&#13;
			<p>You can navigate to BigQuery by searching for it in the search bar of the GCP portal, as in the following scr<a id="_idTextAnchor031"/>eenshot:</p>&#13;
			<div>&#13;
				<div id="_idContainer019" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.13.jpg" alt="Figure 1.13 – Searching for BigQuery&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.13 – Searching for BigQuery</p>&#13;
			<p>You will see <strong class="bold">BigQuery</strong> being <a id="_idIndexMarker024"/>suggested. Click on it and it will take you to the BigQuery portal: </p>&#13;
			<div>&#13;
				<div id="_idContainer020" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.14.jpg" alt="Figure 1.14 – BigQuery and the data warehouse query portal&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.14 – BigQuery and the data warehouse query portal</p>&#13;
			<p>Here are the steps to create a persistent table in the BigQuery data warehouse:</p>&#13;
			<ol>&#13;
				<li value="1">Select <strong class="bold">Create dataset</strong>:<div id="_idContainer021" class="IMG---Figure"><img src="Images/Figure_1.15.jpg" alt="Figure 1.15 – Creating a dataset for the project&#13;&#10;"/></div><p class="figure-caption">Figure 1.15 – Creating a dataset for the project</p></li>&#13;
				<li>Make sure you are in the dataset that you just created. Now click <strong class="bold">CREATE TABLE</strong>:<div id="_idContainer022" class="IMG---Figure"><img src="Images/Figure_1.16.jpg" alt="Figure 1.16 – Creating a table for the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 1.16 – Creating a table for the dataset</p><p>In the <strong class="bold">Source</strong> section, under <strong class="bold">Source</strong>, in the <strong class="bold">Create table from</strong> section, select <strong class="bold">Google Cloud Storage</strong>:</p><div id="_idContainer023" class="IMG---Figure"><img src="Images/Figure_1.17.jpg" alt="Figure 1.17 – Populating the table by specifying a data source&#13;&#10;"/></div><p class="figure-caption">Figure 1.17 – Populating the table by specifying a data source</p></li>&#13;
				<li>Then it will transition to another dialog box. You may enter the name of the file or use the <strong class="bold">Browse</strong> option to find the file stored in the bucket. In this case, a CSV file has already been put in my Google Cloud Storage bucket. You may either put your own CSV file into <a id="_idIndexMarker025"/>the storage bucket, or download the one I used from <a href="https://data.mendeley.com/datasets/7xwsksdpy3/1">https://data.mendeley.com/datasets/7xwsksdpy3/1</a>. Also, enter the column names and datatypes as the schema:<div id="_idContainer024" class="IMG---Figure"><img src="Images/Figure_1.18.jpg" alt="Figure 1.18 – An example of populating a table using an existing CSV file stored in the bucket&#13;&#10;"/></div><p class="figure-caption">Figure 1.18 – An example of populating a table using an existing CSV file stored in the bucket</p></li>&#13;
				<li>In the <strong class="bold">Schema</strong> section, use <strong class="bold">Auto-detect</strong>, and in the <strong class="bold">Advanced options</strong>, since the first row is an array of column names, we need to tell it to skip the first row:<div id="_idContainer025" class="IMG---Figure"><img src="Images/Figure_1.19.jpg" alt="Figure 1.19 – Handling column names for the table&#13;&#10;"/></div><p class="figure-caption">Figure 1.19 – Handling column names for the table</p></li>&#13;
				<li>Once the <a id="_idIndexMarker026"/>table is created, you can click <strong class="bold">QUERY TABLE</strong> to update the SQL query syntax, or just enter this query:<p class="source-code">SELECT * FROM `project1-190517.myworkdataset.iris` LIMIT 1000</p></li>&#13;
				<li>Execute the preceding query and now click on <strong class="bold">Run</strong>:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer026" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.20.jpg" alt="Figure 1.20 – Running a query to examine the table&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.20 – Running a query to examine the table</p>&#13;
			<p>There are many different data source types, as well as many different ways to create a data warehouse from raw data. This is just a simple example for structured data. For more information on <a id="_idIndexMarker027"/>other data sources and types, please refer to the BigQuery documentation at <a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#console">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#console</a>.</p>&#13;
			<p>Now you have learned <a id="_idIndexMarker028"/>how to create a persistent table in your BigQuery data warehouse using the raw data in your storage bucket. </p>&#13;
			<p>We used a CSV file as an example and added it to BigQuery as a table. In the next section, we are going to see how to connect TensorFlow to our data stored in BigQuery and the Cloud Storage bucket. Now we are ready to launch an instance of TensorFlow Enterprise running on AI Pla<a id="_idTextAnchor032"/>tform.  </p>&#13;
			<h1 id="_idParaDest-23"><a id="_idTextAnchor033"/>Using TensorFlow Enterprise in AI Platform</h1>&#13;
			<p>In this section, we are going to see firsthand how easy it is to access data stored in one of the Google Cloud <a id="_idIndexMarker029"/>Storage options, such as a <a id="_idIndexMarker030"/>storage bucket or BigQuery. To do so, we need to configure an environment to execute some example TensorFlow API code and command-line tools in this section. The easiest way to use TensorFlow Enterprise is through the AI Platform Notebook in Google Cloud:</p>&#13;
			<ol>&#13;
				<li value="1">In the GCP portal, search for <strong class="source-inline">AI Platform</strong>.</li>&#13;
				<li>Then select <strong class="bold">NEW INSTANCE</strong>, with <strong class="bold">TensorFlow Enterprise 2.3</strong> and <strong class="bold">Without GPUs</strong>. Then click <strong class="bold">OPEN JUPYTERLAB</strong>:<div id="_idContainer027" class="IMG---Figure"><img src="Images/Figure_1.21.jpg" alt="Figure 1.21 – The Google Cloud AI Platform and instance creation&#13;&#10;"/></div><p class="figure-caption">Figure 1.21 – The Google Cloud AI Platform and instance creation</p></li>&#13;
				<li>Click on <strong class="bold">Python 3</strong>, and it will provide a new notebook to execute the remainder of this chapter's examples:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer028" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.22.jpg" alt="Figure 1.22 – A JupyterLab environment hosted by AI Platform&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.22 – A JupyterLab environment hosted by AI Platform</p>&#13;
			<p>An instance of TensorFlow <a id="_idIndexMarker031"/>Enterprise running <a id="_idIndexMarker032"/>on AI Platform is now ready for use. Next, we are going to use this platform to perform some <a id="_idTextAnchor034"/>data I/O. </p>&#13;
			<h1 id="_idParaDest-24"><a id="_idTextAnchor035"/>Accessing the data sources</h1>&#13;
			<p>TensorFlow Enterprise <a id="_idIndexMarker033"/>can easily access data sources in Google Cloud Storage as well as BigQuery. Either of these data sources can easily host gigabytes to terabytes of data. Reading training data into the JupyterLab runtime at this magnitude of size is definitely out of question, however. Therefore, streaming data as batches through training is the way to handle data ingestion. The <strong class="source-inline">tf.data</strong> API is the way to build a data ingestion pipeline that aggregates data from files in a distributed system. After this step, the data object can go through transformation steps and evolve into a new data object for training.</p>&#13;
			<p>In this section, we are going to learn basic coding patterns for the following tasks:</p>&#13;
			<ul>&#13;
				<li>Reading data from a Cloud Storage bucket</li>&#13;
				<li>Reading data from a BigQuery table</li>&#13;
				<li>Writing data into a Cloud Storage bucket</li>&#13;
				<li>Writing data into BigQuery table</li>&#13;
			</ul>&#13;
			<p>After this, you will have a <a id="_idIndexMarker034"/>good grasp of reading and writing data to a Google Cloud Storage option and persisting your data or objects produced as a result of your TensorFlo<a id="_idTextAnchor036"/>w runtime.  </p>&#13;
			<h2 id="_idParaDest-25"><a id="_idTextAnchor037"/>Cloud Storage Reader</h2>&#13;
			<p><strong class="bold">Cloud Storage Reader</strong> is integrated <a id="_idIndexMarker035"/>with <strong class="source-inline">tf.data</strong>, so a <strong class="source-inline">tf.data</strong> object can easily access data in Google Cloud Storage. For example, the following code snippet demonstrates how to read a <strong class="source-inline">tfrecord</strong> dataset:</p>&#13;
			<p class="source-code">my_train_dataset = tf.data.TFRecordDataset('gs://&lt;BUCKET_NAME&gt;/&lt;FILE_NAME&gt;*.tfrecord')</p>&#13;
			<p class="source-code">my_train_dataset = my_train_dataset.repeat()</p>&#13;
			<p class="source-code">my_train_dataset = my_train_dataset.batch()</p>&#13;
			<p class="source-code">…</p>&#13;
			<p class="source-code">model.fit(my_train_dataset, …)</p>&#13;
			<p>In the example preceding pattern, the file stored in the bucket is serialized into <strong class="source-inline">tfrecord</strong>, which is a binary format of your original data. This is a very common way of storing and serializing large amounts of data or files in the cloud for TensorFlow consumption. This format enables a more efficient read for data being streamed over a network. We will discuss <strong class="source-inline">tfrecord</strong> in more detail in a fut<a id="_idTextAnchor038"/>ure chapter. </p>&#13;
			<h2 id="_idParaDest-26"><a id="_idTextAnchor039"/>BigQuery Reader</h2>&#13;
			<p>Likewise, <strong class="bold">BigQuery Reader</strong> is also integrated into the TensorFlow Enterprise environment, so <a id="_idIndexMarker036"/>training data or derived datasets stored in BigQuery can be consumed by TensorFlow Enterprise. </p>&#13;
			<p>There are three commonly used methods to read a table stored in a BigQuery data warehouse. The first way is the <strong class="source-inline">%%bigquery</strong> <em class="italic">magic command</em>. The second way is using the <em class="italic">BigQuery API in a general Python runtime</em>, and the third way is to <em class="italic">use TensorFlow I/O</em>. Each has i<a id="_idTextAnchor040"/>ts advantages.</p>&#13;
			<p>The BigQuery magic command</p>&#13;
			<p>This method is perfect for running SQL statements directly in a JupyterLab cell. This is equivalent to <a id="_idIndexMarker037"/>switching the cell's command interpreter. The <strong class="source-inline">%%bigquery</strong> interpreter executes a standard SQL query and the results are returned as a pan<a id="_idTextAnchor041"/>das DataFrame. </p>&#13;
			<p>The following code snippet shows how to use the <strong class="source-inline">%%bigquery</strong> interpreter and assign a pandas DataFrame name to the result. Each step is a JupyterLab cell:</p>&#13;
			<ol>&#13;
				<li value="1">Specify a project ID. This JupyterLab cell uses a default interpreter. Therefore, this is a Python variable. If the BigQuery table is in the same project, then you don't need to specify the project ID:<p class="source-code">project_id = '&lt;PROJECT-XXXXX&gt;'</p></li>&#13;
				<li>Invoke the <strong class="source-inline">%%bigquery</strong> magic command, and assign the project ID and a DataFrame name to hold the result:<p class="source-code">%%bigquery --project $project_id mydataframe</p><p class="source-code">SELECT <strong class="source-inline">*</strong> from `<strong class="source-inline">bigquery-public-data.covid19_jhu_csse.summary</strong>` limit 5</p><p>If the table is in the same project as you currently running from, you don't need --project argument.</p></li>&#13;
				<li>Verify the result is a pandas DataFrame:<p class="source-code">type(mydataframe)</p></li>&#13;
				<li>Show the DataFrame:<p class="source-code">mydataframe</p></li>&#13;
			</ol>&#13;
			<p>The complete code snippet for this example is as follows:</p>&#13;
			<div>&#13;
				<div id="_idContainer029" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.23.jpg" alt="Figure 1.23 – Code snippet for BigQuery and Python runtime integration&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.23 – Code snippet for BigQuery and Python runti<a id="_idTextAnchor042"/>me integration</p>&#13;
			<p>Here are the key takeaways:</p>&#13;
			<ul>&#13;
				<li>It is <a id="_idIndexMarker038"/>required to have a project ID in order to use the BigQuery API.</li>&#13;
				<li>You may pass a Python variable such as the project ID as a value into the cell that runs the <strong class="source-inline">%%bigquery</strong> interpreter using the <strong class="source-inline">$</strong> prefix.</li>&#13;
				<li>In order for the result to be reusable further by the Python preprocessing functionality or for TensorFlow consumption, you need to specify a name for the DataFrame that will hold t<a id="_idTextAnchor043"/>h<a id="_idTextAnchor044"/>e query result.</li>&#13;
			</ul>&#13;
			<h3>The Python BigQuery API</h3>&#13;
			<p>The second method by <a id="_idIndexMarker039"/>which we can invoke <a id="_idIndexMarker040"/>the BigQuery API is through Google Cloud's BigQuery client. This will give us direct access to the data, execute the query, and allow us to receive the results right away. This method does not require the user to know about the table schema. In fact, it simply wraps a SQL statement inside the BigQuery client instantiated throug<a id="_idTextAnchor045"/>h a library call.</p>&#13;
			<p>This code snippet demonstrates how to invoke the BigQuery API and use it to return the results in a <a id="_idTextAnchor046"/>pandas DataFrame:</p>&#13;
			<p class="source-code">from <strong class="source-inline">google.cloud</strong> import bigquery</p>&#13;
			<p class="source-code">project_id ='project-xxxxx'</p>&#13;
			<p class="source-code">client = bigquery.Client(project=project_id)</p>&#13;
			<p class="source-code">sample_count = 1000</p>&#13;
			<p class="source-code">row_count = client.query('''</p>&#13;
			<p class="source-code">  SELECT </p>&#13;
			<p class="source-code">    COUNT(*) as total</p>&#13;
			<p class="source-code">  FROM `bigquery-public-data.covid19_jhu_csse.summary`''').to_dataframe().total[0]</p>&#13;
			<p class="source-code">df = client.query('''</p>&#13;
			<p class="source-code">  SELECT</p>&#13;
			<p class="source-code">    *</p>&#13;
			<p class="source-code">  FROM</p>&#13;
			<p class="source-code">    `bigquery-public-data.covid19_jhu_csse.summary`</p>&#13;
			<p class="source-code">  WHERE RAND() &lt; %d/%d</p>&#13;
			<p class="source-code">''' % (sample_count, row_count)).to_dataframe()</p>&#13;
			<p class="source-code">print('Full dataset has %d rows' % row_count)</p>&#13;
			<p>The <a id="_idIndexMarker041"/>output of the preceding code is <a id="_idIndexMarker042"/>as follows:</p>&#13;
			<div>&#13;
				<div id="_idContainer030" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.24.jpg" alt="Figure 1.24 – Code output&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure <a id="_idTextAnchor047"/>1.24 – Code output</p>&#13;
			<p>Let's take a closer look at the preceding code:</p>&#13;
			<ul>&#13;
				<li>An import of the BigQuery library is required to create a BigQuery client.</li>&#13;
				<li>The project ID is required for using this API to create a BigQuery client.</li>&#13;
				<li>This client wraps a SQL statement and executes it.</li>&#13;
				<li>The returned data can be easily converted to a pandas DataFrame right away.</li>&#13;
			</ul>&#13;
			<p>The pandas DataFrame rendition of the BigQuery table has the following columns:</p>&#13;
			<div>&#13;
				<div id="_idContainer031" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.25.jpg" alt="Figure 1.25 – The pandas DataFrame rendition of the BigQuery table&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.25 – The pandas DataFrame rendition of the BigQuery table</p>&#13;
			<p>This is ready for further consumption. It is now a pandas DataFrame that occupies memory space in your Python runtime. </p>&#13;
			<p>This method is very straightforward, as it can help you explore the data schema and do simple aggregation <a id="_idIndexMarker043"/>and filtering, and since it is basically a SQL statement wrapper, it is very easy to just get the data out of the warehouse and <a id="_idIndexMarker044"/>start using it. You didn't have to know much about the table schema to do this.</p>&#13;
			<p>However, the problem with this approach is when the table is big enough to overflow your memory. TensorFlow I/O can help <a id="_idTextAnchor048"/>solve this problem.</p>&#13;
			<h3>TensorFlow I/O </h3>&#13;
			<p>For TensorFlow consumption of BigQuery data, it is better if we use TensorFlow I/O to invoke the BigQuery API. This is <a id="_idIndexMarker045"/>because TensorFlow I/O will <a id="_idIndexMarker046"/>provide us with a dataset object that represents the query results, rather than the entire results, as in the previous method. A dataset object is the means to stream training data for a model during training. Therefore not all training data has to be in memory at once. This complements mini-batch training, which is arguably the most common implementation of gradient descent optimization used in deep learning. However, this is a bit more complicated than the previous method. It requires you to know the schema of the table. This example uses a public dataset hos<a id="_idTextAnchor049"/>ted by Google Cloud. </p>&#13;
			<p>We need to start with the columns of our interest from the table. We can use the previous method to examine the column names and datatypes, and create a session definition:</p>&#13;
			<ol>&#13;
				<li value="1">Load the required libraries and set up the variables as follows:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow_io.bigquery import BigQueryClient</p><p class="source-code">PROJECT_ID = 'project-xxxxx' # This is from what you created in your Google Cloud Account. </p><p class="source-code">DATASET_GCP_PROJECT_ID = 'bigquery-public-data'</p><p class="source-code">DATASET_ID = 'covid19_jhu_csse'</p><p class="source-code">TABLE_ID = 'summary'</p></li>&#13;
				<li>Instantiate a BigQuery client and specify the batch size:<p class="source-code">batch_size = 2048</p><p class="source-code">client = BigQueryClient()</p></li>&#13;
				<li>Use the client to <a id="_idIndexMarker047"/>create a read session and specify the columns and datatypes of interest. Notice that when using the BigQuery client, you <a id="_idIndexMarker048"/>need to know the correct column names and their respective datatypes:<p class="source-code">read_session = client.read_session(</p><p class="source-code">    'projects/' + PROJECT_ID,</p><p class="source-code">    DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,</p><p class="source-code">    ['province_state',</p><p class="source-code">       'country_region',</p><p class="source-code">       'confirmed',</p><p class="source-code">       'deaths',</p><p class="source-code">       'date',</p><p class="source-code">       'recovered'</p><p class="source-code">       ],</p><p class="source-code">    [tf.string,</p><p class="source-code">       tf.string,</p><p class="source-code">       tf.int64,</p><p class="source-code">       tf.int64,</p><p class="source-code">       tf.int32,</p><p class="source-code">       tf.int64],</p><p class="source-code">      requested_streams=10</p><p class="source-code">)</p></li>&#13;
				<li>Now we can use the session object created to execute a read operation:<p class="source-code">dataset = read_session.parallel_read_rows(sloppy=True).batch(batch_size)</p></li>&#13;
				<li>Let's take a <a id="_idIndexMarker049"/>look at the dataset <a id="_idIndexMarker050"/>with <strong class="source-inline">type()</strong>:<p class="source-code">type(dataset)</p><p>Here's the output:</p><div id="_idContainer032" class="IMG---Figure"><img src="Images/Figure_1.26.jpg" alt="Figure 1.26 – Output&#13;&#10;"/></div><p class="figure-caption">Figure 1.26 – Output</p></li>&#13;
				<li>In order to actually see the data, we need to convert the dataset ops to a Python iterator and use <strong class="source-inline">next()</strong> to see the content of the first batch:<p class="source-code">itr = tf.compat.v1.data.make_one_shot_iterator(</p><p class="source-code">    dataset</p><p class="source-code">)</p><p class="source-code"> next(itr)</p></li>&#13;
			</ol>&#13;
			<p>The output of the preceding command shows it is organized as an ordered dictionary, where the keys are column names and the values are Tensors:</p>&#13;
			<div>&#13;
				<div id="_idContainer033" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.27.jpg" alt="Figure 1.27 – Raw data as an iterator&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.27 – <a id="_idTextAnchor050"/>Raw data as an iterator</p>&#13;
			<p>Here are the key takeaways:</p>&#13;
			<ul>&#13;
				<li>TensorFlow I/O's BigQuery Client requires setting up a read session, which consists of column names from your table of interest.</li>&#13;
				<li>This client then executes a read operation that also includes data batching.</li>&#13;
				<li>The output of the <a id="_idIndexMarker051"/>read operation is a TensorFlow <a id="_idIndexMarker052"/>ops.</li>&#13;
				<li>This ops may be converted to a Python iterator, so it can output the actual data read by the ops.</li>&#13;
				<li>This improves the efficiency of memory use during training, as data is sent <a id="_idTextAnchor051"/>for training in batches. </li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-27"><a id="_idTextAnchor052"/>Persisting data in BigQuery </h2>&#13;
			<p>We have looked at how to read data stored in Google Storage solutions, such as Cloud Storage buckets <a id="_idIndexMarker053"/>or a BigQuery data warehouse, and how to <a id="_idIndexMarker054"/>enable the data for consumption by AI Platform's TensorFlow Enterprise instance running in JupyterLab. Now let's take a look at some ways to write data back, or persist our working data, into our cloud Storage.</p>&#13;
			<p>Our first example concerns writing a file stored in JupyterLab runtime's directory (in some TensorFlow Enterprise <a id="_idIndexMarker055"/>documentations, this is also referred to as a <em class="italic">local</em> file). The process in general is as follows:</p>&#13;
			<ol>&#13;
				<li value="1">For convenience, execute a BigQuery SQL <strong class="source-inline">read</strong> command on a table from a public dataset.</li>&#13;
				<li>Store the result locally as a <strong class="bold">comma-separated file</strong> (<strong class="bold">CSV</strong>).</li>&#13;
				<li>Write the CSV file to a tab<a id="_idTextAnchor053"/>le in our BigQuery dataset. </li>&#13;
			</ol>&#13;
			<p>Each step is a code cell. The following step-by-step code snippet applies to JupyterLab in any of the three AI platforms (AI Notebook, AI Deep Learning VM, and Deep Learning Container):</p>&#13;
			<ol>&#13;
				<li value="1">Designate a project ID:<p class="source-code">project_id = 'project1-190517'</p></li>&#13;
				<li>Execute the BigQuery SQL command and assign the result to a pandas DataFrame:<p class="source-code">%%bigquery --project $project_id mydataframe</p><p class="source-code">SELECT <strong class="source-inline">*</strong> from <strong class="source-inline">`bigquery-public-data.covid19_jhu_csse.summary`</strong></p><p>The BigQuery <a id="_idIndexMarker056"/>results come back as a pandas DataFrame by default. In this <a id="_idIndexMarker057"/>case, we designate the DataFrame name to be <strong class="source-inline">mydataframe</strong>.</p></li>&#13;
				<li>Write the pandas DataFrame to a CSV file in a local directory. In this case, we used the <strong class="source-inline">/home</strong> directory of this JupyterLab runtime:<p class="source-code">import pandas as pd</p><p class="source-code">mydataframe.to_csv('my_new_data.csv')</p></li>&#13;
				<li>Designate a dataset name:<p class="source-code">dataset_id = 'my_new_dataset'</p></li>&#13;
				<li>Use the BigQuery command-line tool to create an empty table in this project's dataset. This command starts with <strong class="source-inline">!bq</strong>:<p class="source-code"><strong class="bold">!bq --location=US mk --dataset $dataset_id</strong></p><p>This command creates a new dataset. This dataset doesn't have any tables yet. We are going to write a new table into this dataset in the next step.</p></li>&#13;
				<li>Write the local CSV file to a new table:<p class="source-code"><strong class="bold">!bq \</strong></p><p class="source-code"><strong class="bold">    --location=US \</strong></p><p class="source-code"><strong class="bold">    load \</strong></p><p class="source-code"><strong class="bold">    --autodetect \</strong></p><p class="source-code"><strong class="bold">    --skip_leading_rows=1 \</strong></p><p class="source-code"><strong class="bold">    --source_format=CSV \</strong></p><p class="source-code"><strong class="bold">    {dataset_id}.my_new_data_table \</strong></p><p class="source-code"><strong class="bold">    'my_new_data.csv'</strong></p><p>In this <a id="_idIndexMarker058"/>command, since the CSV file is stored in the current directory, its <a id="_idIndexMarker059"/>filename of <strong class="source-inline">'my_new_data.csv'</strong> will suffice. Otherwise, a full path is required. Also, <strong class="source-inline">{dataset_id}.my_new_data_table</strong> indicates that we want to write the CSV file into this particular dataset and the table name.</p></li>&#13;
				<li>Now you can navigate to the BigQuery portal, and you will find the dataset and the table:<div id="_idContainer034" class="IMG---Figure"><img src="Images/Figure_1.28.jpg" alt="Figure 1.28 – The BigQuery portal and navigation to the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 1.28 – The BigQuery portal and navigation to the dataset</p><p>In <a id="_idIndexMarker060"/>this case, we <a id="_idIndexMarker061"/>have one dataset, which contains one table.</p></li>&#13;
				<li>Then, execute a simple query, as follows:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer035" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.29.jpg" alt="Figure 1.29 – A query for examining the table&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.29 – A query for examining the table</p>&#13;
			<p>This is a very simple query where we just want to show 1,000 randomly selected rows. You can now execute this query and the output will be as shown in the following screenshot. </p>&#13;
			<p>The following <a id="_idIndexMarker062"/>query output shows <a id="_idIndexMarker063"/>the data from the BigQuery table we just created:</p>&#13;
			<div>&#13;
				<div id="_idContainer036" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.30.jpg" alt="Figure 1.30 – Example table output&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">F<a id="_idTextAnchor054"/>igure 1.30 – Example table output</p>&#13;
			<p>Here are the key takeaways:</p>&#13;
			<ul>&#13;
				<li>Data generated during the TensorFlow workflow in the AI Platform's JupyterLab runtime can be seamlessly persisted as a table in BigQuery.</li>&#13;
				<li>Persisting data in a structured format, such as a pandas DataFrame or a CSV file, in BigQuery can easily be done using the BigQuery command-line tool.</li>&#13;
				<li>When you need to move a data object (such as a table) between the JupyterLab runtime and BigQuery, use the BigQuery command-line tool <a id="_idTextAnchor055"/>with <strong class="source-inline">!bq</strong> to save time and effort.</li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-28"><a id="_idTextAnchor056"/>Persisting data in a storage bucket</h2>&#13;
			<p>In the <a id="_idIndexMarker064"/>previous <em class="italic">Persisting data in BigQuery</em> section, we saw how a structured data source such as a CSV file or a pandas DataFrame can be persisted in a BigQuery dataset as a table. In this <a id="_idIndexMarker065"/>section, we are going to see how to persist working data such as a NumPy array. In this case, the suitable target storage is a Google Cloud Storage bucket. </p>&#13;
			<p>The workflow for this demonstration is as follows:</p>&#13;
			<ol>&#13;
				<li value="1">For convenience, read a NumPy array from <strong class="source-inline">tf.keras.dataset</strong>.</li>&#13;
				<li>Save the NumPy array as a pickle (<strong class="source-inline">pkl</strong>) file. (FYI: The pickle file format, while convenient and easy to use for serializing Python objects, also has its downsides. For one, it may be slow and creates a larger object than the original. Second, a pickle file may contain bugs or security risks for any process that opens it. It is used only for convenience here.)</li>&#13;
				<li>Use the <strong class="source-inline">!gsutil</strong> storage command-line tool to transfer files from JupyterLab's <strong class="source-inline">/home</strong> directory (in some documentation, this is referred <a id="_idIndexMarker066"/>to as the <em class="italic">local directory</em>) to the storage bucket.</li>&#13;
				<li>Use <strong class="source-inline">!gsutil</strong> to transfer the content in the bu<a id="_idTextAnchor057"/>cket back to the JupyterLab runtime. Since we will use Python with <strong class="source-inline">!gsutil</strong>, we need to keep the content in separate cells. </li>&#13;
			</ol>&#13;
			<p>Follow these steps to complete the workflow:</p>&#13;
			<ol>&#13;
				<li value="1">Let's use the IMDB dataset because it is already provided in NumPy format:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pickle as pkl</p><p class="source-code">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(</p><p class="source-code">    path='imdb.npz',</p><p class="source-code">    num_words=None,</p><p class="source-code">    skip_top=0,</p><p class="source-code">    maxlen=None,</p><p class="source-code">    seed=113,</p><p class="source-code">    start_char=1,</p><p class="source-code">    oov_char=2,</p><p class="source-code">    index_from=3</p><p class="source-code">)</p><p class="source-code">with open('/home/jupyter/x_train.pkl','wb') as f:</p><p class="source-code">    pkl.dump(x_train, f)</p><p><strong class="source-inline">x_train</strong>, <strong class="source-inline">y_train</strong>, <strong class="source-inline">x_test</strong>, and <strong class="source-inline">y_test</strong> are returned as NumPy arrays. Let's use <strong class="source-inline">x_train</strong> for the purposes of this demonstration. The <strong class="source-inline">x_train</strong> array is going to be saved as a <strong class="source-inline">pkl</strong> file in the JupyterLab runtime.</p><p>The <a id="_idIndexMarker067"/>preceding code opens the IMDB movie review dataset that is distributed as a part of TensorFlow. This <a id="_idIndexMarker068"/>dataset is formatted as tuples of NumPy arrays and separated as training and test partitions. Then we proceed to save the <strong class="source-inline">x_train</strong> array as a pickle file in the runtime's <strong class="source-inline">/home</strong> directory. This pickle file will then be persisted in a storage bucket in the next step.</p></li>&#13;
				<li>Designate a name for the new storage bucket:<p class="source-code">bucket_name = 'ai-platform-bucket'</p></li>&#13;
				<li>Create a new bucket with the designated name:<p class="source-code">!gsutil mb gs://{bucket_name}/</p><p>Use <strong class="source-inline">!gsutil</strong> to move the <strong class="source-inline">pkl</strong> file from the runtime to the storage bucket:</p><p class="source-code">!gsutil cp /home/jupyter/x_train.pkl gs://{bucket_name}/</p></li>&#13;
				<li>Read the <strong class="source-inline">pkl</strong> file back:<p class="source-code">!gsutil cp gs://{bucket_name}/x_train.pkl /home/jupyter/x_train_readback.pkl</p></li>&#13;
				<li>Now let's inspect the Cloud Storage bucket: </li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer037" class="IMG---Figure">&#13;
					<img src="Images/Figure_1.31.jpg" alt="Figure 1.31 – Serializing an object in a bucket from the workflow in AI Platform&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 1.31 – Serializing an object in a bucket<a id="_idTextAnchor058"/> from the workflow in AI Platform</p>&#13;
			<p>Here <a id="_idIndexMarker069"/>are the key <a id="_idIndexMarker070"/>takeaways:</p>&#13;
			<ul>&#13;
				<li>Working data generated during the TensorFlow workflow can be persisted as a serialized object in the storage bucket.</li>&#13;
				<li>Google AI Platform's JupyterLab environment provides seamless integration between the TensorFlow runtime and the Cloud Storage command-line tool, <strong class="source-inline">gsutil</strong>.</li>&#13;
				<li>When you need to transfer content between Google Cloud Storage and AI Platf<a id="_idTextAnchor059"/>orm, use the <strong class="source-inline">!gsutil</strong> command-line tool.</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-29"><a id="_idTextAnchor060"/>Summary</h1>&#13;
			<p>This chapter provided a broad overview of the TensorFlow Enterprise environment hosted by Google Cloud AI Platform. We also saw how this platform seamlessly integrates specific tools such as command-line APIs to facilitate the easy transfer of data or objects between the JupyterLab environment and our storage solutions. These tools make it easy to access data stored in BigQuery or in storage buckets, which are the two most commonly used data sources in TensorFlow. </p>&#13;
			<p>In the next chapter, we will take a closer look at the three ways available in AI Platform to use TensorFlow Enterprise: the Notebook, Deep Learning VM, and Deep Learning Containers.</p>&#13;
		</div>&#13;
	</div></body></html>