<html><head></head><body>
  <div id="_idContainer166">
    <h1 class="chapterNumber">7</h1>
    <h1 id="_idParaDest-114" class="chapterTitle">Multi-Modal Networks and Image Captioning with ResNets and Transformer Networks</h1>
    <p class="normal">"A picture is worth a thousand words" is a famous adage. In this chapter, we'll put this adage to the test and generate captions for an image. In doing so, we'll work with <strong class="keyword">multi-modal</strong> networks. Thus far, we have operated on text as input. Humans can handle multiple sensory inputs together to make sense of the environment around them. We can watch a video with subtitles and combine the information provided to understand the scene. We can use facial expressions and lip movement along with sounds to understand speech. We can recognize text in an image, and we can answer natural language questions about images. In other words, we have the ability to process information from different modalities at the same time, and then put them together to understand the world around us. The future of artificial intelligence and deep learning is in building multi-modal networks as they closely mimic human cognitive functions. </p>
    <p class="normal">Recent advances in image, speech, and text processing lay a solid foundation for multi-modal networks. This chapter transitions you from the world of NLP to the world of multi-modal learning, where we will combine visual and textual features using the familiar Transformer architecture. </p>
    <p class="normal">We will cover the following topics in this chapter:</p>
    <ul>
      <li class="bullet">Overview of multi-modal deep learning</li>
      <li class="bullet">Vision and language tasks</li>
      <li class="bullet">Detailed overview of the Image Captioning task and the MS-COCO dataset</li>
      <li class="bullet">Architecture of a residual network, specifically ResNet</li>
      <li class="bullet">Extracting features from images using pre-trained ResNet50</li>
      <li class="bullet">Building a full Transformer model from scratch</li>
      <li class="bullet">Ideas for improving the performance of image captioning</li>
    </ul>
    <p class="normal">Our journey starts with an overview of the various tasks in the visual understanding domain, with a focus on tasks that combine language and images.</p>
    <h1 id="_idParaDest-115" class="title">Multi-modal deep learning</h1>
    <p class="normal">The dictionary<a id="_idIndexMarker480"/> definition of "modality" states that it is "a particular mode in which something exists or is experienced or expressed." Sensory modalities, like touch, taste, smell, vision, and sound, allow humans to experience the world around them. Suppose you are out at the farm picking strawberries, and your friend tells you to pick ripe and red strawberries. The instruction, <em class="italic">ripe and red strawberries</em>, is processed and converted into a visual and haptic criterion. As you see strawberries and feel them, you know instinctively if they match the criteria of <em class="italic">ripe and red</em>. This task is an example of multiple modalities working together for a task. As you can imagine, these capabilities are essential for robotics.</p>
    <p class="normal">As a direct application of the preceding example, consider a harvesting robot that needs to pick ripe and ready fruit. In December 1976, Harry McGurk and John MacDonald published a piece of research titled <em class="italic">Hearing lips and seeing voices</em> (<a href="https://www.nature.com/articles/264746a0"><span class="url">https://www.nature.com/articles/264746a0</span></a>) in the reputed journal, Nature. They recorded a video of a young woman talking, where utterances of the syllable <em class="italic">ba</em> had been dubbed onto the lip movement of the syllable <em class="italic">ga</em>. When this video was played back to adults, people repeated hearing the syllable <em class="italic">da</em>. When the audio track was played without the video, the right syllable was reported. This research paper highlighted the role of vision in speech recognition. Speech recognition models using lip-reading information were<a id="_idIndexMarker481"/> developed in the field of <strong class="keyword">Audio-Visual Speech Recognition</strong> (<strong class="keyword">AVSR</strong>). There are several exciting applications of multi-modal deep learning models in medical devices and diagnosis, learning technology, and other <strong class="keyword">Artificial Intelligence</strong> (<strong class="keyword">AI</strong>) areas.</p>
    <p class="normal">Let's drill down into the specific interaction of vision and language and the various tasks we can perform.</p>
    <h2 id="_idParaDest-116" class="title">Vision and language tasks</h2>
    <p class="normal">A combination of <strong class="keyword">Computer Vision</strong> (<strong class="keyword">CV</strong>) and <strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>) allows us to build smart AI systems that <a id="_idIndexMarker482"/>can see and talk. CV and NLP together <a id="_idIndexMarker483"/>produce interesting tasks for model <a id="_idIndexMarker484"/>development. Taking an image and generating a caption for it is a well-known task. A practical application of this task is generating alt-text tags for images on web pages. Visually impaired readers use screen readers, which can read these tags while reading the page, improving the accessibility of web pages. Other topics in this area include video captioning and storytelling – composing a story from a sequence of images. The<a id="_idIndexMarker485"/> following image shows some examples of images and <a id="_idIndexMarker486"/>captions. Our primary focus in this chapter is on image captioning:</p>
    <figure class="mediaobject"><img src="image/B16252_07_01.png" alt="A picture containing photo, room, bunch, many  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.1: Example images with captions</p>
    <p class="normal"><strong class="keyword">Visual Question Answering</strong> (<strong class="keyword">VQA</strong>) is the <a id="_idIndexMarker487"/>challenging task of answering questions about objects in the image. The following image shows some examples from the VQA dataset. Compared to image captioning, where prominent objects are reflected in the caption, VQA is a more complex task. Answering the question may also<a id="_idIndexMarker488"/> require some reasoning. </p>
    <p class="normal">Consider the bottom-right panel in the following image. Answering the question, "Does this person have 20/20 vision?" requires reasoning. Datasets for VQA are available at <a href="http://visualqa.org"><span class="url">visualqa.org</span></a>:</p>
    <figure class="mediaobject"><img src="image/B16252_07_02.png" alt="A person posing for a photo  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.2: Examples from the VQA Dataset (Source: VQA: Visual Question Answering by Agrawal et al.)</p>
    <p class="normal">Reasoning leads to another<a id="_idIndexMarker489"/> challenging but fascinating task – <strong class="keyword">Visual Commonsense Reasoning</strong> (<strong class="keyword">VCR</strong>). When we look at an image, we can guess emotions, actions, and frame a hypothesis of what is<a id="_idIndexMarker490"/> happening. Such a task is quite easy for <a id="_idIndexMarker491"/>people and may even happen without conscious effort. The aim of the VCR task is to build models that can perform such a task. These models should also be able to explain or choose an appropriate reason for the logical inference that's been <a id="_idIndexMarker492"/>made. The following image shows an example from the VCR dataset. More details on the VCR dataset can be found at <a href="http://visualcommonsense.com"><span class="url">visualcommonsense.com</span></a>:</p>
    <figure class="mediaobject"><img src="image/B16252_07_03.png" alt="A screenshot of a social media post  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.3: VCR example (Source: From Recognition to Cognition: Visual Commonsense Reasoning by Zellers et al.)</p>
    <p class="normal">Thus far, we have gone from images to text. The reverse is also possible and is an active area of research. In this task, images or videos are generated from text using GANs and other generative <a id="_idIndexMarker493"/>architectures. Imagine being able to generate an illustrative <a id="_idIndexMarker494"/>comic book from the text of a story! This particular task is at the forefront of research currently.</p>
    <p class="normal">A critical concept in this area is <strong class="keyword">visual grounding</strong>. Grounding enables tying concepts in language to the real world. Simply put, it matches words to <a id="_idIndexMarker495"/>objects in a picture. By combining vision and language, we can ground concepts from languages to parts of an image. For example, mapping the word "basketball" to something that looks like one in an image is called visual grounding. There can be more abstract concepts that can be grounded. For example, a short elephant and a short person have different measurements. Grounding provides us with a way to see what models are learning and helps us guide them in the right direction.</p>
    <p class="normal">Now that we have a proper perspective on vision and language tasks, let's dive deep into an image captioning task.</p>
    <h1 id="_idParaDest-117" class="title">Image captioning</h1>
    <p class="normal">Image captioning is all about describing the contents of an image in a sentence. Captions can help in content-based<a id="_idIndexMarker496"/> image retrieval and visual search. We already discussed how captions could improve the accessibility of websites by making it easier for screen readers to summarize the content of an image. A caption can be considered a summary of the image. Once we frame the problem as an image summarization problem, we can adapt the seq2seq model from the previous chapter to solve this problem. In text summarization, the input is a sequence of the long-form article, and the output is a short sequence summarizing the content. In image captioning, the output is similar in format to summarization. However, it may not be obvious how to structure an image that consists of pixels as a sequence of embeddings to be fed into the Encoder.</p>
    <p class="normal">Secondly, the summarization architecture used <strong class="keyword">Bi-directional Long Short-Term Memory networks</strong> (<strong class="keyword">BiLSTMs</strong>), with the underlying principle that words that are closer together to each other are similar to each other in meaning. BiLSTMs exploited this property by looking at the input sequence from both sides and generated encoded representations. Generating a representation for an image that works for the Encoder requires some thought.</p>
    <p class="normal">A naïve solution for representing images as a sequence could be expressing them as a list of pixels. So, an image of size 28x28 pixels becomes a sequence of 784 tokens. When the tokens represent text, an Embedding layer learns the representation of each token. If this Embedding layer had a dimension of 64, then each token would be represented by a 64-dimensional vector. This embedding vector was learned during training. Extending our analogy of using a pixel as a token, a straightforward solution is to use the value of the Red/Green/Blue channels of the pixel in an image to generate a three-dimensional embedding. However, training these three dimensions does not sound like a logical approach. More importantly, pixels are laid out in a 2D representation, while the text is laid out in a 1D representation. This concept is illustrated in the following image. Words are related to words next to each other. When pixels are laid out in a<a id="_idIndexMarker497"/> sequence, the <strong class="keyword">data locality</strong> of these pixels is broken since the content of a pixel is related to the pixels all around it, not just to the left and right of it. This idea is shown by the following super zoomed in image of a tulip:</p>
    <figure class="mediaobject"><img src="image/B16252_07_04.png" alt=""/> </figure>
    <p class="packt_figref">Figure 7.4: Data locality in text versus images</p>
    <p class="normal">Data locality and translation invariance are two critical properties of images. Translation invariance is the idea that an object<a id="_idIndexMarker498"/> can appear in various spots in an image. In a fully connected model, the model would try to learn the position of the object, which would prevent the model from generalizing. The specialized architecture of <strong class="keyword">Convolutional Neural Networks</strong> (<strong class="keyword">CNNs</strong>) can be used to exploit these properties and extract signals from the image. At a high level, we use CNNs, specifically the <strong class="keyword">ResNet50</strong> architecture, to convert the image into a tensor that can be fed to a seq2seq architecture. Our model will combine the best of CNNs and RNNs to handle the image and text parts under the seq2seq model. The following diagram shows our architecture at a very high level:</p>
    <figure class="mediaobject"><img src="image/B16252_07_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 7.5: High-level image captioning model architecture</p>
    <p class="normal">While a comprehensive explanation of CNNs is beyond the scope of this book, we will review the key concepts in short. Since we will be using a pre-trained CNN model, we won't have to go into much depth about CNNs. <em class="italic">Python Machine Learning, Third Edition</em>, published by Packt, is an excellent resource for reading up on CNNs. </p>
    <p class="normal">In the previous chapter on text summarization, we built a seq2seq model with attention. In this chapter, we will build a Transformer model. Transformer models are currently state of the art in NLP. The Encoder part of the Transformer is the core of the <strong class="keyword">Bidirectional Encoder Representations from Transformers</strong> (<strong class="keyword">BERT</strong>) architecture. The Decoder part of the Transformer is the core of the <strong class="keyword">Generative Pre-trained Transformer</strong> (<strong class="keyword">GPT</strong>) family of architectures. There is a specific<a id="_idIndexMarker499"/> advantage of the Transformer architecture that is relevant to the image captioning problem. In the seq2seq architecture, we used BiLSTMS, which tries to learn relationships via co-occurrence. In the Transformer architecture, there is no recurrence. Instead, positional encodings and self-attention model relationships are made between inputs. This change enables us to feed in processed image patches as input and hope that the relationships between the image patches will be learned.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Implementing the image captioning model requires a large amount of code as we will implement several pieces, like pre-processing images, with ResNet50 and a complete implementation of Transformer architecture from scratch. This chapter contains much more code than the other chapters. We will rely on code fragments to highlight the most important aspects of the code rather than going over every line of code in detail, as we have been doing so far.</p>
    </div>
    <p class="normal">The main steps of building our model are <a id="_idIndexMarker500"/>summarized here:</p>
    <ol>
      <li class="numbered"><strong class="keyword">Downloading the data</strong>: Given the large size of the dataset, this is a time-consuming activity.</li>
      <li class="numbered"><strong class="keyword">Pre-processing captions</strong>: Since the captions are in JSON format, they are flattened into a CSV for easier processing.</li>
      <li class="numbered"><strong class="keyword">Feature extraction</strong>: We pass the image files through ResNet50 to extract features and save them to speed up training.</li>
      <li class="numbered"><strong class="keyword">Transformer training</strong>: A full Transformer model with positional encoding, multi-head attention, an Encoder, and a Decoder is trained on the processed data.</li>
      <li class="numbered"><strong class="keyword">Inference</strong>: Use the trained model to caption some images!</li>
      <li class="numbered"><strong class="keyword">Evaluating performance</strong>: <strong class="keyword">Bilingual Evaluation Understudy</strong> (<strong class="keyword">BLEU</strong>) scores are used to compare the trained models with ground truth data.</li>
    </ol>
    <p class="normal">Let's start with the dataset first.</p>
    <h1 id="_idParaDest-118" class="title">MS-COCO dataset for image captioning</h1>
    <p class="normal">Microsoft published the <strong class="keyword">Common Objects in Context</strong> or <strong class="keyword">COCO</strong> dataset in 2014. All the versions<a id="_idIndexMarker501"/> of the dataset can be found at <a href="http://cocodataset.org"><span class="url">cocodataset.org</span></a>. The COCO dataset is a big dataset that's used for object detection, segmentation, and captioning, among other annotations. Our focus will be <a id="_idIndexMarker502"/>on the 2014 training and validation images, where five captions per image are available. There are roughly 83K images in the training set and 41K images in the<a id="_idIndexMarker503"/> validation set. The training and validation images and captions need to be downloaded from the COCO website.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">Large download warning</strong>: The training image dataset is approximately 13 GB, while the validation dataset is over 6 GB. The annotations for the image files, which include captions, are about 214 MB in size. Please be careful of your internet bandwidth usage and potential costs as you download this dataset.</p>
      <p class="Information-Box--PACKT-">Google has also published a new Conceptual Captions dataset at <a href="https://ai.google.com/research/ConceptualCaptions"><span class="url">https://ai.google.com/research/ConceptualCaptions</span></a>. It contains over 3M images. Having a large dataset allows deep models to train better. There is a corresponding competition where you can submit your models and see how they compete with others.</p>
    </div>
    <p class="normal">Given that these are large downloads, you may wish to use the download that's the most comfortable to you. If <code class="Code-In-Text--PACKT-">wget</code> is available on your environment, you could use it to download the files, like so:</p>
    <pre class="programlisting con"><code class="hljs-con">$ wget http://images.cocodataset.org/zips/train2014.zip
$ wget http://images.cocodataset.org/zips/val2014.zip
$ wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip
</code></pre>
    <p class="normal">Note that the annotations for the training and validation sets are in one compressed archive. Once the files have been downloaded, they need to be unzipped. Each of these compressed files creates its own folder and puts the contents in there. We will create a folder called <code class="Code-In-Text--PACKT-">data</code> and move all the expanded contents inside it:</p>
    <pre class="programlisting con"><code class="hljs-con">$ mkdir data
$ mv train2014 data/
$ mv val2014 data/
$ mv annotations data/
</code></pre>
    <p class="normal">All the images are either in the <code class="Code-In-Text--PACKT-">train2014</code> or <code class="Code-In-Text--PACKT-">val2014</code> folder. The code for the initial pre-processing of the data is in the <code class="Code-In-Text--PACKT-">data-download-preprocess.py</code> file. Captions for the training and validation<a id="_idIndexMarker504"/> images can be found in the <code class="Code-In-Text--PACKT-">captions_train2014.json</code> or <code class="Code-In-Text--PACKT-">captions_val2014.json</code> JSON file inside the <code class="Code-In-Text--PACKT-">annotations</code> subfolder. Both of these files are in a similar format. The files have four main keys – info, image, license, and annotation. The image key contains a record per image, along with<a id="_idIndexMarker505"/> information about the size, URL, name, and a unique ID that is used to refer to that image in the dataset. Captions are stored as a tuple of the image ID and caption text, along with a unique ID for the caption. We use the Python <code class="Code-In-Text--PACKT-">json</code> module to read and process these files:</p>
    <pre class="programlisting code"><code class="hljs-code">valcaptions = json.load(<span class="hljs-built_in">open</span>(
    <span class="hljs-string">'./data/annotations/captions_val2014.json'</span>, <span class="hljs-string">'r'</span>))
trcaptions = json.load(<span class="hljs-built_in">open</span>(
    <span class="hljs-string">'./data/annotations/captions_train2014.json'</span>, <span class="hljs-string">'r'</span>))
<span class="hljs-comment"># inspect the annotations</span>
print(trcaptions.keys())
dict_keys([<span class="hljs-string">'info'</span>, <span class="hljs-string">'images'</span>, <span class="hljs-string">'licenses'</span>, <span class="hljs-string">'annotations'</span>])
</code></pre>
    <p class="normal">Our objective is to produce a single simple file with two columns – one for the image file name and another containing the caption for that file. Note that the validation set contains half the number of images of the training set. In a seminal paper on captioning titled <em class="italic">Deep Visual-Semantic Alignment for Generating Image Descriptions</em>, Andrej Karpathy and Fei-Fei Li proposed training on all the training and validation images after reserving 5,000 images from the validation set for testing. We will follow this approach by processing the image names and IDs into a dictionary:</p>
    <pre class="programlisting code"><code class="hljs-code">prefix = <span class="hljs-string">"./data/"</span>
val_prefix = prefix + <span class="hljs-string">'val2014/'</span>
train_prefix = prefix + <span class="hljs-string">'train2014/'</span>
<span class="hljs-comment"># training images</span>
trimages = {x[<span class="hljs-string">'id'</span>]: x[<span class="hljs-string">'file_name'</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> trcaptions[<span class="hljs-string">'images'</span>]}
<span class="hljs-comment"># validation images</span>
<span class="hljs-comment"># take all images from validation except 5k - karpathy split</span>
valset = <span class="hljs-built_in">len</span>(valcaptions[<span class="hljs-string">'images'</span>]) - <span class="hljs-number">5000</span> <span class="hljs-comment"># leave last 5k </span>
valimages = {x[<span class="hljs-string">'id'</span>]: x[<span class="hljs-string">'file_name'</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> valcaptions[<span class="hljs-string">'images'</span>][:valset]}
truevalimg = {x[<span class="hljs-string">'id'</span>]: x[<span class="hljs-string">'file_name'</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> valcaptions[<span class="hljs-string">'images'</span>][valset:]}
</code></pre>
    <p class="normal">Since each image has five captions, the validation set cannot be split based on captions. Otherwise, there will be<a id="_idIndexMarker506"/> leakage of data from the training set into the <a id="_idIndexMarker507"/>validation/test set. In the preceding code, we reserved the last 5K images for the validation set. </p>
    <p class="normal">Now, let's go over the captions for the training and validation images and create a combined list. We will create empty lists to store the tuples of image paths and captions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># we flatten to (caption, image_path) structure</span>
data = <span class="hljs-built_in">list</span>()
errors = <span class="hljs-built_in">list</span>()
validation = <span class="hljs-built_in">list</span>()
</code></pre>
    <p class="normal">Next, we will process all the training captions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> trcaptions[<span class="hljs-string">'annotations'</span>]:
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">int</span>(item[<span class="hljs-string">'image_id'</span>]) <span class="hljs-keyword">in</span> trimages:
        fpath = train_prefix + trimages[<span class="hljs-built_in">int</span>(item[<span class="hljs-string">'image_id'</span>])]
        caption = item[<span class="hljs-string">'caption'</span>]
        data.append((caption, fpath))
    <span class="hljs-keyword">else</span>:
        errors.append(item)
</code></pre>
    <p class="normal">For the validation captions, the logic is similar, but we need to ensure that no captions are included for the images that have been reserved:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> valcaptions[<span class="hljs-string">'annotations'</span>]:
    caption = item[<span class="hljs-string">'caption'</span>]
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">int</span>(item[<span class="hljs-string">'image_id'</span>]) <span class="hljs-keyword">in</span> valimages:
        fpath = val_prefix + valimages[<span class="hljs-built_in">int</span>(item[<span class="hljs-string">'image_id'</span>])]
        data.append((caption, fpath))
    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">int</span>(item[<span class="hljs-string">'image_id'</span>]) <span class="hljs-keyword">in</span> truevalimg: <span class="hljs-comment"># reserved</span>
        fpath = val_prefix + truevalimg[<span class="hljs-built_in">int</span>(item[<span class="hljs-string">'image_id'</span>])]
        validation.append((caption, fpath))
    <span class="hljs-keyword">else</span>:
        errors.append(item)
</code></pre>
    <p class="normal">Hopefully, there should <a id="_idIndexMarker508"/>not be any errors. If you encounter errors, this could be due to corrupted downloads or errors while unzipping the files. The <a id="_idIndexMarker509"/>training dataset is shuffled to aid in training. Finally, two CSV files are persisted with the training and testing data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># persist for future use</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(prefix + <span class="hljs-string">'data.csv'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> file:
    writer = csv.writer(file, quoting=csv.QUOTE_ALL)
    writer.writerows(data)
<span class="hljs-comment"># persist for future use</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(prefix + <span class="hljs-string">'validation.csv'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> file:
    writer = csv.writer(file, quoting=csv.QUOTE_ALL)
    writer.writerows(validation)
print(<span class="hljs-string">"TRAINING: Total Number of Captions: {},  Total Number of Images: {}"</span>.<span class="hljs-built_in">format</span>(
    <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(trimages) + <span class="hljs-built_in">len</span>(valimages)))
print(<span class="hljs-string">"VALIDATION/TESTING: Total Number of Captions: {},  Total Number of Images: {}"</span>.<span class="hljs-built_in">format</span>(
    <span class="hljs-built_in">len</span>(validation), <span class="hljs-built_in">len</span>(truevalimg)))
print(<span class="hljs-string">"Errors: "</span>, errors)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">TRAINING: Total Number of Captions: 591751,  Total Number of Images: 118287
VALIDATION/TESTING: Total Number of Captions: 25016,  Total Number of Images: 5000
Errors:  []
</code></pre>
    <p class="normal">At this point, the data download and pre-processing phases are complete. The next step is to pre-process all the images using ResNet50 to extract features. Before we write the code for that, we will take <a id="_idIndexMarker510"/>a short detour and look at CNNs <a id="_idIndexMarker511"/>and the ResNet architecture. If you are already comfortable with CNNs, you may skip ahead to the code part.</p>
    <h1 id="_idParaDest-119" class="title">Image processing with CNNs and ResNet50</h1>
    <p class="normal">In the world of <a id="_idIndexMarker512"/>deep learning, specific architectures have been developed to handle specific modalities. CNNs have been incredibly successful in processing images and are the standard architecture for CV tasks. A good mental <a id="_idIndexMarker513"/>model for using a pre-trained model for extracting features from images is that of using pre-trained word embeddings like GloVe for text. In this particular case, we use a <a id="_idIndexMarker514"/>specific architecture called ResNet50. While a<a id="_idIndexMarker515"/> comprehensive treatment of CNNs is outside the scope of this book, a brief overview of CNNs and ResNet will be provided in this section. If you are already comfortable with these concepts, you may skip ahead to the section titled <em class="italic">Image feature extraction with ResNet50</em>.</p>
    <h2 id="_idParaDest-120" class="title">CNNs</h2>
    <p class="normal">CNNs are an architecture <a id="_idIndexMarker516"/>designed to learn from the following key properties, which are relevant to image recognition:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Data locality</strong>: The pixels in an image are highly correlated to the pixels around them.</li>
      <li class="bullet"><strong class="keyword">Translation invariance</strong>: An object of interest, for example, a bird, may appear at different places in an image. The model should be able to identify the object, irrespective of the object's position in the image.</li>
      <li class="bullet"><strong class="keyword">Scale invariance</strong>: An object of interest may have a smaller or large size, depending on the zoom. Ideally, the model should be able to identify objects of interest in an image, irrespective of their size.</li>
    </ul>
    <p class="normal">Convolution and pooling layers are key components that aid CNNs in extracting features from images.</p>
    <h3 id="_idParaDest-121" class="title">Convolutions</h3>
    <p class="normal">A convolution is a mathematical <a id="_idIndexMarker517"/>operation that is performed on patches taken from an image with a filter. A filter is a matrix, usually square and with 3x3, 5x5, and 7x7 as common dimensions. The following image shows an example of a 3x3 convolution matrix applied to a 5x5 image. The image patches are taken from left to right and then top to bottom. The number of pixels this patch shifts by <a id="_idIndexMarker518"/>every step is called the <strong class="keyword">stride length</strong>. A stride length of 1 in a horizontal and vertical direction reduces a 5x5 image to a 3x3 image, as shown here:</p>
    <figure class="mediaobject"><img src="image/B16252_07_06.png" alt="A close up of a green screen  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.6: Example of a convolution operation</p>
    <p class="normal">The specific filter that was applied here is an edge detection filter. Prior to CNNs, CV relied heavily on handcrafted filters. Sobel filters are an example of a special filter for the purpose of edge detection. The <code class="Code-In-Text--PACKT-">convolution-example.ipynb</code> notebook provides an example of detecting edges using the Sobel filter. The code is quite straightforward. After the imports, the image file is loaded and converted into a grayscale image:</p>
    <pre class="programlisting code"><code class="hljs-code">tulip = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">"chap7-tulip.jpg"</span>) 
<span class="hljs-comment"># convert to gray scale image</span>
tulip_grey = tulip.convert(<span class="hljs-string">'L'</span>)
tulip_ar = np.array(tulip_grey)
</code></pre>
    <p class="normal">Next, we define and apply the Sobel filters to the image:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Sobel Filter</span>
kernel_1 = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-1</span>],
                     [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-2</span>],
                     [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-1</span>]])        <span class="hljs-comment"># Vertical edge </span>
kernel_2 = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">-1</span>, <span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>]])      <span class="hljs-comment"># Horizontal edge </span>
out1 = convolve2d(tulip_ar, kernel_1)    <span class="hljs-comment"># vertical filter</span>
out2 = convolve2d(tulip_ar, kernel_2)    <span class="hljs-comment"># horizontal filter</span>
<span class="hljs-comment"># Create a composite image from the two edge detectors</span>
out3 = np.sqrt(out1**<span class="hljs-number">2</span> + out2**<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">The original image, along with the intermediate versions, are shown in the following image:</p>
    <figure class="mediaobject"><img src="image/B16252_07_07.png" alt="A screen shot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.7: Edge detection using Sobel filters</p>
    <p class="normal">Constructing such filters is very tedious. However, CNNs can learn many such filters by treating the filter matrices as learnable parameters. CNNs often pass an image through hundreds or thousands<a id="_idIndexMarker519"/> of such filters, referred to as channels, and stack them together. You can think of each filter as detecting some features, like vertical lines, horizontal lines, arcs, circles, trapezoids, and so on. However, the magic happens when multiple such layers are put together. Stacking multiple layers leads to learning hierarchical representations. An easy way to understand this concept is by imagining that earlier layers are learning simple shapes like lines and arcs, middle layers are learning shapes like circles and hexagons, and the top layers are learning complex objects like stop signs and steering wheels. The convolution operation is the key innovation that exploits data locality and extracts features that enable translation invariance.</p>
    <p class="normal">A consequence of this layering is the amount of data flowing through the model increasing. Pooling is an operation that helps reduce the dimensions of the data flowing through and further highlights these features.</p>
    <h3 id="_idParaDest-122" class="title">Pooling</h3>
    <p class="normal">Once the values from the convolution operation have been computed, a pooling operation can be applied to patches to further concentrate the signal in the image. The most common form of<a id="_idIndexMarker520"/> pooling is called <strong class="keyword">Max pooling</strong> and is demonstrated in the<a id="_idIndexMarker521"/> following diagram. It is as simple as taking the maximum value in a patch. </p>
    <p class="normal">The following diagram shows max pooling on non-overlapping 2x2 patches:</p>
    <figure class="mediaobject"><img src="image/B16252_07_08.png" alt="A close up of a colorful background  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.8: Max pooling operation</p>
    <p class="normal">Another way to pool is by averaging the values. While pooling reduces the complexity and computation load, it also helps modestly with scale invariance. However, there is a chance that such a model overfits and does not generalize well. Dropout is a technique that helps with regularization and enables such models to generalize better.</p>
    <h3 id="_idParaDest-123" class="title">Regularization with dropout</h3>
    <p class="normal">You may recall that we used<a id="_idIndexMarker522"/> dropout settings in previous chapters with the LSTM and BiLSTM settings. The core idea behind dropout is shown in the following diagram:</p>
    <figure class="mediaobject"><img src="image/B16252_07_09.png" alt="A close up of a logo  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.9: Dropout</p>
    <p class="normal">Rather than connecting every unit from a lower layer to every unit in the next higher layer of the model, some of the connections are randomly dropped during training time. Inputs are dropped only during training time. Since dropping inputs reduces the total input reaching a node compared to test/inference time, inputs are upscaled in the proportion of dropout to ensure the relative magnitudes are preserved. Dropping some of the inputs during<a id="_idIndexMarker523"/> training forces the model to learn more from each of the inputs. This is because it cannot rely on the presence of a specific input. This helps the network build resilience to missing inputs and consequently helps generalize the models.</p>
    <p class="normal">A combination of these techniques helped build deeper and deeper networks. A challenge that showed up as networks got deeper was that the signal from the inputs became quite small in the higher layers. Residual connections is a technique that helps deal with this problem.</p>
    <h3 id="_idParaDest-124" class="title">Residual connections and ResNets</h3>
    <p class="normal">Intuition suggests that adding more layers should make performance better. A deeper network has more<a id="_idIndexMarker524"/> model capacity, so it should be able to model more complex distributions compared to shallower networks. As deeper and deeper models were built, a degradation in accuracy was observed. Since the<a id="_idIndexMarker525"/> reduction happened even on the training data, overfitting can be ruled out as a probable cause. As inputs pass through more and more layers, the optimizers have a harder time adjusting the gradients to the point where learning is impaired in the model. Kaiming He and his collaborators published the ResNet architecture in their seminal paper titled <em class="italic">Deep Residual Learning for Image Recognition</em>.</p>
    <p class="normal">We must understand residual connections before understanding ResNets. The core concept of the residual connection is shown in the following diagram. In a regular dense layer, the input is first multiplied by the weights. Then, biases are added in, which is a linear operation. The output is passed through an activation function, like ReLU, which introduces non-linearity in the layer. The output from the activation function is the final output of the layer. </p>
    <p class="normal">However, residual connections introduce a summation in-between the linear computation and the activation function, as shown on the right-hand side of the following diagram:</p>
    <figure class="mediaobject"><img src="image/B16252_07_10.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.10: A conceptual residual connection</p>
    <p class="normal">Note that the preceding diagram is only for illustrating the core concept behind residual connections. In ResNets, the residual connection is made between multiple blocks. The following diagram shows the basic building blocks of ResNet50, also referred to as the bottleneck design. This<a id="_idIndexMarker526"/> design is called the bottleneck design because the 1x1 convolution blocks reduce the dimensions of the <a id="_idIndexMarker527"/>inputs before <a id="_idIndexMarker528"/>passing them to the 3x3 convolution. The last 1x1 block scales the inputs out again for the next layer:</p>
    <figure class="mediaobject"><img src="image/B16252_07_11.png" alt="A close up of a keyboard  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.11: ResNet50 bottleneck building block</p>
    <p class="normal">ResNet50 is composed of several such blocks stacked on top of each other. There are four groups, each consisting of three to<a id="_idIndexMarker529"/> seven such blocks. <strong class="keyword">BatchNorm</strong> or batch normalization was proposed by Sergey Ioffe and Christian Szegedy in their paper titled <em class="italic">Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate Shift</em> in 2015. Batch normalization aims to reduce the variance of the outputs coming from one layer being fed into the next layer. By reducing this variance, BatchNorm acts like L2 regularization, which attempts to do the same thing by adding the penalties of the magnitude of the weights to the cost function. The main motivation of BatchNorm is to efficiently backpropagate gradient updates through a large number of layers, while minimizing the risk that this update could result in divergence. In stochastic gradient descent, gradients are used to update the weights of all the layers at the same time, assuming that the output of one layer doesn't impact any other layers. However, this is not a completely valid assumption. For an <em class="italic">n</em>-layer network, computing this would need <em class="italic">n</em>th order gradients, which is intractable. Instead, batch-norm is used, which works on one mini-batch at a time and the constraints of the updates to reduce this unwanted shift in the distribution of weights. It does this by normalizing the outputs before they are fed into the next layer.</p>
    <p class="normal">The last two layers<a id="_idIndexMarker530"/> of ResNet50 are dense layers that classify the outputs from the last block into an object category. Covering ResNets comprehensively is a tough ask, but hopefully, this crash course on CNNs and ResNets has given you enough background on how they work. You are encouraged to read the referenced<a id="_idIndexMarker531"/> papers and <em class="italic">Deep Learning with TensorFlow 2 and Keras, Second Edition</em>, published by Packt, for a detailed treatment of this topic. Fortunately for us, TensorFlow provides a pre-trained ResNet50 model that is ready for use. In the next section, we'll use this pre-trained ResNet50 model for extracting image features.</p>
    <h1 id="_idParaDest-125" class="title">Image feature extraction with ResNet50</h1>
    <p class="normal">ResNet50 models are trained on the ImageNet dataset. This dataset contains millions of images in over 20,000<a id="_idIndexMarker532"/> categories. The large-scale visual recognition challenge, ILSVRC, focuses on the top 1,000 categories for models to compete on recognizing images. Consequently, the top layers of the ResNet50 that perform <a id="_idIndexMarker533"/>classification have a dimension of 1,000. The idea behind using a pre-trained ResNet50 model is that it is already able to parse out objects that may be useful in image captioning.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">tensorflow.keras.applications</code> package provides pre-trained models like ResNet50. At the time of writing, all the pre-trained models provided are related to CV. Loading up the pre-trained model is quite easy. All the code for this section is in the <code class="Code-In-Text--PACKT-">feature-extraction.py</code> file in this chapter's folder on GitHub. The main reason for using a separate file is that it gives us the ability to run feature extraction as a script. </p>
    <p class="normal">Given that we will be processing over 100,000 images, this process may take a while. CNNs benefit greatly from a GPU in computation. Let's get into the code now. First, we must set up the paths for the CSV file we created from the JSON annotations in the previous chapter:</p>
    <pre class="programlisting code"><code class="hljs-code">prefix = <span class="hljs-string">'./data/'</span>
save_prefix = prefix + <span class="hljs-string">"features/"</span>  <span class="hljs-comment"># for storing prefixes</span>
annot = prefix + <span class="hljs-string">'data.csv'</span>
<span class="hljs-comment"># load the pre-processed file</span>
inputs = pd.read_csv(annot, header=<span class="hljs-literal">None</span>, names=[<span class="hljs-string">"caption"</span>, <span class="hljs-string">"image"</span>])
</code></pre>
    <p class="normal">ResNet50 expects each image to be 224x224 pixels with three channels. The input images from the COCO set have different sizes. Hence, we must convert the input files into the standard that ResNet was trained on:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># We are going to use the last residual block of # the ResNet50 architecture</span>
<span class="hljs-comment"># which has dimension 7x7x2048 and store into individual file</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">load_image</span><span class="hljs-functio">(</span><span class="hljs-params">image_path, size=(</span><span class="hljs-number">224</span><span class="hljs-params">, </span><span class="hljs-number">224</span><span class="hljs-params">)</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># pre-processes images for ResNet50 in batches </span>
    image = tf.io.read_file(image_path)
    image = tf.io.decode_jpeg(image, channels=<span class="hljs-number">3</span>)
    image = tf.image.resize(image, size)
    image = <span class="code-highlight"><strong class="hljs-slc">preprocess_input(image)</strong></span>  <span class="hljs-comment"># from keras.applications.ResNet50</span>
    <span class="hljs-keyword">return</span> image, image_path
</code></pre>
    <p class="normal">The highlighted <a id="_idIndexMarker534"/>code shows a special pre-processing function provided by the ResNet50 package. The pixels in the input image are loaded into an <a id="_idIndexMarker535"/>array via the <code class="Code-In-Text--PACKT-">decode_jpeg()</code> function. Each pixel has a value between 0 and 255 for each color channel. The <code class="Code-In-Text--PACKT-">preprocess_input()</code> function normalizes the pixel values so that their mean is 0. Since each input image has five captions, we should only process the unique images in the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">uniq_images = <span class="hljs-built_in">sorted</span>(inputs[<span class="hljs-string">'image'</span>].unique())  
print(<span class="hljs-string">"Unique images: "</span>, <span class="hljs-built_in">len</span>(uniq_images))  <span class="hljs-comment"># 118,287 images</span>
</code></pre>
    <p class="normal">Next, we must convert the dataset into a <code class="Code-In-Text--PACKT-">tf.dat.Dataset</code>, which makes it easier to batch and process the input files using the convenience function defined previously:</p>
    <pre class="programlisting code"><code class="hljs-code">image_dataset = tf.data.Dataset.from_tensor_slices(uniq_images)
image_dataset = image_dataset.<span class="hljs-built_in">map</span>(
    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(<span class="hljs-number">16</span>)
</code></pre>
    <p class="normal">For efficiently processing and generating features, we must process 16 image files at a time. The next step is loading a pre-trained ResNet50 model:</p>
    <pre class="programlisting code"><code class="hljs-code">rs50 = tf.keras.applications.ResNet50(
    include_top=<span class="hljs-literal">False</span>,
    weights=<span class="hljs-string">"imagenet"</span>, 
    input_shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>)
)
new_input = rs50.<span class="hljs-built_in">input</span>
hidden_layer = rs50.layers[<span class="hljs-number">-1</span>].output
features_extract = tf.keras.Model(new_input, hidden_layer)
features_extract.summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">__________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________
&lt;CONV BLOCK 1&gt;
__________________________________________________________________
&lt;CONV BLOCK 2&gt;
__________________________________________________________________
&lt;CONV BLOCK 3&gt;
__________________________________________________________________
&lt;CONV BLOCK 4&gt;
__________________________________________________________________
&lt;CONV BLOCK 5&gt;
==================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
__________________________________________________________________
</code></pre>
    <p class="normal">The preceding output has been abbreviated for brevity. The model contains over 23 million trainable parameters. We don't need the top classification layer as we are using the model for feature<a id="_idIndexMarker536"/> extraction. We defined a new model with the input and output layer. Here, we took the output from the last layer. We<a id="_idIndexMarker537"/> could take output from different parts of ResNet by changing the definition of the <code class="Code-In-Text--PACKT-">hidden_layer</code> variable. In fact, this variable can be a list of layers, in which case the output of the <code class="Code-In-Text--PACKT-">features_extract</code> model will be the output from each of the layers in the list. </p>
    <p class="normal">Next, a directory must be set up to store the extracted features:</p>
    <pre class="programlisting code"><code class="hljs-code">save_prefix = prefix + <span class="hljs-string">"features/"</span>
<span class="hljs-keyword">try</span>:
    <span class="hljs-comment"># Create this directory </span>
    os.mkdir(save_prefix)
<span class="hljs-keyword">except</span> FileExistsError:
    <span class="hljs-keyword">pass</span> <span class="hljs-comment"># Directory already exists</span>
</code></pre>
    <p class="normal">The feature extraction model can work on batches of images and predict the output. The output is 2,048 patches of 7x7 pixels for each image. If a batch of 16 images is supplied, then the output from the model will be a tensor of dimensions [16, 7, 7, 2048]. We store the<a id="_idIndexMarker538"/> features of each image file as a separate file while<a id="_idIndexMarker539"/> flattening the dimensions to [49, 2048]. Each image has now been converted into a sequence of 49 pixels, with an embedding size of 2,048. The following code performs this action:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> img, path <span class="hljs-keyword">in</span> tqdm(image_dataset):
    batch_features = features_extract(img)
    batch_features = tf.reshape(batch_features,
                                (batch_features.shape[<span class="hljs-number">0</span>], <span class="hljs-number">-1</span>,                                  batch_features.shape[<span class="hljs-number">3</span>]))
    <span class="hljs-keyword">for</span> feat, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(batch_features, path):
        filepath = p.numpy().decode(<span class="hljs-string">"utf-8"</span>)
        filepath = save_prefix + filepath.split(<span class="hljs-string">'/'</span>)[<span class="hljs-number">-1</span>][:<span class="hljs-number">-3</span>] + <span class="hljs-string">"npy"</span>
        np.save(filepath, feat.numpy())
print(<span class="hljs-string">"Images saved as npy files"</span>)
</code></pre>
    <p class="normal">This could be a time-consuming operation, depending on your computing environment. On my Ubuntu Linux box with an RTX 2070 GPU, this took ~23 minutes.</p>
    <p class="normal">The last step in data pre-processing is to train the Subword Encoder. This part should be quite familiar to you as it is identical to what we've done in previous chapters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Now, read the labels and create a subword tokenizer with it</span>
<span class="hljs-comment"># ~8K vocab size</span>
cap_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    inputs[<span class="hljs-string">'caption'</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.lower().strip()).tolist(),
    target_vocab_size=<span class="hljs-number">2</span>**<span class="hljs-number">13</span>, reserved_tokens=[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>])
cap_tokenizer.save_to_file(<span class="hljs-string">"captions"</span>)
</code></pre>
    <p class="normal">Note that we included two special tokens to signal the start and end of the sequences. You may recall this technique from <em class="chapterRef">Chapter 5</em>, <em class="italic">Generating Text with RNNs and GPT-2</em>. Here, we used a slightly different way of accomplishing the same technique to show how you can accomplish the same objective in different ways.</p>
    <p class="normal">With that, pre-processing and <a id="_idIndexMarker540"/>feature extraction is complete. The next step is defining the Transformer model. Then, we will be ready to <a id="_idIndexMarker541"/>train the model.</p>
    <h1 id="_idParaDest-126" class="title">The Transformer model</h1>
    <p class="normal">The Transformer model<a id="_idIndexMarker542"/> was discussed in <em class="chapterRef">Chapter 4</em>, <em class="italic">Transfer Learning with BERT</em>. It was inspired by the seq2seq model and has an Encoder and a Decoder part. Since the Transformer model does not rely on RNNs, input sequences need to be annotated with positional encodings, which allow the model to learn about the relationships between inputs. Removing recurrence improves the speed of the model vastly while reducing the memory footprint. This innovation of the Transformer model has made very large-sized models such as BERT and GPT-3 possible. The Encoder part of the Transformer model was shown in the aforementioned chapter. The full Transformer model was shown in <em class="chapterRef">Chapter 5</em>, <em class="italic">Generating Text with RNNs and GPT-2</em>. We will start with a modified version of the full Transformer. Specifically, we will modify the Encoder part of the Transformer to create a visual Encoder, which takes image data as input instead of text sequences. There are some other small modifications to be made to accommodate images as input to the Encoder. The Transformer model we are going to build is shown in the following diagram. The main difference here is how the input sequence is encoded. In the case of text, we will tokenize the text using a Subword Encoder and pass it through an Embedding layer, which is trainable. </p>
    <p class="normal">As training proceeds, the embeddings of the tokens are also learned. In the case of image captioning, we will pre-process the images into a sequence of 49 pixels, each with an "embedding" size of 2,048. This actually simplifies padding the inputs. All the images are pre-processed so that they're the same length. Consequently, padding and masking the inputs is not required:</p>
    <figure class="mediaobject"><img src="image/B16252_07_12.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.12: Transformer model with a visual Encoder</p>
    <p class="normal">The following pieces of code need to be<a id="_idIndexMarker543"/> implemented to build the Transformer model:</p>
    <ul>
      <li class="bullet">Positional encoding of the inputs, along with input and output masks. Our inputs are of a fixed length, but the output and captions are of a variable length.</li>
      <li class="bullet">Scaled dot-product attention and multi-head attention to enable the Encoders and Decoders to focus on specific aspects of the data.</li>
      <li class="bullet">An Encoder that consists of multiple repeating blocks.</li>
      <li class="bullet">A Decoder that uses the outputs from the Encoder through its repeating blocks.</li>
    </ul>
    <p class="normal">The code for the Transformer has been taken from the TensorFlow tutorial titled <em class="italic">Transformer model for language understanding</em>. We will be using this code as the base and adapting it for the image captioning use case. One of the beautiful things about the Transformer architecture is that if we can cast a problem as a sequence-to-sequence problem, then we can apply the Transformer model. As we describe the implementation, the main points of the code will be highlighted. Note that the code for this section is in the <code class="Code-In-Text--PACKT-">visual_transformer.py</code> file.</p>
    <p class="normal">Implementing the full <a id="_idIndexMarker544"/>Transformer model does take a little bit of code. If you are already familiar with the Transformer model or want to only know where our model differs from the standard Transformer model, please focus on the next section and the <em class="italic">VisualEncoder</em> section. You can read the rest of the sections at your leisure.</p>
    <h2 id="_idParaDest-127" class="title">Positional encoding and masks</h2>
    <p class="normal">Transformer models don't use RNNs. This<a id="_idIndexMarker545"/> allows them to compute all the outputs in one step, leading to significant improvements in speed and also the ability to learn dependencies across long inputs. However, it comes at the cost of the<a id="_idIndexMarker546"/> model not knowing anything about the relationship between neighboring words or tokens. A positional encoding vector, with values for the odd and even positions of the tokens to help the model learn relationships between the positions of inputs, helps compensate for the lack of information about the ordering of tokens.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Embeddings help place tokens that are similar in meaning close to each other in the embedding space. Positional encodings put tokens closer to each other based on their position in the sentence. Put together, the two are quite powerful.</p>
    </div>
    <p class="normal">In image captioning, this is important for captions. Technically, we don't need to provide these positional encodings for the image inputs as ResNet50 should have produced appropriate patches. Positional encoding can, however, still be used for the inputs as well. Positional encoding uses a <em class="italic">sin</em> function for even positions and a <em class="italic">cos</em> function for odd positions. The formula for computing the encodings for a position is:</p>
    <figure class="mediaobject"><img src="image/B16252_07_001.png" alt="" style="max-height:40px;"/></figure>
    <p class="normal">Here, <em class="italic">w</em><sub class="" style="font-style: italic;">i</sub> is defined as:</p>
    <figure class="mediaobject"><img src="image/B16252_07_002.png" alt="" style="max-height:40px;"/></figure>
    <p class="normal">In the preceding formula, <em class="italic">pos</em> refers to the position of a given token, <em class="italic">d</em><sub class="" style="font-style: italic;">model</sub> refers to the dimensions of the embeddings, and <em class="italic">i</em> is the specific dimension being computed. The positional encoding process produces a vector with the same dimensions as the embedding for each token. You may be wondering why this complex formulation is used for computing these <a id="_idIndexMarker547"/>positional encodings. Wouldn't numbering the tokens from one side to the other suffice? It turns out that the positional encoding algorithm must have a few characteristics. First, the values must generalize easily to sequences of a variable length. Using a straight-up numbering scheme would<a id="_idIndexMarker548"/> prevent inputs that have sequences longer than those in the training data. The output should be unique for each token's position. Furthermore, the distance between any two positions should be consistent across different lengths of input sequences. This formulation is relatively simple to implement. The code for this is in the Positional Encoder section of the file.</p>
    <p class="normal">First, we must compute the <em class="italic">angle</em>, as shown in the preceding <em class="italic">w</em><sub class="" style="font-style: italic;">i</sub> formula, like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">get_angles</span><span class="hljs-functio">(</span><span class="hljs-params">pos, i, d_model</span><span class="hljs-functio">):</span>
    angle_rates = <span class="hljs-number">1</span> / np.power(<span class="hljs-number">10000</span>, (<span class="hljs-number">2</span> * (i // <span class="hljs-number">2</span>)) / np.float32(d_model))
    <span class="hljs-keyword">return</span> pos * angle_rates
</code></pre>
    <p class="normal">Then, we must compute the vector of positional encodings:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">positional_encoding</span><span class="hljs-functio">(</span><span class="hljs-params">position, d_model</span><span class="hljs-functio">):</span>
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)
    <span class="hljs-comment"># apply sin to even indices in the array; 2i</span>
    angle_rads[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = np.sin(angle_rads[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>])
    <span class="hljs-comment"># apply cos to odd indices in the array; 2i+1</span>
    angle_rads[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = np.cos(angle_rads[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])
    pos_encoding = angle_rads[np.newaxis, ...]
    <span class="hljs-keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)
</code></pre>
    <p class="normal">The next step is to compute the masks for input and output. Let's focus on the Decoder for a second. Since we are not using an RNN, the entire output is fed to the Decoder at once. However, we don't want the Decoder to look at data from future timesteps. So, the outputs must be masked. In <a id="_idIndexMarker549"/>terms of the Encoder, masks are needed if the input is padded to a fixed length. However, in our case, the inputs are always <a id="_idIndexMarker550"/>exactly a length of 49. So, the mask is a fixed vector of ones:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">create_padding_mask</span><span class="hljs-functio">(</span><span class="hljs-params">seq</span><span class="hljs-functio">):</span>
    seq = tf.cast(tf.math.equal(seq, <span class="hljs-number">0</span>), tf.float32)
    <span class="hljs-comment"># add extra dimensions to add the padding</span>
    <span class="hljs-comment"># to the attention logits.</span>
    <span class="hljs-keyword">return</span> seq[:, tf.newaxis, tf.newaxis, :]  
<span class="hljs-keyword">    </span><span class="hljs-comment"># (batch_size, 1, 1, seq_len)</span>
<span class="hljs-comment"># while decoding, we dont have recurrence and dont want Decoder</span>
<span class="hljs-comment"># to see tokens from the future</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">create_look_ahead_mask</span><span class="hljs-functio">(</span><span class="hljs-params">size</span><span class="hljs-functio">):</span>
    mask = <span class="hljs-number">1</span> - tf.linalg.band_part(tf.ones((size, size)), <span class="hljs-number">-1</span>, <span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> mask  <span class="hljs-comment"># (seq_len, seq_len)</span>
</code></pre>
    <p class="normal">The first method is used to mask inputs if they are padded. This method has been included for the sake of completeness, but you will see later that we pass it a sequence of ones. So, all this method does is reshape the masks. The second mask function is used for masking Decoder inputs so that it can only see the positions it has generated.</p>
    <p class="normal">The layers of the transfer Encoder and Decoder use a specific form of attention. This is a fundamental building block of the architecture and will be implemented next.</p>
    <h2 id="_idParaDest-128" class="title">Scaled dot-product and multi-head attention</h2>
    <p class="normal">The purpose of the <a id="_idIndexMarker551"/>attention function is to match a query to a set of key-value pairs. The output is a sum of the values, weighted by the correspondence <a id="_idIndexMarker552"/>between the query and the key. multi-head attention learns multiple ways to compute the scaled dot-product attention and combines it.</p>
    <p class="normal">Scaled dot-product attention is computed by multiplying the query vector by the key vector. This product is scaled by the square root of the dimensions of the query and key. Note that this formulation assumes that the key and query vectors have the same dimensions. Practically, the dimensions of the query, key, and value vectors are all set to the size of the embedding. </p>
    <p class="normal">This was referred to as <em class="italic">d</em><sub class="" style="font-style: italic;">model</sub> in the position encoding. After computing the scaled product of the key<a id="_idIndexMarker553"/> and query vector, a softmax is applied, and the result of the softmax is<a id="_idIndexMarker554"/> multiplied by the value vector. A mask is used to mask the product of the query and keys:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">scaled_dot_product_attention</span><span class="hljs-functio">(</span><span class="hljs-params">q, k, v, mask</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># (..., seq_len_q, seq_len_k)</span>
    matmul_qk = tf.matmul(q, k, transpose_b=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># scale matmul_qk</span>
    dk = tf.cast(tf.shape(k)[<span class="hljs-number">-1</span>], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
    <span class="hljs-comment"># add the mask to the scaled tensor.</span>
    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        scaled_attention_logits += (mask * <span class="hljs-number">-1e9</span>)
    <span class="hljs-comment"># softmax is normalized on the last axis (seq_len_k)     # so that the scores</span>
    <span class="hljs-comment"># add up to 1.</span>
    attention_weights = tf.nn.softmax(
                        scaled_attention_logits,
                        axis=<span class="hljs-number">-1</span>)  <span class="hljs-comment"># (..., seq_len_q, seq_len_k)</span>
    output = tf.matmul(attention_weights, v)  
<span class="hljs-comment">    # (..., seq_len_q, depth_v)</span>
    <span class="hljs-keyword">return</span> output, attention_weights
</code></pre>
    <p class="normal">Multi-ahead attention concatenates outputs from multiple scaled dot-product attention units and passes them through a linear layer. The dimensions of the embedding inputs are divided by the number of heads to compute the dimensions of the key and value vectors. Multi-head attention is implemented as a custom layer. First, we must create the constructor:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">MultiHeadAttention</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, d_model, num_heads</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        <span class="code-highlight"><strong class="hljs-keyword-slc">assert</strong><strong class="hljs-slc"> d_model % self.num_heads == </strong><strong class="hljs-number-slc">0</strong></span>
        self.depth = d_model // self.num_heads
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
</code></pre>
    <p class="normal">Note the <code class="Code-In-Text--PACKT-">assert</code> statement that is highlighted. When the Transformer model is instantiated, it is vital to <a id="_idIndexMarker555"/>choose some parameters so that the number of heads divides the model size or<a id="_idIndexMarker556"/> embedding dimensions completely. The main computation of this layer is in the <code class="Code-In-Text--PACKT-">call()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, v, k, q, mask</span><span class="hljs-functio">):</span>
        batch_size = tf.shape(q)[<span class="hljs-number">0</span>]
        q = self.wq(q)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
        k = self.wk(k)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
        v = self.wv(v)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
        <span class="hljs-comment"># (batch_size, num_heads, seq_len_q, depth)</span>
        <span class="code-highlight"><strong class="hljs-slc">q = self.split_heads(q, batch_size)</strong></span>
        <span class="hljs-comment"># (batch_size, num_heads, seq_len_k, depth)</span>
        <span class="code-highlight"><strong class="hljs-slc">k = self.split_heads(k, batch_size)</strong></span>
        <span class="hljs-comment"># (batch_size, num_heads, seq_len_v, depth)</span>
        <span class="code-highlight"><strong class="hljs-slc">v = self.split_heads(v, batch_size)</strong></span>
        <span class="hljs-comment"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span>
        <span class="hljs-comment"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)
        <span class="hljs-comment"># (batch_size, seq_len_q, num_heads, depth)</span>
        scaled_attention = tf.transpose(scaled_attention, 
                                                   perm=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])
        concat_attention = tf.reshape(scaled_attention,
                                        (batch_size, <span class="hljs-number">-1</span>,
                              self.d_model))  
        <span class="hljs-comment"># (batch_size, seq_len_q, d_model)</span>
        <span class="hljs-comment"># (batch_size, seq_len_q, d_model)</span>
        output = self.dense(concat_attention)
        <span class="hljs-keyword">return</span> output, attention_weights
</code></pre>
    <p class="normal">The three highlighted rows<a id="_idIndexMarker557"/> show splitting the vectors into multiple heads. <code class="Code-In-Text--PACKT-">split_heads()</code> is defined like so:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">split_heads</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, batch_size</span><span class="hljs-functio">):</span>
        <span class="hljs-string">"""
        Split the last dimension into (num_heads, depth).</span>
<span class="hljs-string">        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)</span>
<span class="hljs-string">        """</span>
        x = tf.reshape(x, (batch_size, <span class="hljs-number">-1</span>, 
self.num_heads, self.depth))
        <span class="hljs-keyword">return</span> tf.transpose(x, perm=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])
</code></pre>
    <p class="normal">This completes the<a id="_idIndexMarker558"/> multi-head attention implementation. This is the key part of the Transformer model. There is a small detail surrounding a Dense layer, which is used to aggregate the outputs from multi-head attention. It is quite simple:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">point_wise_feed_forward_network</span><span class="hljs-functio">(</span><span class="hljs-params">d_model, dff</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">return</span> tf.keras.Sequential([
        <span class="hljs-comment"># (batch_size, seq_len, dff)</span>
        tf.keras.layers.Dense(dff, activation=<span class="hljs-string">'relu'</span>),
        tf.keras.layers.Dense(d_model)
        <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
    ])
</code></pre>
    <p class="normal">Thus far, we have looked at the following parameters for specifying a Transformer mode:</p>
    <ul>
      <li class="bullet"><em class="italic">d</em><sub class="" style="font-style: italic;">model</sub> is used for the size of the embeddings and primary flow of inputs</li>
      <li class="bullet"><em class="italic">d</em><sub class="" style="font-style: italic;">ff</sub> is the size of the output from the intermediate Dense layer in the FeedForward part</li>
      <li class="bullet"><em class="italic">h</em> specifies the number of heads for multi-head attention</li>
    </ul>
    <p class="normal">Next, we will implement a visual Encoder, which has been modified to accommodate images as input.</p>
    <h2 id="_idParaDest-129" class="title">VisualEncoder</h2>
    <p class="normal">The diagram shown in the <em class="italic">The Transformer model</em> section shows the Encoder's structure. The Encoder processes the inputs with positional encodings and masks, and then passes them through<a id="_idIndexMarker559"/> stacks of multi-head attention and feed-forward blocks. The implementation deviates from the TensorFlow tutorial as the input in the tutorial is text. In our case, we are passing 49x2,048 vectors that were generated by passing images through ResNet50. The main difference is in how the inputs are handled. <code class="Code-In-Text--PACKT-">VisualEncoder</code> is built as a layer to allow composition into the eventual Transform model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">VisualEncoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, num_layers, d_model, num_heads, dff,</span>
<span class="hljs-params">                 maximum_position_encoding=</span><span class="code-highlight"><strong class="hljs-number-slc">49</strong></span><span class="hljs-params">, dropout_rate=</span><span class="hljs-number">0.1</span><span class="hljs-params">,</span>
<span class="hljs-params">                 use_pe=</span><span class="hljs-literal">True</span><span class="hljs-functio">):</span>
        <span class="hljs-comment"># we have 7x7 images from ResNet50, </span>
        <span class="hljs-comment"># and each pixel is an input token</span>
        <span class="hljs-comment"># which has been embedded into 2048 dimensions by ResNet</span>
        <span class="hljs-built_in">super</span>(VisualEncoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        
        <span class="code-highlight"><strong class="hljs-comment-slc"># FC layer replaces embedding layer in traditional encoder</strong></span>
        <span class="code-highlight"><strong class="hljs-comment-slc"># this FC layers takes 49x2048 image </strong></span>
        <span class="code-highlight"><strong class="hljs-comment-slc"># and projects into model dims</strong></span>
        <span class="code-highlight"><strong class="hljs-slc">self.fc = tf.keras.layers.Dense(d_model, activation=</strong><strong class="hljs-string-slc">'relu'</strong><strong class="hljs-slc">)</strong></span>
        self.pos_encoding = positional_encoding(
                                         maximum_position_encoding,
                                      self.d_model)
        self.enc_layers = [EncoderLayer(d_model, num_heads, 
                                        dff, dropout_rate)
                           <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        
        self.use_pe = use_pe
</code></pre>
    <p class="normal">The constructor is shown next. A new parameter that states the number of layers is introduced. The original paper used 6 layers, 512 as <em class="italic">d</em><sub class="" style="font-style: italic;">model</sub>, 8 multi-attention heads, and 2,048 as the size of the intermediate feed-forward output. Note the highlighted lines in the preceding code. The dimensions of the pre-processed images can vary depending on the layer of ResNet50 from which output is pulled. We pass the input through a dense layer, <code class="Code-In-Text--PACKT-">fc</code>, to the size inputs <a id="_idIndexMarker560"/>according to the model. This allows us to experiment with different models to pre-process images such as VGG19 or Inception without changing the architecture. Also, note that the maximum position encoding is hardcoded to 49, since that is the dimension of the output of the ResNet50 model. Lastly, we add a flag that can switch positional encoding on or off in the Visual Encoder. You should experiment with training models with and without positional encodings in the input to see if this helps or hinders learning.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">VisualEncoder</code> is composed of multiple multi-head attention and feed-forward blocks. We can utilize a convenience class, <code class="Code-In-Text--PACKT-">EncoderLayer</code>, to define one such block. A stack of these blocks is created based on the input parameters. We will examine the internals of <code class="Code-In-Text--PACKT-">EncoderLayer</code> momentarily. First, let's see how inputs pass through <code class="Code-In-Text--PACKT-">VisualEncoder</code>. The <code class="Code-In-Text--PACKT-">call()</code> function is used to produce the outputs for the given inputs:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, training, mask</span><span class="hljs-functio">):</span>
        <span class="hljs-comment"># all inp image sequences are always 49, so mask not needed</span>
        seq_len = tf.shape(x)[<span class="hljs-number">1</span>]
        <span class="hljs-comment"># adding embedding and position encoding.</span>
        <span class="hljs-comment"># input size should be batch_size, 49, 2048)</span>
<span class="hljs-comment">        # output dims should be (batch_size, 49, d_model)</span>
        x = self.fc(x)
        <span class="hljs-comment"># scaled dot product attention</span>
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) 
        <span class="hljs-keyword">if</span> self.use_pe:
            x += self.pos_encoding[:, :seq_len, :]
        x = self.dropout(x, training=training)
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):
            x = self.enc_layers[i](
                x, training, mask)  <span class="hljs-comment"># mask shouldnt be needed</span>
        <span class="hljs-keyword">return</span> x  <span class="hljs-comment"># (batch_size, 49, d_model)</span>
</code></pre>
    <p class="normal">This code is fairly simple due to the abstractions defined previously. Note the use of the training flag to turn dropout on or off. Now, let's see how <code class="Code-In-Text--PACKT-">EncoderLayer</code> is defined. Each Encoder building is composed of two sub-blocks. The first sub-block passes inputs through multi-head attention, while<a id="_idIndexMarker561"/> the second sub-block passes the output of the first sub-block through the 2-layer feed-forward layer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">EncoderLayer</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, d_model, num_heads, dff, rate=</span><span class="hljs-number">0.1</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(EncoderLayer, self).__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(
                                                        epsilon=<span class="hljs-number">1e-6</span>)
        self.layernorm2 = tf.keras.layers.LayerNormalization(
                                                        epsilon=<span class="hljs-number">1e-6</span>)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, training, mask</span><span class="hljs-functio">):</span>
        <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
        attn_output, _ = self.mha(x, x, x, mask)
        attn_output = self.dropout1(attn_output, 
                                      training=training)
        <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
        <span class="code-highlight"><strong class="hljs-slc">out1 = self.layernorm1(x + attn_output) </strong><strong class="hljs-comment-slc"># Residual connection</strong></span>
        
        <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
        ffn_output = self.ffn(out1)  
        ffn_output = self.dropout2(ffn_output, training=training)
        <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
        <span class="code-highlight"><strong class="hljs-slc">out2 = self.layernorm2(out1 + ffn_output) </strong><strong class="hljs-comment-slc"># Residual conx</strong></span>
        <span class="hljs-keyword">return</span> out2
</code></pre>
    <p class="normal">Each layer first computes the output from multi-head attention and passes it through dropout. A residual connection passes the sum of the output and input through LayerNorm. The second part of this block passes the output of the first LayerNorm through the feed-forward layer and another dropout layer. </p>
    <p class="normal">Again, a residual connection combines the output<a id="_idIndexMarker562"/> and input to the feed-forward part before passing it through LayerNorm. Note the use of dropout and residual connections, which were developed for CV in the Transformer architecture.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">Layer normalization or LayerNorm</strong></p>
      <p class="Information-Box--PACKT-">LayerNorm was proposed in 2016 in a paper by the same name as an alternative to BatchNorm for RNNs. BatchNorm, as described in the <em class="italic">CNNs</em> section, normalizes the outputs across the entire batch. But sequences can be of variable length in the case of RNNs. A different formulation is required for normalization that can handle variable sequence lengths. LayerNorm normalizes across all the hidden units in a given layer. It is independent of the batch size, and the normalization is the same for all the units in a given layer. LayerNorm results in a significant speedup of training and convergence of seq2seq style models.</p>
    </div>
    <p class="normal">With <code class="Code-In-Text--PACKT-">VisualEncoder</code> in place, we are ready to implement the Decoder before we put this all together into the full Transformer.</p>
    <h2 id="_idParaDest-130" class="title">Decoder</h2>
    <p class="normal">The Decoder is also <a id="_idIndexMarker563"/>composed of blocks, just like the Encoder. Each block of the Decoder, however, contains three sub-blocks, as shown in the diagram in the <em class="italic">The Transformer model</em> section. There is a masked multi-head attention sub-block, followed by a multi-head attention block, and finally a feed-forward sub-block. The feed-forward sub-block is identical to the Encoder sub-block. We must define a Decoder layer that can be stacked to construct the Decoder. The constructor for this is shown here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">DecoderLayer</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, d_model, num_heads, dff, rate=</span><span class="hljs-number">0.1</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(DecoderLayer, self).__init__()
        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(
                                                       epsilon=<span class="hljs-number">1e-6</span>)
        self.layernorm2 = tf.keras.layers.LayerNormalization(
                                                       epsilon=<span class="hljs-number">1e-6</span>)
        self.layernorm3 = tf.keras.layers.LayerNormalization(
                                                       epsilon=<span class="hljs-number">1e-6</span>)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)
</code></pre>
    <p class="normal">Three sub-blocks should be quite evident based on the preceding variables. Input passes through this layer and is <a id="_idIndexMarker564"/>converted into output, as defined by the computations in the <code class="Code-In-Text--PACKT-">call()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, enc_output, training,</span>
<span class="hljs-params">             look_ahead_mask, padding_mask</span><span class="hljs-functio">):</span>
        <span class="hljs-comment"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>
        <span class="code-highlight"><strong class="hljs-slc">attn1, attn_weights_block1 = self.mha1(</strong></span>
            <span class="code-highlight"><strong class="hljs-slc">x, x, x, look_ahead_mask)</strong></span>
        <span class="hljs-comment"># args ^ =&gt; (batch_size, target_seq_len, d_model)</span>
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x) <span class="hljs-comment"># residual</span>
        attn2, attn_weights_block2 = self.mha2(
            enc_output, enc_output, out1, padding_mask)  
        <span class="hljs-comment"># args ^ =&gt;  (batch_size, target_seq_len, d_model)</span>
        attn2 = self.dropout2(attn2, training=training)
        <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
        out2 = self.layernorm2(attn2 + out1)
        ffn_output = self.ffn(out2)  
        ffn_output = self.dropout3(ffn_output, training=training)
        <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
        out3 = self.layernorm3(ffn_output + out2)
        <span class="hljs-keyword">return</span> out3, attn_weights_block1, attn_weights_block2
</code></pre>
    <p class="normal">The first sub-block, also referred to as the masked multi-head attention block, uses the output tokens, masked to the current position being generated. The outputs, in our case, are the tokens that make up the caption. The look-ahead mask masks tokens that haven't been generated yet. </p>
    <p class="normal">Note that this sub-block does not use the output of the Encoder. It is trying to predict the relationship of the next token to the previous token that was generated. The<a id="_idIndexMarker565"/> second sub-block uses the output of the Encoder, along with the output of the previous sub-block, to generate the outputs. Finally, the feed-forward network generates the final output by operating on the output of the second sub-block. Both the multi-head attention sub-blocks have their own attention weights.</p>
    <p class="normal">We define the Decoder as a custom layer that is composed of multiple <code class="Code-In-Text--PACKT-">DecoderLayer</code> blocks. The structure of the Transformer is symmetrical. The number of Encoder and Decoder blocks is the same. The constructor is defined first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Decoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, num_layers, d_model, num_heads, </span>
<span class="hljs-params">                 dff, target_vocab_size,</span>
<span class="hljs-params">                 maximum_position_encoding, rate=</span><span class="hljs-number">0.1</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(Decoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.embedding = tf.keras.layers.Embedding(
                                        target_vocab_size, d_model)
        self.pos_encoding = positional_encoding(
                                maximum_position_encoding, 
                                  d_model)
        self.dec_layers = [DecoderLayer(d_model, num_heads, 
                                           dff, rate)
                           <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)
</code></pre>
    <p class="normal">The output of the Decoder is computed by the <code class="Code-In-Text--PACKT-">call()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, enc_output, training,</span>
<span class="hljs-params">             look_ahead_mask, padding_mask</span><span class="hljs-functio">):</span>
        seq_len = tf.shape(x)[<span class="hljs-number">1</span>]
        attention_weights = {}
        x = self.embedding(x)  
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]
        x = self.dropout(x, training=training)
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output,
                       training, look_ahead_mask, padding_mask)
        attention_weights[<span class="hljs-string">'decoder_layer{}_block1'</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>)]  = block1
        attention_weights[<span class="hljs-string">'decoder_layer{}_block2'</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>)] = block2
        <span class="hljs-comment"># x.shape == (batch_size, target_seq_len, d_model)</span>
        <span class="hljs-keyword">return</span> x, attention_weights
</code></pre>
    <p class="normal">Whew, that was a fair amount <a id="_idIndexMarker566"/>of code. The structure of the Transformer model is so elegant. The beauty of the model allows us to stack more Encoder and Decoder layers to create more powerful models, as demonstrated by GPT-3 recently. Let's put the Encoder and Decoder together to create a full Transformer.</p>
    <h2 id="_idParaDest-131" class="title">Transformer</h2>
    <p class="normal">The Transformer is composed<a id="_idIndexMarker567"/> of the Encoder, the Decoder, and the final Dense layer for generating output token distributions across the subword vocabulary:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Transformer</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, num_layers, d_model, num_heads, dff,</span>
<span class="hljs-params">                 target_vocab_size, pe_input, pe_target, rate=</span><span class="hljs-number">0.1</span><span class="hljs-params">,</span>
<span class="hljs-params">                 use_pe=</span><span class="hljs-literal">True</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(Transformer, self).__init__()
        self.encoder = VisualEncoder(num_layers, d_model, 
                                     num_heads, dff,
                                     pe_input, rate, use_pe)
        self.decoder = Decoder(num_layers, d_model, num_heads, 
                       dff, target_vocab_size, pe_target, rate)
        self.final_layer = tf.keras.layers.Dense(
                                       target_vocab_size)
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, inp, tar, training, enc_padding_mask,</span>
<span class="hljs-params">             look_ahead_mask, dec_padding_mask</span><span class="hljs-functio">):</span>
        <span class="hljs-comment"># (batch_size, inp_seq_len, d_model)</span>
        enc_output = self.encoder(inp, training, enc_padding_mask)
        <span class="hljs-comment"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
        dec_output, attention_weights = self.decoder(
                                tar, enc_output, training, 
                                look_ahead_mask, dec_padding_mask)
        <span class="hljs-comment"># (batch_size, tar_seq_len, target_vocab_size)</span>
        final_output = self.final_layer(dec_output)
        <span class="hljs-keyword">return</span> final_output, attention_weights
</code></pre>
    <p class="normal">That was a whirlwind tour of the full Transformer code. Ideally, Keras in TensorFlow will provide a higher-level API for defining a Transformer model without you having to write the code out. If this was too much to absorb, then focus on the masks and VisualEncoder as they are the only deviations from the standard Transformer architecture.</p>
    <p class="normal">We are now ready to train the <a id="_idIndexMarker568"/>model. We'll take a very similar approach to the one we adopted in the previous chapter, by setting up learning rate annealing and checkpointing.</p>
    <h1 id="_idParaDest-132" class="title">Training the Transformer model with VisualEncoder</h1>
    <p class="normal">Training the Transformer model can<a id="_idIndexMarker569"/> take hours as <a id="_idIndexMarker570"/>we want to train for around 20 epochs. It is best to put the training code into a file so that it can be<a id="_idIndexMarker571"/> run from the command line. Note that the model will be able to show some results even after 4 epochs of training. The training code is in the <code class="Code-In-Text--PACKT-">caption-training.py</code> file. At a high level, the following steps need to be performed before starting training. First, the CSV file with captions and image names is loaded in, and the corresponding paths for the files with extracted image features are appended. The Subword Encoder is also loaded in. A <code class="Code-In-Text--PACKT-">tf.data.Dataset</code> is created with the encoded captions and image features for easy batching and feeding them into the model for training. A loss function, an<a id="_idIndexMarker572"/> optimizer with a learning rate<a id="_idIndexMarker573"/> schedule, is created for use in training. A custom training loop is used to<a id="_idIndexMarker574"/> train the Transformer model. Let's go over these steps in detail.</p>
    <h2 id="_idParaDest-133" class="title">Loading training data</h2>
    <p class="normal">The following code loads the CSV file we<a id="_idIndexMarker575"/> generated in the pre-processing step:</p>
    <pre class="programlisting code"><code class="hljs-code">prefix = <span class="hljs-string">'./data/'</span>
save_prefix = prefix + <span class="hljs-string">"features/"</span>  <span class="hljs-comment"># for storing prefixes</span>
annot = prefix + <span class="hljs-string">'data.csv'</span>
inputs = pd.read_csv(annot, header=<span class="hljs-literal">None</span>, 
                      names=[<span class="hljs-string">"caption"</span>, <span class="hljs-string">"image"</span>])
print(<span class="hljs-string">"Data file loaded"</span>)
</code></pre>
    <p class="normal">The captions in the data are tokenized using the Subword Encoder we generated and persisted to disk earlier:</p>
    <pre class="programlisting code"><code class="hljs-code">cap_tokenizer = \
          tfds.features.text.SubwordTextEncoder.load_from_file(
                                                    <span class="hljs-string">"captions"</span>)
print(cap_tokenizer.encode(
                  <span class="hljs-string">"A man riding a wave on top of a surfboard."</span>.lower())
)
print(<span class="hljs-string">"Tokenizer hydrated"</span>)
<span class="hljs-comment"># Max length of captions split by spaces</span>
lens = inputs[<span class="hljs-string">'caption'</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x.split()))
<span class="hljs-comment"># Max length of captions after tokenization</span>
<span class="hljs-comment"># tfds demonstrated in earlier chapters</span>
<span class="hljs-comment"># This is a quick way if data fits in memory</span>
lens = inputs[<span class="hljs-string">'caption'</span>].<span class="hljs-built_in">map</span>(
                <span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(cap_tokenizer.encode(x.lower()))
)
<span class="hljs-comment"># We will set this as the max length of captions</span>
<span class="hljs-comment"># which cover 99% of the captions without truncation</span>
max_len = <span class="hljs-built_in">int</span>(lens.quantile(<span class="hljs-number">0.99</span>) + <span class="hljs-number">1</span>)  <span class="hljs-comment"># for special tokens</span>
</code></pre>
    <p class="normal">The maximum length of the<a id="_idIndexMarker576"/> captions is generated to accommodate 99% of the caption lengths. All the captions are truncated or padded to this maximum length:</p>
    <pre class="programlisting code"><code class="hljs-code">start = <span class="hljs-string">'&lt;s&gt;'</span>
end = <span class="hljs-string">'&lt;/s&gt;'</span>
inputs[<span class="hljs-string">'tokenized'</span>] = inputs[<span class="hljs-string">'caption'</span>].<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: start + x.lower().strip() + end)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">tokenize_pad</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    x = cap_tokenizer.encode(x)
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(x) &lt; max_len:
        x = x + [<span class="hljs-number">0</span>] * <span class="hljs-built_in">int</span>(max_len - <span class="hljs-built_in">len</span>(x))
    <span class="hljs-keyword">return</span> x[:max_len]
inputs[<span class="hljs-string">'tokens'</span>] = inputs.tokenized.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: tokenize_pad(x))
</code></pre>
    <p class="normal">Image features are persisted to disk. When training begins, those features need to be read from the disk and fed in, along with the encoded captions. The name of the file containing the image features is then added to the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># now to compute a column with the new name of the saved </span>
<span class="hljs-comment"># image feature file</span>
inputs[<span class="hljs-string">'img_features'</span>] = inputs[<span class="hljs-string">'image'</span>].<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:
                                             save_prefix +
                                             x.split(<span class="hljs-string">'/'</span>)[<span class="hljs-number">-1</span>][:<span class="hljs-number">-3</span>]
                                             + <span class="hljs-string">'npy'</span>)
</code></pre>
    <p class="normal">A <code class="Code-In-Text--PACKT-">tf.data.Dataset</code> is created and a map function that reads image features while enumerating batches is set up:</p>
    <pre class="programlisting code"><code class="hljs-code">captions = inputs.tokens.tolist()
img_names = inputs.img_features.tolist()
<span class="hljs-comment"># Load the numpy file with extracted ResNet50 feature</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">load_image_feature</span><span class="hljs-functio">(</span><span class="hljs-params">img_name, cap</span><span class="hljs-functio">):</span>
    img_tensor = np.load(img_name.decode(<span class="hljs-string">'utf-8'</span>))
    <span class="hljs-keyword">return</span> img_tensor, cap
dataset = tf.data.Dataset.from_tensor_slices((img_train, 
                                              cap_train))
<span class="hljs-comment"># Use map to load the numpy files in parallel</span>
dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> item1, item2: tf.numpy_function(
    load_image_feature, [item1, item2], [tf.float32, tf.int32]),
    num_parallel_calls=tf.data.experimental.AUTOTUNE)
</code></pre>
    <p class="normal">Now that the dataset has <a id="_idIndexMarker577"/>been prepared, we are ready to instantiate the Transformer model.</p>
    <h2 id="_idParaDest-134" class="title">Instantiating the Transformer model</h2>
    <p class="normal">We will instantiate a small <a id="_idIndexMarker578"/>model in terms of the number of layers, attention heads, embedding dimensions, and feed-forward units:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Small Model</span>
num_layers = <span class="hljs-number">4</span>
d_model = <span class="hljs-number">128</span>
dff = d_model * <span class="hljs-number">4</span>
num_heads = <span class="hljs-number">8</span>
</code></pre>
    <p class="normal">For comparison, the BERT base model contains the following parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># BERT Base Model</span>
<span class="hljs-comment"># num_layers = 12</span>
<span class="hljs-comment"># d_model = 768</span>
<span class="hljs-comment"># dff = d_model * 4</span>
<span class="hljs-comment"># num_heads = 12</span>
</code></pre>
    <p class="normal">These settings are available in the file but commented out. Using these settings slows down training and requires a large amount of GPU memory. A couple of other parameters need to be set up and the Transformer instantiated:</p>
    <pre class="programlisting code"><code class="hljs-code">target_vocab_size = cap_tokenizer.vocab_size  
<span class="hljs-comment"># already includes start/end tokens</span>
dropout_rate = <span class="hljs-number">0.1</span>
EPOCHS = <span class="hljs-number">20</span>  <span class="hljs-comment"># should see results in 4-10 epochs also</span>
transformer = vt.Transformer(num_layers, d_model, num_heads, dff,
                             target_vocab_size,
                             pe_input=<span class="hljs-number">49</span>,  <span class="hljs-comment"># 7x7 pixels</span>
                             pe_target=target_vocab_size,
                             rate=dropout_rate,
                             use_pe=<span class="hljs-literal">False</span>
                             )
</code></pre>
    <p class="normal">This model contains over 4 million<a id="_idIndexMarker579"/> trainable parameters. It is a smaller model than we have seen previously:</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "transformer"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
visual_encoder (VisualEncode multiple                  1055360   
_________________________________________________________________
decoder (Decoder)            multiple                  2108544   
_________________________________________________________________
dense_65 (Dense)             multiple                  1058445   
=================================================================
Total params: 4,222,349
Trainable params: 4,222,349
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">However, the model summary is not available since the input dimensions have not yet been supplied. The summary will be available once we've run a training example through the model.</p>
    <p class="normal">A custom learning rate schedule is created for training the model. A custom learning rate schedule anneals or reduces the learning rate as the model improves its accuracy, resulting in better accuracy. This process is called learning rate decay or learning rate annealing and was discussed in detail in <em class="chapterRef">Chapter 5</em>, <em class="italic">Generating Text with RNNs and GPT-2</em>.</p>
    <h2 id="_idParaDest-135" class="title">Custom learning rate schedule</h2>
    <p class="normal">This rate schedule is identical to the<a id="_idIndexMarker580"/> one proposed in the <em class="italic">Attention Is All You Need</em> paper:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">CustomSchedule</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.optimizers.schedules.LearningRateSchedule</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, d_model, warmup_steps=</span><span class="hljs-number">4000</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(CustomSchedule, self).__init__()
        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)
        self.warmup_steps = warmup_steps
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__call__</span><span class="hljs-functio">(</span><span class="hljs-params">self, step</span><span class="hljs-functio">):</span>
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** <span class="hljs-number">-1.5</span>)
        <span class="hljs-keyword">return</span> tf.math.rsqrt(self.d_model) * \
                tf.math.minimum(arg1, arg2)
learning_rate = CustomSchedule(d_model)
optimizer = tf.keras.optimizers.Adam(learning_rate, 
                                     beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.98</span>,
                                     epsilon=<span class="hljs-number">1e-9</span>)
</code></pre>
    <p class="normal">The following graph shows the learning schedule:</p>
    <figure class="mediaobject"><img src="image/B16252_07_13.png" alt="A close up of a person  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.13: Custom learning rate schedule</p>
    <p class="normal">When training starts, a higher learning rate is used as the loss is high. As the model learns more and more, the<a id="_idIndexMarker581"/> loss starts decreasing, which requires a lower learning rate. Using the preceding learning rate schedule significantly speeds up training and convergence. We also need a loss function to optimize.</p>
    <h2 id="_idParaDest-136" class="title">Loss and metrics</h2>
    <p class="normal">The loss function is <a id="_idIndexMarker582"/>based on categorical cross-entropy. It is a common loss function that we have used in previous chapters. In addition<a id="_idIndexMarker583"/> to the loss, an accuracy metric is also defined to track how the model is doing on the training set:</p>
    <pre class="programlisting code"><code class="hljs-code">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
                              from_logits=<span class="hljs-literal">True</span>, reduction=<span class="hljs-string">'none'</span>)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">loss_function</span><span class="hljs-functio">(</span><span class="hljs-params">real, pred</span><span class="hljs-functio">):</span>
    mask = tf.math.logical_not(tf.math.equal(real, <span class="hljs-number">0</span>))
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    <span class="hljs-keyword">return</span> tf.reduce_sum(loss_) / tf.reduce_sum(mask)
train_loss = tf.keras.metrics.Mean(name=<span class="hljs-string">'train_loss'</span>)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
                        name=<span class="hljs-string">'train_accuracy'</span>)
</code></pre>
    <p class="normal">This formulation has been used in previous chapters as well. We are almost ready to start training. There are two <a id="_idIndexMarker584"/>more steps we must follow before we get into the custom training function. We need to set up<a id="_idIndexMarker585"/> checkpoints to save progress in case of failures, and we also need to mask inputs for the Encoder and Decoder.</p>
    <h2 id="_idParaDest-137" class="title">Checkpoints and masks</h2>
    <p class="normal">We need to specify a <a id="_idIndexMarker586"/>checkpoint directory for TensorFlow to save progress. We will use a <code class="Code-In-Text--PACKT-">CheckpointManager</code> here, which automatically manages the checkpoints and stores a limited number of them. A<a id="_idIndexMarker587"/> checkpoint can be quite large. Five checkpoints for the small model would take up approximately 243 MB of space. Larger models would take up more space:</p>
    <pre class="programlisting code"><code class="hljs-code">checkpoint_path = <span class="hljs-string">"./checkpoints/train-small-model-40ep"</span>
ckpt = tf.train.Checkpoint(transformer=transformer,
                           optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, 
                           max_to_keep=<span class="hljs-number">5</span>)
<span class="hljs-comment"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="hljs-keyword">if</span> ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Latest checkpoint restored!!'</span>)
</code></pre>
    <p class="normal">Next, a method that will create masks for the input images and captions must be defined:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">create_masks</span><span class="hljs-functio">(</span><span class="hljs-params">inp, tar</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># Encoder padding mask - This should just be 1's</span>
    <span class="hljs-comment"># input shape should be (batch_size, 49, 2048)</span>
    inp_seq = tf.ones([inp.shape[<span class="hljs-number">0</span>], inp.shape[<span class="hljs-number">1</span>]])  
    enc_padding_mask = vt.create_padding_mask(inp_seq)
    <span class="hljs-comment"># Used in the 2nd attention block in the Decoder.</span>
    <span class="hljs-comment"># This padding mask is used to mask the encoder outputs.</span>
    dec_padding_mask = vt.create_padding_mask(inp_seq)
    <span class="hljs-comment"># Used in the 1st attention block in the Decoder.</span>
    <span class="hljs-comment"># It is used to pad and mask future tokens in the input </span>
    <span class="hljs-comment"># received by the decoder.</span>
    look_ahead_mask = vt.create_look_ahead_mask(tf.shape(tar)[<span class="hljs-number">1</span>])
    dec_target_padding_mask = vt.create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, 
                                  look_ahead_mask)
    <span class="hljs-keyword">return</span> enc_padding_mask, combined_mask, dec_padding_mask
</code></pre>
    <p class="normal">Inputs are always a <a id="_idIndexMarker588"/>constant length, so the input sequence is set as ones. Only the captions, which are used by the Decoder, are masked. There are two types of masks for the Decoder. The first mask is the padding mask. Since the captions are set to the maximum length to handle 99% of the captions, which works out at about 22 tokens, any captions that are smaller than this number of tokens have padding <a id="_idIndexMarker589"/>appended to the end of them. The padding mask helps separate caption tokens from padding tokens. The second mask is the look-ahead mask. It prevents the Decoder from seeing tokens from the future or tokens it has not generated yet. Now, we are ready to train the model.</p>
    <h2 id="_idParaDest-138" class="title">Custom training</h2>
    <p class="normal">Similar to the summarization <a id="_idIndexMarker590"/>model, teacher forcing will be used for training. Consequently, a custom training function will be used. First, we must define a function that will train on one batch of data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">train_step</span><span class="hljs-functio">(</span><span class="hljs-params">inp, tar</span><span class="hljs-functio">):</span>
    tar_inp = tar[:, :<span class="hljs-number">-1</span>]
    tar_real = tar[:, <span class="hljs-number">1</span>:]
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        predictions, _ = transformer(inp, tar_inp,
                                     <span class="hljs-literal">True</span>,
                                     enc_padding_mask,
                                     combined_mask,
                                     dec_padding_mask)
        loss = loss_function(tar_real, predictions)
    gradients = tape.gradient(loss, 
                                transformer.trainable_variables)
    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, 
                                   transformer.trainable_variables))
    train_loss(loss)
    train_accuracy(tar_real, predictions)
</code></pre>
    <p class="normal">This method is very similar to the <a id="_idIndexMarker591"/>summarization training code. All we need to do now is define the number of epochs and batch size and start training:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># setup training parameters</span>
BUFFER_SIZE = <span class="hljs-number">1000</span>
BATCH_SIZE = <span class="hljs-number">64</span>  <span class="hljs-comment"># can +/- depending on GPU capacity</span>
<span class="hljs-comment"># Shuffle and batch</span>
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
<span class="hljs-comment"># Begin Training</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):
    start_tm = time.time()
    train_loss.reset_states()
    train_accuracy.reset_states()
    <span class="hljs-comment"># inp -&gt; images, tar -&gt; caption</span>
    <span class="hljs-keyword">for</span> (batch, (inp, tar)) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataset):
        train_step(inp, tar)
        <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            ts = datetime.datetime.now().strftime(
                                      <span class="hljs-string">"%d-%b-%Y (%H:%M:%S)"</span>)
            print(<span class="hljs-string">'[{}] Epoch {} Batch {} Loss {:.6f} Accuracy'</span>+\ 
                   <span class="hljs-string">'{:.6f}'</span>.<span class="hljs-built_in">format</span>(ts, epoch + <span class="hljs-number">1</span>, batch,
                                   train_loss.result(),
                                   train_accuracy.result()))
    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:
        ckpt_save_path = ckpt_manager.save()
        print(<span class="hljs-string">'Saving checkpoint for epoch {} at {}'</span>.<span class="hljs-built_in">format</span>(
                               epoch + <span class="hljs-number">1</span>,
                               ckpt_save_path))
    print(<span class="hljs-string">'Epoch {} Loss {:.6f} Accuracy {:.6f}'</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>,
                                       train_loss.result(),
                                       train_accuracy.result()))
    print(<span class="hljs-string">'Time taken for 1 epoch: {} secs\n'</span>.<span class="hljs-built_in">format</span>(
                                     time.time() - start_tm))
</code></pre>
    <p class="normal">Training can be started from the command line:</p>
    <pre class="programlisting con"><code class="hljs-con">(tf24nlp) $ python caption-training.py
</code></pre>
    <p class="normal">This training may take<a id="_idIndexMarker592"/> some time. An epoch of training takes about 11 minutes on my GPU-enabled machine. If you contrast this to the summarization model, this model is training extremely fast. Compared to the summarization model, which contains 13 million parameters, it is much smaller and trains very fast. This speed boost is due to the lack of recurrence.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The state-of-the-art summarization models use the Transformer architecture along with subword encoding. Given that you have all the pieces of the Transformer, a good exercise to test your understanding would be editing the VisualEncoder to process text and rebuild the summarization model as a Transformer. You will then be able to experience these speedup and accuracy improvements.</p>
    </div>
    <p class="normal">A longer training time<a id="_idIndexMarker593"/> allows the model to learn better. However, this model can give reasonable results in as few as 5-10 epochs of training. Once training is complete, we can try the model on some images.</p>
    <h1 id="_idParaDest-139" class="title">Generating captions</h1>
    <p class="normal">First, you need to be congratulated! You made it through a whirlwind implementation of the Transformer. I am sure you must have<a id="_idIndexMarker594"/> noticed a number of common building blocks that were used in previous chapters. Since the Transformer model is complex, we left it for this chapter to look at other techniques like Bahdanau attention, custom layers, custom rate schedules, custom training using teacher forcing, and checkpointing so that we could cover a lot of ground quickly in this chapter. You should consider all these building blocks an important part of your toolkit when you try and solve an NLP problem.</p>
    <p class="normal">Without further ado, let's try and caption some images. Again, we will use a Jupyter notebook for inference so that we can quickly try out different images. All the code for inference is in the <code class="Code-In-Text--PACKT-">image-captioning-inference.ipynb</code> file.</p>
    <p class="normal">The inference code needs to load the Subword Encoder, set up masking, instantiate a ResNet50 model to extract features from test images, and generate captions a token at a time until the end of the sequence or a maximum sequence length is reached. Let's go over these steps one at a time.</p>
    <p class="normal">Once we've done the appropriate imports and optionally initialized the GPU, we can load the Subword Encoder that was saved when we pre-processed the data:</p>
    <pre class="programlisting code"><code class="hljs-code">cap_tokenizer = tfds.features.text.SubwordTextEncoder.load_from_file(<span class="hljs-string">"captions"</span>)
</code></pre>
    <p class="normal">We must now instantiate the Transformer model. This is an important step to ensure the parameters are the same as the checkpoint ones:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Small Model</span>
num_layers = <span class="hljs-number">4</span>
d_model = <span class="hljs-number">128</span>
dff = d_model * <span class="hljs-number">4</span>
num_heads = <span class="hljs-number">8</span>
target_vocab_size = cap_tokenizer.vocab_size  <span class="hljs-comment"># already includes </span>
<span class="hljs-comment">                                              # start/end tokens</span>
dropout_rate = <span class="hljs-number">0.</span> <span class="hljs-comment"># immaterial during inference</span>
transformer = vt.Transformer(num_layers, d_model, num_heads, dff,
                             target_vocab_size,
                             pe_input=<span class="hljs-number">49</span>,  <span class="hljs-comment"># 7x7 pixels</span>
                             pe_target=target_vocab_size,
                             rate=dropout_rate
                             )
</code></pre>
    <p class="normal">Restoring the model from the checkpoint requires the optimizer, even though we are not training the model. So, we will reuse the custom scheduler from the training code. As this code was provided <a id="_idIndexMarker595"/>previously, it has been omitted here. For the checkpoint, I used a model that was trained for 40 epochs, but without positional encoding in the Encoder:</p>
    <pre class="programlisting code"><code class="hljs-code">checkpoint_path = <span class="hljs-string">"./checkpoints/train-small-model-nope-40ep"</span>
ckpt = tf.train.Checkpoint(transformer=transformer,
                           optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, 
                                            max_to_keep=<span class="hljs-number">5</span>)
<span class="hljs-comment"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="hljs-keyword">if</span> ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Latest checkpoint restored!!'</span>)
</code></pre>
    <p class="normal">Finally, we must set up the masking function for the generated captions. Note that the look ahead masks don't really help during inference as future tokens have not been generated yet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Helper function for creating masks</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">create_masks</span><span class="hljs-functio">(</span><span class="hljs-params">inp, tar</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># Encoder padding mask - This should just be 1's</span>
    <span class="hljs-comment"># input shape should be (batch_size, 49, 2048)</span>
    inp_seq = tf.ones([inp.shape[<span class="hljs-number">0</span>], inp.shape[<span class="hljs-number">1</span>]])  
    enc_padding_mask = vt.create_padding_mask(inp_seq)
    <span class="hljs-comment"># Used in the 2nd attention block in the Decoder.</span>
    <span class="hljs-comment"># This padding mask is used to mask the encoder outputs.</span>
    dec_padding_mask = vt.create_padding_mask(inp_seq)
    <span class="hljs-comment"># Used in the 1st attention block in the Decoder.</span>
    <span class="hljs-comment"># It is used to pad and mask future tokens in the input received by</span>
    <span class="hljs-comment"># the decoder.</span>
    look_ahead_mask = vt.create_look_ahead_mask(tf.shape(tar)[<span class="hljs-number">1</span>])
    dec_target_padding_mask = vt.create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, 
                                 look_ahead_mask)
    <span class="hljs-keyword">return</span> enc_padding_mask, combined_mask, dec_padding_mask
</code></pre>
    <p class="normal">The main code for inference is in an <code class="Code-In-Text--PACKT-">evaluate()</code> function. This method takes in the image features generated by ResNet50 as input and seeds the output caption sequence with the start token. Then, it<a id="_idIndexMarker596"/> runs in a loop to generate a token at a time while updating the masks, until an end of sequence token is encountered or the maximum length of the caption is reached:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">evaluate</span><span class="hljs-functio">(</span><span class="hljs-params">inp_img, max_len=</span><span class="hljs-number">21</span><span class="hljs-functio">):</span>
    start_token = cap_tokenizer.encode(<span class="hljs-string">"&lt;s&gt;"</span>)[<span class="hljs-number">0</span>]
    end_token = cap_tokenizer.encode(<span class="hljs-string">"&lt;/s&gt;"</span>)[<span class="hljs-number">0</span>]
    
    encoder_input = inp_img <span class="hljs-comment"># batch of 1</span>
    
    <span class="hljs-comment"># start token for caption</span>
    decoder_input = [start_token]
    output = tf.expand_dims(decoder_input, <span class="hljs-number">0</span>)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_len):
        enc_padding_mask, combined_mask, dec_padding_mask = \
                create_masks(encoder_input, output)
        
        <span class="hljs-comment"># predictions.shape == (batch_size, seq_len, vocab_size)</span>
        predictions, attention_weights = transformer(
                                               encoder_input, 
                                               output,
                                               <span class="hljs-literal">False</span>,
                                               enc_padding_mask,
                                               combined_mask,
                                               dec_padding_mask)
        <span class="hljs-comment"># select the last word from the seq_len dimension</span>
        predictions = predictions[: ,<span class="hljs-number">-1</span>:, :]  
        predicted_id = tf.cast(tf.argmax(predictions, axis=<span class="hljs-number">-1</span>), 
                                  tf.int32)
        
        <span class="hljs-comment"># return the result if predicted_id is equal to end token</span>
        <span class="hljs-keyword">if</span> predicted_id == end_token:
            <span class="hljs-keyword">return</span> tf.squeeze(output, axis=<span class="hljs-number">0</span>), attention_weights
        
        <span class="hljs-comment"># concatenate the predicted_id to the output which is </span>
        <span class="hljs-comment"># given to the decoder  as its input.</span>
        output = tf.concat([output, predicted_id], axis=<span class="hljs-number">-1</span>)
    <span class="hljs-keyword">return</span> tf.squeeze(output, axis=<span class="hljs-number">0</span>), attention_weights
</code></pre>
    <p class="normal">A wrapper method is used to call the evaluation method and print out the caption:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">caption</span><span class="hljs-functio">(</span><span class="hljs-params">image</span><span class="hljs-functio">):</span>
    end_token = cap_tokenizer.encode(<span class="hljs-string">"&lt;/s&gt;"</span>)[<span class="hljs-number">0</span>]
    result, attention_weights = evaluate(image)
    
    predicted_sentence = cap_tokenizer.decode([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> result 
                                              <span class="hljs-keyword">if</span> i &gt; end_token])
    print(<span class="hljs-string">'Predicted Caption: {}'</span>.<span class="hljs-built_in">format</span>(predicted_sentence))
</code></pre>
    <p class="normal">The only thing remaining now is<a id="_idIndexMarker597"/> instantiating a ResNet50 model to extract features from image files on the fly:</p>
    <pre class="programlisting code"><code class="hljs-code">rs50 = tf.keras.applications.ResNet50(
    include_top=<span class="hljs-literal">False</span>,
    weights=<span class="hljs-string">"imagenet"</span>,  <span class="hljs-comment"># no pooling</span>
    input_shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>)
)
new_input = rs50.<span class="hljs-built_in">input</span>
hidden_layer = rs50.layers[<span class="hljs-number">-1</span>].output
features_extract = tf.keras.Model(new_input, hidden_layer)
</code></pre>
    <p class="normal">It's the moment of truth, finally! Let's try out the model on an image. We will load the image, pre-process it for <a id="_idIndexMarker598"/>ResNet50, and extract the features from it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># from keras</span>
image = load_img(<span class="hljs-string">"./beach-surf.jpg"</span>, target_size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)) 
image = img_to_array(image)
image = np.expand_dims(image, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># batch of one</span>
image = preprocess_input(image)  <span class="hljs-comment"># from resnet</span>
eval_img = features_extract.predict(image)
caption(eval_img)
</code></pre>
    <p class="normal">The following is the example image and its caption:</p>
    <figure class="mediaobject"><img src="image/B16252_07_14.png" alt="A person riding a wave on a surfboard in the ocean  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.14: Generated caption - A man is riding a surfboard on a wave</p>
    <p class="normal">This looks like an amazing caption for the given image! However, the overall accuracy of the model is in the low 30s. There is a lot of scope for improvement in the model. The next section talks about the state-of-the-art techniques for image captioning and also proposes some simpler ideas that you can try and play around with.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that you may see slightly different results. The reviewer for this book got the result <em class="italic">A man in a black shirt is riding a surfboard</em> while running this code. This is expected as slight differences in the probabilities and the exact place where the model stops training in the loss surface is not exact. We are operating in the probabilistic realm here, so there may be slight differences. You may have experienced similar differences in the text generation and summarization code in the previous chapters as well.</p>
    </div>
    <p class="normal">The following image shows <a id="_idIndexMarker599"/>some more examples of images and their captions. The notebook contains several good, as well as some atrocious, examples of the generated labels:</p>
    <figure class="mediaobject"><img src="image/B16252_07_15.png" alt="A picture containing photo, different, various, group  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.15: Examples of images and their generated captions</p>
    <p class="normal">None of these images were in the training set. The caption quality goes down from top to bottom. Our model understands close up, cake, groups of people, sandy beaches, streets, and luggage, among other things. However, the bottom two examples are concerning. They hint at some <strong class="keyword">bias</strong> in the model. In both of the bottom two images, the model is misinterpreting gender. </p>
    <p class="normal">The images were deliberately chosen to show a woman in a business suit and women playing basketball. In both cases, the model proposes men in the captions. When the model was tried with a female tennis player's image, it guessed the right gender, but it changed genders in an image from a women's soccer game. Bias in models is a very important concern. In cases such as image captioning, this bias<a id="_idIndexMarker600"/> is immediately apparent. In fact, over 600,000 images were removed from the ImageNet database (<a href="https://bit.ly/3qk4FgN"><span class="url">https://bit.ly/3qk4FgN</span></a>) in 2019 after bias was found in how it classifies and tags people in its pictures. ResNet50 is pre-trained on ImageNet. However, in other models, the bias may be harder to detect. Building fair deep learning models and reducing bias in models are active areas of research in the ML community.</p>
    <p class="normal">You may have noticed that we skipped running the model on an evaluation set and on the test set. This was done for brevity, and also because those techniques were covered previously.</p>
    <p class="normal">A quick note on metrics for evaluating the quality of captions. We saw ROUGE metrics in the previous chapters. ROUGE-L is still applicable in the case of image captioning. You can use a mental model of the caption as a summary of an image, as opposed to the summary of a paragraph in text summarization. There can be more than one way to express the summary, and ROUGE-L tries to capture the intent. There are two other commonly reported metrics:</p>
    <ul>
      <li class="bullet"><strong class="keyword">BLEU</strong>: This stands for <strong class="keyword">Bilingual Evaluation Understudy</strong> and is the most popular metric in machine<a id="_idIndexMarker601"/> translation. We can cast the image captioning problem as a machine translation problem as well. It relies on n-grams for computing the overlap of the predicted text with a number of reference texts and combines the results into one score.</li>
      <li class="bullet"><strong class="keyword">CIDEr</strong>: This stands for <strong class="keyword">Consensus-Based Image Description Evaluation</strong> and was proposed in a paper by the same name in 2015. It tries to deal with the difficulty of automatic evaluation<a id="_idIndexMarker602"/> when multiple captions could be reasonable by combining TF-IDF and n-grams. The metric tries to compare the captions generated by the model against multiple captions by human annotators and tries to score them based on consensus.</li>
    </ul>
    <p class="normal">Before wrapping up this chapter, let's spend a little time discussing ways to improve performance and state-of-the-art models.</p>
    <h1 id="_idParaDest-140" class="title">Improving performance and state-of-the-art models</h1>
    <p class="normal">Let's first talk through some simple <a id="_idIndexMarker603"/>experiments you can try to improve performance before talking about the latest models. Recall our discussion on positional encodings for inputs in the Encoder. Adding or removing positional encodings helps or hinders performance. In the<a id="_idIndexMarker604"/> previous chapter, we implemented the beam search algorithm for generating summaries. You can adapt the beam search code and see an improvement in the results with beam search. Another avenue of exploration is the ResNet50. We used a pre-trained network and did not fine-tune it further. It is possible to build an architecture where ResNet is part of the architecture and not a pre-processing step. Image files are loaded in, and features are extracted from ResNet50 as part of the VisualEncoder. ResNet50 layers can be trained from the get-go, or only in the last few iterations. This idea is implemented in the <code class="Code-In-Text--PACKT-">resnet-finetuning.py</code> file for you to try. Another line of thinking is using a different object detection model than ResNet50 or using the output from a different layer. You can try a more complex version of ResNet like ResNet152, or a different object detection model like Detectron from Facebook or other models. It should be quite easy to use a different model in our code as it is quite modular.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">When you use a different model for extracting image features, the key will be to make sure tensor dimensions are flowing properly through the Encoder. The Decoder should not require any changes. Depending on the complexity of the model, you can either pre-process and store the image features or compute them on the fly.</p>
    </div>
    <p class="normal">Recall that we just used the pixels from the image directly. This was based on a paper published recently at CVPR titled <em class="italic">Pixel-BERT</em>. Most models use region proposals extracted from images instead of the pixels directly. Object detection in an image involves drawing a boundary around that object in the image. Another way to perform the same task is to classify each pixel into an object or background. These region proposals can be in the form of bounding boxes in an image. State-of-the-art models use bounding boxes or region proposals as input.</p>
    <p class="normal">The second-biggest gain in image captioning comes from pre-training. Recall that BERT and GPT are pre-trained on specific pre-training objectives. Models differ based on whether the Encoder is pre-trained or both the Encoder and Decoder are pre-trained. A common pre-training objective is a version of the BERT MLM task. Recall that BERT inputs are structured as <code class="Code-In-Text--PACKT-">[CLS] I1 I2 … In [SEP] J1 J2 … Jk [SEP]</code>, where some of the tokens from the input sequence are masked. This is adapted for image captioning, where the image features and caption tokens in the input are concatenated. Caption tokens are masked similar to how they are in the BERT model, and the pre-training objective is for the model to predict the masked token. After pre-training, the output of the CLS token can be used for <a id="_idIndexMarker605"/>classification or fed to the Decoder to generate the caption. Care must be exercised to not pre-train on the same dataset, like that for evaluation. An example<a id="_idIndexMarker606"/> of the setup could be using the Visual Genome and Flickr30k datasets for pre-training and COCO for fine-tuning.</p>
    <p class="normal">Image captioning is an active area of research. The research is just getting started on multi-modal networks in general. Now, let's recap everything we've learned in this chapter.</p>
    <h1 id="_idParaDest-141" class="title">Summary</h1>
    <p class="normal">In the world of deep learning, specific architectures have been developed to handle specific modalities. <strong class="keyword">Convolutional Neural Networks</strong> (<strong class="keyword">CNNs</strong>) have been incredibly effective in processing images and is the standard architecture for CV tasks. However, the world of research is moving toward the world of multi-modal networks, which can take multiple types of inputs, like sounds, images, text, and so on and perform cognition like humans. After reviewing multi-modal networks, we dived into vision and language tasks as a specific focus. There are a number of problems in this particular area, including image captioning, visual question answering, VCR, and text-to-image, among others.</p>
    <p class="normal">Building on our learnings from previous chapters on seq2seq architectures, custom TensorFlow layers and models, custom learning schedules, and custom training loops, we implemented a Transformer model from scratch. Transformers are state of the art at the time of writing. We took a quick look at the basic concepts of CNNs to help with the image side of things. We were able to build a model that may not be able to generate a thousand words for a picture but is definitely able to generate a human-readable caption. Its performance still needs improvement, and we discussed a number of possibilities so that we can try to do so, including the latest techniques.</p>
    <p class="normal">It is apparent that deep models perform very well when they contain a lot of data. The BERT and GPT models have shown the value of pre-training on massive amounts of data. It is still very hard to get good quality labeled data for use in pre-training or fine-tuning. In the world of NLP, we have a lot of text data, but not enough labeled data. The next chapter focuses on weak supervision to build classification models that can label data for pre-training or even fine-tuning tasks.</p>
  </div>
</body></html>