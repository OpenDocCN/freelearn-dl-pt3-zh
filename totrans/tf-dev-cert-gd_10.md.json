["```\n    import requests\n    ```", "```\n    from bs4 import BeautifulSoup\n    ```", "```\n    import re\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    ```", "```\n    # Define the URL of the page\n    ```", "```\n    url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n    ```", "```\n    # Send a GET request to the webpage\n    ```", "```\n    response = requests.get(url)\n    ```", "```\n    # Parse the HTML content of the page with BeautifulSoup\n    ```", "```\n    soup = BeautifulSoup(response.content, 'html.parser')\n    ```", "```\n    # Extract the text from all paragraph tags on the page\n    ```", "```\n    passage = \" \".join([\n    ```", "```\n        p.text for p in soup.find_all('p')])\n    ```", "```\n    # Define a simple list of stopwords\n    ```", "```\n    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\",\n    ```", "```\n        \"ours\", \"ourselves\", \"you\", \"your\",\n    ```", "```\n        \"yours\", \"yourself\", \"yourselves\", \"he\",\n    ```", "```\n        \"him\", \"his\", \"himself\", \"she\", \"her\",\n    ```", "```\n        \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n    ```", "```\n        \"they\", \"them\", \"their\", \"theirs\",\n    ```", "```\n        \"themselves\", \"what\", \"which\", \"who\",\n    ```", "```\n        \"whom\", \"this\", \"that\", \"these\", \"those\",\n    ```", "```\n        \"am\", \"is\", \"are\", \"was\", \"were\", \"be\",\n    ```", "```\n        \"been\", \"being\", \"have\", \"has\", \"had\",\n    ```", "```\n        \"having\", \"do\", \"does\", \"did\", \"doing\",\n    ```", "```\n        \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\n    ```", "```\n        \"or\", \"because\", \"as\", \"until\", \"while\",\n    ```", "```\n        \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n    ```", "```\n        \"against\", \"between\", \"into\", \"through\",\n    ```", "```\n        \"during\", \"before\", \"after\", \"above\",\n    ```", "```\n        \"below\", \"to\", \"from\", \"up\", \"down\",\n    ```", "```\n        \"in\", \"out\", \"on\", \"off\", \"over\",\n    ```", "```\n        \"under\", \"again\", \"further\", \"then\",\n    ```", "```\n        \"once\", \"here\", \"there\", \"when\", \"where\",\n    ```", "```\n        \"why\", \"how\", \"all\", \"any\", \"both\",\n    ```", "```\n        \"each\", \"few\", \"more\", \"most\", \"other\",\n    ```", "```\n        \"some\", \"such\", \"no\", \"nor\", \"not\",\n    ```", "```\n        \"only\", \"own\", \"same\", \"so\", \"than\",\n    ```", "```\n        \"too\", \"very\", \"s\", \"t\", \"can\", \"will\",\n    ```", "```\n        \"just\", \"don\", \"should\", \"now\"]\n    ```", "```\n    passage = passage.lower()\n    ```", "```\n    # Remove HTML tags using regex\n    ```", "```\n    passage = re.sub(r'<[^>]+>', '', passage)\n    ```", "```\n    # Remove unwanted special characters\n    ```", "```\n    passage = re.sub('[^a-zA-Z\\s]', '', passage)\n    ```", "```\n    # Remove stopwords\n    ```", "```\n    passage = ' '.join(word for word in passage.split() if word not in stopwords)\n    ```", "```\n    # Print the cleaned passage\n    ```", "```\n    print(passage[:500])  # print only first 500 characters for brevity\n    ```", "```\n    machine learning ml field devoted understanding building methods let machines learn methods leverage data improve computer performance set tasks machine learning algorithms build model based sample data known training data order make predictions decisions without explicitly programmed machine learning algorithms used wide variety applications medicine email filtering speech recognition agriculture computer vision difficult unfeasible develop conventional algorithms perform needed tasks subset ma\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    ```", "```\n    text = \"Machine learning is fascinating. It is a field full of challenges!\"\n    ```", "```\n    # Define the tokenizer and fit it on the text\n    ```", "```\n    tokenizer = Tokenizer()\n    ```", "```\n    tokenizer.fit_on_texts([text])\n    ```", "```\n    # Print out the word index to see how words are tokenized\n    ```", "```\n    print(tokenizer.word_index)\n    ```", "```\n    {'is': 1, 'machine': 2, 'learning': 3, 'fascinating': 4, 'it': 5, 'a': 6, 'field': 7, 'full': 8, 'of': 9, 'challenges': 10}\n    ```", "```\n# Define the tokenizer and fit it on the text\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts([text])\n# Print out the character index to see how characters are tokenized\nprint(tokenizer.word_index)\n```", "```\n{' ': 1, 'i': 2, 'a': 3, 'n': 4, 'l': 5, 'e': 6, 's': 7,\n    'f': 8, 'c': 9, 'g': 10, 'h': 11, 't': 12, 'm': 13,\n    'r': 14, '.': 15, 'd': 16, 'u': 17, 'o': 18, '!': 19}\n```", "```\n# Convert the text to sequences\nsequence = tokenizer.texts_to_sequences([text])\nprint(sequence)\n```", "```\n[[2, 3, 1, 4, 5, 1, 6, 7, 8, 9, 10]]\n```", "```\n    sentences = [\n    ```", "```\n        \"I love reading books.\",\n    ```", "```\n        \"The cat sat on the mat.\",\n    ```", "```\n        \"It's a beautiful day outside!\",\n    ```", "```\n        \"Have you done your homework?\"\n    ```", "```\n    ]\n    ```", "```\n    [[2, 3, 4, 5], [1, 6, 7, 8, 1, 9],\n    ```", "```\n        [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]\n    ```", "```\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    ```", "```\n    padded = pad_sequences(sequences)\n    ```", "```\n    print(padded)\n    ```", "```\n    [[ 0  0  2  3  4  5]\n    ```", "```\n     [ 1  6  7  8  1  9]\n    ```", "```\n     [ 0 10 11 12 13 14]\n    ```", "```\n     [ 0 15 16 17 18 19]]\n    ```", "```\n    # post padding\n    ```", "```\n    padded_sequences = pad_sequences(sequences,\n    ```", "```\n        padding='post')\n    ```", "```\n    print(padded_sequences)\n    ```", "```\n    [[ 2  3  4  5  0  0]\n    ```", "```\n     [ 1  6  7  8  1  9]\n    ```", "```\n     [10 11 12 13 14  0]\n    ```", "```\n     [15 16 17 18 19  0]]\n    ```", "```\n    Sentences = [\n    ```", "```\n        \"I love reading books.\",\n    ```", "```\n        \"The cat sat on the mat.\",\n    ```", "```\n        \"It's a beautiful day outside!\",\n    ```", "```\n        \"Have you done your homework?\",\n    ```", "```\n        \"Machine Learning is a very interesting subject that enables \n    ```", "```\n         you build amazing solutions beyond your imagination.\"\n    ```", "```\n    ]\n    ```", "```\n    [[ 5  6  7  8  0  0  0  0  0  0  0  0  0  0  0  0]\n    ```", "```\n     [ 1  9 10 11  1 12  0  0  0  0  0  0  0  0  0  0]\n    ```", "```\n     [13  2 14 15 16  0  0  0  0  0  0  0  0  0  0  0]\n    ```", "```\n     [17  3 18  4 19  0  0  0  0  0  0  0  0  0  0  0]\n    ```", "```\n     [20 21 22  2 23 24 25 26 27  3 28 29 30 31  4 32]]\n    ```", "```\n    # Define the max length\n    ```", "```\n    max_length = 10\n    ```", "```\n    # Pad the sequences\n    ```", "```\n    padded = pad_sequences(sequences, padding='post',\n    ```", "```\n        maxlen=max_length)\n    ```", "```\n    print(padded))\n    ```", "```\n    [[ 5  6  7  8  0  0  0  0  0  0]\n    ```", "```\n     [ 1  9 10 11  1 12  0  0  0  0]\n    ```", "```\n     [13  2 14 15 16  0  0  0  0  0]\n    ```", "```\n     [17  3 18  4 19  0  0  0  0  0]\n    ```", "```\n     [25 26 27  3 28 29 30 31  4 32]]\n    ```", "```\n    # Pad the sequences\n    ```", "```\n    padded = pad_sequences(sequences, padding='post',\n    ```", "```\n        truncating='post', maxlen=max_length)\n    ```", "```\n    print(padded)\n    ```", "```\n    [[ 5  6  7  8  0  0  0  0  0  0]\n    ```", "```\n     [ 1  9 10 11  1 12  0  0  0  0]\n    ```", "```\n     [13  2 14 15 16  0  0  0  0  0]\n    ```", "```\n     [17  3 18  4 19  0  0  0  0  0]\n    ```", "```\n     [20 21 22  2 23 24 25 26 27  3]]\n    ```", "```\n# Define the tokenizer with an OOV token\ntokenizer = Tokenizer(oov_token=\"<OOV>\")\n# Fit the tokenizer on the texts\ntokenizer.fit_on_texts(sentences)\n# Convert the texts to sequences\nsequences = tokenizer.texts_to_sequences(sentences)\n# Let's look at the word index\nprint(tokenizer.word_index)\n```", "```\n{'<OOV>': 1, 'the': 2, 'a': 3, 'you': 4, 'your': 5, 'i': 6, 'love': 7, 'reading': 8, 'books': 9, 'cat': 10, 'sat': 11, 'on': 12, 'mat': 13, \"it's\": 14, 'beautiful': 15, 'day': 16, 'outside': 17, 'have': 18, 'done': 19, 'homework': 20, 'machine': 21, 'learning': 22, 'is': 23, 'very': 24, 'interesting': 25, 'subject': 26, 'that': 27, 'enables': 28, 'build': 29, 'amazing': 30, 'solutions': 31, 'beyond': 32, 'imagination': 33}\n```", "```\n# Now let's convert a sentence with some OOV words\ntest_sentence = \"I love playing chess\"\ntest_sequence = tokenizer.texts_to_sequences(\n    [test_sentence])\nprint(test_sequence)\n```", "```\n[[6, 7, 1, 1]]\n```", "```\n    import tensorflow as tf\n    ```", "```\n    import tensorflow_datasets as tfds\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    ```", "```\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    ```", "```\n    from sklearn.model_selection import train_test_split\n    ```", "```\n    import numpy as np\n    ```", "```\n    import io\n    ```", "```\n    # Load the Yelp Polarity Reviews dataset\n    ```", "```\n    (train_dataset, test_dataset),\n    ```", "```\n        dataset_info = tfds.load('yelp_polarity_reviews',\n    ```", "```\n            split=['train', 'test'], shuffle_files=True,\n    ```", "```\n            with_info=True, as_supervised=True)\n    ```", "```\n    def get_reviews(dataset, num_samples=5):\n    ```", "```\n        reviews = []\n    ```", "```\n        for text, label in dataset.take(num_samples):\n    ```", "```\n            reviews.append((text.numpy().decode('utf-8'), \n    ```", "```\n                label.numpy()))\n    ```", "```\n        return reviews\n    ```", "```\n    def dataset_insights(dataset, num_samples=2000):\n    ```", "```\n        total_reviews = 0\n    ```", "```\n        total_positive = 0\n    ```", "```\n        total_negative = 0\n    ```", "```\n        total_length = 0\n    ```", "```\n        min_length = float('inf')\n    ```", "```\n        max_length = 0\n    ```", "```\n        for text, label in dataset.take(num_samples):\n    ```", "```\n            total_reviews += 1\n    ```", "```\n            review_length = len(text.numpy().decode(\n    ```", "```\n                'utf-8').split())\n    ```", "```\n            total_length += review_length\n    ```", "```\n            if review_length < min_length:\n    ```", "```\n                min_length = review_length\n    ```", "```\n            if review_length > max_length:\n    ```", "```\n                max_length = review_length\n    ```", "```\n            if label.numpy() == 1:\n    ```", "```\n                total_positive += 1\n    ```", "```\n            else:\n    ```", "```\n                total_negative += 1\n    ```", "```\n        avg_length = total_length / total_reviews\n    ```", "```\n        return min_length, max_length, avg_length,\n    ```", "```\n            total_positive, total_negative\n    ```", "```\n    def plot_reviews(positive, negative):\n    ```", "```\n        labels = ['Positive', 'Negative']\n    ```", "```\n        counts = [positive, negative]\n    ```", "```\n        plt.bar(labels, counts, color=['blue', 'red'])\n    ```", "```\n        plt.xlabel('Review Type')\n    ```", "```\n        plt.ylabel('Count')\n    ```", "```\n        plt.title('Distribution of Reviews')\n    ```", "```\n        plt.show()\n    ```", "```\n    # Check out some reviews\n    ```", "```\n    print(\"Training Set Reviews:\")\n    ```", "```\n    train_reviews = get_reviews(train_dataset, 7)\n    ```", "```\n    for review, label in train_reviews:\n    ```", "```\n        print(f\"Label: {label}, Review: {review[:100]}\")\n    ```", "```\n    Training Set Reviews:\n    ```", "```\n    Label: 1, Review: The Groovy P. and I ventured to his old stomping grounds for lunch today.  The '5 and Diner' on 16th...\n    ```", "```\n    Label: 0, Review: Mediocre burgers - if you are in the area and want a fast food burger, Fatburger is  a better bet th...\n    ```", "```\n    Label: 0, Review: Not at all impressed...our server was not very happy to be there...food was very sub-par and it was ...\n    ```", "```\n    Label: 0, Review: I wish I would have read Megan P's review before I decided to cancel my dinner reservations because ...\n    ```", "```\n    Label: 1, Review: A large selection of food from all over the world. Great atmosphere and ambiance.  Quality of food i...\n    ```", "```\n    Label: 1, Review: I know, I know a review for Subway, come on.  But I have to say that the service at this subway is t...\n    ```", "```\n    Label: 1, Review: We came in for a pre-bachelor party madness meal and I have to say it was one of the best dining exp...\n    ```", "```\n    min_length, max_length, avg_length, total_positive, \n    ```", "```\n        total_negative = dataset_insights(train_dataset)\n    ```", "```\n    # Display the results\n    ```", "```\n    print(f\"Shortest Review Length: {min_length}\")\n    ```", "```\n    print(f\"Longest Review Length: {max_length}\")\n    ```", "```\n    print(f\"Average Review Length: {avg_length:.2f}\")\n    ```", "```\n    print(f\"Total Positive Reviews: {total_positive}\")\n    ```", "```\n    print(f\"Total Negative Reviews: {total_negative}\")\n    ```", "```\n    Shortest Review Length: 1\n    ```", "```\n    Longest Review Length: 942\n    ```", "```\n    Average Review Length: 131.53\n    ```", "```\n    Total Positive Reviews: 1030\n    ```", "```\n    Total Negative Reviews: 970\n    ```", "```\n    plot_reviews(total_positive, total_negative)\n    ```", "```\n    # Define parameters\n    ```", "```\n    vocab_size = 10000\n    ```", "```\n    embedding_dim = 16\n    ```", "```\n    max_length = 132\n    ```", "```\n    trunc_type='post'\n    ```", "```\n    padding_type='post'\n    ```", "```\n    oov_tok = \"<OOV>\"\n    ```", "```\n    num_epochs = 10\n    ```", "```\n    # Build the Tokenizer\n    ```", "```\n    tokenizer = Tokenizer(num_words=vocab_size,\n    ```", "```\n        oov_token=oov_tok)\n    ```", "```\n    # Fetch and decode the training data\n    ```", "```\n    train_text = []\n    ```", "```\n    train_label = []\n    ```", "```\n    for example in train_dataset.take(20000):\n    ```", "```\n        text, label = example\n    ```", "```\n        train_text.append(text.numpy().decode('utf-8'))\n    ```", "```\n        train_label.append(label.numpy())\n    ```", "```\n    # Convert labels to numpy array\n    ```", "```\n    train_labels = np.array(train_label)\n    ```", "```\n    # Fit the tokenizer on the training texts\n    ```", "```\n    tokenizer.fit_on_texts(train_text)\n    ```", "```\n    # Get the word index from the tokenizer\n    ```", "```\n    word_index = tokenizer.word_index\n    ```", "```\n    # Convert texts to sequences\n    ```", "```\n    train_sequences = tokenizer.texts_to_sequences(\n    ```", "```\n        train_text)\n    ```", "```\n    # Fetch and decode the test data\n    ```", "```\n    test_text = []\n    ```", "```\n    test_label = []\n    ```", "```\n    for example in test_dataset.take(8000):\n    ```", "```\n        text, label = example\n    ```", "```\n        test_text.append(text.numpy().decode('utf-8'))\n    ```", "```\n        test_label.append(label.numpy())\n    ```", "```\n    # Convert labels to numpy array\n    ```", "```\n    test_labels = np.array(test_label)\n    ```", "```\n    # Convert texts to sequences\n    ```", "```\n    test_sequences = tokenizer.texts_to_sequences(\n    ```", "```\n        test_text)\n    ```", "```\n    # Pad the sequences\n    ```", "```\n    train_padded = pad_sequences(train_sequences,\n    ```", "```\n        maxlen=max_length, padding=padding_type,\n    ```", "```\n        truncating=trunc_type)\n    ```", "```\n    test_padded = pad_sequences(test_sequences,\n    ```", "```\n        maxlen=max_length, padding=padding_type,\n    ```", "```\n        truncating=trunc_type)\n    ```", "```\n    # Split the data into training and validation sets\n    ```", "```\n    train_padded, val_padded, train_labels,\n    ```", "```\n        val_labels = train_test_split(train_padded,\n    ```", "```\n            train_labels, test_size=0.2, random_state=42)\n    ```", "```\n    # Define the model\n    ```", "```\n    model = tf.keras.Sequential([\n    ```", "```\n        tf.keras.layers.Embedding(vocab_size,\n    ```", "```\n            embedding_dim, input_length=max_length),\n    ```", "```\n        tf.keras.layers.GlobalAveragePooling1D(),\n    ```", "```\n        tf.keras.layers.Dense(24, activation='relu'),\n    ```", "```\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ```", "```\n    # because it's binary classification\n    ```", "```\n    ])\n    ```", "```\n    # Compile the model\n    ```", "```\n    model.compile(loss='binary_crossentropy',\n    ```", "```\n        optimizer='adam', metrics=['accuracy'])\n    ```", "```\n    # Train the model\n    ```", "```\n    history = model.fit(train_padded, train_labels,\n    ```", "```\n        epochs=num_epochs, validation_data=(val_padded,\n    ```", "```\n            val_labels))\n    ```", "```\n    Epoch 6/10\n    ```", "```\n    625/625 [==============================] - 4s 7ms/step - loss: 0.1293 - accuracy: 0.9551 - val_loss: 0.3149 - val_accuracy: 0.8875\n    ```", "```\n    Epoch 7/10\n    ```", "```\n    625/625 [==============================] - 4s 6ms/step - loss: 0.1116 - accuracy: 0.9638 - val_loss: 0.3330 - val_accuracy: 0.8880\n    ```", "```\n    Epoch 8/10\n    ```", "```\n    625/625 [==============================] - 5s 9ms/step - loss: 0.0960 - accuracy: 0.9697 - val_loss: 0.3703 - val_accuracy: 0.8813\n    ```", "```\n    Epoch 9/10\n    ```", "```\n    625/625 [==============================] - 4s 6ms/step - loss: 0.0828 - accuracy: 0.9751 - val_loss: 0.3885 - val_accuracy: 0.8796\n    ```", "```\n    Epoch 10/10\n    ```", "```\n    625/625 [==============================] - 4s 6ms/step - loss: 0.0727 - accuracy: 0.9786 - val_loss: 0.4258 - val_accuracy: 0.8783\n    ```", "```\n    # Evaluate the model on the test set\n    ```", "```\n    results = model.evaluate(test_padded, test_labels)\n    ```", "```\n    print(\"Test Loss: \", results[0])\n    ```", "```\n    print(\"Test Accuracy: \", results[1])\n    ```", "```\n    def plot_history(history):\n    ```", "```\n        plt.figure(figsize=(12, 4))\n    ```", "```\n        # Plot training & validation accuracy values\n    ```", "```\n        plt.subplot(1, 2, 1)\n    ```", "```\n        plt.plot(history.history['accuracy'])\n    ```", "```\n        plt.plot(history.history['val_accuracy'])\n    ```", "```\n        plt.title('Model accuracy')\n    ```", "```\n        plt.ylabel('Accuracy')\n    ```", "```\n        plt.xlabel('Epoch')\n    ```", "```\n        plt.legend(['Train', 'Validation'],\n    ```", "```\n            loc='upper left')\n    ```", "```\n        # Plot training & validation loss values\n    ```", "```\n        plt.subplot(1, 2, 2)\n    ```", "```\n        plt.plot(history.history['loss'])\n    ```", "```\n        plt.plot(history.history['val_loss'])\n    ```", "```\n        plt.title('Model loss')\n    ```", "```\n        plt.ylabel('Loss')\n    ```", "```\n        plt.xlabel('Epoch')\n    ```", "```\n        plt.legend(['Train', 'Validation'],\n    ```", "```\n            loc='upper right')\n    ```", "```\n        plt.tight_layout()\n    ```", "```\n        plt.show()\n    ```", "```\n    # New sentences\n    ```", "```\n    new_sentences = [\"The restaurant was absolutely fantastic. The staff were kind and the food was delicious.\",  # positive\n    ```", "```\n        \"I've had an incredible day at the beach, the weather was beautiful.\",  # positive\n    ```", "```\n        \"The movie was a big disappointment. I wouldn't recommend it to anyone.\",  # negative\n    ```", "```\n        \"I bought a new phone and it stopped working after a week. Terrible product.\"]  # negative\n    ```", "```\n    # Preprocess the sentences in the same way as the training data\n    ```", "```\n    new_sequences = tokenizer.texts_to_sequences(\n    ```", "```\n        new_sentences)\n    ```", "```\n    new_padded = pad_sequences(new_sequences,\n    ```", "```\n        maxlen=max_length, padding=padding_type,\n    ```", "```\n        truncating=trunc_type)\n    ```", "```\n    # Use the model to predict the sentiment of the new sentences\n    ```", "```\n    predictions = model.predict(new_padded)\n    ```", "```\n    # Print out the sequences and the corresponding predictions\n    ```", "```\n    for i in range(len(new_sentences)):\n    ```", "```\n        print(\"Sequence:\", new_sequences[i])\n    ```", "```\n        print(\"Predicted sentiment (\n    ```", "```\n            probability):\", predictions[i])\n    ```", "```\n        if predictions[i] > 0.5:\n    ```", "```\n            print(\"Interpretation: Positive sentiment\")\n    ```", "```\n        else:\n    ```", "```\n            print(\"Interpretation: Negative sentiment\")\n    ```", "```\n        print(\"\\n\")\n    ```", "```\n    1/1 [==============================] - 0s 21ms/step\n    ```", "```\n    Sequence: [2, 107, 7, 487, 533, 2, 123, 27, 290, 3, 2, 31, 7, 182]\n    ```", "```\n    Predicted sentiment (probability): [0.9689689]\n    ```", "```\n    Interpretation: Positive sentiment\n    ```", "```\n    Sequence: [112, 25, 60, 1251, 151, 26, 2, 3177, 2, 2079, 7, 634]\n    ```", "```\n    Predicted sentiment (probability): [0.9956489]\n    ```", "```\n    Interpretation: Positive sentiment\n    ```", "```\n    Sequence: [2, 1050, 7, 6, 221, 1174, 4, 454, 234, 9, 5, 528]\n    ```", "```\n    Predicted sentiment (probability): [0.43672907]\n    ```", "```\n    Interpretation: Negative sentiment\n    ```", "```\n    Sequence: [4, 764, 6, 161, 483, 3, 9, 695, 524, 83, 6, 393, 464, 1341]\n    ```", "```\n    Predicted sentiment (probability): [0.36306405]\n    ```", "```\n    Interpretation: Negative sentiment\n    ```", "```\n    weights = model.get_layer(\n    ```", "```\n        'embedding').get_weights()[0]\n    ```", "```\n    vocab = tokenizer.word_index\n    ```", "```\n    print(weights.shape)\n    ```", "```\n    # shape: (vocab_size, embedding_dim)\n    ```", "```\n    out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n    ```", "```\n    out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n    ```", "```\n    for word, index in vocab.items():\n    ```", "```\n        if index < vocab_size:\n    ```", "```\n            vec = weights[index]\n    ```", "```\n            out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n    ```", "```\n            out_m.write(word + \"\\n\")\n    ```", "```\n    out_v.close()\n    ```", "```\n    out_m.close()\n    ```", "```\n    try:\n    ```", "```\n        from google.colab import files\n    ```", "```\n        files.download('vectors.tsv')\n    ```", "```\n        files.download('metadata.tsv')\n    ```", "```\n    except Exception:\n    ```", "```\n        pass\n    ```", "```\n# Increasing the vocab_size\nvocab_size = 10000 #Change from 10000 to 20000\nembedding_dim = 16\ntraining_size = 20000\nnum_epochs=10\nmodel_1, history_1 = sentiment_model(vocab_size,\n    embedding_dim, training_size, num_epochs)\n```", "```\nvocab_size = 10000\nembedding_dim = 32 #Change from 16 to 32\ntrain_size = 20000\nnum_epochs=10\nmodel_2, history_2 = sentiment_model(vocab_size,\n    embedding_dim, train_size, num_epochs)\n```", "```\nvocab_size = 10000\nembedding_dim = 16\ntrain_size = 40000\nmodel_3, history_3 = sentiment_model(vocab_size,\n    embedding_dim, train_size)\n```", "```\nmodel_4  = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n    input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n# Dropout layer with 50% dropout rate\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n# Compile the model\nmodel_4.compile(loss='binary_crossentropy',\n    optimizer='adam',metrics=['accuracy'])\n```", "```\n# Initialize the optimizer\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n# Compile the model\nmodel_7.compile(loss='binary_crossentropy',\n    optimizer=optimizer, metrics=['accuracy'])\n# Train the model\nhistory_7 = model_7.fit(train_padded, train_labels,\n    epochs=num_epochs, validation_data=(val_padded,\n        val_labels))\n```"]