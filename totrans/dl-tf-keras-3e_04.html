<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer187">
<h1 class="chapterNumber">4</h1>
<h1 class="chapterTitle" id="_idParaDest-98">Word Embeddings</h1>
<p class="normal">In the previous chapter, we talked about convolutional networks, which have been very successful against image data. Over the next few chapters, we will switch tracks to focus on strategies and networks to handle text data.</p>
<p class="normal">In this chapter, we will first look at the idea behind word embeddings, and then cover the two earliest implementations – Word2Vec and GloVe. We will learn how to build word embeddings from scratch using the popular library Gensim on our own corpus and navigate the embedding space we create.</p>
<p class="normal">We will also learn how to use pretrained third-party embeddings as a starting point for our own NLP tasks, such as spam detection, that is, learning to automatically detect unsolicited and unwanted emails. We will then learn about various ways to leverage the idea of word embeddings for unrelated tasks, such as constructing an embedded space for making item recommendations.</p>
<p class="normal">We will then look at extensions to these foundational word embedding techniques that have occurred in the last decade since Word2Vec – adding syntactic similarity with fastText, adding the effect of context using neural networks such as ELMo and Google Universal Sentence Encoder, sentence encodings such as InferSent and skip-thoughts, and the introduction of language models such as ULMFiT and BERT.</p>
<p class="normal">In this chapter, we’ll learn about the following:</p>
<ul>
<li class="bulletList">Word embeddings – origins and fundamentals</li>
<li class="bulletList">Distributed representations</li>
<li class="bulletList">Static embeddings</li>
<li class="bulletList">Creating your own embedding with Gensim</li>
<li class="bulletList">Exploring the embedding space with Gensim</li>
<li class="bulletList">Using word embedding for spam detection</li>
<li class="bulletList">Neural embedding – not just for words</li>
<li class="bulletList">Character and subword embedding</li>
<li class="bulletList">Dynamic embeddings</li>
<li class="bulletList">Sentence and paragraph embeddings</li>
<li class="bulletList">Language-based model embeddings</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp4"><span class="url">https://packt.link/dltfchp4</span></a>.</p>
</div>
<p class="normal">Let’s begin!</p>
<h1 class="heading-1" id="_idParaDest-99">Word embedding ‒ origins and fundamentals</h1>
<p class="normal">Wikipedia defines word<a id="_idIndexMarker343"/> embedding as the collective name for a set of language modeling and feature learning techniques in <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>) where <a id="_idIndexMarker344"/>words or phrases from a vocabulary are mapped to vectors of real numbers.</p>
<p class="normal">Deep learning models, like other machine learning models, typically don’t work directly with text; the text needs to be converted to numbers instead. The process of converting text to numbers is a process called vectorization. An<a id="_idIndexMarker345"/> early technique for vectorizing words was one-hot encoding, which you learned about in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>. As you will recall, a major problem with one-hot encoding is that it treats each word as completely independent from all the others, since the similarity between any two words (measured by the dot product of the two word vectors) is always zero.</p>
<p class="normal">The dot product is an algebraic operation that operates on two vectors <img alt="" height="50" src="../Images/B18331_04_001.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="267"/> and <img alt="" height="50" src="../Images/B18331_04_002.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="254"/>of equal length and returns a number. It is also known as the inner product or scalar product:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_04_003.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="579"/></p>
<p class="normal">Why is the dot product of one-hot vectors of two words always 0? Consider two words <em class="italic">w</em><sub class="subscript">i</sub> and <em class="italic">w</em><sub class="subscript">j</sub>. Assuming a vocabulary size of <em class="italic">V</em>, their corresponding one-hot vectors are a zero vector of rank <em class="italic">V</em> with positions <em class="italic">i</em> and <em class="italic">j</em> set to 1. When combined using the dot product operation, the 1 in a[i] is multiplied by 0 in b[i], and 1 in b[j] is multiplied by 0 in a[j], and all other elements in both vectors are 0, so the resulting dot product is also 0.</p>
<p class="normal">To overcome the limitations of one-hot encoding, the NLP community has borrowed techniques<a id="_idIndexMarker346"/> from <strong class="keyWord">Information Retrieval</strong> (<strong class="keyWord">IR</strong>) to vectorize text using the document<a id="_idIndexMarker347"/> as the context. Notable techniques are <strong class="keyWord">Term Frequency-Inverse Document Frequency</strong> (<strong class="keyWord">TF-IDF</strong>) [35], <strong class="keyWord">Latent Semantic Analysis</strong> (<strong class="keyWord">LSA</strong>) [36], and topic <a id="_idIndexMarker348"/>modeling [37]. These representations attempt to capture a document-centric idea of semantic similarity between words. Of<a id="_idIndexMarker349"/> these, one-hot and TF-IDF are relatively sparse embeddings, since vocabularies are usually quite large, and a word is unlikely to occur in more than a few documents in the corpus.</p>
<p class="normal">The development of word embedding techniques began around 2000. These techniques differ from previous IR-based techniques in that they use neighboring words as their context, leading to a more natural semantic similarity from a human understanding perspective. Today, word embedding is a foundational technique for all kinds of NLP tasks, such as text classification, document clustering, part-of-speech tagging, named entity recognition, sentiment analysis, and many more. Word embeddings result in dense, low-dimensional vectors, and along with LSA and topic models can be thought of as a vector of latent features for the word.</p>
<p class="normal">Word embeddings are based on the distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings. Hence the class of word embedding-based encodings is also known as distributed representations, which we will talk about next.</p>
<h1 class="heading-1" id="_idParaDest-100">Distributed representations</h1>
<p class="normal">Distributed representations attempt to<a id="_idIndexMarker350"/> capture the meaning of a word by considering its relations with other words in its context. The idea behind the distributed hypothesis is captured in this quote from <em class="italic">J. R. Firth</em>, a linguist, who first proposed this idea:</p>
<blockquote class="packt_quote">
<p class="quote">You shall know a word by the company it keeps.</p>
</blockquote>
<p class="normal">How does this work? By way of example, consider the following pair of sentences:</p>
<p class="normal"><em class="italic">Paris is the capital of France</em>.</p>
<p class="normal"><em class="italic">Berlin is the capital of Germany</em>.</p>
<p class="normal">Even assuming no knowledge of world geography, the sentence pair implies some sort of relationship between the entities Paris, France, Berlin, and Germany that could be represented as:</p>
<p class="normal"><code class="inlineCode">"Paris" is to "France" as "Berlin" is to "Germany."</code></p>
<p class="normal">Distributed representations are based on the idea that there exists some transformation, as follows:</p>
<p class="normal"><code class="inlineCode">Paris : France :: Berlin : Germany</code></p>
<p class="normal">In other words, a distributed embedding space is <a id="_idIndexMarker351"/>one where words that are used in similar contexts are close to one another. Therefore, the similarity between the word vectors in this space would roughly correspond to the semantic similarity between the words.</p>
<p class="normal"><em class="italic">Figure 4.1</em> shows a TensorBoard visualization of word embedding of words around the word “important” in the embedding space. As you can see, the neighbors of the word tend to be closely related, or interchangeable with the original word.</p>
<p class="normal">For example, “crucial” is virtually a synonym, and it is easy to see how the words “historical” or “valuable” could be substituted in certain situations:</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated with low confidence" height="579" src="../Images/B18331_04_01.png" width="876"/></figure>
<p class="packt_figref">Figure 4.1: Visualization of nearest neighbors of the word “important” in a word embedding dataset, from the TensorFlow Embedding Guide (https://www.tensorflow.org/guide/embedding)</p>
<p class="normal">In the next section, we will look at<a id="_idIndexMarker352"/> various types of distributed representations (or word embeddings).</p>
<h1 class="heading-1" id="_idParaDest-101">Static embeddings</h1>
<p class="normal">Static embeddings are the oldest type of word embedding. The embeddings are generated against a large corpus <a id="_idIndexMarker353"/>but the number of words, though large, is finite. You can think of a static embedding as a dictionary, with words as the keys and their corresponding vector as the value. If you have a word whose embedding needs to be looked up that was not in the original corpus, then you are out of luck. In addition, a word has the same embedding regardless of how it is used, so static embeddings cannot address the problem of polysemy, that is, words with multiple meanings. We will explore this issue further when we cover non-static embeddings later in this chapter.</p>
<h2 class="heading-2" id="_idParaDest-102">Word2Vec</h2>
<p class="normal">The models known as Word2Vec were first created in 2013 by a team of researchers at Google led by <em class="italic">Tomas Mikolov</em> [1, 2, 3]. The models <a id="_idIndexMarker354"/>are self-supervised, that is, they <a id="_idIndexMarker355"/>are supervised models that depend on the structure of natural language to provide labeled training data.</p>
<p class="normal">The two architectures for Word2Vec<a id="_idIndexMarker356"/> are as follows:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Continuous Bag of Words</strong> (<strong class="keyWord">CBOW</strong>)</li>
<li class="bulletList">Skip-gram</li>
</ul>
<figure class="mediaobject"><img alt="" height="459" src="../Images/B18331_04_02.png" width="770"/></figure>
<p class="packt_figref">Figure 4.2: Architecture of the CBOW and Skip-gram Word2Vec models</p>
<p class="normal">In the CBOW architecture, the model <a id="_idIndexMarker357"/>predicts the current word given a window of surrounding words. The order of context words does not influence the prediction (that is, the bag of words assumption, hence the name). In the skip-gram architecture, the model<a id="_idIndexMarker358"/> predicts the surrounding words given the context word. According to the Word2Vec website, CBOW is faster, but skip-gram does a better job at predicting infrequent words.</p>
<p class="normal"><em class="italic">Figure 4.2</em> summarizes the CBOW and skip-gram architectures. To understand the inputs and outputs, consider the following example sentence:</p>
<p class="normal"><em class="italic">The Earth travels around the Sun once per year.</em></p>
<p class="normal">Assuming a window size of 5, that is, two context words to the left and right of the content word, the resulting context windows are shown as follows. The word in bold is the word under consideration, and the other words are the context words within the window:</p>
<p class="normal">[_, _, <strong class="keyWord">The</strong>, Earth, travels]</p>
<p class="normal">[_, The, <strong class="keyWord">Earth</strong>, travels, around]</p>
<p class="normal">[The, Earth, <strong class="keyWord">travels</strong>, around, the]</p>
<p class="normal">[Earth, travels, <strong class="keyWord">around</strong>, the, Sun]</p>
<p class="normal">[travels, around, <strong class="keyWord">the</strong>, Sun, once]</p>
<p class="normal">[around, the, <strong class="keyWord">Sun</strong>, once, per]</p>
<p class="normal">[the, Sun, <strong class="keyWord">once</strong>, per, year]</p>
<p class="normal">[Sun, <strong class="keyWord">once</strong>, per, year, _]</p>
<p class="normal">[<strong class="keyWord">once</strong>, per, year, _, _]</p>
<p class="normal">For the CBOW model, the input and label tuples for<a id="_idIndexMarker359"/> the first three context windows are as follows. In the <a id="_idIndexMarker360"/>following first example, the CBOW model would learn to predict the word “The” given the set of words (“Earth,” “travels”), and so on. More correctly, the input of sparse vectors <a id="_idIndexMarker361"/>for the words “Earth” and “travels.” The model will learn to predict a dense vector whose highest value, or probability, corresponds to the word “The”:</p>
<p class="normal">([Earth, travels], <strong class="keyWord">The</strong>)</p>
<p class="normal">([The, travels, around], <strong class="keyWord">Earth</strong>)</p>
<p class="normal">([The, Earth, around, the], <strong class="keyWord">travels</strong>)</p>
<p class="normal">For the skip-gram model, the first three<a id="_idIndexMarker362"/> context windows correspond to the following input and label tuples. We can simplify the skip-gram model objective of predicting a context word given a target word to basically predicting if a pair of words are contextually related. Contextually related means that a pair of words within a context window are somehow related. That is, the input to the skip-gram model for the following first example would be the sparse vectors for the context words “The” and “Earth,” and the output would be the value 1:</p>
<p class="normal">([<strong class="keyWord">The</strong>, Earth], 1)</p>
<p class="normal">([<strong class="keyWord">The</strong>, travels], 1)</p>
<p class="normal">([<strong class="keyWord">Earth</strong>, The], 1)</p>
<p class="normal">([<strong class="keyWord">Earth</strong>, travels], 1)</p>
<p class="normal">([<strong class="keyWord">Earth</strong>, around], 1)</p>
<p class="normal">([<strong class="keyWord">travels</strong>, The], 1)</p>
<p class="normal">([<strong class="keyWord">travels</strong>, Earth], 1)</p>
<p class="normal">([<strong class="keyWord">travels</strong>, around], 1)</p>
<p class="normal">([<strong class="keyWord">travels</strong>, the], 1)</p>
<p class="normal">We also need negative samples to train a model properly, so we generate additional negative samples by<a id="_idIndexMarker363"/> pairing each input word with some random word in the vocabulary. This<a id="_idIndexMarker364"/> process is called negative sampling and might result in the following additional inputs:</p>
<p class="normal">([<strong class="keyWord">Earth</strong>, aardvark], 0)</p>
<p class="normal">([<strong class="keyWord">Earth</strong>, zebra], 0)</p>
<p class="normal">A model trained with all of these inputs is<a id="_idIndexMarker365"/> called a <strong class="keyWord">Skip-Gram with Negative Sampling</strong> (<strong class="keyWord">SGNS</strong>) model.</p>
<p class="normal">It is important to understand that we are not interested in the ability of these models to classify; rather, we are interested in the side effect of training – the learned weights. These learned weights are what we call the embedding.</p>
<p class="normal">While it may be instructive to<a id="_idIndexMarker366"/> implement the models on your own as an academic exercise, at this point Word2Vec is so commoditized, you are unlikely to ever need to do this. For the curious, you will find code to implement the CBOW and skip-gram models in the files <code class="inlineCode">tf2_cbow_model.py</code> and <code class="inlineCode">tf2_cbow_skipgram.py</code> in the source code accompanying this chapter.</p>
<p class="normal">The Word2Vec model was trained in a self-supervised manner by Google on roughly 100 billion words from the Google News dataset and contains a vocabulary of 3 million words. Google then released the pretrained model for anyone to download and use. The pretrained Word2Vec <a id="_idIndexMarker367"/>model is available here (<a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"><span class="url">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit</span></a>). The output vector dimensionality is 300. It is available as a BIN file and can be opened using Gensim by using <code class="inlineCode">gensim.models.Word2Vec.load_word2vec_format()</code> or using the <code class="inlineCode">gensim()</code> data downloader.</p>
<p class="normal">The other<a id="_idIndexMarker368"/> early implementation of <a id="_idIndexMarker369"/>word embedding is GloVe, which we will talk about next.</p>
<h2 class="heading-2" id="_idParaDest-103">GloVe</h2>
<p class="normal">The <strong class="keyWord">Global vectors for word representation</strong> (<strong class="keyWord">GloVe</strong>) embeddings were created by <em class="italic">Jeffrey Pennington</em>, <em class="italic">Richard Socher</em>, and <em class="italic">Christopher Manning</em> [4]. The authors describe GloVe as an<a id="_idIndexMarker370"/> unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated <a id="_idIndexMarker371"/>global word-word co-occurrence statistics from a corpus, and the resulting representations show similar clustering behavior between similar words as seen in Word2Vec.</p>
<p class="normal">GloVe differs from Word2Vec in that Word2Vec is a predictive model while GloVe is a count-based model. The first step is to construct a large matrix of (word, context) pairs that co-occur in the training corpus. Rows correspond to words and columns correspond to contexts, usually a sequence of one or more words. Each element of the matrix represents how often the word co-occurs in the context.</p>
<p class="normal">The GloVe process factorizes this co-occurrence matrix into a pair of (word, feature) and (feature, context) matrices. The <a id="_idIndexMarker372"/>process is known as matrix factorization and is done using <strong class="keyWord">Stochastic Gradient Descent</strong> (<strong class="keyWord">SGD</strong>), an iterative<a id="_idIndexMarker373"/> numerical method. For example, consider that we want to factorize a matrix <em class="italic">R</em> into its factors <em class="italic">P</em> and <em class="italic">Q</em>:</p>
<p class="center"><img alt="" height="46" src="../Images/B18331_04_004.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="258"/></p>
<p class="normal">The SGD process will start with <em class="italic">P</em> and <em class="italic">Q</em> composed of random values and attempt to reconstruct the matrix <em class="italic">R’</em> by multiplying them. The difference between the matrices <em class="italic">R</em> and <em class="italic">R’</em> represents the loss and is usually computed as the mean-squared error between the two matrices. The loss dictates how much the values of <em class="italic">P</em> and <em class="italic">Q</em> need to change for <em class="italic">R’</em> to move closer to <em class="italic">R</em> to minimize the reconstruction loss. This process is repeated multiple times until the loss is within some acceptable threshold. At that point, the (word, feature) matrix <em class="italic">P</em> is the GloVe embedding.</p>
<p class="normal">The GloVe process is much more resource-intensive than Word2Vec. This is because Word2Vec learns the embedding by training over batches of word vectors, while GloVe factorizes the entire co-occurrence matrix in one shot. In order to make the process scalable, SGD is often used in parallel mode, as outlined in the HOGWILD! paper [5].</p>
<p class="normal">Levy and Goldberg have also pointed out equivalences between the Word2Vec and GloVe approaches in their paper [6], showing that the Word2Vec SGNS model implicitly factorizes a word-context matrix.</p>
<p class="normal">As with Word2Vec, you<a id="_idIndexMarker374"/> are unlikely to ever need to generate your own GloVe embedding, and far more likely to use embeddings pre-generated against large corpora and made available for download. If you are curious, you will find code to implement matrix factorization in <code class="inlineCode">tf2_matrix_factorization.py</code> in the source code download accompanying this chapter.</p>
<p class="normal">GloVe vectors trained on<a id="_idIndexMarker375"/> various large corpora (number of tokens ranging from 6 billion to 840 billion, vocabulary size from 400 thousand to 2.2 million) and of various <a id="_idIndexMarker376"/>dimensions (50, 100, 200, 300) are available from the GloVe project download page (<a href="https://nlp.stanford.edu/projects/glove/"><span class="url">https://nlp.stanford.edu/projects/glove/</span></a>). It can be downloaded directly from the site or using Gensim or spaCy data downloaders.</p>
<h1 class="heading-1" id="_idParaDest-104">Creating your own embeddings using Gensim</h1>
<p class="normal">We will create an embedding <a id="_idIndexMarker377"/>using Gensim and a small<a id="_idIndexMarker378"/> text corpus, called text8.</p>
<p class="normal">Gensim is an open-source <a id="_idIndexMarker379"/>Python library designed to extract semantic meaning from text documents. One of its features is an excellent implementation of the Word2Vec algorithm, with an easy-to-use API that allows you to train and query your own Word2Vec model. To learn more about Gensim, see <a href="https://radimrehurek.com/gensim/index.xhtml"><span class="url">https://radimrehurek.com/gensim/index.xhtml</span></a>. To install Gensim, please <a id="_idIndexMarker380"/>follow the instructions at <a href="https://radimrehurek.com/gensim/install.xhtml"><span class="url">https://radimrehurek.com/gensim/install.xhtml</span></a>.</p>
<p class="normal">The text8 dataset is the first 10<sup class="superscript">8</sup> bytes of the Large Text Compression Benchmark, which consists of the first 10<sup class="superscript">9</sup> bytes of English Wikipedia [7]. The text8 dataset is accessible from within the Gensim API as an iterable of tokens, essentially a list of tokenized sentences. To download the text8 corpus, create a Word2Vec model from it, and save it for later use, run the following few lines of code (available in <code class="inlineCode">create_embedding_with_text8.py</code> in the source code for this chapter):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gensim.downloader <span class="hljs-keyword">as</span> api
<span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> Word2Vec
dataset = api.load(<span class="hljs-string">"text8"</span>)
model = Word2Vec(dataset)
model.save(<span class="hljs-string">"data/text8-word2vec.bin"</span>)
</code></pre>
<p class="normal">This will train a Word2Vec model on the text8 dataset and save it as a binary file. The Word2Vec model has many parameters, but we will just use the defaults. In this case, it trains a CBOW model (<code class="inlineCode">sg=0</code>) with window size 5 (<code class="inlineCode">window=5</code>) and will produce 100 dimensional embeddings (<code class="inlineCode">size=100</code>). The full set of parameters is described on the Word2Vec documentation page [8]. To run this code, execute the following commands at the command line:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mkdir</span> data
<span class="hljs-con-meta">$ </span>python create_embedding_with_text8.py
</code></pre>
<p class="normal">The code should <a id="_idIndexMarker381"/>run for 5-10 minutes, after which it will write out a trained model into the <code class="inlineCode">data</code> folder. We will examine this trained model in the next<a id="_idIndexMarker382"/> section.</p>
<div class="note">
<p class="normal">Word embeddings are <a id="_idIndexMarker383"/>central to text processing; however, at the time of writing this book, there is no comparable API within TensorFlow that allows you to work with embeddings at the same level of abstraction. For this reason, we have used Gensim in this chapter to work with Word2Vec models. The online Tensorflow tutorial contains an example of how to train a Word2Vec model from scratch (<a href="https://www.tensorflow.org/tutorials/text/word2vec"><span class="url">https://www.tensorflow.org/tutorials/text/word2vec</span></a>) but that is not our focus here.</p>
</div>
<h1 class="heading-1" id="_idParaDest-105">Exploring the embedding space with Gensim</h1>
<p class="normal">Let us reload the Word2Vec model we <a id="_idIndexMarker384"/>just built and explore it using<a id="_idIndexMarker385"/> the Gensim API. The actual word vectors can be accessed as a custom Gensim class from the model’s <code class="inlineCode">wv</code> attribute:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> KeyedVectors
model = KeyedVectors.load(<span class="hljs-string">"data/text8-word2vec.bin"</span>)
word_vectors = model.wv
</code></pre>
<p class="normal">We can take a look at the first few words in the vocabulary and check to see if specific words are available:</p>
<pre class="programlisting code"><code class="hljs-code">words = word_vectors.vocab.keys()
<span class="hljs-built_in">print</span>([x <span class="hljs-keyword">for</span> i, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(words) <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>])
<span class="hljs-keyword">assert</span>(<span class="hljs-string">"king"</span> <span class="hljs-keyword">in</span> words)
</code></pre>
<p class="normal">The preceding snippet of code produces the following output:</p>
<pre class="programlisting con"><code class="hljs-con">['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']
</code></pre>
<p class="normal">We can look for similar words to a given word (“king”), shown as follows:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">print_most_similar</span><span class="hljs-function">(</span><span class="hljs-params">word_conf_pairs, k</span><span class="hljs-function">):</span>
   <span class="hljs-keyword">for</span> i, (word, conf) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_conf_pairs):
       <span class="hljs-built_in">print</span>(<span class="hljs-string">"{:.3f} {:s}"</span>.<span class="hljs-built_in">format</span>(conf, word))
       <span class="hljs-keyword">if</span> i &gt;= k-<span class="hljs-number">1</span>:
           <span class="hljs-keyword">break</span>
   <span class="hljs-keyword">if</span> k &lt; <span class="hljs-built_in">len</span>(word_conf_pairs):
       <span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">..."</span>)
print_most_similar(word_vectors.most_similar(<span class="hljs-string">"king"</span>), <span class="hljs-number">5</span>)
</code></pre>
<p class="normal">The <code class="inlineCode">most_similar()</code> method with <a id="_idIndexMarker386"/>a single parameter produces the following<a id="_idIndexMarker387"/> output. Here, the floating-point score is a measure of the similarity, higher values being better than lower values. As you can see, the similar words seem to be mostly accurate:</p>
<pre class="programlisting con"><code class="hljs-con">0.760 prince
0.701 queen
0.700 kings
0.698 emperor
0.688 throne
...
</code></pre>
<p class="normal">You can also do vector arithmetic similar to the country-capital example we described earlier. Our objective is to see if the relation Paris : France :: Berlin : Germany holds true. This is equivalent to saying that the distance in embedding space between Paris and France should be the same as that between Berlin and Germany. In other words, France - Paris + Berlin should give us Germany. In code, then, this would translate to:</p>
<pre class="programlisting code"><code class="hljs-code">print_most_similar(word_vectors.most_similar(
   positive=[<span class="hljs-string">"france"</span>, <span class="hljs-string">"berlin"</span>], negative=[<span class="hljs-string">"paris"</span>]), <span class="hljs-number">1</span>
)
</code></pre>
<p class="normal">This returns the following result, as expected:</p>
<pre class="programlisting con"><code class="hljs-con">0.803 germany
</code></pre>
<p class="normal">The preceding similarity value reported is cosine similarity, but a better measure of similarity was proposed by <em class="italic">Levy</em> and <em class="italic">Goldberg</em> [9], which is also implemented in the Gensim API. This measure <a id="_idIndexMarker388"/>essentially computes the distance on a log scale thereby <a id="_idIndexMarker389"/>amplifying the difference between shorter distances and reducing the difference between longer ones.</p>
<pre class="programlisting code"><code class="hljs-code">print_most_similar(word_vectors.most_similar_cosmul(
   positive=[<span class="hljs-string">"france"</span>, <span class="hljs-string">"berlin"</span>], negative=[<span class="hljs-string">"paris"</span>]), <span class="hljs-number">1</span>
)
</code></pre>
<p class="normal">And this also yields the expected result, but with higher similarity:</p>
<pre class="programlisting con"><code class="hljs-con">0.984 germany
</code></pre>
<p class="normal">Gensim also provides a <code class="inlineCode">doesnt_match()</code> function, which can be used to detect the odd one out of a list of words:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(word_vectors.doesnt_match([<span class="hljs-string">"hindus"</span>, <span class="hljs-string">"parsis"</span>, <span class="hljs-string">"singapore"</span>, <span class="hljs-string">"christians"</span>]))
</code></pre>
<p class="normal">This gives us <code class="inlineCode">singapore</code> as expected, since it is the only country among a set of words identifying religions.</p>
<p class="normal">We can also calculate the similarity between two words. Here we demonstrate that the distance between related words is less than that of unrelated words:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> [<span class="hljs-string">"woman"</span>, <span class="hljs-string">"</span><span class="hljs-string">dog"</span>, <span class="hljs-string">"whale"</span>, <span class="hljs-string">"tree"</span>]:
   <span class="hljs-built_in">print</span>(<span class="hljs-string">"similarity({:s}, {:s}) = {:.3f}"</span>.<span class="hljs-built_in">format</span>(
       <span class="hljs-string">"man"</span>, word,
       word_vectors.similarity(<span class="hljs-string">"man"</span>, word)
   ))
</code></pre>
<p class="normal">This gives the following interesting result:</p>
<pre class="programlisting con"><code class="hljs-con">similarity(man, woman) = 0.759
similarity(man, dog) = 0.474
similarity(man, whale) = 0.290
similarity(man, tree) = 0.260
</code></pre>
<p class="normal">The <code class="inlineCode">similar_by_word()</code> function is functionally equivalent to <code class="inlineCode">similar()</code> except that the latter normalizes the vector before comparing by default. There is also a related <code class="inlineCode">similar_by_vector()</code> function, which <a id="_idIndexMarker390"/>allows you to find similar words by specifying a vector as input. Here we try to find words that are similar to “singapore”:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(print_most_similar(
   word_vectors.similar_by_word(<span class="hljs-string">"singapore"</span>), <span class="hljs-number">5</span>)
)
</code></pre>
<p class="normal">And we get the following output, which seems to be mostly correct, at least from a geographical point of view:</p>
<pre class="programlisting con"><code class="hljs-con">0.882 malaysia
0.837 indonesia
0.826 philippines
0.825 uganda
0.822 thailand
...
</code></pre>
<p class="normal">We can also compute the distance between two words in the embedding space using the <code class="inlineCode">distance()</code> function. This is really just <code class="inlineCode">1 - similarity()</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">distance(singapore, malaysia) = {:.3f}"</span>.<span class="hljs-built_in">format</span>(
   word_vectors.distance(<span class="hljs-string">"singapore"</span>, <span class="hljs-string">"malaysia"</span>)
))
</code></pre>
<p class="normal">We can also look up<a id="_idIndexMarker391"/> vectors for a vocabulary word either directly from the <code class="inlineCode">word_vectors</code> object, or by using the <code class="inlineCode">word_vec()</code> wrapper, shown as follows:</p>
<pre class="programlisting code"><code class="hljs-code">vec_song = word_vectors[<span class="hljs-string">"song"</span>]
vec_song_2 = word_vectors.word_vec(<span class="hljs-string">"song"</span>, use_norm=<span class="hljs-literal">True</span>)
</code></pre>
<p class="normal">There are a few other functions that you may find useful depending on your use case. The documentation page for KeyedVectors contains a list of all the available functions [10].</p>
<p class="normal">The code shown here can be found in the <code class="inlineCode">explore_text8_embedding.py</code> file in the code accompanying this book.</p>
<h1 class="heading-1" id="_idParaDest-106">Using word embeddings for spam detection</h1>
<p class="normal">Because of the widespread<a id="_idIndexMarker392"/> availability of various robust embeddings generated from large corpora, it has become quite common to use one of these embeddings to convert text input for use with machine learning models. Text is treated as a sequence of tokens. The embedding provides a dense fixed dimension vector for each token. Each token is replaced with its vector, and this converts the sequence <a id="_idIndexMarker393"/>of text into a matrix of examples, each of which has a fixed number of features corresponding to the dimensionality of the embedding.</p>
<p class="normal">This matrix of examples can be used directly as input to standard (non-neural network based) machine learning programs, but since this book is about deep learning and TensorFlow, we will demonstrate its<a id="_idIndexMarker394"/> use with a one-dimensional version of the <strong class="keyWord">Convolutional Neural Network</strong> (<strong class="keyWord">CNN</strong>) that you learned about in <em class="chapterRef">Chapter 3</em>, <em class="italic">Convolutional Neural Networks</em>. Our example is a spam detector that will classify <strong class="keyWord">Short Message Service</strong> (<strong class="keyWord">SMS</strong>) or text<a id="_idIndexMarker395"/> messages as either “ham” or “spam.” The example is very similar to a sentiment analysis example we’ll cover in <em class="chapterRef">Chapter 20</em>, <em class="italic">Advanced Convolutional Neural Networks</em>, that uses a one-dimensional CNN, but our focus here will be on the embedding layer.</p>
<p class="normal">Specifically, we will see how the program learns an embedding from scratch that is customized to the spam detection task. Next, we will see how to use an external third-party embedding like the ones we have learned about in this chapter, a process similar to transfer learning in computer vision. Finally, we will learn how to combine the two approaches, starting with a third-party embedding and letting the network use that as a starting point for its custom embedding, a process similar to fine-tuning in computer vision.</p>
<p class="normal">As usual, we will start with our imports:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> gensim.downloader <span class="hljs-keyword">as</span> api
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, confusion_matrix
</code></pre>
<div class="note">
<p class="normal">Scikit-learn is an open-source Python machine learning toolkit that contains many efficient and easy-to-use<a id="_idIndexMarker396"/> tools for data mining and data analysis. In this chapter, we have used two of its predefined metrics, <code class="inlineCode">accuracy_score</code> and <code class="inlineCode">confusion_matrix</code>, to evaluate our model after it is trained.</p>
<p class="normal">You can learn more <a id="_idIndexMarker397"/>about scikit-learn at <a href="https://scikit-learn.org/stable/"><span class="url">https://scikit-learn.org/stable/</span></a>.</p>
</div>
<h2 class="heading-2" id="_idParaDest-107">Getting the data</h2>
<p class="normal">The data for our model is available <a id="_idIndexMarker398"/>publicly and comes from the SMS spam collection dataset from the UCI Machine Learning Repository [11]. The following code will download the file and parse it to produce a list of SMS messages and their corresponding labels:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">download_and_read</span><span class="hljs-function">(</span><span class="hljs-params">url</span><span class="hljs-function">):</span>
   local_file = url.split(<span class="hljs-string">'/'</span>)[-<span class="hljs-number">1</span>]
   p = tf.keras.utils.get_file(local_file, url,
       extract=<span class="hljs-literal">True</span>, cache_dir=<span class="hljs-string">"."</span>)
   labels, texts = [], []
   local_file = os.path.join(<span class="hljs-string">"datasets"</span>, <span class="hljs-string">"SMSSpamCollection"</span>)
   <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(local_file, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> fin:
       <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fin:
           label, text = line.strip().split(<span class="hljs-string">'\t'</span>)
           labels.append(<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> label == <span class="hljs-string">"spam"</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
           texts.append(text)
   <span class="hljs-keyword">return</span> texts, labels
DATASET_URL = <span class="hljs-string">"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"</span>
texts, labels = download_and_read(DATASET_URL)
</code></pre>
<p class="normal">The dataset contains 5,574 SMS records, 747 of which are marked as “spam” and the other 4,827 are marked as “ham” (not spam). The text of the SMS records is contained in the variable <code class="inlineCode">texts</code>, and the corresponding numeric labels (0 = ham, 1 = spam) are contained in the variable labels.</p>
<h2 class="heading-2" id="_idParaDest-108">Making the data ready for use</h2>
<p class="normal">The next step is to process the<a id="_idIndexMarker399"/> data so it can be consumed by the network. The SMS text needs to be fed into the network as a sequence of integers, where each word is represented by its corresponding ID in the vocabulary. We will use the Keras tokenizer to convert each SMS text into a sequence of words, and then create the vocabulary using the <code class="inlineCode">fit_on_texts()</code> method on the tokenizer.</p>
<p class="normal">We then convert the SMS messages to a sequence of integers using <code class="inlineCode">texts_to_sequences()</code>. Finally, since the network can only work with fixed-length sequences of integers, we call the <code class="inlineCode">pad_sequences()</code> function to pad the shorter SMS messages with zeros.</p>
<p class="normal">The longest SMS message in our dataset has 189 tokens (words). In many applications where there may be a few outlier sequences that are very long, we would restrict the length to a smaller number by setting the <code class="inlineCode">maxlen</code> flag. In that case, sentences longer than <code class="inlineCode">maxlen</code> tokens would be truncated, and sentences shorter than <code class="inlineCode">maxlen</code> tokens would be padded:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># tokenize and pad text</span>
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(texts)
text_sequences = tokenizer.texts_to_sequences(texts)
text_sequences = tf.keras.preprocessing.sequence.pad_sequences(
    text_sequences)
num_records = <span class="hljs-built_in">len</span>(text_sequences)
max_seqlen = <span class="hljs-built_in">len</span>(text_sequences[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">"{:d} sentences, max length: {:d}"</span>.<span class="hljs-built_in">format</span>(
    num_records, max_seqlen))
</code></pre>
<p class="normal">We will also convert our labels to categorical or one-hot encoding format, because the loss function we would like to choose (categorical cross-entropy) expects to see the labels in that format:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># labels</span>
NUM_CLASSES = <span class="hljs-number">2</span>
cat_labels = tf.keras.utils.to_categorical(
    labels, num_classes=NUM_CLASSES)
</code></pre>
<p class="normal">The tokenizer allows access to the vocabulary created through the <code class="inlineCode">word_index</code> attribute, which is basically a dictionary of vocabulary words to their index positions in the vocabulary. We also build the reverse index that enables us to go from index position to the word itself. In addition, we create entries for the <code class="inlineCode">PAD</code> character:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># vocabulary</span>
word2idx = tokenizer.word_index
idx2word = {v:k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word2idx.items()}
word2idx[<span class="hljs-string">"PAD"</span>] = <span class="hljs-number">0</span>
idx2word[<span class="hljs-number">0</span>] = <span class="hljs-string">"PAD"</span>
vocab_size = <span class="hljs-built_in">len</span>(word2idx)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"vocab size: {:d}"</span>.<span class="hljs-built_in">format</span>(vocab_size))
</code></pre>
<p class="normal">Finally, we create the <code class="inlineCode">dataset</code> object that our network will work with. The <code class="inlineCode">dataset</code> object allows us to set up some properties, such as the batch size, declaratively. Here, we build up a dataset from our <a id="_idIndexMarker400"/>padded sequence of integers and categorical labels, shuffle the data, and split it into training, validation, and test sets. Finally, we set the batch size for each of the three datasets:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># dataset</span>
dataset = tf.data.Dataset.from_tensor_slices(
    (text_sequences, cat_labels))
dataset = dataset.shuffle(<span class="hljs-number">10000</span>)
test_size = num_records // <span class="hljs-number">4</span>
val_size = (num_records - test_size) // <span class="hljs-number">10</span>
test_dataset = dataset.take(test_size)
val_dataset = dataset.skip(test_size).take(val_size)
train_dataset = dataset.skip(test_size + val_size)
BATCH_SIZE = <span class="hljs-number">128</span>
test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=<span class="hljs-literal">True</span>)
val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=<span class="hljs-literal">True</span>)
train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=<span class="hljs-literal">True</span>)
</code></pre>
<h2 class="heading-2" id="_idParaDest-109">Building the embedding matrix</h2>
<p class="normal">The Gensim toolkit provides access to<a id="_idIndexMarker401"/> various trained embedding models, as you can see from running the following command at the Python prompt:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> gensim.downloader <span class="hljs-con-keyword">as</span> api
<span class="hljs-con-meta">&gt;&gt;&gt;</span> api.info(<span class="hljs-con-string">"models"</span>).keys()
</code></pre>
<p class="normal">This will return (at the time of writing this book) the following trained word embeddings:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Word2Vec</strong>: Two flavors, one trained on Google news (3 million word vectors based on 3 billion tokens), and one<a id="_idIndexMarker402"/> trained on Russian corpora (word2vec-ruscorpora-300, word2vec-google-news-300).</li>
<li class="bulletList"><strong class="keyWord">GloVe</strong>: Two flavors, one<a id="_idIndexMarker403"/> trained on the Gigawords corpus (400,000 word vectors based on 6 billion tokens), available as 50d, 100d, 200d, and 300d vectors, and one trained on Twitter (1.2 million word vectors based on 27 billion tokens), available as 25d, 50d, 100d, and 200d vectors (glove-wiki-gigaword-50, glove-wiki-gigaword-100, glove-wiki-gigaword-200, glove-wiki-gigaword-300, glove-twitter-25, glove-twitter-50, glove-twitter-100, glove-twitter-200). Smaller embedding sizes would result in greater compression of the input and consequently a greater degree of approximation.</li>
<li class="bulletList"><strong class="keyWord">fastText</strong>: One million word <a id="_idIndexMarker404"/>vectors trained with subword information on Wikipedia 2017, the UMBC web corpus, and statmt.org news dataset (16B tokens) (fastText-wiki-news-subwords-300).</li>
<li class="bulletList"><strong class="keyWord">ConceptNet Numberbatch</strong>: An ensemble<a id="_idIndexMarker405"/> embedding that uses the ConceptNet <a id="_idIndexMarker406"/>semantic network, the <strong class="keyWord">paraphrase database</strong> (<strong class="keyWord">PPDB</strong>), Word2Vec, and GloVe as input. Produces 600d vectors [12, 13].</li>
</ul>
<p class="normal">For our example, we chose the 300d GloVe embeddings trained on the Gigaword corpus.</p>
<p class="normal">In order to keep our model size small, we <a id="_idIndexMarker407"/>want to only consider embeddings for words that exist in our vocabulary. This is done using the following code, which creates a smaller embedding matrix for each word in the vocabulary. Each row in the matrix corresponds to a word, and the row itself is the vector corresponding to the embedding for the word:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_embedding_matrix</span><span class="hljs-function">(</span><span class="hljs-params">sequences, word2idx, embedding_dim,</span>
<span class="hljs-params">       embedding_file</span><span class="hljs-function">):</span>
   <span class="hljs-keyword">if</span> os.path.exists(embedding_file):
       E = np.load(embedding_file)
   <span class="hljs-keyword">else</span>:
       vocab_size = <span class="hljs-built_in">len</span>(word2idx)
       E = np.zeros((vocab_size, embedding_dim))
       word_vectors = api.load(EMBEDDING_MODEL)
       <span class="hljs-keyword">for</span> word, idx <span class="hljs-keyword">in</span> word2idx.items():
           <span class="hljs-keyword">try</span>:
               E[idx] = word_vectors.word_vec(word)
           <span class="hljs-keyword">except</span> KeyError:   <span class="hljs-comment"># word not in embedding</span>
               <span class="hljs-keyword">pass</span>
       np.save(embedding_file, E)
   <span class="hljs-keyword">return</span> E
EMBEDDING_DIM = <span class="hljs-number">300</span>
DATA_DIR = <span class="hljs-string">"data"</span>
EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, <span class="hljs-string">"</span><span class="hljs-string">E.npy"</span>)
EMBEDDING_MODEL = <span class="hljs-string">"glove-wiki-gigaword-300"</span>
E = build_embedding_matrix(text_sequences, word2idx, 
   EMBEDDING_DIM,
   EMBEDDING_NUMPY_FILE)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Embedding matrix:"</span>, E.shape)
</code></pre>
<p class="normal">The output shape for the<a id="_idIndexMarker408"/> embedding matrix is (9010, 300), corresponding to the 9,010 tokens in the vocabulary, and 300 features in the third-party GloVe embeddings.</p>
<h2 class="heading-2" id="_idParaDest-110">Defining the spam classifier</h2>
<p class="normal">We are now ready to define our classifier. We will <a id="_idIndexMarker409"/>use a <strong class="keyWord">one-dimensional Convolutional Neural Network or ConvNet</strong> (<strong class="keyWord">1D CNN</strong>), similar to the<a id="_idIndexMarker410"/> network you have seen <a id="_idIndexMarker411"/>already in <em class="chapterRef">Chapter 3</em>, <em class="italic">Convolutional Neural Networks</em>.</p>
<p class="normal">The input is a sequence of integers. The first layer is an embedding layer, which converts each input integer to a vector of size (<code class="inlineCode">embedding_dim</code>). Depending on the run mode, that is, whether we will learn the embeddings from scratch, do transfer learning, or do fine-tuning, the embedding layer in the network would be slightly different. When the network starts with randomly initialized embedding weights (<code class="inlineCode">run_mode == "scratch"</code>) and learns the weights during the training, we set the <code class="inlineCode">trainable</code> parameter to <code class="inlineCode">True</code>. In the transfer learning case (<code class="inlineCode">run_mode == "vectorizer"</code>), we set the weights from our embedding matrix <code class="inlineCode">E</code> but set the <code class="inlineCode">trainable</code> parameter to <code class="inlineCode">False</code>, so it doesn’t train. In the fine-tuning case (<code class="inlineCode">run_mode == "finetuning"</code>), we set the embedding weights from our external matrix <code class="inlineCode">E</code>, as well as setting the layer to trainable.</p>
<p class="normal">The output of the embedding is fed into a convolutional layer. Here, fixed-size 3-token-wide 1D windows (<code class="inlineCode">kernel_size=3</code>), also called time steps, are convolved against 256 random filters (<code class="inlineCode">num_filters=256</code>) to produce vectors of size 256 for each time step. Thus, the output vector shape is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">time_steps</code>, <code class="inlineCode">num_filters</code>).</p>
<p class="normal">The output of the convolutional layer is sent to a 1D spatial dropout layer. Spatial dropout will randomly drop entire feature maps output from the convolutional layer. This is a regularization technique to prevent over-fitting. This is then sent through a global max pool layer, which takes the maximum value from each time step for each filter, resulting in a vector of shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">num_filters</code>).</p>
<p class="normal">The output of the dropout layer is fed into a pooling layer to flatten it, and then into a dense layer, which converts the<a id="_idIndexMarker412"/> vector of shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">num_filters</code>) to (<code class="inlineCode">batch_size</code>, <code class="inlineCode">num_classes</code>). A softmax activation will convert the scores for each of (spam, ham) into a probability distribution, indicating the probability of the input SMS being spam or ham respectively:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">SpamClassifierModel</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
   <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_sz, embed_sz, input_length,</span>
<span class="hljs-params">           num_filters, kernel_sz, output_sz,</span>
<span class="hljs-params">           run_mode, embedding_weights,</span>
<span class="hljs-params">           **kwargs</span><span class="hljs-function">):</span>
       <span class="hljs-built_in">super</span>(SpamClassifierModel, self).__init__(**kwargs)
       <span class="hljs-keyword">if</span> run_mode == <span class="hljs-string">"scratch"</span>:
           self.embedding = tf.keras.layers.Embedding(vocab_sz,
               embed_sz,
               input_length=input_length,
               trainable=<span class="hljs-literal">True</span>)
       <span class="hljs-keyword">elif</span> run_mode == <span class="hljs-string">"vectorizer"</span>:
           self.embedding = tf.keras.layers.Embedding(vocab_sz,
               embed_sz,
               input_length=input_length,
               weights=[embedding_weights],
               trainable=<span class="hljs-literal">False</span>)
       <span class="hljs-keyword">else</span>:
           self.embedding = tf.keras.layers.Embedding(vocab_sz,
               embed_sz,
               input_length=input_length,
               weights=[embedding_weights],
               trainable=<span class="hljs-literal">True</span>)
       self.conv = tf.keras.layers.Conv1D(filters=num_filters,
           kernel_size=kernel_sz,
           activation=<span class="hljs-string">"relu"</span>)
       self.dropout = tf.keras.layers.SpatialDropout1D(<span class="hljs-number">0.2</span>)
       self.pool = tf.keras.layers.GlobalMaxPooling1D()
       self.dense = tf.keras.layers.Dense(output_sz,
           activation=<span class="hljs-string">"softmax"</span>)
   <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x</span><span class="hljs-function">):</span>
       x = self.embedding(x)
       x = self.conv(x)
       x = self.dropout(x)
       x = self.pool(x)
       x = self.dense(x)
       <span class="hljs-keyword">return</span> x
<span class="hljs-comment"># model definition</span>
conv_num_filters = <span class="hljs-number">256</span>
conv_kernel_size = <span class="hljs-number">3</span>
model = SpamClassifierModel(
   vocab_size, EMBEDDING_DIM, max_seqlen,
   conv_num_filters, conv_kernel_size, NUM_CLASSES,
   run_mode, E)
model.build(input_shape=(<span class="hljs-literal">None</span>, max_seqlen))
</code></pre>
<p class="normal">Finally, we compile the model using the categorical cross entropy loss function and the Adam optimizer:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># compile</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">"adam"</span>, loss=<span class="hljs-string">"categorical_crossentropy"</span>, metrics=[<span class="hljs-string">"accuracy"</span>])
</code></pre>
<h2 class="heading-2" id="_idParaDest-111">Training and evaluating the model</h2>
<p class="normal">One thing to notice is that the <a id="_idIndexMarker413"/>dataset is somewhat imbalanced; there are only 747 instances of spam compared to 4,827 instances of ham. The network could achieve close to 87% accuracy simply by always predicting the <a id="_idIndexMarker414"/>majority class. To alleviate this problem, we set class weights to indicate that an error on a spam SMS is eight times as expensive as an error on a ham SMS. This is indicated by the <code class="inlineCode">CLASS_WEIGHTS</code> variable, which is passed into the <code class="inlineCode">model.fit()</code> call as an additional parameter.</p>
<p class="normal">After training for 3 epochs, we evaluate the model against the test set, and report the accuracy and confusion matrix of the model against the test set. However, for imbalance data, even with the use of class weights, the model may end up learning to always predict the majority class. Therefore, it is generally advisable to report accuracy on a per-class basis to make sure that the model learns to distinguish each class effectively. This can be done quite easily using the confusion matrix by dividing the diagonal element for each row<a id="_idIndexMarker415"/> by the sum of<a id="_idIndexMarker416"/> elements for that row, where each row corresponds to a labeled class:</p>
<pre class="programlisting code"><code class="hljs-code">NUM_EPOCHS = <span class="hljs-number">3</span>
<span class="hljs-comment"># data distribution is 4827 ham and 747 spam (total 5574), which</span>
<span class="hljs-comment"># works out to approx 87% ham and 13% spam, so we take reciprocals</span>
<span class="hljs-comment"># and this works out to being each spam (1) item as being </span>
<span class="hljs-comment"># approximately 8 times as important as each ham (0) message.</span>
CLASS_WEIGHTS = { <span class="hljs-number">0</span>: <span class="hljs-number">1</span>, <span class="hljs-number">1</span>: <span class="hljs-number">8</span> }
<span class="hljs-comment"># train model</span>
model.fit(train_dataset, epochs=NUM_EPOCHS,
   validation_data=val_dataset,
   class_weight=CLASS_WEIGHTS)
<span class="hljs-comment"># evaluate against test set</span>
labels, predictions = [], []
<span class="hljs-keyword">for</span> Xtest, Ytest <span class="hljs-keyword">in</span> test_dataset:
   Ytest_ = model.predict_on_batch(Xtest)
   ytest = np.argmax(Ytest, axis=<span class="hljs-number">1</span>)
   ytest_ = np.argmax(Ytest_, axis=<span class="hljs-number">1</span>)
   labels.extend(ytest.tolist())
   predictions.extend(ytest.tolist())
<span class="hljs-built_in">print</span>(<span class="hljs-string">"test accuracy: {:.3f}"</span>.<span class="hljs-built_in">format</span>(accuracy_score(labels, predictions)))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"confusion matrix"</span>)
<span class="hljs-built_in">print</span>(confusion_matrix(labels, predictions))
</code></pre>
<h2 class="heading-2" id="_idParaDest-112">Running the spam detector</h2>
<p class="normal">The three scenarios we <a id="_idIndexMarker417"/>want to look at are:</p>
<ul>
<li class="bulletList">Letting the network learn the embedding for the task.</li>
<li class="bulletList">Starting with a fixed external third-party embedding where the embedding matrix is treated like a vectorizer to transform the sequence of integers into a sequence of vectors.</li>
<li class="bulletList">Starting with an external third-party embedding which is further fine-tuned to the task during the training.</li>
</ul>
<p class="normal">Each scenario can be evaluated by setting the value of the <code class="inlineCode">mode</code> argument as shown in the following command:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python spam_classifier --mode [scratch|vectorizer|finetune]
</code></pre>
<p class="normal">The dataset is small, and the model is fairly simple. We were able to achieve very good results (validation set accuracies in the high 90s, and perfect test set accuracy) with only minimal training (3 epochs). In all three cases, the network achieved a perfect score, accurately predicting the 1,111 ham messages, as well as the 169 spam cases.</p>
<p class="normal">The change in validation accuracies, shown in <em class="italic">Figure 4.3</em>, illustrates the differences between the three approaches:</p>
<figure class="mediaobject"><img alt="" height="471" src="../Images/B18331_04_03.png" width="707"/></figure>
<p class="packt_figref">Figure 4.3: Comparison of validation accuracy across training epochs for different embedding techniques</p>
<p class="normal">In the learning from scratch case, at the end of the first epoch, the validation accuracy is 0.93, but over the next two epochs, it rises to 0.98. In the vectorizer case, the network gets something of a head start from the third-party embeddings and ends up with a validation accuracy of almost 0.95 at the end of the first epoch. However, because the embedding weights are <a id="_idIndexMarker418"/>not allowed to change, it is not able to customize the embeddings to the spam detection task, and the validation accuracy at the end of the third epoch is the lowest among the three. The fine-tuning case, like the vectorizer, also gets a head start, but can customize the embedding to the task as well, and therefore is able to learn at the most rapid rate among the three cases. The fine-tuning case has the highest validation accuracy at the end of the first epoch and reaches the same validation accuracy at the end of the second epoch that the scratch case achieves at the end of the third.</p>
<p class="normal">In the next section, we will see that distributional similarity is not restricted to word embeddings; it applies to other scenarios as well.</p>
<h1 class="heading-1" id="_idParaDest-113">Neural embeddings – not just for words</h1>
<p class="normal">Word embedding technology has evolved in various ways since Word2Vec and GloVe. One such direction is the application of <a id="_idIndexMarker419"/>word embeddings to non-word settings, also known as neural embeddings. As you will recall, word embeddings leverage the distributional hypothesis that words occurring in similar contexts tend to have similar meanings, where context is usually a fixed-size (in number of words) window around the target word.</p>
<p class="normal">The idea of neural embeddings is very similar; that is, entities that occur in similar contexts tend to be strongly related to each other. The way in which these contexts are constructed is usually situation-dependent. We will describe two techniques here that are foundational and general enough to be applied easily to a variety of use cases.</p>
<h2 class="heading-2" id="_idParaDest-114">Item2Vec</h2>
<p class="normal">The Item2Vec embedding model was<a id="_idIndexMarker420"/> originally proposed by Barkan and Koenigstein [14] for the collaborative filtering use case, that is, recommending items to users <a id="_idIndexMarker421"/>based on purchases by other users that have similar purchase histories to this user. It uses items in a web-store as the “words” and the itemset (the sequence of items purchased by a user over time) as the “sentence” from which the “word context” is derived.</p>
<p class="normal">For example, consider the problem of recommending items to shoppers in a supermarket. Assume that our supermarket sells 5,000 items, so each item can be represented as a sparse one-hot encoded vector of size 5,000. Each user is represented by their shopping cart, which is a sequence of such vectors. Applying a context window similar to the one we saw in the Word2Vec section, we can train a skip-gram model to predict likely item pairs. The learned embedding model maps the items to a dense low-dimensional space where similar items are close together, which can be used to make similar item recommendations.</p>
<h2 class="heading-2" id="_idParaDest-115">node2vec</h2>
<p class="normal">The <a id="_idIndexMarker422"/>node2vec embedding model was<a id="_idIndexMarker423"/> proposed by Grover and Leskovec [15], as a scalable way to learn features for nodes in a graph. It learns an embedding of the structure of the graph by executing a large number of fixed-length random walks on the graph. The nodes are the “words” and the random walks are the “sentences” from which the “word context” is derived in node2vec.</p>
<p class="normal">The <strong class="keyWord">Something2Vec</strong> page [40] provides a comprehensive list of ways in which researchers have tried to apply the distributional hypothesis to entities other than words. Hopefully, this list will spark ideas for your own “Something2Vec” representation.</p>
<p class="normal">To illustrate how easy it is to create your own neural embedding, we will generate a node2vec-like model<a id="_idIndexMarker424"/> or, more accurately, a predecessor graph-based embedding called DeepWalk, proposed by<a id="_idIndexMarker425"/> Perozzi, et al. [42] for papers presented at the NeurIPS conference from 1987-2015, by leveraging word co-occurrence relationships between them.</p>
<p class="normal">The dataset is a 11,463 × 5,812 matrix of word counts, where the rows represent words, and columns represent <a id="_idIndexMarker426"/>conference papers. We will use this to construct a graph of papers, where an edge between two papers represents a word that occurs in both of them. Both node2vec and DeepWalk assume that the graph is undirected and unweighted. Our graph is undirected, since a relationship between a pair of papers is bidirectional. However, our edges could have weights based on the number of word co-occurrences between the two documents. For our example, we will consider any number of co-occurrences above 0 to be a valid unweighted edge.</p>
<p class="normal">As usual, we will start by declaring our imports:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gensim
<span class="hljs-keyword">import</span> logging
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> scipy.sparse <span class="hljs-keyword">import</span> csr_matrix
<span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity
logging.basicConfig(<span class="hljs-built_in">format</span>=<span class="hljs-string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)
</code></pre>
<p class="normal">The next step is to download the data from the UCI repository and convert it to a sparse term-document matrix, TD, then construct a document-document matrix E by multiplying the transpose of the term-document matrix by itself. Our graph is represented as an adjacency or edge matrix by the document-document matrix. Since each element represents a similarity between two documents, we will binarize the matrix <code class="inlineCode">E</code> by <a id="_idIndexMarker427"/>setting any non-zero elements to 1:</p>
<pre class="programlisting code"><code class="hljs-code">DATA_DIR = <span class="hljs-string">"./data"</span>
UCI_DATA_URL = <span class="hljs-string">"https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv"</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">download_and_read</span><span class="hljs-function">(</span><span class="hljs-params">url</span><span class="hljs-function">):</span>
   local_file = url.split(<span class="hljs-string">'/'</span>)[-<span class="hljs-number">1</span>]
   p = tf.keras.utils.get_file(local_file, url, cache_dir=<span class="hljs-string">"."</span>)
   row_ids, col_ids, data = [], [], []
   rid = <span class="hljs-number">0</span>
   f = <span class="hljs-built_in">open</span>(p, <span class="hljs-string">"r"</span>)
   <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:
       line = line.strip()
       <span class="hljs-keyword">if</span> line.startswith(<span class="hljs-string">"\"\","</span>):
           <span class="hljs-comment"># header</span>
           <span class="hljs-keyword">continue</span>
       <span class="hljs-comment"># compute non-zero elements for current row</span>
       counts = np.array([<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> line.split(<span class="hljs-string">','</span>)[<span class="hljs-number">1</span>:]])
       nz_col_ids = np.nonzero(counts)[<span class="hljs-number">0</span>]
       nz_data = counts[nz_col_ids]
       nz_row_ids = np.repeat(rid, <span class="hljs-built_in">len</span>(nz_col_ids))
       rid += <span class="hljs-number">1</span>
       <span class="hljs-comment"># add data to big lists</span>
       row_ids.extend(nz_row_ids.tolist())
       col_ids.extend(nz_col_ids.tolist())
       data.extend(nz_data.tolist())
   f.close()
   TD = csr_matrix((
       np.array(data), (
           np.array(row_ids), np.array(col_ids)
           )
       ),
       shape=(rid, counts.shape[<span class="hljs-number">0</span>]))
   <span class="hljs-keyword">return</span> TD
<span class="hljs-comment"># read data and convert to Term-Document matrix</span>
TD = download_and_read(UCI_DATA_URL)
<span class="hljs-comment"># compute undirected, unweighted edge matrix</span>
E = TD.T * TD
<span class="hljs-comment"># binarize</span>
E[E &gt; <span class="hljs-number">0</span>] = <span class="hljs-number">1</span>
</code></pre>
<p class="normal">Once we have our<a id="_idIndexMarker428"/> sparse binarized adjacency matrix, <code class="inlineCode">E</code>, we can then generate random walks from each of the vertices. From each node, we construct 32 random walks of a maximum length of 40 nodes. The walks have a random restart probability of 0.15, which means that for any node, the particular random walk could end with a 15% probability. The following code will construct the random walks and write them out to a file given by <code class="inlineCode">RANDOM_WALKS_FILE</code>. To give an idea of the input, we have provided a snapshot of the first 10 lines of this file, showing random walks starting from node 0:</p>
<pre class="programlisting con"><code class="hljs-con">0 1405 4845 754 4391 3524 4282 2357 3922 1667
0 1341 456 495 1647 4200 5379 473 2311
0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273
0 906 3498 2286 4755 2567 2632
0 5769 638 3574 79 2825 3532 2363 360 1443 4789 229 4515 3014 3683 2967 5206 2288 1615 1166
0 2469 1353 5596 2207 4065 3100
0 2236 1464 1596 2554 4021
0 4688 864 3684 4542 3647 2859
0 4884 4590 5386 621 4947 2784 1309 4958 3314
0 5546 200 3964 1817 845
</code></pre>
<p class="normal">Note that this is a very <a id="_idIndexMarker429"/>slow process. A copy of the output is provided along with the source code for this chapter in case you prefer to skip the random walk <a id="_idIndexMarker430"/>generation process:</p>
<pre class="programlisting code"><code class="hljs-code">NUM_WALKS_PER_VERTEX = <span class="hljs-number">32</span>
MAX_PATH_LENGTH = <span class="hljs-number">40</span>
RESTART_PROB = <span class="hljs-number">0.15</span>
RANDOM_WALKS_FILE = os.path.join(DATA_DIR, <span class="hljs-string">"random-walks.txt"</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">construct_random_walks</span><span class="hljs-function">(</span><span class="hljs-params">E, n, alpha, l, ofile</span><span class="hljs-function">):</span>
   <span class="hljs-keyword">if</span> os.path.exists(ofile):
       <span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">random walks generated already, skipping"</span>)
       <span class="hljs-keyword">return</span>
   f = <span class="hljs-built_in">open</span>(ofile, <span class="hljs-string">"w"</span>)
   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(E.shape[<span class="hljs-number">0</span>]):  <span class="hljs-comment"># for each vertex</span>
       <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
           <span class="hljs-built_in">print</span>(<span class="hljs-string">"{:d} random walks generated from {:d} vertices"</span>
               .<span class="hljs-built_in">format</span>(n * i, i))
       <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):       <span class="hljs-comment"># construct n random walks</span>
           curr = i
           walk = [curr]
           target_nodes = np.nonzero(E[curr])[<span class="hljs-number">1</span>]
           <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(l):   <span class="hljs-comment"># each of max length l</span>
               <span class="hljs-comment"># should we restart?</span>
               <span class="hljs-keyword">if</span> np.random.random() &lt; alpha <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(walk) &gt; <span class="hljs-number">5</span>:
                   <span class="hljs-keyword">break</span>
               <span class="hljs-comment"># choose one outgoing edge and append to walk</span>
               <span class="hljs-keyword">try</span>:
                   curr = np.random.choice(target_nodes)
                   walk.append(curr)
                   target_nodes = np.nonzero(E[curr])[<span class="hljs-number">1</span>]
               <span class="hljs-keyword">except</span> ValueError:
                   <span class="hljs-keyword">continue</span>
           f.write(<span class="hljs-string">"{:s}\n"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-string">" "</span>.join([<span class="hljs-built_in">str</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> walk])))
   <span class="hljs-built_in">print</span>(<span class="hljs-string">"{:d} random walks generated from {:d} vertices, COMPLETE"</span>
       .<span class="hljs-built_in">format</span>(n * i, i))
   f.close()
<span class="hljs-comment"># construct random walks (caution: very long process!)</span>
construct_random_walks(E, NUM_WALKS_PER_VERTEX, RESTART_PROB, MAX_PATH_LENGTH, RANDOM_WALKS_FILE)
</code></pre>
<p class="normal">A few lines from the <code class="inlineCode">RANDOM_WALKS_FILE</code> are shown below. You could imagine that these look like sentences in a language where the vocabulary of words is all the node IDs in our graph. We have<a id="_idIndexMarker431"/> learned that word embeddings exploit the structure of language to generate a distributional <a id="_idIndexMarker432"/>representation for words. Graph embedding schemes such as DeepWalk and node2vec do the exact same thing with these “sentences” created out of random walks. Such embeddings can capture similarities between nodes in a graph that go beyond immediate neighbors, as we shall see:</p>
<pre class="programlisting con"><code class="hljs-con">0 1405 4845 754 4391 3524 4282 2357 3922 1667
0 1341 456 495 1647 4200 5379 473 2311
0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273
0 906 3498 2286 4755 2567 2632
0 5769 638 3574 79 2825 3532 2363 360 1443 4789 229 4515 3014 3683 2967 5206 2288 1615 1166
0 2469 1353 5596 2207 4065 3100
0 2236 1464 1596 2554 4021
0 4688 864 3684 4542 3647 2859
0 4884 4590 5386 621 4947 2784 1309 4958 3314
0 5546 200 3964 1817 845
</code></pre>
<p class="normal">We are now ready to create our word embedding model. The Gensim package offers a simple API that allows us to declaratively create and train a Word2Vec model, using the following code. The trained model will be serialized to the file given by <code class="inlineCode">W2V_MODEL_FILE</code>. The <code class="inlineCode">Documents</code> class allows us to stream large input files to train the Word2Vec model without running into memory issues. We will train the Word2Vec model in skip-gram mode with a window size of 10, which means we train it to predict up to five neighboring vertices <a id="_idIndexMarker433"/>given a central vertex. The resulting embedding for each vertex is a dense vector of size 128:</p>
<pre class="programlisting code"><code class="hljs-code">W2V_MODEL_FILE = os.path.join(DATA_DIR, <span class="hljs-string">"</span><span class="hljs-string">w2v-neurips-papers.model"</span>)
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Documents</span><span class="hljs-class">(</span><span class="hljs-built_in">object</span><span class="hljs-class">):</span>
   <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, input_file</span><span class="hljs-function">):</span>
       self.input_file = input_file
   <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__iter__</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
       <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(self.input_file, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> f:
           <span class="hljs-keyword">for</span> i, line <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(f):
               <span class="hljs-keyword">if</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:
                   logging.info(<span class="hljs-string">"{:d} random walks extracted"</span>.<span class="hljs-built_in">format</span>(i))
               <span class="hljs-keyword">yield</span> line.strip().split()
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train_word2vec_model</span><span class="hljs-function">(</span><span class="hljs-params">random_walks_file, model_file</span><span class="hljs-function">):</span>
   <span class="hljs-keyword">if</span> os.path.exists(model_file):
       <span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">Model file {:s} already present, skipping training"</span>
           .<span class="hljs-built_in">format</span>(model_file))
       <span class="hljs-keyword">return</span>
   docs = Documents(random_walks_file)
   model = gensim.models.Word2Vec(
       docs,
       size=<span class="hljs-number">128</span>,    <span class="hljs-comment"># size of embedding vector</span>
       window=<span class="hljs-number">10</span>,   <span class="hljs-comment"># window size</span>
       sg=<span class="hljs-number">1</span>,        <span class="hljs-comment"># skip-gram model</span>
       min_count=<span class="hljs-number">2</span>,
       workers=<span class="hljs-number">4</span>
   )
   model.train(
       docs,
       total_examples=model.corpus_count,
       epochs=<span class="hljs-number">50</span>)
   model.save(model_file)
<span class="hljs-comment"># train model</span>
train_word2vec_model(RANDOM_WALKS_FILE, W2V_MODEL_FILE)
</code></pre>
<p class="normal">Our resulting DeepWalk <a id="_idIndexMarker434"/>model is just a Word2Vec model, so anything you can do with Word2Vec in the context of words, you can do with this model in the context of vertices. Let us use the model to discover similarities between documents:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">evaluate_model</span><span class="hljs-function">(</span><span class="hljs-params">td_matrix, model_file, source_id</span><span class="hljs-function">):</span>
   model = gensim.models.Word2Vec.load(model_file).wv
   most_similar = model.most_similar(<span class="hljs-built_in">str</span>(source_id))
   scores = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> most_similar]
   target_ids = [x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> most_similar]
   <span class="hljs-comment"># compare top 10 scores with cosine similarity </span>
   <span class="hljs-comment"># between source and each target</span>
   X = np.repeat(td_matrix[source_id].todense(), <span class="hljs-number">10</span>, axis=<span class="hljs-number">0</span>)
   Y = td_matrix[target_ids].todense()
   cosims = [cosine_similarity(X[i], Y[i])[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)]
   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):
       <span class="hljs-built_in">print</span>(<span class="hljs-string">"{:d} {:s} {:.3f} {:.3f}"</span>.<span class="hljs-built_in">format</span>(
           source_id, target_ids[i], cosims[i], scores[i]))
source_id = np.random.choice(E.shape[<span class="hljs-number">0</span>])
evaluate_model(TD, W2V_MODEL_FILE, source_id)
</code></pre>
<p class="normal">The following output is shown. The first and second columns are the source and target vertex IDs. The third column is the cosine similarity between the term vectors corresponding to<a id="_idIndexMarker435"/> the source and target documents, and the fourth is the similarity score reported by the Word2Vec<a id="_idIndexMarker436"/> model. As you can see, cosine similarity reports a similarity only between 2 of the 10 document pairs, but the Word2Vec model is able to detect latent similarities in the embedding space. This is similar to the behavior we have noticed between one-hot encoding and dense embeddings:</p>
<pre class="programlisting con"><code class="hljs-con">src_id dst_id cosine_sim w2v_score
1971   5443        0.000     0.348
1971   1377        0.000     0.348
1971   3682        0.017     0.328
1971   51          0.022     0.322
1971   857         0.000     0.318
1971   1161        0.000     0.313
1971   4971        0.000     0.313
1971   5168        0.000     0.312
1971   3099        0.000     0.311
1971   462         0.000     0.310
</code></pre>
<p class="normal">The code for this embedding strategy is available in <code class="inlineCode">neurips_papers_node2vec.py</code> in the source code folder accompanying this chapter. Next, we will move on to look at character and subword<a id="_idIndexMarker437"/> embeddings.</p>
<h1 class="heading-1" id="_idParaDest-116">Character and subword embeddings</h1>
<p class="normal">Another evolution of the basic word embedding strategy has been to look at character and subword embeddings instead <a id="_idIndexMarker438"/>of word embeddings. Character-level embeddings were first proposed by <em class="italic">Xiang</em> and <em class="italic">LeCun</em> [17] and have some key advantages over word embeddings.</p>
<p class="normal">First, a character <a id="_idIndexMarker439"/>vocabulary is finite and small – for example, a vocabulary for English would contain around 70 characters (26 characters, 10 numbers, and the rest special characters), leading to character models that are also small and compact. Second, unlike word embeddings, which provide vectors for a large but finite set of words, there is no concept of out-of-vocabulary for character embeddings, since any word can be represented by the vocabulary. Third, character embeddings tend to be better for rare and misspelled words because there is much less imbalance for character inputs than for word inputs.</p>
<p class="normal">Character embeddings tend to work better for applications that require the notion of syntactic rather than semantic similarity. However, unlike word embeddings, character embeddings tend to be task-specific and are usually generated inline within a network to support the task. For this reason, third-party character embeddings are generally not available.</p>
<p class="normal">Subword embeddings combine the idea of character and word embeddings by treating a word as a bag of character n-grams, that is, sequences of <em class="italic">n</em> consecutive words. They were first proposed by Bojanowski, et al. [18] based on<a id="_idIndexMarker440"/> research from <strong class="keyWord">Facebook AI Research</strong> (<strong class="keyWord">FAIR</strong>), which they later released as fastText embeddings. fastText embeddings are available for 157 languages, including English. The paper has reported state-of-the-art performance on a number of NLP tasks, especially word analogies and language tasks for languages with rich morphologies.</p>
<p class="normal">fastText computes embeddings for character n-grams where n is between 3 and 6 characters (default settings can be changed), as well as for the words themselves. For example, character n-grams for n=3 for the word “green” would be “&lt;gr”, “gre”, “ree”, “een”, and “en&gt;”. The beginning and end of words are marked with “&lt;” and “&gt;” characters respectively, to distinguish between short words and their n-grams such as “&lt;cat&gt;” and “cat”.</p>
<p class="normal">During lookup, you can look up a vector from the fastText embedding using the word as the key if the word exists in the embedding. However, unlike traditional word embeddings, you can<a id="_idIndexMarker441"/> still construct a fastText vector for a word that does not exist in the<a id="_idIndexMarker442"/> embedding. This is done by decomposing the word into its constituent trigram subwords as shown in the preceding example, looking up the vectors for the subwords, and then taking the average of these subword vectors. The fastText Python API [19] will do this automatically, but you will need to do this manually if you use other APIs to access fastText word embeddings, such as Gensim or NumPy.</p>
<p class="normal">Next up, we will look at dynamic embeddings.</p>
<h1 class="heading-1" id="_idParaDest-117">Dynamic embeddings</h1>
<p class="normal">So far, all the embeddings we have considered have been static; that is, they are deployed as a dictionary of words (and subwords) mapped to fixed dimensional vectors. The vector corresponding to a<a id="_idIndexMarker443"/> word in these embeddings is going to be the same regardless of whether it is being used as a noun or verb in the sentence, for example, the word “ensure” (the name of a health supplement when used as a noun, and to make certain when used as a verb). It also provides the same vector for polysemous words or words with multiple meanings, such as “bank” (which can mean different things depending on whether it co-occurs with the word “money” or “river”). In both cases, the meaning of the word changes depending on clues available in its context, the sentence. Dynamic embeddings attempt to use these signals to provide different vectors for words based on their context.</p>
<p class="normal">Dynamic embeddings are deployed as trained networks that convert your input (typically a sequence of one-hot vectors) into a lower-dimensional dense fixed-size embedding by looking at the entire sequence, not just individual words. You can either preprocess your input to this dense embedding and then use this as input to your task-specific network, or wrap the network and treat it similar to the <code class="inlineCode">tf.keras.layers.Embedding</code> layer for static embeddings. Using a dynamic embedding network in this way is usually much more expensive compared to generating it ahead of time (the first option) or using traditional embeddings.</p>
<p class="normal">The earliest dynamic embedding was proposed by McCann, et al. [20], and was called <strong class="keyWord">Contextualized Vectors</strong> (<strong class="keyWord">CoVe</strong>). This involved <a id="_idIndexMarker444"/>taking the output of the encoder from the encoder-decoder pair of a machine translation network and concatenating it with word vectors for the same word. </p>
<p class="normal">You will learn more about seq2seq networks in the next chapter. The researchers found that this strategy improved the performance of a wide variety of NLP tasks.</p>
<p class="normal">Another dynamic embedding proposed by Peters, et al. [21], was <strong class="keyWord">Embeddings from Language Models</strong> (<strong class="keyWord">ELMo</strong>). ELMo computes<a id="_idIndexMarker445"/> contextualized word representations using character-based word representation and<a id="_idIndexMarker446"/> bidirectional <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>). You will learn more about LSTMs in the next chapter. In the meantime, a trained ELMo network is available from<a id="_idIndexMarker447"/> TensorFlow’s model repository TensorFlow Hub. You can access it and use it for generating ELMo embeddings as follows.</p>
<p class="normal">The full set of models available on TensorFlow Hub that are TensorFlow 2.0 compatible can be found on the TensorFlow Hub site for TensorFlow 2.0 [16]. Here I have used an array of sentences, where the model will figure out tokens by using its default strategy of tokenizing on whitespace:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub
 
elmo = hub.load(<span class="hljs-string">"https://tfhub.dev/google/elmo/3"</span>)
embeddings = elmo.signatures[<span class="hljs-string">"default"</span>](
    tf.constant([
      <span class="hljs-string">"i like green eggs and ham"</span>,
      <span class="hljs-string">"would you eat them in a box"</span>
    ]))[<span class="hljs-string">"elmo"</span>]
<span class="hljs-built_in">print</span>(embeddings.shape)
</code></pre>
<p class="normal">The output is (<code class="inlineCode">2, 7, 1024</code>). The first index tells us that our input contained 2 sentences. The second index refers to the maximum number of words across all sentences, in this case, 7. The model automatically pads the output to the longest sentence. The third index gives us the size of the contextual word embedding created by ELMo; each word is converted to a vector of size (1024).</p>
<p class="normal">You can also integrate the ELMo embedding layer into your TF2 model by wrapping it in a <code class="inlineCode">tf.keras.KerasLayer</code> adapter. In this simple model, the model will return the embedding for the entire string:</p>
<pre class="programlisting code"><code class="hljs-code">embed = hub.KerasLayer(<span class="hljs-string">"https://tfhub.dev/google/elmo/3"</span>,input_shape=[], dtype=tf.string)
model = tf.keras.Sequential([embed])
embeddings = model.predict([
    <span class="hljs-string">"i i like green eggs and ham"</span>,
    <span class="hljs-string">"would you eat them in a box"</span>
])
<span class="hljs-built_in">print</span>(embeddings.shape)
</code></pre>
<p class="normal">Dynamic embeddings such as ELMo are able to provide different embeddings for the same word when used in different contexts and represent an improvement over static embeddings such as <a id="_idIndexMarker448"/>Word2Vec or GloVe. A logical next step is embeddings that represent larger units of text, such as sentences and paragraphs. This is what we will look at in the next section.</p>
<h1 class="heading-1" id="_idParaDest-118">Sentence and paragraph embeddings</h1>
<p class="normal">A simple, yet surprisingly effective<a id="_idIndexMarker449"/> solution for generating useful sentence and paragraph embeddings is to average the word vectors of their constituent words. Even though we will describe some popular sentence and paragraph embeddings in this <a id="_idIndexMarker450"/>section, it is generally always advisable to try averaging the word vectors as a baseline.</p>
<p class="normal">Sentence (and paragraph) embeddings can also be created in a task-optimized way by treating them as a sequence of words and representing each word using some standard word vector. The sequence of word vectors is used as input to train a network for some specific task. Vectors extracted from one of the later layers of the network just before the classification layer generally tend to produce a very good vector representation for the sequence. However, they tend to be very task-specific, and are of limited use as a general vector representation.</p>
<p class="normal">An idea for generating general vector representations for sentences that could be used across tasks was proposed by Kiros, et al. [22]. They proposed using the continuity of text from books to construct an encoder-decoder model that is trained to predict surrounding sentences given a sentence. The vector representation of a sequence of words constructed by an encoder-decoder network is<a id="_idIndexMarker451"/> typically called a “thought vector.” In addition, the proposed model works on a very similar basis to skip-gram, where we try to predict the surrounding words given a word. For these reasons, these sentence vectors were called skip-thought vectors. The project released a Theano-based model<a id="_idIndexMarker452"/> that could be used to generate embeddings from sentences. Later, the model was re-implemented with TensorFlow by the Google Research team [23]. The Skip-Thoughts model emits vectors of size (2048) for each sentence. Using the model is not very straightforward, but the <code class="inlineCode">README.md</code> file on the repository [23] provides instructions if you would like to use it.</p>
<p class="normal">A more convenient source of sentence embeddings is the Google Universal Sentence Encoder, available on TensorFlow Hub. There are two flavors of the encoder in terms of implementation. The first flavor is fast but <a id="_idIndexMarker453"/>not so accurate and is based on the <strong class="keyWord">Deep Averaging Network</strong> (<strong class="keyWord">DAN</strong>) proposed by Iyer, et al. [24], which combines embeddings for words and bigrams and sends it through a <a id="_idIndexMarker454"/>fully connected network. The second flavor is <a id="_idIndexMarker455"/>much more accurate but slower and is based on the encoder component of the transformer network proposed by Vaswani, et al. [25]. We will cover the transformer network in more detail in <em class="chapterRef">Chapter 6</em>, <em class="italic">Transformers</em>.</p>
<p class="normal">As with ELMo, the Google Universal Sentence Encoder can also be loaded from TensorFlow Hub into your TF2 code. Here is some code that calls it with two of our example sentences:</p>
<pre class="programlisting code"><code class="hljs-code">embed = hub.load(<span class="hljs-string">"https://tfhub.dev/google/universal-sentence-encoder-large/4"</span>)
embeddings = embed([
<span class="hljs-string">"i like green eggs and ham"</span>,
<span class="hljs-string">"would you eat them in a box"</span>
])[<span class="hljs-string">"outputs"</span>]
<span class="hljs-built_in">print</span>(embeddings.shape)
</code></pre>
<p class="normal">The output is (<code class="inlineCode">2</code>, <code class="inlineCode">512</code>); that is, each sentence is represented by a vector of size (512). It is important to note that the Google Universal Sentence Encoder can handle any length of word sequence—so you could legitimately use it to get word embeddings on one end as well as paragraph embeddings on the other. However, as the sequence length gets longer, the quality of the embeddings tends to get “diluted.”</p>
<p class="normal">A much earlier related line of work in producing embeddings for long sequences such as paragraphs and documents was proposed by Le and Mikolov [26] soon after Word2Vec was proposed. It is now known interchangeably as Doc2Vec or Paragraph2Vec. The Doc2Vec algorithm is an extension of Word2Vec that uses surrounding words to predict a word. In the case of Doc2Vec, an additional parameter, the paragraph ID, is provided during training. At the end of the training, the Doc2Vec network learns an embedding for every word and an embedding for every paragraph. During inference, the network is given a paragraph with some missing words. The network uses the known part of the paragraph to produce a paragraph embedding, then uses this paragraph embedding and the word embeddings to infer the missing words in<a id="_idIndexMarker456"/> the paragraph. The Doc2Vec algorithm comes in two flavors—the <strong class="keyWord">Paragraph Vectors - Distributed Memory</strong> (<strong class="keyWord">PV-DM</strong>) and <strong class="keyWord">Paragraph Vectors - Distributed Bag of Words</strong> (<strong class="keyWord">PV-DBOW</strong>), roughly analogous to <a id="_idIndexMarker457"/>CBOW and skip-gram in<a id="_idIndexMarker458"/> Word2Vec. We will not look at Doc2Vec further in this book, except to<a id="_idIndexMarker459"/> note that the Gensim toolkit provides prebuilt implementations that you can train with your own corpus.</p>
<p class="normal">Having looked at the different forms of static and dynamic embeddings, we will now switch gears a bit and look at language model-based embeddings.</p>
<h1 class="heading-1" id="_idParaDest-119">Language model-based embeddings</h1>
<p class="normal">Language model-based <a id="_idIndexMarker460"/>embeddings represent the next step in the evolution of word embeddings. A language model is a probability distribution over sequences of words. Once we have a model, we can ask it to predict the most likely next word given a particular sequence of words. Similar to traditional word embeddings, both static and dynamic, they are trained to predict the next word (or previous word as well, if the language model is bidirectional) given a partial sentence from the corpus. Training does not involve active labeling, since it leverages the natural grammatical structure of large volumes of text, so in a sense, this is a self-supervised learning process.</p>
<p class="normal">The main difference between a language model as a word embedding and more traditional embeddings is that traditional embeddings are applied as a single initial transformation on the data and are then fine-tuned for specific tasks. In contrast, language models are trained on large external corpora and represent a model of a particular language, say English. This step is called<a id="_idIndexMarker461"/> pretraining. The computing cost to pretrain these language models is usually fairly high; however, the people who pretrain these models generally make them available for use by others so we usually do not need to worry about this step. The next step is to fine-tune these general-purpose language models for your particular application domain. For example, if you are working in the travel or healthcare industry, you would fine-tune the language model with text from your own domain. Fine-tuning involves retraining the last few layers with your own text. Once fine-tuned, you can reuse this model for multiple tasks within your domain. The fine-tuning step is generally much less expensive compared to the pretraining step.</p>
<p class="normal">Once you have the fine-tuned language model, you remove the last layer of the language model and replace<a id="_idIndexMarker462"/> it with a one-to-two-layer fully connected network that converts the language model embedding for your input into the final categorical or regression output that your task needs. The idea is identical to transfer learning, which you learned about in <em class="chapterRef">Chapter 3</em>, <em class="italic">Convolutional Neural Networks</em>, the only difference here is that you are doing transfer learning on text instead of images. As with transfer learning with images, these language model-based embeddings allow us to get surprisingly good results with very little labeled data. Not surprisingly, language model embeddings have been referred to as the “ImageNet moment” for natural language processing.</p>
<p class="normal">The language model-based embedding idea has its roots in the ELMo [21] network, which you have already seen in this chapter. ELMo learns about its language by being trained on a large text corpus to learn to predict the next and previous words given a sequence of words. ELMo is based on a bidirectional LSTM, which you will learn more about in <em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>.</p>
<p class="normal">The first viable language model<a id="_idIndexMarker463"/> embedding was proposed by Howard and Ruder [27] via their <strong class="keyWord">Universal Language Model Fine-Tuning</strong> (<strong class="keyWord">ULMFiT</strong>) model, which was trained on the wikitext-103 dataset consisting of 28,595 Wikipedia articles and 103 million words. ULMFiT provides the same benefits that transfer learning provides for image tasks—better results from supervised learning tasks with comparatively less labeled data.</p>
<p class="normal">Meanwhile, the transformer architecture has become the preferred network for machine translation tasks, replacing the LSTM network because it allows for parallel operations and better handling of long-term dependencies. We will learn more about the transformer architecture in <em class="chapterRef">Chapter 6</em>, <em class="italic">Transformers</em>. The OpenAI team of Radford, et al. [29] proposed using the decoder stack from the standard transformer network instead of the LSTM network used in ULMFiT. Using this, they built a language model embedding called <strong class="keyWord">Generative Pretraining</strong> (<strong class="keyWord">GPT</strong>) that <a id="_idIndexMarker464"/>achieved state of the art results for many language processing tasks. The paper proposes several configurations for supervised tasks involving single-and multi-sentence tasks such as classification, entailment, similarity, and multiple-choice question answering.</p>
<p class="normal">The OpenAI team later followed this up by building even larger language models called GPT-2 and GPT-3 respectively. GPT-2 was initially not released because of fears of misuse of the technology by malicious operators [30].</p>
<p class="normal">One problem with the OpenAI transformer architecture is that it is unidirectional whereas its predecessors ELMo and ULMFiT were bidirectional. <strong class="keyWord">Bidirectional Encoder Representations for Transformers</strong> (<strong class="keyWord">BERT</strong>), proposed by<a id="_idIndexMarker465"/> the Google AI team [28], uses the encoder stack of the Transformer architecture and achieves bidirectionality safely by masking up to 15% of its input, which it asks the model to predict.</p>
<p class="normal">As with the OpenAI paper, BERT proposes configurations for using it for several supervised learning tasks such as single- and multiple-sentence classification, question answering, and tagging.</p>
<p class="normal">The BERT model comes in two major flavors—BERT-base and BERT-large. BERT-base has 12 encoder layers, 768 hidden units, and 12 attention heads, with 110 million parameters in all. BERT-large <a id="_idIndexMarker466"/>has 24 encoder layers, 1,024 hidden units, and 16 attention heads, with 340 million parameters. More details can be found in the BERT GitHub repository [33].</p>
<p class="normal">BERT pretraining is an<a id="_idIndexMarker467"/> expensive process and can currently only be achieved using <strong class="keyWord">Tensor Processing Units</strong> (<strong class="keyWord">TPUs</strong>) or large distributed <strong class="keyWord">Graphics Processing Units</strong> (<strong class="keyWord">GPUs</strong>) clusters. TPUs <a id="_idIndexMarker468"/>are only available from Google via its Colab network [31] or Google Cloud Platform [32]. However, fine-tuning the BERT-base with custom datasets is usually achievable on GPU instances.</p>
<p class="normal">Once the BERT model is fine-tuned for your domain, the embeddings from the last four hidden layers usually produce good results for downstream tasks. Which embedding or combination of embeddings (via summing, averaging, max-pooling, or concatenating) to use is usually based on the type of task.</p>
<p class="normal">In the following section, we will look at how to extract embeddings from the BERT language model.</p>
<h2 class="heading-2" id="_idParaDest-120">Using BERT as a feature extractor</h2>
<p class="normal">The BERT project [33] provides a<a id="_idIndexMarker469"/> set of Python scripts that can be run from the command line to fine-tune BERT:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>git <span class="hljs-con-built_in">clone</span> https://github.com/google-research/bert.git
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> bert
</code></pre>
<p class="normal">We then download the appropriate BERT model we want to fine-tune. As mentioned earlier, BERT comes in two sizes—BERT-base and BERT-large. In addition, each model has a cased and uncased version. The cased version differentiates between upper and lowercase words, while the uncased version does not. For our example, we will use the BERT-base-uncased pretrained model. You can find the download URL for this and the other models further down the <code class="inlineCode">README.md</code> page:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mkdir</span> data
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> data
<span class="hljs-con-meta">$ </span>wget \ 
https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
<span class="hljs-con-meta">$ </span>unzip -a uncased_L-12_H-768_A-12.zip
</code></pre>
<p class="normal">This will create the following folder under the <code class="inlineCode">data</code> directory of your local BERT project. The <code class="inlineCode">bert_config.json</code> file is the configuration file used to create the original pretrained model, and the <code class="inlineCode">vocab.txt</code> is the vocabulary used for the model, consisting of 30,522 words and word pieces:</p>
<pre class="programlisting con"><code class="hljs-con">uncased_L-12_H-768_A-12/
 ├── bert_config.json
 ├── bert_model.ckpt.data-00000-of-00001
 ├── bert_model.ckpt.index
 ├── bert_model.ckpt.meta
 └── vocab.txt
</code></pre>
<p class="normal">The pretrained language model can be directly used as a text feature extractor for simple machine learning pipelines. This can be useful for situations where you want to just vectorize your text input, leveraging the distributional property of embeddings to get a denser and richer representation than one-hot encoding.</p>
<p class="normal">The input in this case is just a file with one sentence per line. Let us call it <code class="inlineCode">sentences.txt</code> and put it into our <code class="inlineCode">${CLASSIFIER_DATA}</code> folder. You can generate the embeddings from the last hidden layers by identifying them as -1 (last hidden layer), -2 (hidden layer before that), and so on. The command to extract BERT embeddings for your input sentences is as follows:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> BERT_BASE_DIR=./data/uncased_L-12_H-768_A-12
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> CLASSIFIER_DATA=./data/my_data
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> TRAINED_CLASSIFIER=./data/my_classifier
<span class="hljs-con-meta">$ </span>python extract_features.py \
    --input_file=<span class="hljs-con-variable">${CLASSIFIER_DATA}</span>/sentences.txt \
    --output_file=<span class="hljs-con-variable">${CLASSIFIER_DATA}</span>/embeddings.jsonl \
    --vocab_file=<span class="hljs-con-variable">${BERT_BASE_DIR}</span>/vocab.txt \
    --bert_config_file=<span class="hljs-con-variable">${BERT_BASE_DIR}</span>/bert_config.json \
    --init_checkpoint=<span class="hljs-con-variable">${BERT_BASE_DIR}</span>/bert_model.ckpt \
    --layers=-1,-2,-3,-4 \
    --max_seq_length=128 \
    --batch_size=8
</code></pre>
<p class="normal">The command will extract the BERT embeddings from the last four hidden layers of the model and write them out into a line-oriented JSON file called <code class="inlineCode">embeddings.jsonl</code> in the same directory as the input file. These embeddings can then be used as input to downstream models<a id="_idIndexMarker470"/> that specialize in some specific task, such as sentiment analysis. Because BERT was pretrained on large quantities of English text, it learns a lot about the nuances of the language, which turn out to be useful for these downstream tasks. The downstream model does not have to be a neural network, it can be a non-neural model such as SVM or XGBoost as well.</p>
<p class="normal">There is much more you can do with BERT. The previous use case corresponds to transfer learning in computer vision. As in computer vision, it is also possible to fine-tune BERT (and other transformer models) for specific tasks, where the appropriate “head” network is attached to BERT, and the combined network is fine-tuned for a specific task. You will learn more about these techniques in <em class="chapterRef">Chapter 6</em>, <em class="italic">Transformers</em>.</p>
<h1 class="heading-1" id="_idParaDest-121">Summary</h1>
<p class="normal">In this chapter, we have learned about the concepts behind distributional representations of words and their various implementations, starting from static word embeddings such as Word2Vec and GloVe.</p>
<p class="normal">We then looked at improvements to the basic idea, such as subword embeddings, sentence embeddings that capture the context of the word in the sentence, and the use of entire language models for generating embeddings. While language model-based embeddings are achieving state-of-the-art results nowadays, there are still plenty of applications where more traditional approaches yield very good results, so it is important to know them all and understand the tradeoffs.</p>
<p class="normal">We also looked briefly at other interesting uses of word embeddings outside the realm of natural language, where the distributional properties of other kinds of sequences are leveraged to make predictions in domains such as information retrieval and recommendation systems.</p>
<p class="normal">You are now ready to use embeddings, not only for your text-based neural networks, which we will look at in greater depth in the next chapter, but also in other areas of machine learning.</p>
<h1 class="heading-1" id="_idParaDest-122">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Mikolov, T., et al. (2013, Sep 7) <em class="italic">Efficient Estimation of Word Representations in Vector Space</em>. arXiv:1301.3781v3 [cs.CL].</li>
<li class="numberedList">Mikolov, T., et al. (2013, Sep 17). <em class="italic">Exploiting Similarities among Languages for Machine Translation</em>. arXiv:1309.4168v1 [cs.CL].</li>
<li class="numberedList">Mikolov, T., et al. (2013). <em class="italic">Distributed Representations of Words and Phrases and their Compositionality</em>. Advances in Neural Information Processing Systems 26 (NIPS 2013).</li>
<li class="numberedList">Pennington, J., Socher, R., Manning, C. (2014). <em class="italic">GloVe: Global Vectors for Word Representation</em>. D14-1162, Proceedings of the 2014 Conference on <em class="italic">Empirical Methods in Natural Language Processing</em> (<em class="italic">EMNLP</em>).</li>
<li class="numberedList">Niu, F., et al (2011, 11 Nov). <em class="italic">HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</em>. arXiv:1106.5730v2 [math.OC].</li>
<li class="numberedList">Levy, O., Goldberg, Y. (2014). <em class="italic">Neural Word Embedding as Implicit Matrix Factorization</em>. Advances in Neural Information Processing Systems 27 (NIPS 2014).</li>
<li class="numberedList">Mahoney, M. (2011, 1 Sep). text8 dataset: <a href="http://mattmahoney.net/dc/textdata.xhtml"><span class="url">http://mattmahoney.net/dc/textdata.xhtml</span></a></li>
<li class="numberedList">Rehurek, R. (2019, 10 Apr). gensim documentation for Word2Vec model: <a href="https://radimrehurek.com/gensim/models/word2vec.xhtml"><span class="url">https://radimrehurek.com/gensim/models/word2vec.xhtml</span></a></li>
<li class="numberedList">Levy, O., Goldberg, Y. (2014, 26-27 June). <em class="italic">Linguistic Regularities in Sparse and Explicit Word Representations</em>. Proceedings of the Eighteenth Conference on Computational Language Learning, pp 171-180 (ACL 2014).</li>
<li class="numberedList">Rehurek, R. (2019, 10 Apr). gensim documentation for KeyedVectors: <a href="https://radimrehurek.com/gensim/models/keyedvectors.xhtml"><span class="url">https://radimrehurek.com/gensim/models/keyedvectors.xhtml</span></a></li>
<li class="numberedList">Almeida, T. A., Gamez Hidalgo, J. M., and Yamakami, A. (2011). Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG): <a href="https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf?ref=https://githubhelp.com"><span class="url">https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf?ref=https://githubhelp.com</span></a></li>
<li class="numberedList">Speer, R., Chin, J. (2016, 6 Apr). <em class="italic">An Ensemble Method to Produce High-Quality Word Embeddings</em>. arXiv:1604.01692v1 [cs.CL].</li>
<li class="numberedList">Speer, R. (2016, 25 May). <em class="italic">ConceptNet Numberbatch: a new name for the best Word Embeddings you can download</em>: <a href="http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/"><span class="url">http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-name-for-the-best-word-embeddings-you-can-download/</span></a></li>
<li class="numberedList">Barkan, O., Koenigstein, N. (2016, 13-16 Sep). <em class="italic">Item2Vec: Neural Item Embedding for Collaborative Filtering</em>. IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP 2016).</li>
<li class="numberedList">Grover, A., Leskovec, J. (2016, 13-17 Aug). <em class="italic">node2vec: Scalable Feature Learning for Networks</em>. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. (KDD 2016).</li>
<li class="numberedList">TensorFlow 2.0 Models on TensorFlow Hub: <a href="https://tfhub.dev/s?q=tf2-preview"><span class="url">https://tfhub.dev/s?q=tf2-preview</span></a></li>
<li class="numberedList">Zhang, X., LeCun, Y. (2016, 4 Apr). <em class="italic">Text Understanding from Scratch</em>. arXiv 1502.01710v5 [cs.LG].</li>
<li class="numberedList">Bojanowski, P., et al. (2017, 19 Jun). <em class="italic">Enriching Word Vectors with Subword Information</em>. arXiv: 1607.04606v2 [cs.CL].</li>
<li class="numberedList">Facebook AI Research, fastText (2017). GitHub repository: <a href="https://github.com/facebookresearch/fastText"><span class="url">https://github.com/facebookresearch/fastText</span></a></li>
<li class="numberedList">McCann, B., Bradbury, J., Xiong, C., Socher, R. (2017). <em class="italic">Learned in Translation: Contextualized Word Vectors</em>. Neural Information Processing Systems, 2017.</li>
<li class="numberedList">Peters, M., et al. (2018, 22 Mar). <em class="italic">Deep contextualized word representations</em>. arXiv: 1802.05365v2 [cs.CL].</li>
<li class="numberedList">Kiros, R., et al. (2015, 22 June). <em class="italic">Skip-Thought Vectors</em>. arXiv: 1506.06727v1 [cs.CL].</li>
<li class="numberedList">Kiros, R, et al (2017). GitHub repository: <a href="https://github.com/ryankiros/skip-thoughts"><span class="url">https://github.com/ryankiros/skip-thoughts</span></a></li>
<li class="numberedList">Iyer, M., Manjunatha, V., Boyd-Graber, J., Daume, H. (2015, July 26-31). <em class="italic">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</em>. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015).</li>
<li class="numberedList">Vaswani, A., et al. (2017, 6 Dec). <em class="italic">Attention Is All You Need</em>. arXiv: 1706.03762v5 [cs.CL].</li>
<li class="numberedList">Le, Q., Mikolov, T. (2014) <em class="italic">Distributed Representation of Sentences and Documents</em>. arXiv: 1405.4053v2 [cs.CL].</li>
<li class="numberedList">Howard, J., Ruder, S. (2018, 23 May). <em class="italic">Universal Language Model Fine-Tuning for Text Classification</em>. arXiv: 1801.06146v5 [cs.CL].</li>
<li class="numberedList">Devlin, J., Chang, M., Lee, K., Toutanova, K. (2018, 11 Oct). <em class="italic">BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding</em>. arXiv: 1810.04805v1 [cs.CL]: <a href="https://arxiv.org/pdf/1810.04805.pdf"><span class="url">https://arxiv.org/pdf/1810.04805.pdf</span></a></li>
<li class="numberedList">Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. (2018). <em class="italic">Improving Language Understanding with Unsupervised Learning</em>: <a href="https://openai.com/blog/language-unsupervised/"><span class="url">https://openai.com/blog/language-unsupervised/</span></a></li>
<li class="numberedList">Radford, A., et al. (2019). <em class="italic">Language Models are unsupervised Multitask Learners</em>. OpenAI Blog 2019: <a href="http://www.persagen.com/files/misc/radford2019language.pdf"><span class="url">http://www.persagen.com/files/misc/radford2019language.pdf</span></a></li>
<li class="numberedList">Google Collaboratory: <a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com</span></a></li>
<li class="numberedList">Google Cloud Platform. <a href="https://cloud.google.com/"><span class="url">https://cloud.google.com/</span></a></li>
<li class="numberedList">Google Research, BERT (2019). GitHub repository: <a href="https://github.com/google-research/bert"><span class="url">https://github.com/google-research/bert</span></a></li>
<li class="numberedList">Nemeth (2019). Simple BERT using Tensorflow 2.0. <em class="italic">Towards Data Science blog</em>: <a href="https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22"><span class="url">https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22</span></a></li>
<li class="numberedList">TF-IDF. Wikipedia. Retrieved May 2019: <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"><span class="url">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</span></a></li>
<li class="numberedList">Latent Semantic Analysis. Wikipedia. Retrieved May 2019: <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis"><span class="url">https://en.wikipedia.org/wiki/Latent_semantic_analysis</span></a></li>
<li class="numberedList">Topic Model. Wikipedia. Retrieved May 2019: <a href="https://en.wikipedia.org/wiki/Topic_model"><span class="url">https://en.wikipedia.org/wiki/Topic_model</span></a></li>
<li class="numberedList">Warstadt, A., Singh, A., and Bowman, S. (2018). <em class="italic">Neural Network Acceptability Judgements</em>. arXiv 1805:12471 [cs.CL]: <a href="https://nyu-mll.github.io/CoLA/"><span class="url">https://nyu-mll.github.io/CoLA/</span></a></li>
<li class="numberedList">Microsoft Research Paraphrase Corpus. (2018): <a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398"><span class="url">https://www.microsoft.com/en-us/download/details.aspx?id=52398</span></a></li>
<li class="numberedList">Nozawa, K. (2019). Something2Vec papers: <a href="https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e"><span class="url">https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e</span></a></li>
<li class="numberedList">Perrone, V., et al. (2016). <em class="italic">Poisson Random Fields for Dynamic Feature Models</em>: <a href="https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015"><span class="url">https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015</span></a></li>
<li class="numberedList">Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). <em class="italic">DeepWalk: Online Learning of Social Representations</em>. arXiv 1403.6652v2 [cs.SI].</li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>