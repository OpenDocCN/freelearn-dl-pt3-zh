<html><head></head><body>
		<div id="_idContainer137">
			<h1 id="_idParaDest-318"><em class="italic"><a id="_idTextAnchor370"/>Chapter 12</em>: Boosting Performance</h1>
			<p>More often than not, the leap between good and great doesn't involve drastic changes, but instead subtle tweaks and fine-tuning.</p>
			<p>It is often said that 20% of the effort can get you 80% of the results (this is known as the <strong class="bold">Pareto principle</strong>). But what about that gap between 80% and 100%? What do we need to do to <a id="_idIndexMarker1154"/>exceed expectations, to improve our solutions, to squeeze as much performance out of our computer vision algorithms as possible? </p>
			<p>Well, as with all things deep learning, the answer is a mixture of art and science. The good news is that in this chapter, we'll focus on simple tools you can use to boost the performance of your neural networks!</p>
			<p>In this chapter, we will cover the following recipes:</p>
			<ul>
				<li>Using convolutional neural network ensembles to improve accuracy</li>
				<li>Using test time augmentation to improve accuracy</li>
				<li>Using rank-N accuracy to evaluate performance</li>
				<li>Using label smoothing to increase performance</li>
				<li>Checkpointing models</li>
				<li>Customizing the training process using <strong class="source-inline">tf.GradientTape</strong></li>
				<li>Visualizing class activation maps to better understand your network</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-319"><a id="_idTextAnchor371"/>Technical requirements</h1>
			<p>As usual, you'll get the most out of these recipes if you can access a GPU, given that some of the examples in this chapter are quite resource-intensive. Also, if there are any preparatory steps you'll need to perform in order to complete a recipe, you'll find them in the <em class="italic">Getting ready</em> sections provided. As a last remark, the code for this chapter is available in the companion repository on GitHub: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/2Ko3H3K">https://bit.ly/2Ko3H3K</a>.<a id="_idTextAnchor372"/><a id="_idTextAnchor373"/></p>
			<h1 id="_idParaDest-320"><a id="_idTextAnchor374"/>Using convolutional neural network ensembles to improve accuracy</h1>
			<p>In machine <a id="_idIndexMarker1155"/>learning, one <a id="_idIndexMarker1156"/>of the most robust classifiers is, in fact, a meta-classifier, known as an ensemble. An ensemble is comprised of what's known as weak classifiers, predictive models just a tad better than random guessing. However, when combined, they result in a rather robust algorithm, especially against high variance (overfitting). Some of the most famous examples of ensembles we may encounter include Random Forest and Gradient Boosting Machines.</p>
			<p>The good news is that we can leverage the same principle when it comes to neural networks, thus creating a whole that's more than the sum of its parts. Do you want to learn how? Keep reading!</p>
			<h2 id="_idParaDest-321"><a id="_idTextAnchor375"/>Getting ready</h2>
			<p>This recipe depends on <strong class="source-inline">Pillow</strong> and <strong class="source-inline">tensorflow_docs</strong>, which can be easily installed like this:</p>
			<p class="source-code">$&gt; pip install Pillow git+https://github.com/tensorflow/docs</p>
			<p>We'll also be using the famous <strong class="source-inline">Caltech 101</strong> dataset, available here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>. Download and decompress <strong class="source-inline">101_ObjectCategories.tar.gz</strong> to your preferred location. For the purposes of this recipe, we'll place it in <strong class="source-inline">~/.keras/datasets/101_ObjectCategories</strong>.</p>
			<p>The following are some sample images:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B14768_12_001.jpg" alt="Figure 12.1 – Caltech 101 sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Caltech 101 sample images</p>
			<p>Let's start this recipe, shall we?</p>
			<h2 id="_idParaDest-322"><a id="_idTextAnchor376"/>How to do it…</h2>
			<p>Follow <a id="_idIndexMarker1157"/>these steps<a id="_idIndexMarker1158"/> to create an<a id="_idIndexMarker1159"/> ensemble of <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>):</p>
			<ol>
				<li>Import all the required modules:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define the <strong class="source-inline">load_images_and_labels()</strong> function, which reads the images and categories of the <strong class="source-inline">Caltech 101</strong> dataset and returns them as NumPy arrays:<p class="source-code">def load_images_and_labels(image_paths, </p><p class="source-code">                           target_size=(64, 64)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                          target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Define<a id="_idIndexMarker1160"/> the <strong class="source-inline">build_model()</strong> function, which is in charge of building a VGG-like <a id="_idIndexMarker1161"/>convolutional neural network: <p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p>Now, build <a id="_idIndexMarker1162"/>the<a id="_idIndexMarker1163"/> fully connected part of the network:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>Define the <strong class="source-inline">plot_model_history()</strong> function, which we'll use to plot the training and validation curves of the networks in the ensemble:<p class="source-code">def plot_model_history(model_history, metric, </p><p class="source-code">                       plot_name):</p><p class="source-code">    plt.style.use('seaborn-darkgrid')</p><p class="source-code">    plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">    plotter.plot({'Model': model_history}, </p><p class="source-code">                  metric=metric)</p><p class="source-code">    plt.title(f'{metric.upper()}')</p><p class="source-code">    plt.ylim([0, 1])</p><p class="source-code">    plt.savefig(f'{plot_name}.png')</p><p class="source-code">    plt.close()</p></li>
				<li>To enhance reproducibility, set a random seed:<p class="source-code">SEED = 999</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Compile<a id="_idIndexMarker1164"/> the <a id="_idIndexMarker1165"/>paths to the images of <strong class="source-inline">Caltech 101</strong>, as well as the classes:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">              'datasets' /</p><p class="source-code">             '101_ObjectCategories')</p><p class="source-code">images_pattern = str(base_path / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(images_pattern)]</p><p class="source-code">image_paths = [p for p in image_paths if</p><p class="source-code">               p.split(os.path.sep)[-2] !='BACKGROUND_Google']</p><p class="source-code">CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}</p></li>
				<li>Load the images and labels while normalizing the images and one-hot encoding the labels:<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p></li>
				<li>Reserve 20% of the data for test purposes and use the rest to train the models:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                  random_state=SEED)</p></li>
				<li>Define the batch size, the number of epochs, and the number of batches per epoch:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE</p><p class="source-code">EPOCHS = 40</p></li>
				<li>We'll use<a id="_idIndexMarker1166"/> data<a id="_idIndexMarker1167"/> augmentation here to perform a series of random transformations, such as horizontal flipping, rotations, and zooming:<p class="source-code">augmenter = ImageDataGenerator(horizontal_flip=True,</p><p class="source-code">                               rotation_range=30,</p><p class="source-code">                               width_shift_range=0.1,</p><p class="source-code">                               height_shift_range=0.1,</p><p class="source-code">                               shear_range=0.2,</p><p class="source-code">                               zoom_range=0.2,</p><p class="source-code">                               fill_mode='nearest')</p></li>
				<li>Our ensemble will be comprised of <strong class="source-inline">5</strong> models. We'll save the predictions of each network in the ensemble in the <strong class="source-inline">ensemble_preds</strong> list:<p class="source-code">NUM_MODELS = 5</p><p class="source-code">ensemble_preds = []</p></li>
				<li>We'll train each model in a similar fashion. We'll start by creating and compiling the network itself:<p class="source-code">for n in range(NUM_MODELS):</p><p class="source-code">    print(f'Training model {n + 1}/{NUM_MODELS}')</p><p class="source-code">    model = build_network(64, 64, 3, len(CLASSES))</p><p class="source-code">    model.compile(loss='categorical_crossentropy',</p><p class="source-code">                  optimizer='rmsprop',</p><p class="source-code">                  metrics=['accuracy'])</p></li>
				<li>Then, we'll <a id="_idIndexMarker1168"/>fit <a id="_idIndexMarker1169"/>the model using data augmentation:<p class="source-code">    train_generator = augmenter.flow(X_train, y_train,</p><p class="source-code">                                     BATCH_SIZE)</p><p class="source-code">    hist = model.fit(train_generator,</p><p class="source-code">                     steps_per_epoch=STEPS_PER_EPOCH,</p><p class="source-code">                     validation_data=(X_test, y_test),</p><p class="source-code">                     epochs=EPOCHS,</p><p class="source-code">                     verbose=2)</p></li>
				<li>Compute the accuracy of the model on the test set, plot its training and validation accuracy curves, and store its predictions in <strong class="source-inline">ensemble_preds</strong>:<p class="source-code">    predictions = model.predict(X_test, </p><p class="source-code">                               batch_size=BATCH_SIZE)</p><p class="source-code">    accuracy = accuracy_score(y_test.argmax(axis=1),</p><p class="source-code">                              predictions.argmax(axis=1))</p><p class="source-code">    print(f'Test accuracy (Model #{n + 1}): {accuracy}')</p><p class="source-code">    plot_model_history(hist, 'accuracy', f'model_{n +1}')</p><p class="source-code">    ensemble_preds.append(predictions)</p></li>
				<li>The last step consists of averaging the predictions of each member of the ensemble, effectively producing a joint prediction for the whole meta-classifier, and then computing the accuracy on the test set:<p class="source-code">ensemble_preds = np.average(ensemble_preds, axis=0)</p><p class="source-code">ensemble_acc = accuracy_score(y_test.argmax(axis=1),</p><p class="source-code">                         ensemble_preds.argmax(axis=1))</p><p class="source-code">print(f'Test accuracy (ensemble): {ensemble_acc}')</p><p>Because we <a id="_idIndexMarker1170"/>are<a id="_idIndexMarker1171"/> training five networks, this program can take a while to complete. When it does, you should see accuracies similar to the following for each member of the ensemble:</p><p class="source-code">Test accuracy (Model #1): 0.6658986175115207</p><p class="source-code">Test accuracy (Model #2): 0.6751152073732719</p><p class="source-code">Test accuracy (Model #3): 0.673963133640553</p><p class="source-code">Test accuracy (Model #4): 0.6491935483870968</p><p class="source-code">Test accuracy (Model #5): 0.6756912442396313</p><p>Here, we can observe the accuracy ranges between 65% and 67.5%. The following figure shows the training and validation curves for models 1 to 5 (from left to right, models 1, 2, and 3 on the top row; models 4 and 5 on the bottom row):</p></li>
			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B14768_12_002.jpg" alt="Figure 12.2 – Curves for the training and validation accuracy for the five models in the ensemble&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Curves for the training and validation accuracy for the five models in the ensemble</p>
			<p>However, the <a id="_idIndexMarker1172"/>most <a id="_idIndexMarker1173"/>interesting result is the accuracy of the ensemble, which is the result of averaging the predictions of each model:</p>
			<p class="source-code">Test accuracy (ensemble): 0.7223502304147466</p>
			<p>Truly impressive! Just by combining the predictions of the five networks, we bumped our accuracy all the way to 72.2%, on a very challenging dataset –  <strong class="source-inline">Caltech 101</strong>! Let's discuss this a bit further in the next section.</p>
			<h2 id="_idParaDest-323"><a id="_idTextAnchor377"/>How it works…</h2>
			<p>In this recipe, we leveraged the power of ensembles by training five neural networks on the challenging <strong class="source-inline">Caltech 101</strong> dataset. It must be noted that our process was pretty straightforward and unremarkable. We started by loading and shaping the data in a format suitable for training and then using the same template to train several copies of a VGG-inspired architecture.</p>
			<p>To create more robust classifiers, we used data augmentation and trained each network for 40 epochs. Besides these details, we didn't change the architecture of the networks, nor did we tweak each particular member. The result is that each model was between 65% and 67% accurate on the test set. However, when combined, they reached a decent 72%!</p>
			<p>Why did this happen, though? The rationale behind ensemble learning is that each model develops its own biases during the training process, which is a consequence of the stochastic nature of deep learning. However, when combining their decisions through a voting process (which is basically what averaging their predictions does), these differences <a id="_idIndexMarker1174"/>smooth <a id="_idIndexMarker1175"/>out and give far more robust results.</p>
			<p>Of course, training several models is a resource-intensive task, and depending on the size and complexity of the problem, it might be outright impossible to do so. Nevertheless, it's a very useful tool that can boost your predicting power just by creating and combining multiple copies of the same network. </p>
			<p>Not bad, huh?</p>
			<h2 id="_idParaDest-324"><a id="_idTextAnchor378"/>See also</h2>
			<p>If you want to understand the mathematical basis behind ensembles, read this article about <strong class="bold">Jensen's Inequality</strong>: <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">https://en.wikipedia.org/wiki/Jensen%27s_inequality</a>.</p>
			<h1 id="_idParaDest-325"><a id="_idTextAnchor379"/>Using test time augmentation to improve accuracy</h1>
			<p>Most of the<a id="_idIndexMarker1176"/> time, when we're<a id="_idIndexMarker1177"/> testing the predictive power of a network, we use a test set to do so. This test set is comprised of images the model has never seen. Then, we present them to the model and ask it what class each belongs to. The thing is… we do it <em class="italic">once</em>.</p>
			<p>What if we were more forgiving and gave the model multiple chances to do this? Would its accuracy improve? Well, more often than not, it does!</p>
			<p>This technique is<a id="_idIndexMarker1178"/> known as <strong class="bold">Test Time Augmentation</strong> (<strong class="bold">TTA</strong>), and it's the focus of this recipe.</p>
			<h2 id="_idParaDest-326"><a id="_idTextAnchor380"/>Getting ready</h2>
			<p>In order to load the images in the dataset, we need <strong class="source-inline">Pillow</strong>. Install it using the following command:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>Then, download the <strong class="source-inline">Caltech 101</strong> dataset, which is available here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>. Download and decompress <strong class="source-inline">101_ObjectCategories.tar.gz</strong> to a location of your choosing. For the rest of this<a id="_idIndexMarker1179"/> recipe, we'll work under the <a id="_idIndexMarker1180"/>assumption that the dataset is in <strong class="source-inline">~/.keras/datasets/101_ObjectCategories</strong>.</p>
			<p>Here's a sample of what you can find inside <strong class="source-inline">Caltech 101</strong>:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B14768_12_003.jpg" alt="Figure 12.3 – Caltech 101 sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Caltech 101 sample images</p>
			<p>We are ready to begin!</p>
			<h2 id="_idParaDest-327"><a id="_idTextAnchor381"/>How to do it…</h2>
			<p>Follow these steps to learn the benefits of TTA:</p>
			<ol>
				<li value="1">Import the dependencies we need:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define <a id="_idIndexMarker1181"/>the <strong class="source-inline">load_images_and_labels()</strong> function<a id="_idIndexMarker1182"/> in order to read the data from <strong class="source-inline">Caltech 101</strong> (in NumPy format):<p class="source-code">def load_images_and_labels(image_paths, </p><p class="source-code">                           target_size=(64, 64)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                        target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Define <a id="_idIndexMarker1183"/>the <strong class="source-inline">build_model()</strong> function, which <a id="_idIndexMarker1184"/>returns a network based on the famous <strong class="bold">VGG</strong> architecture: <p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p>Now, build <a id="_idIndexMarker1185"/>the fully connected<a id="_idIndexMarker1186"/> part of the network:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>The <strong class="source-inline">flip_augment()</strong> function is the basis of our <strong class="bold">TTA</strong> scheme. It takes an image and produces copies of it that can be randomly flipped (horizontally) with a 50% probability:<p class="source-code">def flip_augment(image, num_test=10):</p><p class="source-code">    augmented = []</p><p class="source-code">    for i in range(num_test):</p><p class="source-code">        should_flip = np.random.randint(0, 2)</p><p class="source-code">        if should_flip:</p><p class="source-code">            flipped = np.fliplr(image.copy())</p><p class="source-code">            augmented.append(flipped)</p><p class="source-code">        else:</p><p class="source-code">            augmented.append(image.copy())</p><p class="source-code">    return np.array(augmented)</p></li>
				<li>To <a id="_idIndexMarker1187"/>ensure reproducibility, set <a id="_idIndexMarker1188"/>a random seed:<p class="source-code">SEED = 84</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Compile the paths to the images of <strong class="source-inline">Caltech 101</strong>, as well as its classes:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">             'datasets' /'101_ObjectCategories')</p><p class="source-code">images_pattern = str(base_path / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(images_pattern)]</p><p class="source-code">image_paths = [p for p in image_paths if</p><p class="source-code">               p.split(os.path.sep)[-2] </p><p class="source-code">               !='BACKGROUND_Google']</p><p class="source-code">CLASSES = {p.split(os.path.sep)[-2] for p in </p><p class="source-code">           image_paths}</p></li>
				<li>Load the images and labels while normalizing the images and one-hot encoding the labels:<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p></li>
				<li>Use 20% of<a id="_idIndexMarker1189"/> the data for test<a id="_idIndexMarker1190"/> purposes and leave the rest to train the models:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y, test_size=0.2,</p><p class="source-code">                                  random_state=SEED)</p></li>
				<li>Define the batch size and the number of epochs:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">EPOCHS = 40</p></li>
				<li>We'll randomly horizontally flip the images in the train set:<p class="source-code">augmenter = ImageDataGenerator(horizontal_flip=True)</p></li>
				<li>Build and compile the network:<p class="source-code">model = build_network(64, 64, 3, len(CLASSES))</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='adam',</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model:<p class="source-code">train_generator = augmenter.flow(X_train, y_train,</p><p class="source-code">                                 BATCH_SIZE)</p><p class="source-code">model.fit(train_generator,</p><p class="source-code">          steps_per_epoch=len(X_train) // BATCH_SIZE,</p><p class="source-code">          validation_data=(X_test, y_test),</p><p class="source-code">          epochs=EPOCHS,</p><p class="source-code">          verbose=2)</p></li>
				<li>Make <a id="_idIndexMarker1191"/>predictions on the test set<a id="_idIndexMarker1192"/> and use them to compute the accuracy of the model:<p class="source-code">predictions = model.predict(X_test,</p><p class="source-code">                            batch_size=BATCH_SIZE)</p><p class="source-code">accuracy = accuracy_score(y_test.argmax(axis=1), </p><p class="source-code">                          predictions.argmax(axis=1))</p><p class="source-code">print(f'Accuracy, without TTA: {accuracy}')</p></li>
				<li>Now, we'll use <strong class="bold">TTA</strong> on the test set. We'll store the predictions for each copy of an image in the test set in the predictions list. We'll create 10 copies of each image:<p class="source-code">predictions = []</p><p class="source-code">NUM_TEST = 10</p></li>
				<li>Next, we will iterate over each image of the test set, creating a batch of copies of it and passing it through the model:<p class="source-code">for index in range(len(X_test)):</p><p class="source-code">    batch = flip_augment(X_test[index], NUM_TEST)</p><p class="source-code">    sample_predictions = model.predict(batch)</p></li>
				<li>The final prediction of each image will be the most predicted class in the batch of copies:<p class="source-code">    sample_predictions = np.argmax(</p><p class="source-code">        np.sum(sample_predictions, axis=0))</p><p class="source-code">    predictions.append(sample_predictions)</p></li>
				<li>Finally, we will compute the accuracy on the predictions made by the model using TTA:<p class="source-code">accuracy = accuracy_score(y_test.argmax(axis=1), </p><p class="source-code">                          predictions)</p><p class="source-code">print(f'Accuracy with TTA: {accuracy}')</p><p>After a <a id="_idIndexMarker1193"/>while, we'll see results<a id="_idIndexMarker1194"/> similar to these:</p><p class="source-code">Accuracy, without TTA: 0.6440092165898618</p><p class="source-code">Accuracy with TTA: 0.6532258064516129</p></li>
			</ol>
			<p>The network achieves an accuracy of 64.4% without TTA, while it increases to 65.3% if we give the model more chances to generate correct predictions. Cool, right?</p>
			<p>Let's move on to the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-328"><a id="_idTextAnchor382"/>How it works…</h2>
			<p>In this recipe, we<a id="_idIndexMarker1195"/> learned that <strong class="bold">test time augmentation</strong> is a simple technique that entails only a few changes once the network has been trained. The reasoning behind this is that if we present the network with copies of images in the test set that have been altered in a similar way to the ones it saw during training, the network should do better.</p>
			<p>However, the key is that these transformations, which are done during the evaluation phase, should match the ones that were done during the training period; otherwise, we would be feeding the model incongruent data!</p>
			<p>There's a caveat, though: TTA is really, really slow! After all, we are multiplying the size of the test set by the augmentation factor, which in our case was 10. This means that instead of evaluating one image at a time, the network must process 10 instead.</p>
			<p>Of course, TTA is not<a id="_idIndexMarker1196"/> suitable for <a id="_idIndexMarker1197"/>real-time or speed-constrained applications, but it can be useful when time or speed are not an issue.</p>
			<h1 id="_idParaDest-329"><a id="_idTextAnchor383"/>Using rank-N accuracy to evaluate performance</h1>
			<p>Most of the <a id="_idIndexMarker1198"/>time, when we're training deep learning-based image classifiers, we care about the accuracy, which is a binary measure of a model's performance, based on a one-on-one comparison between its predictions and the ground-truth labels. When the model says there's a <em class="italic">leopard</em> in a photo, is there actually a <em class="italic">leopard</em> there? In other words, we measure how <em class="italic">precise</em> the model is.</p>
			<p>However, for more complex datasets, this way of assessing a network's learning might be counterproductive and even unfair, because it's too restrictive. What if the model didn't classify the feline in the picture as a <em class="italic">leopard</em> but as a <em class="italic">tiger</em>? Moreover, what if the second most probable class was, indeed, a <em class="italic">leopard</em>? This means the model has some more learning to do, but it's getting there! That's valuable! </p>
			<p>This is the reasoning <a id="_idIndexMarker1199"/>behind <strong class="bold">rank-N accuracy</strong>, a more lenient and fairer way of measuring a predictive model's performance, which counts a prediction as correct if the ground-truth label is in the top-N most probable classes output by the model. In this recipe, we'll learn how to implement it and use it.</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-330"><a id="_idTextAnchor384"/>Getting ready</h2>
			<p>Install <strong class="source-inline">Pillow</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>Next, download and unzip the <strong class="source-inline">Caltech 101</strong> dataset, which is available here: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>. Make sure to click on the <strong class="source-inline">101_ObjectCategories.tar.gz</strong> file. Once downloaded, place it in a location of your choosing. For the rest of this recipe, we'll work under the assumption that the dataset is in <strong class="source-inline">~/.keras/datasets/101_ObjectCategories</strong>.</p>
			<p>Here's a sample of <strong class="source-inline">Caltech 101</strong>:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B14768_12_004.jpg" alt="Figure 12.4 – Caltech 101 sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – Caltech 101 sample images</p>
			<p>Let's implement this recipe!</p>
			<h2 id="_idParaDest-331"><a id="_idTextAnchor385"/>How to do it…</h2>
			<p>Follow these <a id="_idIndexMarker1200"/>steps to implement and use <strong class="bold">rank-N accuracy</strong>:</p>
			<ol>
				<li value="1">Import the necessary modules:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define the <strong class="source-inline">load_images_and_labels()</strong> function in order to read the data from <strong class="source-inline">Caltech 101</strong>:<p class="source-code">def load_images_and_labels(image_paths, </p><p class="source-code">                           target_size=(64, 64)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                         target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Define <a id="_idIndexMarker1201"/>the <strong class="source-inline">build_model()</strong> function to create a <strong class="bold">VGG</strong>-inspired network: <p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p>Now, build <a id="_idIndexMarker1202"/>the fully connected part of the network:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>Define the <strong class="source-inline">rank_n()</strong> function, which computes the <strong class="bold">rank-N accuracy</strong> based on the predictions<a id="_idIndexMarker1203"/> and ground-truth labels. Notice that it produces a value between 0 and 1, where a "hit" or correct prediction is accounted for when the ground-truth label is in the N most probable categories:<p class="source-code">def rank_n(predictions, labels, n):</p><p class="source-code">    score = 0.0</p><p class="source-code">    for prediction, actual in zip(predictions, labels):</p><p class="source-code">        prediction = np.argsort(prediction)[::-1]</p><p class="source-code">        if actual in prediction[:n]:</p><p class="source-code">            score += 1</p><p class="source-code">    return score / float(len(predictions))</p></li>
				<li>For the sake of reproducibility, set a random seed:<p class="source-code">SEED = 42</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Compile the paths to the images of <strong class="source-inline">Caltech 101</strong>, as well as its classes:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / 'datasets' /</p><p class="source-code">             '101_ObjectCategories')</p><p class="source-code">images_pattern = str(base_path / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(images_pattern)]</p><p class="source-code">image_paths = [p for p in image_paths if</p><p class="source-code">        p.split(os.path.sep)[-2] !='BACKGROUND_Google']</p><p class="source-code">CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}</p></li>
				<li>Load the<a id="_idIndexMarker1204"/> images and labels while normalizing the images and one-hot encoding the labels:<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p></li>
				<li>Use 20% of the data for test purposes and leave the rest to train the models:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                  random_state=SEED)</p></li>
				<li>Define the batch size and the number of epochs:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">EPOCHS = 40</p></li>
				<li>Define an <strong class="source-inline">ImageDataGenerator()</strong> to augment the images in the training set with random flips, rotations, and other transformations:<p class="source-code">augmenter = ImageDataGenerator(horizontal_flip=True,</p><p class="source-code">                               rotation_range=30,</p><p class="source-code">                               width_shift_range=0.1,</p><p class="source-code">                               height_shift_range=0.1,</p><p class="source-code">                               shear_range=0.2,</p><p class="source-code">                               zoom_range=0.2,</p><p class="source-code">                               fill_mode='nearest')</p></li>
				<li>Build and <a id="_idIndexMarker1205"/>compile the network:<p class="source-code">model = build_network(64, 64, 3, len(CLASSES))</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='adam',</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model:<p class="source-code">train_generator = augmenter.flow(X_train, y_train,</p><p class="source-code">                                 BATCH_SIZE)</p><p class="source-code">model.fit(train_generator,</p><p class="source-code">          steps_per_epoch=len(X_train) // BATCH_SIZE,</p><p class="source-code">          validation_data=(X_test, y_test),</p><p class="source-code">          epochs=EPOCHS,</p><p class="source-code">          verbose=2)</p></li>
				<li>Make predictions on the test set:<p class="source-code">predictions = model.predict(X_test, </p><p class="source-code">                            batch_size=BATCH_SIZE)</p></li>
				<li>Compute rank-1 (regular accuracy), rank-3, rank-5, and rank-10 accuracies:<p class="source-code">y_test = y_test.argmax(axis=1)</p><p class="source-code">for n in [1, 3, 5, 10]:</p><p class="source-code">    rank_n_accuracy = rank_n(predictions, y_test, n=n) * 100</p><p class="source-code">    print(f'Rank-{n}: {rank_n_accuracy:.2f}%')</p><p>Here are the<a id="_idIndexMarker1206"/> results:</p><p class="source-code">Rank-1: 64.29%</p><p class="source-code">Rank-3: 78.05%</p><p class="source-code">Rank-5: 83.01%</p><p class="source-code">Rank-10: 89.69%</p></li>
			</ol>
			<p>Here, we can observe that 64.29% of the time, the network produces an exact match. However, 78.05% of the time, the correct prediction is in the top 3, 83.01% of the time it's in the top 5, and almost 90% of the time it's in the top 10. These are pretty interesting and encouraging results, considering our dataset is comprised of 101 classes that are very different from each other.</p>
			<p>We'll dig deeper in the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-332"><a id="_idTextAnchor386"/>How it works…</h2>
			<p>In this recipe, we <a id="_idIndexMarker1207"/>learned about the existence and utility of rank-N accuracy. We also implemented it with a simple function, <strong class="source-inline">rank_n()</strong>, which we then tested on a network that had been trained on the challenging <strong class="source-inline">Caltech-101</strong> dataset. </p>
			<p>Rank-N, particularly the rank-1 and rank-5 accuracies, are common in the literature of networks that have been trained on massive, challenging datasets, such as COCO or ImageNet, where even humans have a hard time discerning between categories. It is particularly useful when we have fine-grained classes that share a common parent or ancestor, such as <em class="italic">Pug</em> and <em class="italic">Golden Retriever</em>, both being <em class="italic">Dog</em> breeds.</p>
			<p>The reason why rank-N is meaningful is a well-trained model that has truly learned to generalize will produce contextually similar classes in its top-N predictions (typically, the top 5).</p>
			<p>Of course, we can take <a id="_idIndexMarker1208"/>rank-N accuracy too far, to the point where it loses its meaning and utility. For instance, a rank-5 accuracy on <strong class="source-inline">MNIST</strong>, a dataset comprised of 10 categories, would be almost useless.</p>
			<h2 id="_idParaDest-333"><a id="_idTextAnchor387"/>See also</h2>
			<p>Want to see rank-N being <a id="_idIndexMarker1209"/>used in the wild? Take a look at the results section of this paper: <a href="https://arxiv.org/pdf/1610.02357.pdf">https://arxiv.org/pdf/1610.02357<span id="_idTextAnchor388"/><span id="_idTextAnchor389"/>.pdf</a>.</p>
			<h1 id="_idParaDest-334"><a id="_idTextAnchor390"/>Using label smoothing to increase performance</h1>
			<p>One of the<a id="_idIndexMarker1210"/> constant battles we have to fight against in machine learning is overfitting. There are many techniques we can use to prevent a model from losing generalization power, such as dropout, L1 and L2 regularization, and even data augmentation. A recent addition to this<a id="_idIndexMarker1211"/> group is <strong class="bold">label smoothing</strong>, a more forgiving alternative to one-hot encoding. </p>
			<p>Whereas in one-hot encoding we represent each category as a binary vector where the only non-zero element corresponds to the class that's been encoded, with <strong class="bold">label smoothing</strong>, we represent each label as a probability distribution where all the elements have a non-zero probability. The one with the highest probability, of course, is the one that corresponds to the encoded class. </p>
			<p>For instance, a smoothed version of the <em class="italic">[0, 1, 0]</em> vector would be <em class="italic">[0.01, 0.98, 0.01]</em>. </p>
			<p>In this recipe, we'll learn how to use <strong class="bold">label smoothing</strong>. Keep reading! </p>
			<h2 id="_idParaDest-335"><a id="_idTextAnchor391"/>Getting ready</h2>
			<p>Install <strong class="source-inline">Pillow</strong>, which we'll need to manipulate the images in the dataset:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>Head to the <strong class="source-inline">Caltech 101</strong> website: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</a>. Download and unzip the file named <strong class="source-inline">101_ObjectCategories.tar.gz</strong> in a location of your preference. From now on, we'll assume the data is in <strong class="source-inline">~/.keras/datasets/101_ObjectCategories</strong>.</p>
			<p>Here's a sample <a id="_idIndexMarker1212"/>from <strong class="source-inline">Caltech 101</strong>:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B14768_12_005.jpg" alt="Figure 12.5 – Caltech 101 sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – Caltech 101 sample images</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-336"><a id="_idTextAnchor392"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the necessary dependencies:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import CategoricalCrossentropy</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Create the <strong class="source-inline">load_images_and_labels()</strong> function in order to read the data <a id="_idIndexMarker1213"/>from <strong class="source-inline">Caltech 101</strong>:<p class="source-code">def load_images_and_labels(image_paths, </p><p class="source-code">                           target_size=(64, 64)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                         target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Implement<a id="_idIndexMarker1214"/> the <strong class="source-inline">build_model()</strong> function to create a <strong class="bold">VGG</strong>-based network: <p class="source-code">def build_network(width, height, depth, classes):</p><p class="source-code">    input_layer = Input(shape=(width, height, depth))</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(input_layer)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p>Now, build <a id="_idIndexMarker1215"/>the fully connected part of the network:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=512)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input_layer, output)</p></li>
				<li>Set a random seed to enhance reproducibility:<p class="source-code">SEED = 9</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Compile the paths to the images of <strong class="source-inline">Caltech 101</strong>, as well as its classes:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / 'datasets'         </p><p class="source-code">              /'101_ObjectCategories')</p><p class="source-code">images_pattern = str(base_path / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(images_pattern)]</p><p class="source-code">image_paths = [p for p in image_paths if</p><p class="source-code">        p.split(os.path.sep)[-2] !='BACKGROUND_Google']</p><p class="source-code">CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}</p></li>
				<li>Load the images <a id="_idIndexMarker1216"/>and labels while normalizing the images and one-hot encoding the labels:<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p></li>
				<li>Use 20% of the data for test purposes and leave the rest to train the models:<p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                   random_state=SEED)</p></li>
				<li>Define the batch size and the number of epochs:<p class="source-code">BATCH_SIZE = 128</p><p class="source-code">EPOCHS = 40</p></li>
				<li>Define an <strong class="source-inline">ImageDataGenerator()</strong> to augment the images in the training set with random flips, rotations, and other transformations:<p class="source-code">augmenter = ImageDataGenerator(horizontal_flip=True,</p><p class="source-code">                               rotation_range=30,</p><p class="source-code">                               width_shift_range=0.1,</p><p class="source-code">                               height_shift_range=0.1,</p><p class="source-code">                               shear_range=0.2,</p><p class="source-code">                               zoom_range=0.2,</p><p class="source-code">                               fill_mode='nearest')</p></li>
				<li>We'll train two models: one with and an other without <strong class="bold">label smoothing</strong>. This will allow us to <a id="_idIndexMarker1217"/>compare their performance and assess whether <strong class="bold">label smoothing</strong> has an impact on performance. The logic is pretty much the same in both cases, starting with the model creation process: <p class="source-code">for with_label_smoothing in [False, True]:</p><p class="source-code">    model = build_network(64, 64, 3, len(CLASSES))</p></li>
				<li>If <strong class="source-inline">with_label_smoothing</strong> is <strong class="source-inline">True</strong>, then we'll set the smoothing factor to 0.1. Otherwise, the factor will be 0, which implies we'll use regular one-hot encoding:<p class="source-code">    if with_label_smoothing:</p><p class="source-code">        factor = 0.1</p><p class="source-code">    else:</p><p class="source-code">        factor = 0</p></li>
				<li>We apply <strong class="bold">label smoothing</strong> through the <strong class="source-inline">loss</strong> function – in this case, <strong class="source-inline">CategoricalCrossentropy()</strong>:<p class="source-code">    loss = CategoricalCrossentropy(label_smoothing=factor)</p></li>
				<li>Compile and fit the model:<p class="source-code">    model.compile(loss=loss,</p><p class="source-code">                  optimizer='rmsprop',</p><p class="source-code">                  metrics=['accuracy'])</p><p class="source-code">    train_generator = augmenter.flow(X_train, y_train,</p><p class="source-code">                                     BATCH_SIZE)</p><p class="source-code">    model.fit(train_generator,</p><p class="source-code">              steps_per_epoch=len(X_train) // </p><p class="source-code">              BATCH_SIZE,</p><p class="source-code">              validation_data=(X_test, y_test),</p><p class="source-code">              epochs=EPOCHS,</p><p class="source-code">              verbose=2)</p></li>
				<li>Make <a id="_idIndexMarker1218"/>predictions on the test set and compute the accuracy:<p class="source-code">    predictions = model.predict(X_test, </p><p class="source-code">                               batch_size=BATCH_SIZE)</p><p class="source-code">    accuracy = accuracy_score(y_test.argmax(axis=1),</p><p class="source-code">                              predictions.argmax(axis=1))</p><p class="source-code">    print(f'Test accuracy '</p><p class="source-code">          f'{"with" if with_label_smoothing else </p><p class="source-code">           "without"} '</p><p class="source-code">          f'label smoothing: {accuracy * 100:.2f}%')</p><p>The script will train two models: one without <strong class="bold">label smoothing</strong>, using traditional one-hot encoded labels, and a second one with <strong class="bold">label smoothing</strong> applied through the <strong class="source-inline">loss</strong> function. Here are the results:</p><p class="source-code">Test accuracy without label smoothing: 65.09%</p><p class="source-code">Test accuracy with label smoothing: 65.78%</p></li>
			</ol>
			<p>Just by using <strong class="bold">label smoothing</strong>, we improved our test score by almost 0.7%, a non-negligible boost<a id="_idIndexMarker1219"/> considering the size of our dataset and its complexity. We'll dive deeper in the next section.</p>
			<h2 id="_idParaDest-337"><a id="_idTextAnchor393"/>How it works…</h2>
			<p>In this recipe, we learned how to apply <strong class="bold">label smoothing</strong> to a multi-class classification problem and <a id="_idIndexMarker1220"/>witnessed how it improved the performance of our network on the test set. We didn't do anything particularly special, besides passing a smoothing factor to the <strong class="source-inline">CategoricalCrossentropy()</strong> loss function, which is used to measure the network's learning. </p>
			<p>Why does label <a id="_idIndexMarker1221"/>smoothing work, though? Despite its widespread use in many areas of deep learning, including <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) and, of course, <strong class="bold">computer vision</strong>, <strong class="bold">label smoothing</strong> is still poorly<a id="_idIndexMarker1222"/> understood. However, what many have observed (including ourselves, in this example) is that by softening the targets, the generalization and learning speed of a network often improves significantly, preventing it from becoming overconfident, thus shielding us against the harmful effects of overfitting.</p>
			<p>For a very interesting insight into <strong class="bold">label smoothing</strong>, read the paper mentioned in the <em class="italic">See also</em> section.</p>
			<p>See also</p>
			<p>This paper explores <a id="_idIndexMarker1223"/>the reasons why <strong class="bold">label smoothing</strong> helps, as well as when it does not. It's a worthy read! You can download it here: <a href="https://arxiv.org/abs/1906.02629">https://arxiv.org/abs/1906.02629</a>. </p>
			<h1 id="_idParaDest-338"><a id="_idTextAnchor394"/>Checkpointing model</h1>
			<p>Training a deep <a id="_idIndexMarker1224"/>neural network is an expensive process in terms of time, storage, and resources. Retraining a network each time we want to use it is preposterous and impractical. The good news is that we can use a mechanism to automatically save the best versions of a network during the training process.</p>
			<p>In this recipe, we'll talk about such a mechanism, known as checkpointing. </p>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor395"/>How to do it…</h2>
			<p>Follow these steps to learn about the different modalities of checkpointing you have at your disposal in TensorFlow:</p>
			<ol>
				<li value="1">Import the<a id="_idIndexMarker1225"/> modules we will be using:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras.callbacks import ModelCheckpoint</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fm</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import *</p></li>
				<li>Define a function that will load <strong class="source-inline">Fashion-MNIST</strong> into <strong class="source-inline">tf.data.Datasets</strong>:<p class="source-code">def load_dataset():</p><p class="source-code">    (X_train, y_train), (X_test, y_test) = fm.load_data()</p><p class="source-code">    X_train = X_train.astype('float32') / 255.0</p><p class="source-code">    X_test = X_test.astype('float32') / 255.0</p><p class="source-code">    X_train = np.expand_dims(X_train, axis=3)</p><p class="source-code">    X_test = np.expand_dims(X_test, axis=3)</p><p class="source-code">    label_binarizer = LabelBinarizer()</p><p class="source-code">    y_train = label_binarizer.fit_transform(y_train)</p><p class="source-code">    y_test = label_binarizer.fit_transform(y_test)</p></li>
				<li>Use 20% of the training data to validate the dataset:<p class="source-code">    (X_train, X_val,</p><p class="source-code">     y_train, y_val) = train_test_split(X_train, y_train,</p><p class="source-code">                                        train_size=0.8)</p></li>
				<li>Convert the <a id="_idIndexMarker1226"/>train, test, and validation subsets into <strong class="source-inline">tf.data.Datasets</strong>:<p class="source-code">    train_ds = (tf.data.Dataset</p><p class="source-code">                .from_tensor_slices((X_train, </p><p class="source-code">                                     y_train)))</p><p class="source-code">    val_ds = (tf.data.Dataset</p><p class="source-code">              .from_tensor_slices((X_val, y_val)))</p><p class="source-code">    test_ds = (tf.data.Dataset</p><p class="source-code">               .from_tensor_slices((X_test, y_test)))</p><p class="source-code">    train_ds = (train_ds.shuffle(buffer_size=BUFFER_SIZE)</p><p class="source-code">                .batch(BATCH_SIZE)</p><p class="source-code">                .prefetch(buffer_size=BUFFER_SIZE))</p><p class="source-code">    val_ds = (val_ds</p><p class="source-code">              .batch(BATCH_SIZE)</p><p class="source-code">              .prefetch(buffer_size=BUFFER_SIZE))</p><p class="source-code">    test_ds = test_ds.batch(BATCH_SIZE)</p><p class="source-code">    return train_ds, val_ds, test_ds</p></li>
				<li>Define the <strong class="source-inline">build_network()</strong> method, which, as its name suggests, creates the model<a id="_idIndexMarker1227"/> we'll train on <strong class="source-inline">Fashion-MNIST</strong>:<p class="source-code">def build_network():</p><p class="source-code">    input_layer = Input(shape=(28, 28, 1))</p><p class="source-code">    x = Conv2D(filters=20,</p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(input_layer)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Conv2D(filters=50,</p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p>Now, build the fully connected part of the network:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=500)(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Dense(10)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(inputs=input_layer, outputs=output)</p></li>
				<li>Define <a id="_idIndexMarker1228"/>the <strong class="source-inline">train_and_checkpoint()</strong> function, which loads the dataset and then builds, compiles, and fits the network, saving the checkpoints according to the logic established by the <strong class="source-inline">checkpointer</strong> parameter:<p class="source-code">def train_and_checkpoint(checkpointer):</p><p class="source-code">    train_dataset, val_dataset, test_dataset = load_dataset()</p><p class="source-code">    </p><p class="source-code">    model = build_network()</p><p class="source-code">    model.compile(loss='categorical_crossentropy',</p><p class="source-code">                  optimizer='adam',</p><p class="source-code">                  metrics=['accuracy'])</p><p class="source-code">    model.fit(train_dataset,</p><p class="source-code">              validation_data=val_dataset,</p><p class="source-code">              epochs=EPOCHS,</p><p class="source-code">              callbacks=[checkpointer])</p></li>
				<li>Define the batch size, the number of epochs to train the model for, and the buffer size of each subset of data:<p class="source-code">BATCH_SIZE = 256</p><p class="source-code">BUFFER_SIZE = 1024</p><p class="source-code">EPOCHS = 100</p></li>
				<li>The first way to generate checkpoints is by just saving a different model after each iteration. To<a id="_idIndexMarker1229"/> do this, we must pass <strong class="source-inline">save_best_only=False</strong> to <strong class="source-inline">ModelCheckpoint()</strong>: <p class="source-code">checkpoint_pattern = (</p><p class="source-code">    'save_all/model-ep{epoch:03d}-loss{loss:.3f}'</p><p class="source-code">    '-val_loss{val_loss:.3f}.h5')</p><p class="source-code">checkpoint = ModelCheckpoint(checkpoint_pattern,</p><p class="source-code">                             monitor='val_loss',</p><p class="source-code">                             verbose=1,</p><p class="source-code">                             save_best_only=False,</p><p class="source-code">                             mode='min')</p><p class="source-code">train_and_checkpoint(checkpoint)</p><p>Notice that we save all the checkpoints in the <strong class="source-inline">save_all</strong> folder, with the epoch, the loss, and the validation loss in the checkpointed model name.</p></li>
				<li>A more efficient way of checkpointing is to just save the best model so far. We can achieve this by setting <strong class="source-inline">save_best_only</strong> to <strong class="source-inline">True</strong> in <strong class="source-inline">ModelCheckpoint()</strong>: <p class="source-code">checkpoint_pattern = (</p><p class="source-code">    'best_only/model-ep{epoch:03d}-loss{loss:.3f}'</p><p class="source-code">    '-val_loss{val_loss:.3f}.h5')</p><p class="source-code">checkpoint = ModelCheckpoint(checkpoint_pattern,</p><p class="source-code">                             monitor='val_loss',</p><p class="source-code">                             verbose=1,</p><p class="source-code">                             save_best_only=True,</p><p class="source-code">                             mode='min')</p><p class="source-code">train_and_checkpoint(checkpoint)</p><p>We'll save the<a id="_idIndexMarker1230"/> results in the <strong class="source-inline">best_only</strong> directory.</p></li>
				<li>A leaner way to generate checkpoints is to just save one, corresponding to the best model so far, instead of storing each incrementally improved model. To achieve this, we can remove any parameters from the checkpoint name:<p class="source-code">checkpoint_pattern = 'overwrite/model.h5'</p><p class="source-code">checkpoint = ModelCheckpoint(checkpoint_pattern,</p><p class="source-code">                             monitor='val_loss',</p><p class="source-code">                             verbose=1,</p><p class="source-code">                             save_best_only=True,</p><p class="source-code">                             mode='min')</p><p class="source-code">train_and_checkpoint(checkpoint)</p><p>After running these three experiments, we can examine each output folder to see how many checkpoints were generated. In the first experiment, we saved a model after each epoch, as shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B14768_12_006.jpg" alt="Figure 12.6 – Experiment 1 results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 – Experiment 1 results</p>
			<p>The downside of this approach is that we end up with a lot of useless snapshots. The upside is <a id="_idIndexMarker1231"/>that, if we want, we can resume training from any epoch by loading the corresponding epoch. A better approach is to save only the best model so far, which, as the following screenshot shows, produces fewer models. By inspecting the checkpoint names, we can see that each one has a validation loss that's lower than the one before it:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B14768_12_007.jpg" alt="Figure 12.7 – Experiment 2 results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 – Experiment 2 results</p>
			<p>Lastly, we can just save the best model, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B14768_12_008.jpg" alt="Figure 12.8 – Experiment 3 results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 – Experiment 3 results</p>
			<p>Let's move on to the next section.</p>
			<p>How it works…</p>
			<p>In this recipe, we learned how to checkpoint models, which saves us a huge amount of time as we don't need to retrain a model from scratch. Checkpointing is great because we can save the best model according to our own criteria, such as the validation loss, training accu<a id="_idIndexMarker1232"/>racy, or any other measurement. </p>
			<p>By leveraging the <strong class="source-inline">ModelCheckpoint()</strong> callback, we can save a snapshot of the network after each completed epoch, thus keeping only the best model or a history of the best models produced during training. </p>
			<p>Each strategy has its pros and cons. For instance, generating models after each epoch has the upside of allowing us to resume training from any epoch, but at the cost of occupying lots of space on disk, while saving the best model only preserves space but reduces our flexibility to experiment.</p>
			<p>What strategy will you use in your next project?</p>
			<h1 id="_idParaDest-340"><a id="_idTextAnchor396"/>Customizing the training process using tf.GradientTape</h1>
			<p>One of the<a id="_idIndexMarker1233"/> biggest competitors<a id="_idIndexMarker1234"/> of TensorFlow is another well-known framework: PyTorch. What made PyTorch so attractive until the arrival of TensorFlow 2.x was the level of control it gives to its users, particularly when it comes to training neural networks. </p>
			<p>If we are working with somewhat traditional neural networks to solve common problems, such as image classification, we don't need that much control over how to train a model, and therefore can rely on TensorFlow's (or the Keras API's) built-in capabilities, loss functions, and optimizers without a problem.</p>
			<p>But what if we are researchers that are exploring new ways to do things, as well as new architectures and novel strategies to solve challenging problems? That's when, in the past, we had to resort to PyTorch, due to it being considerably easier to customize the training models than using TensorFlow 1.x, but not anymore! TensorFlow 2.x's <strong class="source-inline">tf.GradientTape</strong> allows us to create custom training loops for models implemented in<a id="_idIndexMarker1235"/> Keras and low-level TensorFlow <a id="_idIndexMarker1236"/>more easily, and in this recipe, we'll learn how to use it.</p>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor397"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the modules we will be using:<p class="source-code">import time</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fm</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import categorical_crossentropy</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p><p class="source-code">from tensorflow.keras.utils import to_categorical</p></li>
				<li>Define a function that will load and prepare <strong class="source-inline">Fashion-MNIST</strong>:<p class="source-code">def load_dataset():</p><p class="source-code">    (X_train, y_train), (X_test, y_test) = fm.load_data()</p><p class="source-code">    X_train = X_train.astype('float32') / 255.0</p><p class="source-code">    X_test = X_test.astype('float32') / 255.0</p><p class="source-code">    # Reshape grayscale to include channel dimension.</p><p class="source-code">    X_train = np.expand_dims(X_train, axis=-1)</p><p class="source-code">    X_test = np.expand_dims(X_test, axis=-1)</p><p class="source-code">    y_train = to_categorical(y_train)</p><p class="source-code">    y_test = to_categorical(y_test)</p><p class="source-code">    return (X_train, y_train), (X_test, y_test)</p></li>
				<li>Define <a id="_idIndexMarker1237"/>the <strong class="source-inline">build_network()</strong> method, which, as<a id="_idIndexMarker1238"/> its name suggests, creates the model we'll train on <strong class="source-inline">Fashion-MNIST</strong>:<p class="source-code">def build_network():</p><p class="source-code">    input_layer = Input(shape=(28, 28, 1))</p><p class="source-code">    x = Conv2D(filters=20,</p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(input_layer)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Conv2D(filters=50,</p><p class="source-code">               kernel_size=(5, 5),</p><p class="source-code">               padding='same',</p><p class="source-code">               strides=(1, 1))(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization()(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                     strides=(2, 2))(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p>Now, build <a id="_idIndexMarker1239"/>the fully connected <a id="_idIndexMarker1240"/>part of the network:</p><p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=500)(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = Dropout(0.5)(x)</p><p class="source-code">    x = Dense(10)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(inputs=input_layer, outputs=output)</p></li>
				<li>To demonstrate how to use <strong class="source-inline">tf.GradientTape</strong>, we'll implement the <strong class="source-inline">training_step()</strong> function, which obtains the gradients for a batch of data and then backpropagates them using an optimizer:<p class="source-code">def training_step(X, y, model, optimizer):</p><p class="source-code">    with tf.GradientTape() as tape:</p><p class="source-code">        predictions = model(X)</p><p class="source-code">        loss = categorical_crossentropy(y, predictions)</p><p class="source-code">    gradients = tape.gradient(loss, </p><p class="source-code">                          model.trainable_variables)</p><p class="source-code">    optimizer.apply_gradients(zip(gradients,</p><p class="source-code">                          model.trainable_variables))</p></li>
				<li>Define<a id="_idIndexMarker1241"/> the batch size and the<a id="_idIndexMarker1242"/> number of epochs to train the model for:<p class="source-code">BATCH_SIZE = 256</p><p class="source-code">EPOCHS = 100</p></li>
				<li>Load the dataset: <p class="source-code">(X_train, y_train), (X_test, y_test) = load_dataset()</p></li>
				<li>Create the optimizer and the network:<p class="source-code">optimizer = RMSprop()</p><p class="source-code">model = build_network()</p></li>
				<li>Now, we'll create our custom training loop. First, we'll go over each epoch, measuring the time it takes to complete: <p class="source-code">for epoch in range(EPOCHS):</p><p class="source-code">    print(f'Epoch {epoch + 1}/{EPOCHS}')</p><p class="source-code">    start = time.time()</p></li>
				<li>Now, we'll iterate over each batch of data and pass them, along with the network and the optimizer, to our <strong class="source-inline">training_step()</strong> function:<p class="source-code">    for i in range(int(len(X_train) / BATCH_SIZE)):</p><p class="source-code">        X_batch = X_train[i * BATCH_SIZE:</p><p class="source-code">                          i * BATCH_SIZE + BATCH_SIZE]</p><p class="source-code">        y_batch = y_train[i * BATCH_SIZE:</p><p class="source-code">                          i * BATCH_SIZE + BATCH_SIZE]</p><p class="source-code">        training_step(X_batch, y_batch, model, </p><p class="source-code">                      optimizer)</p></li>
				<li>Then, we'll <a id="_idIndexMarker1243"/>print the epoch's <a id="_idIndexMarker1244"/>elapsed time:<p class="source-code">    elapsed = time.time() - start</p><p class="source-code">    print(f'\tElapsed time: {elapsed:.2f} seconds.')</p></li>
				<li>Lastly, evaluate the network on the test set to make sure it learned without any problems:<p class="source-code">model.compile(loss=categorical_crossentropy,</p><p class="source-code">              optimizer=optimizer,</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">results = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'Loss: {results[0]}, Accuracy: {results[1]}')</p><p>Here are the results:</p><p class="source-code">Loss: 1.7750033140182495, Accuracy: 0.9083999991416931</p></li>
			</ol>
			<p>Let's move on to the next section.</p>
			<h2 id="_idParaDest-342"><a id="_idTextAnchor398"/>How it works…</h2>
			<p>In this recipe, we learned how to create our own custom training loop. Although we didn't do anything particularly interesting in this instance, we highlighted the components (or ingredients, if you will) to cook up a custom deep learning training loop with <strong class="source-inline">tf.GradientTape</strong>:</p>
			<ul>
				<li>The network architecture itself</li>
				<li>The loss<a id="_idIndexMarker1245"/> function used to<a id="_idIndexMarker1246"/> compute the model loss</li>
				<li>The optimizer used to update the model weights based on the gradients</li>
				<li>The step function, which implements a forward pass (compute the gradients) and a backward pass (apply the gradients through the optimizers)</li>
			</ul>
			<p>If you want to study more realistic and appealing uses of <strong class="source-inline">tf.GradientTape</strong>, you can refer to <a href="B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214"><em class="italic">Chapter 6</em></a>, <em class="italic">Generative Models and Adversarial Attacks</em>; <a href="B14768_07_Final_JM_ePub.xhtml#_idTextAnchor248"><em class="italic">Chapter 7</em></a>, <em class="italic">Captioning Images with CNNs and RNNs</em>; and <a href="B14768_08_Final_JM_ePub.xhtml#_idTextAnchor270"><em class="italic">Chapter 8</em></a>, <em class="italic">Fine-Grained Understanding of Images through Segmentation</em>. However, you can just read the next recipe, where we'll learn how to visualize class activation maps in order to debug deep neural networks!</p>
			<p>Visualizing class activation maps to better understand your network</p>
			<p>Despite their<a id="_idIndexMarker1247"/> incontestable power and usefulness, one of the biggest gripes about deep neural networks is their mysterious nature. Most of the time, we use them as black boxes, where we know they work but not why they do. </p>
			<p>In particular, it's truly challenging to say why a network arrived at a particular result, which neurons were activated and why, or where the network is looking at to figure out the class or nature of an object in an image.</p>
			<p>In other words, how can we trust something we don't understand? How can we improve it or fix it if it breaks?</p>
			<p>Fortunately, in <a id="_idIndexMarker1248"/>this recipe, we'll study a novel method to shine some light on these topics, known as <strong class="bold">Gradient Weighted Class Activation Mapping</strong>, or <strong class="bold">Grad-CAM</strong> for short.</p>
			<p>Are you <a id="_idIndexMarker1249"/>ready? Let's get going!</p>
			<h2 id="_idParaDest-343"><a id="_idTextAnchor399"/>Getting ready</h2>
			<p>For this recipe, we need <strong class="source-inline">OpenCV</strong>, <strong class="source-inline">Pillow</strong>, and <strong class="source-inline">imutils</strong>. You can install them in one go like this:</p>
			<p class="source-code">$&gt; pip install Pillow opencv-python imutils</p>
			<p>Now, we are ready to implement this recipe.</p>
			<h2 id="_idParaDest-344"><a id="_idTextAnchor400"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the modules we will be using:<p class="source-code">import cv2</p><p class="source-code">import imutils </p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.applications import *</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define the <strong class="source-inline">GradCAM</strong> class, which will encapsulate the <strong class="bold">Grad-CAM</strong> algorithm, allowing us to produce a heatmap of the activation maps of a given layer. Let's start by defining the constructor:<p class="source-code">class GradGAM(object):</p><p class="source-code">    def __init__(self, model, class_index, </p><p class="source-code">                 layer_name=None):</p><p class="source-code">        self.class_index = class_index</p><p class="source-code">        if layer_name is None:</p><p class="source-code">            for layer in reversed(model.layers):</p><p class="source-code">                if len(layer.output_shape) == 4:</p><p class="source-code">                    layer_name = layer.name</p><p class="source-code">                    break</p><p class="source-code">        self.grad_model = </p><p class="source-code">                  self._create_grad_model(model,</p><p class="source-code">                                                  </p><p class="source-code">                                       layer_name)</p></li>
				<li>Here, we <a id="_idIndexMarker1250"/>are receiving the <strong class="source-inline">class_index</strong> of a class we want to inspect, and the <strong class="source-inline">layer_name</strong> of a layer whose activations we want to visualize. If we don't receive a <strong class="source-inline">layer_name</strong>, we'll take the outermost output layer of our <strong class="source-inline">model</strong> by default. Finally, we create <strong class="source-inline">grad_model</strong> by relying on the <strong class="source-inline">_create_grad_model()</strong> method, as defined here:<p class="source-code">    def _create_grad_model(self, model, layer_name):</p><p class="source-code">        return Model(inputs=[model.inputs],</p><p class="source-code">                     outputs=[</p><p class="source-code">                       model.get_layer(layer_name).</p><p class="source-code">                          output,model.output])</p><p>This model takes the same inputs as <strong class="source-inline">model</strong>, but outputs both the activations of the layer of interest, and the predictions of <strong class="source-inline">model</strong> itself.</p></li>
				<li>Next, we must define the <strong class="source-inline">compute_heatmap()</strong> method. First, we must pass the input image to <strong class="source-inline">grad_model</strong>, obtaining both the activation map of the layer of<a id="_idIndexMarker1251"/> interest and the predictions:<p class="source-code">    def compute_heatmap(self, image, epsilon=1e-8):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            inputs = tf.cast(image, tf.float32)</p><p class="source-code">            conv_outputs, preds = self.grad_model(inputs)</p><p class="source-code">            loss = preds[:, self.class_index]</p></li>
				<li>We can calculate the gradients based on the loss corresponding to the <strong class="source-inline">class_index</strong>:<p class="source-code">grads = tape.gradient(loss, conv_outputs)</p></li>
				<li>We can compute guided gradients by, basically, finding positive values in both <strong class="source-inline">float_conv_outputs</strong> and <strong class="source-inline">float_grads</strong>, and multiplying those by the gradients, which will enable us to visualize what neurons are activating:<p class="source-code">        guided_grads = (tf.cast(conv_outputs &gt; 0, </p><p class="source-code">                      'float32') *</p><p class="source-code">                        tf.cast(grads &gt; 0, 'float32') *</p><p class="source-code">                        grads)</p></li>
				<li>Now, we can compute the gradient weights by averaging the guided gradients, and then use those weights to add the pondered maps to our <strong class="bold">Grad-CAM</strong> visualization:<p class="source-code">        conv_outputs = conv_outputs[0]</p><p class="source-code">        guided_grads = guided_grads[0]</p><p class="source-code">        weights = tf.reduce_mean(guided_grads, </p><p class="source-code">                                 axis=(0, 1))</p><p class="source-code">        cam = tf.reduce_sum(</p><p class="source-code">            tf.multiply(weights, conv_outputs),</p><p class="source-code">            axis=-1)</p></li>
				<li>Then, we <a id="_idIndexMarker1252"/>take the <strong class="bold">Grad-CAM</strong> visualization, resize it to the dimensions of the input image, and min-max normalize it before returning it:<p class="source-code">        height, width = image.shape[1:3]</p><p class="source-code">        heatmap = cv2.resize(cam.numpy(), (width, </p><p class="source-code">                              height))</p><p class="source-code">        min = heatmap.min()</p><p class="source-code">        max = heatmap.max()</p><p class="source-code">        heatmap = (heatmap - min) / ((max - min) + </p><p class="source-code">                                               epsilon)</p><p class="source-code">        heatmap = (heatmap * 255.0).astype('uint8')</p><p class="source-code">        return heatmap</p></li>
				<li>The last method of the <strong class="source-inline">GradCAM</strong> class overlays a heatmap onto the original image. This lets us get a better sense of the visual cues the network is looking at when making predictions:<p class="source-code">    def overlay_heatmap(self,</p><p class="source-code">                        heatmap,</p><p class="source-code">                        image, alpha=0.5,</p><p class="source-code">                        colormap=cv2.COLORMAP_VIRIDIS):</p><p class="source-code">        heatmap = cv2.applyColorMap(heatmap, colormap)</p><p class="source-code">        output = cv2.addWeighted(image,</p><p class="source-code">                                 alpha,</p><p class="source-code">                                 heatmap,</p><p class="source-code">                                 1 - alpha,</p><p class="source-code">                                 0)</p><p class="source-code">        return heatmap, output</p></li>
				<li>Let's<a id="_idIndexMarker1253"/> instantiate a <strong class="bold">ResNet50</strong> trained on ImageNet:<p class="source-code">model = ResNet50(weights='imagenet')</p></li>
				<li>Load the input image, resize it to the dimensions expected by ResNet50, turn it into a NumPy array, and preprocess it:<p class="source-code">image = load_img('dog.jpg', target_size=(224, 224))</p><p class="source-code">image = img_to_array(image)</p><p class="source-code">image = np.expand_dims(image, axis=0)</p><p class="source-code">image = imagenet_utils.preprocess_input(image)</p></li>
				<li>Pass the image through the model and extract the index of the most probable class:<p class="source-code">predictions = model.predict(image)</p><p class="source-code">i = np.argmax(predictions[0])</p></li>
				<li>Instantiate a <strong class="bold">GradCAM</strong> object and calculate the heatmap: <p class="source-code">cam = GradGAM(model, i)</p><p class="source-code">heatmap = cam.compute_heatmap(image)</p></li>
				<li>Overlay the<a id="_idIndexMarker1254"/> heatmap on top of the original image:<p class="source-code">original_image = cv2.imread('dog.jpg')</p><p class="source-code">heatmap = cv2.resize(heatmap, (original_image.shape[1],</p><p class="source-code">                               original_image.shape[0]))</p><p class="source-code">heatmap, output = cam.overlay_heatmap(heatmap, </p><p class="source-code">                                      original_image,</p><p class="source-code">                                      alpha=0.5)</p></li>
				<li>Decode the predictions to make it human-readable: <p class="source-code">decoded = imagenet_utils.decode_predictions(predictions)</p><p class="source-code">_, label, probability = decoded[0][0]</p></li>
				<li>Label the overlaid heatmap with the class and its associated probability:<p class="source-code">cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)</p><p class="source-code">cv2.putText(output, f'{label}: {probability * 100:.2f}%',</p><p class="source-code">            (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8,</p><p class="source-code">            (255, 255, 255), 2)</p></li>
				<li>Lastly, merge the original image, the heatmap, and the labeled overlay into a single image and save it to disk:<p class="source-code">output = np.hstack([original_image, heatmap, output])</p><p class="source-code">output = imutils.resize(output, height=700)</p><p class="source-code">cv2.imwrite('output.jpg', output)</p></li>
			</ol>
			<p>Here is <a id="_idIndexMarker1255"/>the result:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B14768_12_009.jpg" alt="Figure 12.9 – Visualization of Grad-CAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9 – Visualization of Grad-CAM</p>
			<p>As we can see, the network classified my dog as a Pug, which is correct, with a confidence of 85.03%. Moreover, the heatmap reveals the network activates around the nose and eyes of my dog's face, which means these are important features and the model is behaving as expected.</p>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor401"/>How it works…</h2>
			<p>In this recipe, we<a id="_idIndexMarker1256"/> learned and implemented <strong class="bold">Grad-CAM</strong>, a very useful algorithm for visually inspecting the activations of a neural network. This can be an effective way of debugging its behavior as it ensures it's looking at the right parts of an image.</p>
			<p>This is a very important tool because the high accuracy or performance of our model may have less to do with the actual learning, and more to do with factors that have been unaccounted for. For instance, if we are working on a pet classifier to distinguish between dogs and cats, we should use <strong class="bold">Grad-CAM</strong> to verify that the network looks at features inherent to these animals in order to properly classify them, and not at the surroundings, background noise, or less important elements in the images.</p>
			<h2 id="_idParaDest-346"><a id="_idTextAnchor402"/>See also</h2>
			<p>You can expand<a id="_idIndexMarker1257"/> your knowledge of <strong class="bold">Grad-CAM</strong> by reading the following paper: <a href="https://arxiv.org/abs/1610.02391">https://arxiv.org/abs/1610.02391</a>.</p>
		</div>
	</body></html>