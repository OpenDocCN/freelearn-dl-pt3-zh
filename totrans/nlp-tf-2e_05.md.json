["```\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data() \n```", "```\nprint(\"train_images is of shape: {}\".format(train_images.shape))\nprint(\"train_labels is of shape: {}\".format(train_labels.shape))\nprint(\"test_images is of shape: {}\".format(test_images.shape))\nprint(\"test_labels is of shape: {}\".format(test_labels.shape)) \n```", "```\ntrain_images is of shape: (60000, 28, 28)\ntrain_labels is of shape: (60000,)\ntest_images is of shape: (10000, 28, 28)\ntest_labels is of shape: (10000,) \n```", "```\n# Available at: https://www.tensorflow.org/api_docs/python/tf/keras/\n# datasets/fashion_mnist/load_data\nlabel_map = {\n    0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n    5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\",  8: \"Bag\", 9: \"Ankle boot\"\n} \n```", "```\ntrain_images = train_images[:, : , :, None]\ntest_images = test_images[:, : ,: , None] \n```", "```\nprint(\"train_images is of shape: {}\".format(train_images.shape))\nprint(\"test_images is of shape: {}\".format(test_images.shape)) \n```", "```\ntrain_images is of shape: (60000, 28, 28, 1)\ntest_images is of shape: (10000, 28, 28, 1) \n```", "```\nbatch_size = 100 # This is the typical batch size we've been using\nimage_size = 28 # This is the width/height of a single image\n# Number of color channels in an image. These are black and white images \nn_channels = 1 \n# Number of different digits we have images for (i.e. classes)\nn_classes = 10 \n```", "```\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.backend as K\nK.clear_session()\nlenet_like_model = Sequential([\n    # 1st convolutional layer\n    Conv2D(\n        filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', \n        activation='relu', \n        input_shape=(image_size,image_size,n_channels)\n    ), # in 28x28 / out 24x24\n    # 1st max pooling layer\n    MaxPool2D(pool_size=(2,2), strides=(2,2), padding='valid'), \n    # in 24x24 / out 12x12\n    # 2nd convolutional layer\n    Conv2D(filters=16, kernel_size=(5,5), strides=(1,1), \n    padding='valid', activation='relu'), # in 12x12 / out 8x8\n    # 2nd max pooling layer\n    MaxPool2D(pool_size=(2,2), strides=(2,2), padding='valid'), \n    # in 8x8 / out 4x4\n    # 3rd convolutional layer\n    Conv2D(filters=120, kernel_size=(4,4), strides=(1,1), \n    padding='valid', activation='relu'), # in 4x4 / out 1x1\n    # flatten the output of the last layer to suit a fully connected layer\n    Flatten(),\n    # First dense (fully-connected) layer\n    Dense(84, activation='relu'),\n    # Final prediction layer\n    Dense(n_classes, activation='softmax')\n]) \n```", "```\nConv2D(\n        filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', \n        activation='relu', \n        input_shape=(image_size,image_size,n_channels)\n    ) \n```", "```\nMaxPool2D(pool_size=(2,2), strides=(2,2), padding='valid') \n```", "```\nFlatten(), \n```", "```\nDense(84, activation='relu'),\nDense(n_classes, activation='softmax') \n```", "```\nlenet_like_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n```", "```\nlenet_like_model.fit(train_images, train_labels, validation_split=0.2, batch_size=batch_size, epochs=5) \n```", "```\nlenet_like_model.evaluate(test_images, test_labels) \n```", "```\n313/313 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8806 \n```", "```\ndef read_data(filename):\n    '''\n    Read data from a file with given filename\n    Returns a list of strings where each string is a lower case word\n    '''\n    # Holds question strings, categories and sub categories\n    # category/sub_cateory definitions: https://cogcomp.seas.upenn.edu/\n    # Data/QA/QC/definition.html\n    questions, categories, sub_categories = [], [], []     \n\n    with open(filename,'r',encoding='latin-1') as f:        \n        # Read each line\n        for row in f:   \n            # Each string has format <cat>:<sub cat> <question>\n            # Split by : to separate cat and (sub_cat + question)\n            row_str = row.split(\":\")        \n            cat, sub_cat_and_question = row_str[0], row_str[1]\n            tokens = sub_cat_and_question.split(' ')\n            # The first word in sub_cat_and_question is the sub \n            # category rest is the question\n            sub_cat, question = tokens[0], ' '.join(tokens[1:])        \n\n            questions.append(question.lower().strip())\n            categories.append(cat)\n            sub_categories.append(sub_cat)\n\n    return questions, categories, sub_categories\ntrain_questions, train_categories, train_sub_categories = read_data(train_filename)\ntest_questions, test_categories, test_sub_categories = read_data(test_filename) \n```", "```\n# Define training and testing\ntrain_df = pd.DataFrame(\n    {'question': train_questions, 'category': train_categories, \n    'sub_category': train_sub_categories}\n)\ntest_df = pd.DataFrame(\n    {'question': test_questions, 'category': test_categories,\n    'sub_category': test_sub_categories}\n) \n```", "```\n# Shuffle the data for better randomization\ntrain_df = train_df.sample(frac=1.0, random_state=seed) \n```", "```\n# Generate the label to ID mapping\nunique_cats = train_df[\"category\"].unique()\nlabels_map = dict(zip(unique_cats, np.arange(unique_cats.shape[0])))\nprint(\"Label->ID mapping: {}\".format(labels_map))\nn_classes = len(labels_map)\n# Convert all string labels to IDs\ntrain_df[\"category\"] = train_df[\"category\"].map(labels_map)\ntest_df[\"category\"] = test_df[\"category\"].map(labels_map) \n```", "```\nfrom sklearn.model_selection import train_test_split\ntrain_df, valid_df = train_test_split(train_df, test_size=0.1)\nprint(\"Train size: {}\".format(train_df.shape))\nprint(\"Valid size: {}\".format(valid_df.shape)) \n```", "```\nTrain size: (4906, 3)\nValid size: (546, 3) \n```", "```\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# Define a tokenizer and fit on train data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df[\"question\"].tolist()) \n```", "```\n# Convert each list of tokens to a list of IDs, using tokenizer's mapping\ntrain_sequences = tokenizer.texts_to_sequences(train_df[\"question\"].tolist())\nvalid_sequences = tokenizer.texts_to_sequences(valid_df[\"question\"].tolist())\ntest_sequences = tokenizer.texts_to_sequences(test_df[\"question\"].tolist()) \n```", "```\nmax_seq_length = 22\n# Pad shorter sentences and truncate longer ones (maximum length: max_seq_\n# length)\npreprocessed_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    train_sequences, maxlen=max_seq_length, padding='post',\n    truncating='post'\n)\npreprocessed_valid_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    valid_sequences, maxlen=max_seq_length, padding='post', \n    truncating='post'\n)\npreprocessed_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    test_sequences, maxlen=max_seq_length, padding='post', \n    truncating='post'\n) \n```", "```\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.regularizers as regularizers\nfrom tensorflow.keras.models import Model \n```", "```\nK.clear_session() \n```", "```\nInput layer takes word IDs as inputs\nword_id_inputs = layers.Input(shape=(max_seq_length,), dtype='int32') \n```", "```\n# Get the embeddings of the inputs / out [batch_size, sent_length, \n# output_dim]\nembedding_out = layers.Embedding(input_dim=n_vocab, output_dim=64)(word_id_inputs) \n```", "```\n# For all layers: in [batch_size, sent_length, emb_size] / out [batch_\n# size, sent_length, 100]\nconv1_1 = layers.Conv1D(\n    100, kernel_size=3, strides=1, padding='same', \n    activation='relu'\n)(embedding_out)\nconv1_2 = layers.Conv1D(\n    100, kernel_size=4, strides=1, padding='same', \n    activation='relu'\n)(embedding_out)\nconv1_3 = layers.Conv1D(\n    100, kernel_size=5, strides=1, padding='same', \n    activation='relu'\n)(embedding_out) \n```", "```\n# in previous conv outputs / out [batch_size, sent_length, 300]\nconv_out = layers.Concatenate(axis=-1)([conv1_1, conv1_2, conv1_3]) \n```", "```\n# Pooling over time operation. \n# This is doing the max pooling over sequence length\n# in other words, each feature map results in a single output\n# in [batch_size, sent_length, 300] / out [batch_size, 1, 300]\npool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, padding='valid')(conv_out) \n```", "```\n# Flatten the unit length dimension\nflatten_out = layers.Flatten()(pool_over_time_out) \n```", "```\n# Compute the final output\nout = layers.Dense(\n    n_classes, activation='softmax',\n    kernel_regularizer=regularizers.l2(0.001)\n)(flatten_out) \n```", "```\n# Define the model\ncnn_model = Model(inputs=word_id_inputs, outputs=out) \n```", "```\n# Compile the model with loss/optimzier/metrics\ncnn_model.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy']\n) \n```", "```\ncnn_model.summary() \n```", "```\nModel: \"model\"\n______________________________________________________________________\nLayer (type)            Output Shape         Param #     Connected to \n======================================================================\ninput_1 (InputLayer)    [(None, 22)]         0                        \n______________________________________________________________________\nembedding (Embedding)   (None, 22, 64)       504320      input_1[0][0]\n______________________________________________________________________\nconv1d (Conv1D)         (None, 22, 100)      19300     embedding[0][0]\n______________________________________________________________________\nconv1d_1 (Conv1D)       (None, 22, 100)      25700     embedding[0][0]\n______________________________________________________________________\nconv1d_2 (Conv1D)       (None, 22, 100)      32100     embedding[0][0]\n______________________________________________________________________\nconcatenate (Concatenate) (None, 22, 300)    0            conv1d[0][0]\n                                                        conv1d_1[0][0]\n                                                        conv1d_2[0][0]\n______________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 1, 300)    0     concatenate[0][0]\n______________________________________________________________________\nflatten (Flatten)          (None, 300)         0   max_pooling1d[0][0]\n______________________________________________________________________\ndense (Dense)              (None, 6)           1806    flatten[0][0] \n======================================================================\nTotal params: 583,226\nTrainable params: 583,226\nNon-trainable params: 0\n______________________________________________________________________ \n```", "```\n# Call backs\nlr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=3, verbose=1,\n    mode='auto', min_delta=0.0001, min_lr=0.000001\n) \n```", "```\n# Train the model\ncnn_model.fit(\n    preprocessed_train_sequences, train_labels, \n    validation_data=(preprocessed_valid_sequences, valid_labels),\n    batch_size=128, \n    epochs=25,\n    callbacks=[lr_reduce_callback]\n) \n```", "```\nEpoch 1/50\n39/39 [==============================] - 1s 9ms/step - loss: 1.7147 - accuracy: 0.3063 - val_loss: 1.3912 - val_accuracy: 0.5696\nEpoch 2/50\n39/39 [==============================] - 0s 6ms/step - loss: 1.2268 - accuracy: 0.6052 - val_loss: 0.7832 - val_accuracy: 0.7509\n...\nEpoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nEpoch 16/50\n39/39 [==============================] - 0s 6ms/step - loss: 0.0487 - accuracy: 0.9999 - val_loss: 0.3639 - val_accuracy: 0.8846\nRestoring model weights from the end of the best epoch.\nEpoch 00016: early stopping \n```", "```\ncnn_model.evaluate(preprocessed_test_sequences, test_labels, return_dict=True) \n```"]