<html><head></head><body>
		<div id="_idContainer209">
			<h1 id="_idParaDest-184"><em class="italic"><a id="_idTextAnchor195"/>Chapter 10</em>: Road Ahead</h1>
			<p>This is the final chapter of the book. We have learned about and implemented many generative models; and yet there are a lot more models and applications that we have not covered as they are beyond the scope of this book. In this chapter, we will start by summarizing some of the important techniques that we have learned, such as <strong class="bold">optimizer and activation functions</strong>, <strong class="bold">adversarial loss</strong>, <strong class="bold">auxiliary loss</strong>, <strong class="bold">normalization</strong>, and <strong class="bold">regularization</strong>.</p>
			<p>Then, we will look at some of the common pitfalls when using generative models in real-world settings. After that, we will go over some interesting image/video generative models and applications. There is no coding in this chapter, but you will find that many of the new models that we introduce in this chapter are built using techniques we have learned previously. There are also a few links to resources where you can read papers and code to explore the technology. </p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Reviewing GANs</li>
				<li>Putting your skills into practice</li>
				<li>Image processing</li>
				<li>Text to image</li>
				<li>Video retargeting</li>
				<li>Neural rendering</li>
			</ul>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor196"/>Reviewing GANs</h1>
			<p>Apart from PixelCNN, which <a id="_idIndexMarker670"/>we covered in <a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Image Generation Using TensorFlow</em>, which is a CNN, all the other generative models we have learned about are based on (variational) autoencoders or <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>). Strictly speaking, a GAN is not a network but a training method that makes use of two networks – a generator and a discriminator. I tried to fit a lot of content into this book; so, the information can be overwhelming. We will now go over a summary of the important techniques we have learned, by grouping them into the following categories:</p>
			<ul>
				<li>Optimizer and activation functions</li>
				<li>Adversarial loss</li>
				<li>Auxiliary loss</li>
				<li>Normalization</li>
				<li>Regularization</li>
			</ul>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor197"/>Optimizer and activation functions</h2>
			<p><strong class="bold">Adam</strong> is the most popular<a id="_idIndexMarker671"/> optimizer in training GANs, followed by<a id="_idIndexMarker672"/> RMSprop. Typically, the first moment in Adam is set to <strong class="source-inline">0</strong> and the second moment is set to <strong class="source-inline">0.999</strong>. The learning rate for the generator is set to <strong class="source-inline">0.0001</strong>, while the discriminator uses a learning rate that is two to four times larger than that. The discriminator is the key component in a GAN and it needs to learn well before the <a id="_idIndexMarker673"/>generator does. WGAN trains the discriminator more times than the generator in the training step, and an alternative to that is to use a higher learning rate for the discriminator. </p>
			<p>On the other hand, the de facto activation function for internal layers is leaky ReLU with an alpha of <strong class="source-inline">0.01</strong> or <strong class="source-inline">0.02</strong>. The choice of the generator's output activation functions depends on the image normalization, that is, sigmoid for a pixel range of <strong class="source-inline">[0, 1]</strong> or Tanh for <strong class="source-inline">[-1, 1]</strong>. On the other hand, the discriminator uses a linear output activation function for most adversarial losses, apart from sigmoid for the earlier non-saturated loss.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor198"/>Adversarial loss</h2>
			<p>We have seen that an <a id="_idIndexMarker674"/>autoencoder can be used as a generator in a GAN setting. GANs are trained using adversarial loss (sometimes called GAN loss). The following table lists some of the popular adversarial losses:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B14538_10_01.jpg" alt="Figure 10.1 – Important adversarial loss; σ refers to the sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Important adversarial loss; σ refers to the sigmoid function</p>
			<p>Non-saturated loss is <a id="_idIndexMarker675"/>used in vanilla GANs but it is unstable due to disjoining gradients. Wasserstein loss has theory underpinning it that proves it is more stable to train with. However, many GAN models choose to use least-square loss, which has shown to be stable too. In recent years, hinge loss has become the favorite choice of many state-of-the-art models. It is not clear which loss is the best. Nevertheless, we have used least-square and hinge loss in many models in this book and they seem to train well. So, I would suggest you try them first when designing your new GANs. </p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor199"/>Auxiliary loss</h2>
			<p>Apart from adversarial loss, which <a id="_idIndexMarker676"/>acts as the main loss in GAN training, there are various auxiliary losses that help generate better images. Some of them are as follows:</p>
			<ul>
				<li><strong class="bold">Reconstruction loss</strong> <em class="italic">(</em><a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Variational Autoencoder) </em>to encourage pixel-wise accuracy, this is <a id="_idIndexMarker677"/>usually L1 loss.</li>
				<li><strong class="bold">KL divergence loss</strong> <em class="italic">(</em><a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Variational Autoencoder) </em>for the <strong class="bold">variational autoencoder</strong> (<strong class="bold">VAE</strong>) to bring<a id="_idIndexMarker678"/> the latent vector to a standard, multivariate normal distribution.</li>
				<li><strong class="bold">Cycle consistency loss</strong> <em class="italic">(</em><a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation) </em>for bi-direction image<a id="_idIndexMarker679"/> translation.</li>
				<li><strong class="bold">Perceptual loss</strong> <em class="italic">(</em><a href="B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 5</em></a><em class="italic">, Style Transfer) </em>measures high-level perceptual and<a id="_idIndexMarker680"/> semantic differences between images. It can be <a id="_idIndexMarker681"/>further divided into two losses:<p>a) <strong class="bold">Feature-matching loss</strong>, which is<a id="_idIndexMarker682"/> normally the L2 loss of image features extracted from VGG layers. This is also called <strong class="bold">perceptual loss</strong>.</p><p>b) <strong class="bold">Style loss</strong> features are<a id="_idIndexMarker683"/> normally derived from VGG features, such as the Gram matrix or activation statistics, and are calculated using L2 loss.</p></li>
			</ul>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor200"/>Normalization</h2>
			<p>Layer activations are<a id="_idIndexMarker684"/> normalized to stabilize network training. Normalization takes the following general form:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/Formula_10_001.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/Formula_10_002.jpg" alt=""/>
				</div>
			</div>
			<p class="Basic-Paragraph">Here, <em class="italic">x</em> is the activation, <em class="italic">µ</em> is the mean of activations, <em class="italic">σ</em> is the standard deviation of the activations, and <em class="italic">ε</em> is the fudge factor for numerical stability. <em class="italic">γ</em> and <em class="italic">β</em> are learnable parameters; there is one pair for each activation channel. Many of the different normalizations differ only in how <em class="italic">µ</em> and <em class="italic">σ</em> are obtained:</p>
			<ul>
				<li>In <strong class="bold">batch normalization </strong><em class="italic">(</em><a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a><em class="italic">, Generative Adversarial Network)</em>, the mean and standard<a id="_idIndexMarker685"/> deviation are calculated across both batch (<em class="italic">N</em>) and spatial (<em class="italic">H</em>, <em class="italic">W</em>) locations, or in other words, (<em class="italic">N</em>, <em class="italic">H</em>, <em class="italic">W</em>). </li>
				<li><strong class="bold">Instance normalization </strong><em class="italic">(</em><a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation)</em> which is the preferred method<a id="_idIndexMarker686"/> nowadays, only uses the spatial dimension (<em class="italic">H</em>, <em class="italic">W</em>).</li>
				<li><strong class="bold">Adaptive instance normalization</strong> (<strong class="bold">AdaIN</strong>) <em class="italic">(</em><a href="B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 5</em></a><em class="italic">, Style Transfer) </em>serves a different purpose to<a id="_idIndexMarker687"/> merge the content and style activation. It still uses the equation, except that now the parameters have different meanings. <em class="italic">X</em> is still the activation we consider from the content feature. <em class="italic">γ</em> and <em class="italic">β</em> are no longer learnable parameters but the mean and standard deviation from the style features. Like with instance normalization, the statistics are calculated across a spatial dimension of (<em class="italic">H</em>, <em class="italic">W</em>).</li>
				<li><strong class="bold">Spatially-adaptive normalization (SPADE) </strong>(<a href="B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a><em class="italic">, AI Painter</em>) has one <em class="italic">γ</em> and <em class="italic">β</em> value for each of the features (pixels), in other words, they have dimension of (H, W, C). They are produced by running convolutional layers across segmentation map to normalize pixels from different semantic objects separately.</li>
			</ul>
			<p><strong class="bold">Conditional batch normalization</strong> <em class="italic">(</em><a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a><em class="italic">, Self-Attention for Image Generation) </em>is just like<a id="_idIndexMarker688"/> batch normalization except that <em class="italic">γ</em> and <em class="italic">β</em> are now multi-dimensional of (LABELS, C), so there is one set of them per class label. </p>
			<p><strong class="bold">Pixel normalization</strong> <em class="italic">(</em><a href="B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a><em class="italic">, High Fidelity Face Generation ) </em>deviates from the preceding settings. It doesn't<a id="_idIndexMarker689"/> have <em class="italic">µ</em>, <em class="italic">γ</em>, or <em class="italic">β</em>, and <em class="italic">σ</em> is the L2 norm taken across the channel dimension for each spatial location. The normalized activations have a magnitude of <strong class="source-inline">1</strong>. </p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor201"/>Regularization </h2>
			<p>Apart from<a id="_idIndexMarker690"/> adversarial loss and normalization, the other important factor in stabilizing GAN training is regularization. Regularization aims to constrain the growth of the network weights in order to keep the competition between the generator and discriminator in check. This is normally done by adding a loss function that uses weights. The two common regularizations used in GANs aim to enforce the 1-Lipschitz constraint:</p>
			<ul>
				<li><strong class="bold">Gradient penalty</strong> <em class="italic">(</em><a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a><em class="italic">, Generative Adversarial Network) </em>penalizes the growth of gradients, hence<a id="_idIndexMarker691"/> the weights. However, it is not very commonly used due to the additional backpropagation required to calculate the gradients against the input. This slows down the computation considerably. </li>
				<li><strong class="bold">Orthogonal regularization </strong><em class="italic">(</em><a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a><em class="italic">, Self-Attention for Image Generation)</em> aims to make<a id="_idIndexMarker692"/> the weights to be orthonormal matrices, this is because the matrix norm doesn't change if it multiplies with an orthogonal matrix. This can avoid the vanishing or exploding gradient problems. </li>
				<li><strong class="bold">Spectral normalization</strong> <em class="italic">(</em><a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a><em class="italic">, Self-Attention for Image Generation) </em>normalizes the<a id="_idIndexMarker693"/> layer weights by dividing it by its spectral norm. This is different from usual regularizations that use loss function to constrain the weights. Spectral normalization is computationally efficient, easy to implement, and independent of the training loss. You should use it when designing a new GAN.  </li>
			</ul>
			<p>This concludes the summary of GANs techniques. We will now look at new applications and models that we have not explored.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor202"/>Putting your skills into practice</h1>
			<p>Now, you can apply the skills you have learned to implement your own image generation projects. Before you start, there are some pitfalls you should look out for and also some practical advice that you can follow.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor203"/>Don't trust everything you read</h2>
			<p>A new academic paper is published and shows astonishing images generated by their model! Take it with a pinch of salt. Usually, these papers handpick the best result to showcase and hide the failed examples. Furthermore, the images are shrunk down to fit onto the paper, thus the image artifacts may not be visible from the paper. Before investing your time in using or re-implementing the information in the paper, try to find other resources of the claimed results. This can be the author's website or GitHub repository, which may contain the raw, high-definition images and videos.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor204"/>How big is your GPU?</h2>
			<p>Deep learning<a id="_idIndexMarker694"/> models, especially GANs, are computationally expensive. Many of the state-of-the-art results are produced after training tons of data on multiple GPUs for weeks. You will almost certainly need that sort of computing power just to attempt to reproduce those results. Therefore, pay attention to the computation resources used in the papers to avoid disappointment. </p>
			<p>If you don't mind waiting, you can use a single GPU and wait four times longer (assuming the original implementation used four GPUs). However, this will usually mean the batch size will also have to be reduced by four times, and this can have an effect on the results and convergence rate. You may have to reduce the learning rate to match the reduced batch size, which further slows down the training time.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor205"/>Build your model using existing models </h2>
			<p>The renowned AI scientist Dr. Andrej Karpathy, in one<a id="_idIndexMarker695"/> of his talks in 2019, said <em class="italic">"don't be a hero."</em> When you want to create an AI project, do not invent your own model; always start from existing models. Researchers have spent huge amounts of time and resources on creating models. Along the way, they may have thrown in some tricks as well. Therefore, you should start from existing models, then tweak or build on top of them to suit your requirements. </p>
			<p>As we have seen throughout this book, most often, state-of-the-art models do not come out of thin air but have been built on top of pre-existing models or techniques. There are usually implementations of the model available online, either officially by the authors or by re-implementation by enthusiasts in various different machine learning frameworks. One useful web resource to find them is the <a href="http://paperswithcode.com/">http://paperswithcode.com/</a> website. </p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor206"/>Understand the model's limitations</h2>
			<p>A lot of the AI companies I know <a id="_idIndexMarker696"/>do not create their own model architecture, for the reasons mentioned in the preceding sections. So, what is the point of learning to code TensorFlow to create image generation models? The first answer to that is that by writing from scratch, you now understand what the layers and models are, as well as their limitations. Say someone without knowledge of GANs was amazed by what AI could do, so they downloaded pix2pix to train on their own dataset to translate images of cats into trees. That did not work, and they had no clue why it failed; AI was a black box to them. </p>
			<p>As an AI-educated person, we know that pix2pix requires a paired image dataset, and we will need to use CycleGAN for unpaired datasets. The knowledge that you have learned will help you choose the right model and the right data to use. Furthermore, you will now know how to tweak the model architecture for different image sizes, different conditioning, and so on. </p>
			<p>We have now looked at some of the common pitfalls in using generative models. Now, we will look at some of the interesting applications and models that you could use generative models for.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor207"/>Image processing</h1>
			<p>Out of all the things that<a id="_idIndexMarker697"/> image generative models can do, <strong class="bold">image processing</strong> is probably the one that produces the best results for commercial use. In our context, image processing refers to applying some transformation to existing images to produce new ones. We will look at the three applications of image processing in this section – <strong class="bold">image inpainting</strong>, <strong class="bold">image compression</strong>, and <strong class="bold">image super-resolution</strong> (<strong class="bold">ISR</strong>). </p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor208"/>Image inpainting</h2>
			<p>Image inpainting is the process <a id="_idIndexMarker698"/>of filling in missing pixels of an image so that the result is visually realistic. It has practical applications in image editing, such as restoring a damaged image or removing obstructing objects. In the following example, you can see how image inpainting is used to remove people in the background. We first fill the people in with white pixels, then we use a generative model to fill in the pixels:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B14538_10_02.jpg" alt="Figure 10.2 – Image inpainting using DeepFillv2 to remove people in the background (left) original image, (middle) people filled with white masks, (right) restored image&#13;&#10;(source: J. Yu et al., 2018, &quot;Free-Form Image Inpainting with Gated Convolution,&quot; &#13;&#10;https://arxiv.org/abs/1806.03589) "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Image inpainting using DeepFillv2 to remove people in the background (left) original image, (middle) people filled with white masks, (right) restored image (source: J. Yu et al., 2018, "Free-Form Image Inpainting with Gated Convolution," <a href="https://arxiv.org/abs/1806.03589">https://arxiv.org/abs/1806.03589</a>) </p>
			<p>Traditional image inpainting works by finding a background patch with a similar texture and then pasting it into the missing regions. However, this usually only works for simple textures in a small area. One of the first GANs designed for image inpainting is the <strong class="bold">context encoder</strong>. Its architecture is <a id="_idIndexMarker699"/>similar to an autoencoder but trained with adversarial loss in addition to the usual L2 reconstruction loss. The result can appear blurry if there is a large area to be filled. </p>
			<p>One approach to tackle this is to use two networks (course and fine) to train on different scales. Using this <a id="_idIndexMarker700"/>approach, <strong class="bold">DeepFill</strong> (J. Yu et al., 2018, <em class="italic">Generative Image Inpainting with Contextual Attention</em>, <a href="https://arxiv.org/abs/1801.07892">https://arxiv.org/abs/1801.07892</a>) adds an attention layer to better capture the features from a distant spatial location.</p>
			<p>In earlier GANs, a <a id="_idIndexMarker701"/>dataset for image inpainting was created by randomly cutting out square masks (holes), but the technique does not translate well to real-world applications. Yu et al. propose a partial convolution layer to create irregular masks. The layer contains a masked convolution like the one we implemented in PixelCNN in <a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Image Generation Using TensorFlow</em>. The following image examples show the results of using a partial convolution-based network:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B14538_10_03.jpg" alt="Figure 10.3 – Irregular masks and the inpainted results (source: G. Liu et al., 2018, &quot;Image Inpainting for Irregular Holes Using Partial Convolutions,&quot; https://arxiv.org/abs/1804.07723)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Irregular masks and the inpainted results (source: G. Liu et al., 2018, "Image Inpainting for Irregular Holes Using Partial Convolutions," <a href="https://arxiv.org/abs/1804.07723">https://arxiv.org/abs/1804.07723</a>)</p>
			<p><strong class="bold">DeepFillv2</strong> uses gated<a id="_idIndexMarker702"/> convolution to improve and generalize masked convolutions. DeepFill uses only a standard discriminator that predicts real or fake images. However, this does not work well when there can be many holes in free-form inpainting. Therefore, it uses <strong class="bold">spectral-normalized PatchGAN</strong> (<strong class="bold">SN-PatchGAN</strong>) to encourage more<a id="_idIndexMarker703"/> realistic inpainting.</p>
			<p>The following are some additional resources on this topic:</p>
			<ul>
				<li>The TensorFlow v1 source code for DeepFillv1 and v2: <a href="https://github.com/JiahuiYu/generative_inpainting">https://github.com/JiahuiYu/generative_inpainting</a> </li>
				<li>Interactive inpainting demo where you can use your own photo to play with: <a href="https://www.nvidia.com/research/inpainting/">https://www.nvidia.com/research/inpainting/</a> </li>
			</ul>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor209"/>Image compression</h2>
			<p>Image compression is the <a id="_idIndexMarker704"/>process of transforming images from raw pixels into encoded data that is much smaller in size for storage or communication. For example, a JPEG file is a compressed image. When we open a JPEG file, the computer will reverse the compression process to restore the image pixels. The simplified image compression pipeline is as follows:</p>
			<ol>
				<li><strong class="bold">Segmentation</strong>: Divide the<a id="_idIndexMarker705"/> image into small blocks and each of them will be processed individually.</li>
				<li><strong class="bold">Transformation</strong>: Transform<a id="_idIndexMarker706"/> raw pixels into representations that are more compressible. At this stage, a higher compression rate is normally achieved by removing high-frequency content that makes the restored image blurrier. For example, consider a segment of a grayscale image containing [255, 250, 252, 251, ...] pixel values that are nearly white. The differences between them are so small that the human eye cannot pick up on them, so we can just transform all the pixels into 255. This will make the data easier to compress. </li>
				<li><strong class="bold">Quantization</strong>: Use a lower bit<a id="_idIndexMarker707"/> number to represent the data. An example is to convert a grayscale image with 256-pixel values between [0, 255] into two values of black and white of [0, 1]. </li>
				<li><strong class="bold">Symbol encoding</strong> is used to<a id="_idIndexMarker708"/> encode data using some efficient coding. One of the common ones is known as <strong class="bold">run-length coding</strong>. Instead of saving<a id="_idIndexMarker709"/> every 8-bit pixel, we can save just the difference between the pixels. Therefore, instead of saving white pixels of [255, 255, 255, …], we can just encode it into something such as [255] x 100, which says the white pixel repeats 100 times.</li>
			</ol>
			<p>A higher compression rate is achieved by using more extreme quantization or removing more frequency contents. As a result, this information is lost (hence, this is known as lossy compression). The following diagram shows one such GAN for image compression:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B14538_10_04.jpg" alt="Figure 10.4 – Generative compression network. The encoder (E) maps the image into latent feature &#13;&#10;w. It is quantized by a finite quantizer, q, to obtain representation ŵ , which can be encoded &#13;&#10;into a bitstream. The decoder (G) reconstructs the image and D is the discriminator. &#13;&#10;(Source: E. Agustsson et al., 2018, &quot;Generative Adversarial Networks for Extreme Learned Image Compression,&quot; https://arxiv.org/abs/1804.02958)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Generative compression network. The encoder (E) maps the image into latent feature w. It is quantized by a finite quantizer, q, to obtain representation ŵ , which can be encoded into a bitstream. The decoder (G) reconstructs the image and D is the discriminator. (Source: E. Agustsson et al., 2018, "Generative Adversarial Networks for Extreme Learned Image Compression," <a href="https://arxiv.org/abs/1804.02958">https://arxiv.org/abs/1804.02958</a>)</p>
			<p>In general, generative<a id="_idIndexMarker710"/> compression uses autoencoder architecture to compress an image into small, latent code and that is restored using the decoder. </p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor210"/>Image super-resolution</h2>
			<p>We have used the <a id="_idIndexMarker711"/>upsampling layer a lot to increase the spatial resolution of the activation in the generator (GAN) or decoder (autoencoder). It works by spacing out the pixels and filling in the gaps by interpolation. As a result, the enlarged image is usually blurry. </p>
			<p>In a lot of image applications, we want to enlarge the image while keeping its crispness, and this can be done via <strong class="bold">image super resolution</strong> (<strong class="bold">ISR</strong>). ISR aims to increase the image from <strong class="bold">low resolution</strong> (<strong class="bold">LR</strong>) to <strong class="bold">high resolution</strong> (<strong class="bold">HR</strong>). <strong class="bold">Super-Resolution Generative Adversarial Network</strong> (<strong class="bold">SRGAN</strong>) (C. Ledig<a id="_idIndexMarker712"/> et al., 2016, <em class="italic">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</em>, <a href="https://arxiv.org/abs/1609.04802">https://arxiv.org/abs/1609.04802</a>) was the first to use a GAN to do that. </p>
			<p>SRGAN's architecture is similar to that of DCGAN but uses residual blocks instead of a plain convolutional layer. It borrows the perception loss from style transfer literature, that is, the content <a id="_idIndexMarker713"/>loss calculated from VGG features. In retrospect, we knew this was a better measure of visual perception quality rather than pixel-wise loss. We can now see how versatile the autoencoder is for various image processing tasks. Similar autoencoder architecture can be repurposed for other image processing tasks, such as image denoising or deblurring. Next, we will look at an application where the input to the model is not images but words.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor211"/>Text to image</h1>
			<p>Text-to-image GANs are <a id="_idIndexMarker714"/>conditional GANs. However, instead of using class labels as conditions, they use words as the condition to generate images. In earlier practice, GANs used word embeddings as the conditions into the generator and discriminator. Their architectures are similar to conditional GANs, which we learned about in <a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a>, <em class="italic">Image-to-Image Translation</em>. The difference is merely that the embedding of text is generated <a id="_idIndexMarker715"/>using a <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) preprocessing pipeline. The following diagram shows the architecture of a text-conditional GAN:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B14538_10_05.jpg" alt="Figure 10.5 – Text-conditional convolutional GAN architecture where text encoding &#13;&#10;is used by both the generator and discriminator &#13;&#10;(Redrawn from: S. Reed et al., 2016, &quot;Generative Adversarial Text to Image Synthesis,&quot; &#13;&#10;https://arxiv.org/abs/1605.05396)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Text-conditional convolutional GAN architecture where text encoding is used by both the generator and discriminator (Redrawn from: S. Reed et al., 2016, "Generative Adversarial Text to Image Synthesis," <a href="https://arxiv.org/abs/1605.05396">https://arxiv.org/abs/1605.05396</a>)</p>
			<p>Like normal GANs, generated<a id="_idIndexMarker716"/> high-resolution images tend to be blurry. <strong class="bold">StackGAN</strong> resolves this by stacking two networks together. The following diagram shows the text and the generated images at different stages of StackGAN and a vanilla GAN:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B14538_10_06.jpg" alt="Figure 10.6 – Images generated by StackGAN at different generator stages&#13;&#10;(source: H. Zhang et al., 2017, &quot;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks,&quot; https://arxiv.org/abs/1612.03242)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Images generated by StackGAN at different generator stages (source: H. Zhang et al., 2017, "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks," <a href="https://arxiv.org/abs/1612.03242">https://arxiv.org/abs/1612.03242</a>)</p>
			<p>The first generator<a id="_idIndexMarker717"/> produces a low-resolution image from the word embedding. The second generator then takes the generated image and word embedding as input conditions to the second generator to produce refined images. The coarse-to-fine architecture has<a id="_idIndexMarker718"/> appeared in different forms in many high-resolution GANs, as we have learned in this book.</p>
			<p><strong class="bold">AttnGAN</strong> (T. Xu et al., 2017, <em class="italic">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks, at</em> <a href="https://arxiv.org/abs/1711.10485">https://arxiv.org/abs/1711.10485</a>) further improves text-to-image synthesis by using an attention module. The attention module is different from the one used in SAGAN (<a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a><em class="italic">, Self-Attention for Image Generation</em>) but the principle is the same. There are two inputs into the attention module at the start of every stage of the generator – word features and image features. It learns to pay attention to<a id="_idIndexMarker719"/> different words and image regions when moving from coarse to fine generators. Most text-to-image models after that have some form of attention mechanism. </p>
			<p>Text to image is still an unsolved problem; it still struggles to generate complex real-world images from text. As we can see in the following figure, the generated images are still far from perfect. Researchers are beginning to bring in recent advancement from NLP to improve text-to-image performance: </p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B14538_10_07.jpg" alt="Figure 10.7 – Examples of images generated from the given caption from the MS-COCO dataset (A) original images and their image caption in the dataset (B) images generated by StackGAN + object pathway (C) images generated by StackGAN (source: T. Hinz et al., 2019, &quot;Generating Multiple Objects at Spatially Distinct Locations,&quot; https://arxiv.org/abs/1901.00686)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Examples of images generated from the given caption from the MS-COCO dataset (A) original images and their image caption in the dataset (B) images generated by StackGAN + object pathway (C) images generated by StackGAN (source: T. Hinz et al., 2019, "Generating Multiple Objects at Spatially Distinct Locations," <a href="https://arxiv.org/abs/1901.00686">https://arxiv.org/abs/1901.00686</a>)</p>
			<p>Next, we will look at the exciting application of video retargeting.</p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor212"/>Video retargeting</h1>
			<p>Video synthesis is<a id="_idIndexMarker720"/> a broad term used for describing all forms of video generation. This can include generating video from random noise or words, to colorize black-and-white video, and so on, much like image generation. </p>
			<p>In this section, we will look at a subgroup of video synthesis known as <strong class="bold">video retargeting</strong>. We will first look at two applications – face reenactment and pose transfer – and then introduce a powerful model that uses motion to generalize video targeting.</p>
			<p>Face reenactment</p>
			<p><strong class="bold">Face reenactment</strong> was introduced<a id="_idIndexMarker721"/> along with face swapping in <a href="B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175"><em class="italic">Chapter 9</em></a>, <em class="italic">Video Synthesis</em>. Face reenactment in video synthesis involves transferring the facial expression of the driving video to the face in the target video. This is <a id="_idIndexMarker722"/>useful in animation and movie making. Recently, Zakharov et al. proposed a generative model that requires only a few target 2D images. This is done by using facial landmarks as intermediate features, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B14538_10_08.jpg" alt="Figure 10.8 – Transferring the facial expression from the target image to the source image &#13;&#10;(source: E. Zakharov et al., 2019, &quot;Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,&quot; https://arxiv.org/abs/1905.08233)"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 10.8 – Transferring the facial expression from the target image to the source image (source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models," <a href="https://arxiv.org/abs/1905.08233">https://arxiv.org/abs/1905.08233</a>)</p>
			<p>Let's briefly look into the model architecture, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B14538_10_09.jpg" alt="Figure 10.9 – Architecture of few-shot adversarial learning &#13;&#10;(source: E. Zakharov et al., 2019, &quot;Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,&quot; https://arxiv.org/abs/1905.08233)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Architecture of few-shot adversarial learning (source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models," <a href="https://arxiv.org/abs/1905.08233">https://arxiv.org/abs/1905.08233</a>)</p>
			<p>The first thing that you should notice in the preceding diagram is <strong class="bold">AdaIN</strong>, which we immediately know is a style-based model. Therefore, we can see that the landmarks at the top are the content (the target's face shape and pose), while the style (the source's face attributes and expression) is<a id="_idIndexMarker723"/> extracted from the <strong class="bold">embedder</strong>. The generator then uses AdaIN to fuse the content and style to reconstruct the face. </p>
			<p>Recently, a similar model has been deployed by NVIDIA to slash the bit rate of teleconferencing video transmission. You can view their blog at <a href="https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/">https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/</a> to learn how they use many of the AI techniques, such as ISR, face alignment, and face reenactment, in real-world deployment. Next, we will look at how to use AI to transfer the pose of a person.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor213"/>Pose transfer</h2>
			<p><strong class="bold">Pose transfer</strong> is similar to face<a id="_idIndexMarker724"/> reenactment except that now it will transfer the body (and head) pose. There are many ways to perform pose transfer but all of them<a id="_idIndexMarker725"/> involve the use of <strong class="bold">body joints</strong> (also known<a id="_idIndexMarker726"/> as <strong class="bold">keypoints</strong>) as features. The following diagram shows one example of images generated from a condition image and target pose:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B14538_10_10.jpg" alt="Figure 10.10 – Transferring target poses onto the condition image &#13;&#10;(source: Z. Zhu et al., 2019, &quot;Progressive Pose Attention Transfer for Person Image &#13;&#10;Generation,&quot; https://arxiv.org/abs/1904.03349 )"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Transferring target poses onto the condition image (source: Z. Zhu et al., 2019, "Progressive Pose Attention Transfer for Person Image Generation," <a href="https://arxiv.org/abs/1904.03349">https://arxiv.org/abs/1904.03349</a> )</p>
			<p>Pose transfer has many potential applications, including generating a fashion modeling video from single 2D images. This task is more challenging than face reenactment due to the huge variety of human poses. Next, we will look at a motion model that could generalize both face reenactment and pose transfer.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor214"/>Motion transfer</h2>
			<p>The face reenactment and <a id="_idIndexMarker727"/>pose transfer models introduced in the preceding section require object-specific priors, in other words, the facial landmark and human pose keypoints. Those features are normally extracted by separate models trained using a lot of data, which can be expensive to acquire and annotate. </p>
			<p>Recently, an object-agnostic<a id="_idIndexMarker728"/> model known as a <strong class="bold">first-order motion model</strong> (A. Siarohin et al., 2019, <em class="italic">First Order Motion Model for Image Animation</em>, <a href="https://arxiv.org/abs/2003.00196">https://arxiv.org/abs/2003.00196</a>) was introduced. It has rapidly gained popularity for its ease of use as it doesn't need a lot of annotated training data. The following screenshot shows the overall architecture of the model, which exploits the motion in video frames:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/B14538_10_11.jpg" alt="Figure 10.11 – First-order motion model that disentangles appearance and motion (source: &#13;&#10;https://aliaksandrsiarohin.github.io/first-order-model-website/)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – First-order motion model that disentangles appearance and motion (source: <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">https://aliaksandrsiarohin.github.io/first-order-model-website/</a>)</p>
			<p>In style transfer, an image is disentangled into content and style. Using the same terminology, <strong class="bold">motion transfer</strong> disentangles a video into appearance and motion. The motion module <a id="_idIndexMarker729"/>captures the motion of the object in the driving video. The generator network uses the appearance from the source image (similar to VGG content features) and motion information to create a new target video. </p>
			<p>As a result, this model requires only a single source image and a driving video. It could do many of the video tasks we discussed, including face reenactment, pose transfer, and face swapping. You should definitely check out the website to see the demo video.</p>
			<p>Although video retargeting GANs have improved dramatically in recent years, they are still not quite there yet to generate high-resolution images that are perfect for video production. An alternative is to combine 3D modeling with 2D GANs, which we will discuss in the next section.</p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor215"/>Neural rendering</h1>
			<p><strong class="bold">Rendering</strong> is the process of <a id="_idIndexMarker730"/>generating photo-realistic images from 2D or 3D computer models. The term <strong class="bold">neural rendering</strong> has<a id="_idIndexMarker731"/> recently emerged to describe rendering using a neural network. In traditional 3D rendering, we will need to first create a 3D model with a polygon mesh that describes the object's shape, color, and texture. Then, the lighting and camera position will be set and render the view into a 2D image. </p>
			<p>There has been an ongoing research on 3D object generation, but it is still not able to generate satisfying results. We can take advantage of the advancement of GANs by projecting part of the 3D objects into 2D space. We then use GANs to enhance the image in 2D space, for example, to generate a realistic texture using style transfer before projecting that back into the 3D model. The top diagram in the following figure shows the general pipeline of this approach:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B14538_10_12.jpg" alt="Figure 10.12 – Two common frameworks for neural rendering &#13;&#10;(Redrawn from: M-Y. Liu et al., 2020, &quot;Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications,&quot; https://arxiv.org/abs/2008.02793) "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Two common frameworks for neural rendering (Redrawn from: M-Y. Liu et al., 2020, "Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications," <a href="https://arxiv.org/abs/2008.02793">https://arxiv.org/abs/2008.02793</a>) </p>
			<p>Diagram <em class="italic">(b)</em> shows the framework that uses 3D data as input and 3D differentiable operations, such as 3D convolution. Apart from 3D polygons, 3D data can also exist in the form of a point cloud that can be obtained from lidar/radar or computer vision techniques such as structure<a id="_idIndexMarker732"/> from motion. A point cloud is made up of points in 3D space that depict the object's surface. One application of a 3D to 2D deep network framework is to render the point cloud into a 2D image, as shown in the following figure, where the input is the cloud points obtained from a room:</p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/B14538_10_13.jpg" alt="Figure 10.13 – (left) 3D point cloud to 2D rendering, (middle) point cloud synthesis &#13;&#10;image, (right) ground truth&#13;&#10;(source: F. Pittaluga et al., 2019, &quot;Revealing Scenes by Inverting Structure from Motion Reconstructions,&quot; https://arxiv.org/abs/1904.03303)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – (left) 3D point cloud to 2D rendering, (middle) point cloud synthesis image, (right) ground truth (source: F. Pittaluga et al., 2019, "Revealing Scenes by Inverting Structure from Motion Reconstructions," <a href="https://arxiv.org/abs/1904.03303">https://arxiv.org/abs/1904.03303</a>)</p>
			<p>We can also perform rendering in the reverse direction, that is, from a 2D image to a 3D object. This is often<a id="_idIndexMarker733"/> known as <strong class="bold">inverse rendering</strong>. The following figure shows examples of 2D to 3D inverse rendering:</p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B14538_10_14.jpg" alt="Figure 10.14 – Given an input of 2D images (first column), the model predicts the 3D shape &#13;&#10;and texture and renders them into the same viewpoint (second column). Images on &#13;&#10;the right show the rendering in three different viewpoints. &#13;&#10;(Source: Y. Zhang et al., 2020, &quot;Image GANs Meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering,&quot; https://arxiv.org/abs/2010.09125)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Given an input of 2D images (first column), the model predicts the 3D shape and texture and renders them into the same viewpoint (second column). Images on the right show the rendering in three different viewpoints. (Source: Y. Zhang et al., 2020, "Image GANs Meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering," <a href="https://arxiv.org/abs/2010.09125">https://arxiv.org/abs/2010.09125</a>)</p>
			<p>The model by Y. Zhang et al., 2020, uses two <em class="italic">renderers</em>. One is a differentiable graphics renderer to render 2D into 3D, which is outside the scope of this book. The other one is a GAN to generate multi-view image data, or more specifically, StyleGAN. It is interesting to know why they chose to use StyleGAN. The authors learned that StyleGAN could generate faces of <a id="_idIndexMarker734"/>slightly different viewing angles by changing the latent code. Then, they did an extensive study to find that styles in early layers control the camera viewpoint, making it ideal for this task. This is also a good example that shows how we could leverage 2D generative models into the 3D world.</p>
			<p>This concludes our introduction to neural rendering. I<a id="_idIndexMarker735"/>t is an active area and there are many more use cases that are yet to be explored.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor216"/>Summary</h1>
			<p>Since the inception of GANs and VAEs in 2014, significant advancement has been made in 2D image generation. Generating high-fidelity images is still challenging in practice as it requires huge amounts of data, computing power, and hyperparameter tuning. However, as demonstrated by StyleGAN, it seems that we now have the technology to do this, especially in face generation. </p>
			<p>In fact, at the time of writing this book, there haven't really been any major breakthroughs in this area since 2018. With this book, we have included all the important techniques leading to BigGAN. These techniques include the use of AdaIN and self-attention modules, which are now commonplace even in adjacent fields such as video synthesis. This gives us a solid foundation to explore other emerging generative technologies. </p>
			<p>In this chapter, we looked back at the things we have learned and summarized them in different groups, such as losses and normalization techniques. We then looked at some practical advice in training generative models. Finally, we touched upon some of the upcoming technologies, especially in the area of video retargeting. I believe you now have the knowledge, skills, and confidence to explore the new and exciting AI world and I wish you all the best in your new adventure. I hope you have enjoyed reading this book. I welcome your feedback, which will help me improve my writing skills for my next book. Thanks!</p>
		</div>
	</body></html>