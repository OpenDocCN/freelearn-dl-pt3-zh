<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer120">&#13;
			<p id="_idParaDest-132" class="chapter-number"><a id="_idTextAnchor235"/>Chapter 8: </p>&#13;
			<h1 id="_idParaDest-133"><a id="_idTextAnchor236"/>Best Practices for Model Training and Performance</h1>&#13;
			<p>In order for a supervised machine learning model to be well trained, it requires large volumes of training data. In this chapter, we are going to look at a few common examples and patterns for handling input data. We will specifically learn how to access training data regardless of its size and train the model with it. After that, we will look at regularization techniques that help to prevent overfitting. Having large volumes of training data is no guarantee of a well-trained model. In order to prevent overfitting, we may need to apply various regularization techniques in our training processes. We will take a look at a number of such techniques, starting with the typical Lasso (<strong class="bold">L1</strong>), Ridge (<strong class="bold">L2</strong>), and elastic net regularizations, before moving on to a modern regularization technique known as adversarial regularization. With these techniques at our disposal, we put ourselves in a good position vis-à-vis reducing overfitting as a result of training. </p>&#13;
			<p>When it comes to regularization, there is no straightforward way to determine which method works best. It certainly depends on other factors, such as the distribution or sparsity of features and the volume of data. The purpose of this chapter is to provide various examples and give you several choices to try during your own model training process. In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Input handling for loading data</li>&#13;
				<li>Regularization to reduce overfitting</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-134"><a id="_idTextAnchor237"/>Input handling for loading data</h1>&#13;
			<p>Many<a id="_idTextAnchor238"/><a id="_idIndexMarker452"/> common examples that we typically see tend to focus on the modeling aspect, such as how to build a deep learning model using TensorFlow with various layers and patterns. In these examples, the data used is almost always loaded into the runtime memory directly. This is fine as long as the training data is sufficiently small. But what if it is much larger than your runtime memory can handle? The solution is data streaming. We have been using this technique to feed data into our model in the previous chapters, and we are going to take a closer look at data streaming and generalize it to more data types. </p>&#13;
			<p>The <a id="_idIndexMarker453"/>streaming data technique is very similar to a Python generator. Data is ingested into the model training process in batches, meaning that all the data is not sent at one time. In this chapter, we are going to use an example of flower image data. Even though this data is not big by any means, it is a convenient tool for our teaching and learning purposes in this regard. It is multiclass and contains images of different sizes. This reflects what we usually have to deal with in reality, where available training images may be crowdsourced or provided at different scales or dimensions. In addition, an efficient data ingestion workflow is needed as the frontend to the model training process<a id="_idTextAnchor239"/>.</p>&#13;
			<h2 id="_idParaDest-135"><a id="_idTextAnchor240"/>Working with the generator</h2>&#13;
			<p>When it<a id="_idIndexMarker454"/> comes to the generator, TensorFlow now has a very convenient <strong class="source-inline">ImageDataGenerator</strong> API that greatly simplifies and speeds up the code development process. From our experience in using pretrained models for image classification, we have seen that it is often necessary to standardize image dimensions (height and width as measured by the number of pixels) and normalize image pixel values to within a certain range (from [<strong class="source-inline">0</strong>, <strong class="source-inline">255</strong>] to [<strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>]). </p>&#13;
			<p>The <strong class="source-inline">ImageDataGenerator</strong> API provides optional input parameters to make these tasks almost routine and reduce the work of writing your own functions to perform standardization and normalization. So, let's take a look at how to use this API:</p>&#13;
			<ol>&#13;
				<li>Organize raw images. Let's begin by setting up our image collection. For convenience, we are going to use the flower images directly from t<a id="_idTextAnchor241"/>he <strong class="source-inline">tf.keras</strong> API:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">data_dir = tf.keras.utils.get_file(</p><p class="source-code">    'flower_photos', 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',</p><p class="source-code">    untar=True)</p><p>In the <a id="_idIndexMarker455"/>preceding code, we use the <strong class="source-inline">tf.keras</strong> API to download the images of five flower types. </p></li>&#13;
				<li>Next, we will set up <strong class="source-inline">ImageDataGenerator</strong> and streaming objects with <strong class="source-inline">flow_from_directory</strong>. In this step, several operations are defined:<p>a. Image pixel intensity is scaled to a range of [<strong class="source-inline">0</strong>, <strong class="source-inline">255</strong>], along with a cross-validation fraction. The <strong class="source-inline">ImageDataGenerator</strong> API comes with optional input argument rescaling and <strong class="source-inline">validation_split</strong>. These arguments have to be in a dictionary format. Therefore, we can organize the rescale (normalization) factor and fraction for cross-validation together in <strong class="source-inline">datagen_kwargs</strong>.</p><p>b. The image height and width are both reformatted to <strong class="source-inline">224</strong> pixels. The <strong class="source-inline">flow_  from_directory</strong> API contains the optional <strong class="source-inline">target_size</strong>, <strong class="source-inline">batch_size</strong>, and <strong class="source-inline">interpolation</strong> <strong class="source-inline">arguments</strong>. These arguments are designed in a dictionary format. We may use these input arguments to set image size standardization, batch size, and the resampling interpolation algorithm in <strong class="source-inline">dataflow_kwargs</strong>. </p><p>c. The preceding settings are passed to the generator instance. We then pass these  into <strong class="source-inline">ImageDataGenerator</strong> and <strong class="source-inline">flow_from_directory</strong>:</p><p class="source-code">pixels =224</p><p class="source-code">BATCH_SIZE = 32 </p><p class="source-code">IMAGE_SIZE = (pixels, pixels)</p><p class="source-code">NUM_CLASSES = 5</p><p class="source-code">datagen_kwargs = dict(rescale=1./255, validation_split=.20)</p><p class="source-code">dataflow_kwargs = dict(target_size=IMAGE_SIZE, </p><p class="source-code">                       batch_size=BATCH_SIZE,</p><p class="source-code">                   interpolation="bilinear")</p><p class="source-code">valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(</p><p class="source-code">    **datagen_kwargs)</p><p class="source-code">valid_generator = valid_datagen.flow_from_directory(</p><p class="source-code">    data_dir, subset="validation", shuffle=False, **dataflow_kwargs)</p><p class="source-code">train_datagen = valid_datagen</p><p class="source-code">train_generator = train_datagen.flow_from_directory(</p><p class="source-code">data_dir, subset="training", shuffle=True, **dataflow_kwargs)</p><p>The preceding <a id="_idIndexMarker456"/>code demonstrates a typical workflow for creating image generators as a means of ingesting training data in to a model. Two dictionaries are defined and hold the arguments we need. Then, the <strong class="source-inline">ImageDataGenerator</strong> API is invoked, followed by the <strong class="source-inline">flow_from_directory</strong> API. The process is repeated for training data as well. For the results, we have set up an ingestion workflow for training and cross-validation data through <strong class="source-inline">train_generator</strong> and <strong class="source-inline">valid_generator</strong>. </p></li>&#13;
				<li>Retrieve mapping for labels. Since we use <strong class="source-inline">ImageDataGenerator</strong> to create a data pipeline for training, we may use it to retrieve image labels as well:<p class="source-code">labels_idx = (train_generator.class_indices)</p><p class="source-code">idx_labels = dict((v,k) for k,v in labels_idx.items())</p><p class="source-code">print(idx_labels)</p><p>In the preceding code, <strong class="source-inline">idx_labels</strong> is a dictionary that maps the classification model output, which is an index, to the <strong class="source-inline">flower</strong> class. This is <strong class="source-inline">idx_labels</strong>:</p><p class="source-code">{0: 'daisy', 1: 'dandelion', 2: 'roses', 3: 'sunflowers', 4: 'tulips'}</p><p>Since this is a multiclass classification problem, our model prediction will be an array of five probabilities. Therefore, we want the position of the class with the highest probability, and then we will map the position to the name of the corresponding class using <strong class="source-inline">idx_labels</strong>.</p></li>&#13;
				<li>Build and train the model. This step is the same as we performed in the previous chapter, <a href="B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Optimization</em>, where we will build a model by means of transfer learning. The model of choice is a ResNet feature vector, and the final classification layer is a dense layer with five nodes (<strong class="source-inline">NUM_CLASSES</strong> is defined to be <strong class="source-inline">5</strong>, as indicated in <em class="italic">step 2</em>), and these five nodes output probabilities for<a id="_idIndexMarker457"/> each of the five classes:<p class="source-code">mdl = tf.keras.Sequential([</p><p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),</p><p class="source-code">hub.KerasLayer("https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/4", trainable=False),</p><p class="source-code">    tf.keras.layers.Dense(NUM_CLASSES, </p><p class="source-code">             activation='softmax', name = 'custom_class')</p><p class="source-code">])</p><p class="source-code">mdl.build([None, 224, 224, 3])</p><p class="source-code">mdl.compile(</p><p class="source-code">  optimizer=tf.keras.optimizers.SGD(lr=0.005, </p><p class="source-code">                                           momentum=0.9), </p><p class="source-code">  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),</p><p class="source-code">  metrics=['accuracy'])</p><p class="source-code">steps_per_epoch = train_generator.samples </p><p class="source-code">                            // train_generator.batch_size</p><p class="source-code">validation_steps = valid_generator.samples </p><p class="source-code">                            // valid_generator.batch_size</p><p class="source-code">mdl.fit(</p><p class="source-code">    train_generator,</p><p class="source-code">    epochs=5, steps_per_epoch=steps_per_epoch,</p><p class="source-code">    validation_data=valid_generator,</p><p class="source-code">    validation_steps=validation_steps)</p><p>The preceding code shows the general flow of setting up a model's architecture through training. We started by building an <strong class="source-inline">mdl</strong> model using the <strong class="source-inline">tf.keras</strong> sequential API. Once the <strong class="source-inline">loss</strong> function and optimizer were designated, we compiled the model. Since we want to include cross-validation as part of the training routine, we need to set up <strong class="source-inline">step_per_epoch</strong>, which is the total number of data batches for the generator to yield as one epoch. This process is repeated for<a id="_idIndexMarker458"/> cross-validation data. Then we call the Fit API to launch the training process for five epochs.</p></li>&#13;
			</ol>&#13;
			<p>The preceding steps demonstrate how to start with <strong class="source-inline">ImageDataGenerator</strong> to build a pipeline that flows image data from the image directory via <strong class="source-inline">flow_from_directory</strong>, and we are also able to handle image normalization and standardization routines as input argum<a id="_idTextAnchor242"/>ents. </p>&#13;
			<h2 id="_idParaDest-136"><a id="_idTextAnchor243"/>TFRecord dataset – ingestion pipeline</h2>&#13;
			<p>Another means<a id="_idIndexMarker459"/> of streaming training data into the model during the training process is through the TFRecord dataset. TFRecord is a protocol buffer format. Data stored in this format may be used in <strong class="bold">Python</strong>, <strong class="bold">Java</strong>, and <strong class="bold">C++</strong>. In enterprise or production systems, this format may provide versatility and promote reusability of data across different applications. Another caveat for TFRecord is that if you wish to use TPU as your compute target, and you wish to use a pipeline to ingest training data, then TFRecord is the means to achieve it. Currently, TPU does not work with generators. Therefore, the only way to stream data through a pipeline approach is by means of TFRecord. Again, the size of this dataset does not require TFRecord in reality. This is only used for learning purposes. </p>&#13;
			<p>We are going to start with a TFRecord dataset already prepared. It contains the same flower images and classes as seen in the previous section. In addition, this TFRecord dataset is partitioned into training, validation, and test datasets. This TFRecord dataset is available in this book's GitHub repository. You may clone this repository with the following command:</p>&#13;
			<p class="source-code">git clone https://github.com/PacktPublishing/learn-tensorflow-enterprise.git</p>&#13;
			<p>Once this command is complete, get in the following path:</p>&#13;
			<p class="source-code"> learn-tensorflow-enterprise/tree/master/chapter_07/train_base_model/tf_datasets/flower_photos</p>&#13;
			<p>You will see the following TFRecord datasets:</p>&#13;
			<p class="source-code">image_classification_builder-train.tfrecord-00000-of-00002</p>&#13;
			<p class="source-code">image_classification_builder-train.tfrecord-00001-of-00002</p>&#13;
			<p class="source-code">image_classification_builder-validation.tfrecord-00000-of-00001</p>&#13;
			<p class="source-code">image_classification_builder-test.tfrecord-00000-of-00001</p>&#13;
			<p>Make a note of the<a id="_idIndexMarker460"/> file path where these datasets are stored. </p>&#13;
			<p>We will refer to this path as <strong class="source-inline">&lt;PATH_TO_TFRECORD&gt;</strong>. This could be the path in your local system or any cloud notebook environment where you uploaded and mounted these TFRecord files:</p>&#13;
			<ol>&#13;
				<li value="1">Set up the file path. As you can see, in this TFRecord collection, there are multiple parts (two) of <strong class="source-inline">train.tfrecord</strong>. We will use the wildcard (<strong class="source-inline">*</strong>) symbol to denote multiple filenames that follow the same naming pattern. We may use <strong class="source-inline">glob</strong> to keep track of the pattern, pass it to <strong class="source-inline">list_files</strong> to create a list of files, and then let <strong class="source-inline">TFRecordDataset</strong> create a dataset object.</li>&#13;
				<li>Recognize and encode the filename convention. We want to have a pipeline that can handle the data ingestion process. Therefore, we have to create variables to hold the file path and naming convention:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">root_dir = '&lt;PATH_TO_TFRECORD&gt;'</p><p class="source-code">train_file_pattern = "{}/image_classification_builder-train*.tfrecord*".format(root_dir)</p><p class="source-code">val_file_pattern = "{}/image_classification_builder-validation*.tfrecord*".format(root_dir)</p><p class="source-code">test_file_pattern = "{}/image_classification_builder-test*.tfrecord*".format(root_dir)</p><p>Here, we <a id="_idIndexMarker461"/>encoded text string representations of the file path to training, validation, and test data in the <strong class="source-inline">train_file_pattern</strong>, <strong class="source-inline">val_file_pattern</strong>, and <strong class="source-inline">test_file_pattern</strong> variables. Notice that we used the wildcard operator <strong class="source-inline">*</strong> to handle multiple file parts, if any. This is an important way to achieve scalability in data ingestion pipelines. It doesn't matter how many files there are, because now you have a way to find all of them by means of the path pattern.</p></li>&#13;
				<li>Create a file list. To create an object that can handle multiple parts of TFRecord files, we will use <strong class="source-inline">list_files</strong> to keep track of these files:<p class="source-code">train_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(train_file_pattern))</p><p class="source-code">val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))</p><p class="source-code">test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_file_pattern))</p><p>In the preceding code, we use the <strong class="source-inline">tf.io</strong> API to make a reference to the training, validation, and test files. The path to these files is defined by <strong class="source-inline">train_file_pattern</strong>, <strong class="source-inline">val_file_pattern</strong>, and <strong class="source-inline">test_file_pattern</strong>.</p></li>&#13;
				<li>Create a dataset object. We will use <strong class="source-inline">TFRecordDataset</strong> to create dataset objects from training, validation, and test list objects:<p class="source-code">train_all_ds = tf.data.TFRecordDataset(train_all_files, num_parallel_reads = AUTOTUNE)</p><p class="source-code">val_all_ds = tf.data.TFRecordDataset(val_all_files, num_parallel_reads = AUTOTUNE)</p><p class="source-code">test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads = AUTOTUNE)</p><p>The <strong class="source-inline">TFRecordDataset</strong> API reads the <strong class="source-inline">TFRecord</strong> file referenced by the file path variables. </p></li>&#13;
				<li>Inspect the <a id="_idIndexMarker462"/>sample size. So far, there is no quick way to establish the sample size in each TFRecord. The only way to do so is by iterating it:<p class="source-code">print("Sample size for training: {0}".format(sum(1 for _ in tf.data.TFRecordDataset(train_all_files)))</p><p class="source-code">     ,'\n', "Sample size for validation: {0}".format(sum(1 for _ in tf.data.TFRecordDataset(val_all_files)))</p><p class="source-code">     ,'\n', "Sample size for test: {0}".format(sum(1 for _ in tf.data.TFRecordDataset(test_all_files)))) </p><p>The preceding code prints and verifies the sample sizes in each of our TFRecord datasets.</p><p>The output should be as follows:</p><p class="source-code">Sample size for training: 3540 </p><p class="source-code">Sample size for validation: 80 </p><p class="source-code">Sample size for test: 50</p><p>Since we are able to count samples in TFRecord datasets, we know that our data pipeline for TFRecord is set up correctly. </p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-137"><a id="_idTextAnchor244"/>TFRecord dataset – feature engineering and training</h2>&#13;
			<p>When we used a<a id="_idIndexMarker463"/> generator as the ingestion pipeline, the<a id="_idIndexMarker464"/> generator took care of batching and matching data and labels during the training process. However, unlike the generator, in order to use the TFRecord dataset, we have to parse it and perform some necessary feature engineering tasks, such as normalization and standardization, ourselves. The creator of TFRecord has to provide a feature description dictionary as a <strong class="bold">template</strong> for parsing the samples. In this case, the following feature dictionary is provided:</p>&#13;
			<p class="source-code">features = {</p>&#13;
			<p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/format' : tf.io.FixedLenFeature([], tf.string),</p>&#13;
			<p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p>&#13;
			<p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p>&#13;
			<p class="source-code">    })</p>&#13;
			<p>We will go through the following steps to parse the dataset, perform feature engineering tasks, and submit the dataset for training. These steps follow the completion of the <em class="italic">TFRecord dataset – ingestion pipeline</em> section:</p>&#13;
			<ol>&#13;
				<li value="1">Parse TFRecord and resize the images. We will use the preceding dictionary to parse TFRecord in order to extract a single image as a NumPy array and its corresponding<a id="_idIndexMarker465"/> label. We will define a <strong class="source-inline">decode_and_resize</strong> function that<a id="_idIndexMarker466"/> should be used: <p class="source-code">def decode_and_resize(serialized_example):</p><p class="source-code">    # resized image should be [224, 224, 3] and 	 	    # normalized to value range [0, 255]</p><p class="source-code">    # label is integer index of class.</p><p class="source-code">    </p><p class="source-code">    parsed_features = tf.io.parse_single_example(</p><p class="source-code">      serialized_example,</p><p class="source-code">      features = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([], 	 	        	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">    })</p><p class="source-code">    image = tf.io.decode_jpeg(parsed_features[</p><p class="source-code">                            'image/encoded'], channels=3)</p><p class="source-code">    label = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/label'], tf.int32)</p><p class="source-code">    label_txt = tf.cast(parsed_features</p><p class="source-code">                         ['image/class/text'], tf.string)</p><p class="source-code">    label_one_hot = tf.one_hot(label, depth = 5)</p><p class="source-code">    resized_image = tf.image.resize(image, [224, 224], </p><p class="source-code">                                         method='nearest')</p><p class="source-code">    return resized_image, label_one_hot</p><p>The <strong class="source-inline">decode_and_resize</strong> function takes a dataset in TFRecord format, parses it, extracts the metadata and actual image, and then returns the image and its label.</p><p>At a more detailed level inside this function, the TFRecord dataset is parsed with <strong class="source-inline">parsed_feature</strong>. This is how we extract different metadata from the dataset. The image is decoded by the <strong class="source-inline">decode_jpeg</strong> API, and is resized to 224 x 224 pixels. As <a id="_idIndexMarker467"/>for the label, it is extracted and one-hot <a id="_idIndexMarker468"/>encoded. Finally, the function returns the resized image and the corresponding one-hot label.</p></li>&#13;
				<li>Normalize the pixel value. We also need to normalize pixel values within the range [<strong class="source-inline">0</strong>, <strong class="source-inline">255</strong>]. Here, we define a <strong class="source-inline">normalize</strong> function to do this:<p class="source-code">def normalize(image, label):</p><p class="source-code">    #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p><p class="source-code">    image = tf.cast(image, tf.float32) / 255.</p><p class="source-code">    return image, label</p><p>Here, the image is rescaled, pixel-wise, to a range of [<strong class="source-inline">0</strong>, <strong class="source-inline">1.0</strong>] by dividing each pixel by <strong class="source-inline">255</strong>. The results are cast to <strong class="source-inline">float32</strong> to represent floating-point values. This function returns the rescaled image with its label. </p></li>&#13;
				<li>Execute these<a id="_idIndexMarker469"/> functions. These functions (<strong class="source-inline">decode_and_resize</strong> and <strong class="source-inline">normalize</strong>) are designed to be applied to each sample within<a id="_idIndexMarker470"/> TFRecord. We use a map to accomplish this:<p class="source-code">resized_train_ds = train_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_val_ds = val_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_test_ds = test_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_normalized_train_ds = resized_train_ds.map(normalize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_normalized_val_ds = resized_val_ds.map(normalize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_normalized_test_ds = resized_test_ds.map(normalize, num_parallel_calls=AUTOTUNE)</p><p>Here, we apply <strong class="source-inline">decode_and_resize</strong> to all the datasets, and then normalize the dataset at a pixel-wise level.</p></li>&#13;
				<li>Batch datasets for training processes. The final step to be performed on the TFRecord dataset is batching. We will define a few variables for this purpose, and define a function, <strong class="source-inline">prepare_for_model</strong>, for batching:<p class="source-code">pixels =224</p><p class="source-code">IMAGE_SIZE = (pixels, pixels)</p><p class="source-code">TRAIN_BATCH_SIZE = 32</p><p class="source-code"># Validation and test data are small. Use all in a batch.</p><p class="source-code">VAL_BATCH_SIZE = sum(1 for _ in tf.data.TFRecordDataset(val_all_files))</p><p class="source-code">TEST_BATCH_SIZE = sum(1 for _ in tf.data.TFRecordDataset(test_all_files))</p><p class="source-code">def prepare_for_model(ds, BATCH_SIZE, cache=True, TRAINING_DATA=True, shuffle_buffer_size=1000):</p><p class="source-code">  if cache:</p><p class="source-code">    if isinstance(cache, str):</p><p class="source-code">      ds = ds.cache(cache)</p><p class="source-code">    else:</p><p class="source-code">      ds = ds.cache()</p><p class="source-code">  ds = ds.shuffle(buffer_size=shuffle_buffer_size)</p><p class="source-code">  if TRAINING_DATA:</p><p class="source-code">    # Repeat forever</p><p class="source-code">    ds = ds.repeat()</p><p class="source-code">  ds = ds.batch(BATCH_SIZE)</p><p class="source-code">  ds = ds.prefetch(buffer_size=AUTOTUNE)</p><p class="source-code">  return ds</p><p>Cross-validation<a id="_idIndexMarker471"/> and test data are not separated into batches. Therefore, the <a id="_idIndexMarker472"/>entire cross-validation data is a single batch, and likewise for test data. </p><p>The <strong class="source-inline">prepare_for_model</strong> function takes a dataset and then caches it in memory and prefetches it. If this function is applied to the training data, it also repeats it infinitely to make sure you don't run out of data during the training process. </p></li>&#13;
				<li>Execute batching. Use the <strong class="source-inline">map</strong> function to apply the <strong class="source-inline">batching</strong> function:<p class="source-code">NUM_EPOCHS = 5</p><p class="source-code">SHUFFLE_BUFFER_SIZE = 1000</p><p class="source-code">prepped_test_ds = prepare_for_model(resized_normalized_test_ds, TEST_BATCH_SIZE, False, False)</p><p class="source-code">prepped_train_ds = resized_normalized_train_ds.repeat(100).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)</p><p class="source-code">prepped_train_ds = prepped_train_ds.batch(TRAIN_BATCH_SIZE)</p><p class="source-code">prepped_train_ds = prepped_train_ds.prefetch(buffer_size = AUTOTUNE)</p><p class="source-code">prepped_val_ds = resized_normalized_val_ds.repeat(NUM_EPOCHS).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)</p><p class="source-code">prepped_val_ds = prepped_val_ds.batch(80)</p><p class="source-code">prepped_val_ds = prepped_val_ds.prefetch(buffer_size = AUTOTUNE)</p><p>The <a id="_idIndexMarker473"/>preceding code sets up batches of training, validation, and test data. These are ready to be fed into the training routine. We <a id="_idIndexMarker474"/>have now completed the data ingestion pipeline. </p></li>&#13;
				<li>Build and train the model. This part does not vary from the previous section. We will build and train a model with the same architecture as seen in the generator:<p class="source-code">FINE_TUNING_CHOICE = False</p><p class="source-code">NUM_CLASSES = 5</p><p class="source-code">IMAGE_SIZE = (224, 224)</p><p class="source-code">mdl = tf.keras.Sequential([</p><p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + </p><p class="source-code">                               (3,), name='input_layer'),</p><p class="source-code">    hub.KerasLayer("https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/4", trainable=FINE_TUNING_CHOICE, name = 'resnet_fv'), </p><p class="source-code">    tf.keras.layers.Dense(NUM_CLASSES, </p><p class="source-code">             activation='softmax', name = 'custom_class')</p><p class="source-code">])</p><p class="source-code">mdl.build([None, 224, 224, 3])</p><p class="source-code">mdl.compile(</p><p class="source-code">  optimizer=tf.keras.optimizers.SGD(lr=0.005, </p><p class="source-code">                                           momentum=0.9), </p><p class="source-code">  loss=tf.keras.losses.CategoricalCrossentropy(</p><p class="source-code">                  from_logits=True, label_smoothing=0.1),</p><p class="source-code">  metrics=['accuracy'])</p><p class="source-code">mdl.fit(</p><p class="source-code">    prepped_train_ds,</p><p class="source-code">    epochs=5, steps_per_epoch=100,</p><p class="source-code">    validation_data=prepped_val_ds,</p><p class="source-code">    validation_steps=1)</p><p>Notice that the<a id="_idIndexMarker475"/> training and validation datasets are <a id="_idIndexMarker476"/>passed into the model as <strong class="source-inline">prepped_train_ds</strong> and <strong class="source-inline">prepped_val_ds</strong>, respectively. In this regard, it is no different to how we passed generators into the model for training. However, the extra work we had to do in terms of parsing, standardizing, and normalizing these datasets is substantially more complex compared to generators. </p></li>&#13;
			</ol>&#13;
			<p>The benefit of TFRecord is that if you have a large dataset, then breaking it up and storing it as TFRecord in multiple parts will help you stream the data into the model faster than using a generator. Also, if your compute target is TPU, then you cannot stream training data using a generator; you will have to use the TFRecord dataset to stream training data into the model for training.</p>&#13;
			<h1 id="_idParaDest-138"><a id="_idTextAnchor245"/>Regularization</h1>&#13;
			<p>During the training<a id="_idIndexMarker477"/> process, the model is learning to find the best set of weights and biases that minimize the <strong class="source-inline">loss</strong> function. As the model architecture becomes more complex, or simply starts to take on more layers, the model is being fitted with more parameters. Although this may help to produce a better fit during training, having to use more parameters may also lead to overfitting. </p>&#13;
			<p>In this section, we will dive into some regularization techniques that can be implemented in a straightforward fashion in the <strong class="source-inline">tf.keras</strong> API. </p>&#13;
			<h2 id="_idParaDest-139"><a id="_idTextAnchor246"/>L1 and L2 regularization</h2>&#13;
			<p>Traditional methods to address the concern of overfitting involve introducing a penalty term in the <strong class="source-inline">loss</strong> function. This is known as regularization. The penalty term is directly related to model complexity, which is largely determined by the number of non-zero weights. To be more specific, there are three traditional types of regularization used in machine learning:</p>&#13;
			<ul>&#13;
				<li><strong class="bold">L1 regularization</strong> (also known as Lasso): Here is<a id="_idIndexMarker478"/> the <strong class="source-inline">loss</strong> function <a id="_idIndexMarker479"/>with L1 regularization:<div id="_idContainer117" class="IMG---Figure"><img src="Images/Formula_08_001.jpg" alt=""/></div></li>&#13;
			</ul>&#13;
			<p>It uses the sum of the absolute values of the weights, <strong class="source-inline">w</strong>, multiplied by a user-defined penalty value, <strong class="source-inline">λ</strong>, to measure complexity (that is, the number of parameters that are fitted to the model indicate how complex it is). The idea is that the more parameters, or weights, that are used, the higher the penalty applied. We want the best model with the fewest parameters. </p>&#13;
			<ul>&#13;
				<li><strong class="bold">L2 regularization</strong> (also known as Ridge): Here<a id="_idIndexMarker480"/> is the <strong class="source-inline">loss</strong> function with<a id="_idIndexMarker481"/> L2 regularization:</li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer118" class="IMG---Figure">&#13;
					<img src="Images/Formula_08_002.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>It uses the sum of the squares of the weights, <strong class="source-inline">w</strong>, multiplied by a user-defined penalty value, <strong class="source-inline">λ</strong>, to measure complexity.</p>&#13;
			<ul>&#13;
				<li><strong class="bold">Elastic net regularization</strong>: Here<a id="_idIndexMarker482"/> is the <strong class="source-inline">loss</strong> function <a id="_idIndexMarker483"/>with L1 and L2 regularization:</li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer119" class="IMG---Figure">&#13;
					<img src="Images/Formula_08_003.jpg" alt=""/>&#13;
				</div>&#13;
			</div>&#13;
			<p>It uses a combination of L1 and L2 to measure complexity. Each regularization term has its own penalty factor.</p>&#13;
			<p>(Reference: <em class="italic">pp. 38-39</em>, <em class="italic">Antonio Gulli and Sujit Pal</em>, <em class="italic">Deep Learning with Keras</em>, <em class="italic">Packt 2017</em>, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/">https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/</a>) </p>&#13;
			<p>These are the keyword input parameters available for model layer definition, including dense or convolutional layers, such as Conv1D, Conv2D, and Conv3D: </p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">kernel_regularizer</strong>: A regularizer applied to the weight matrix</li>&#13;
				<li><strong class="source-inline">bias_regularizer</strong>: A regularizer applied to the bias vector</li>&#13;
				<li><strong class="source-inline">activity_regularizer</strong>: A regularizer applied to the output of the layer</li>&#13;
			</ul>&#13;
			<p>(Reference: <em class="italic">p. 63</em>, <em class="italic">Antonio Gulli and Sujit Pal</em>, <em class="italic">Deep Learning with Keras</em>, <em class="italic">Packt 2017</em>, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer">https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer</a>)</p>&#13;
			<p>Now we will take a look at how to implement some of these parameters. As an example, we will leverage the model architecture built in the previous section, namely, a ResNet feature vector layer followed by a dense layer as the classification head:</p>&#13;
			<p class="source-code">KERNEL_REGULARIZER = tf.keras.regularizers.l2(l=0.1)</p>&#13;
			<p class="source-code">ACTIVITY_REGULARIZER = tf.keras.regularizers.L1L2(l1=0.1,l2=0.1)</p>&#13;
			<p class="source-code">mdl = tf.keras.Sequential([</p>&#13;
			<p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),</p>&#13;
			<p class="source-code">    hub.KerasLayer("https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4",trainable=FINE_TUNING_CHOICE), </p>&#13;
			<p class="source-code">    tf.keras.layers.Dense(NUM_CLASSES </p>&#13;
			<p class="source-code">                         ,activation='softmax'</p>&#13;
			<p class="source-code">                         ,kernel_regularizer=KERNEL_REGULARIZER</p>&#13;
			<p class="source-code">                          ,activity_regularizer = </p>&#13;
			<p class="source-code">                          ACTIVITY_REGULARIZER</p>&#13;
			<p class="source-code">                          ,name = 'custom_class')</p>&#13;
			<p class="source-code">])</p>&#13;
			<p class="source-code">mdl.build([None, 224, 224, 3])</p>&#13;
			<p>Notice that we are using an alias to <a id="_idIndexMarker484"/>define regularizers of interest to us outside the layer. This will make it easy to adjust the hyperparameters (<strong class="source-inline">l1</strong>, <strong class="source-inline">l2</strong>) that determine how strongly we want the regularization term to penalize the <strong class="source-inline">loss</strong> function for potential overfit:</p>&#13;
			<p class="source-code">KERNEL_REGULARIZER = tf.keras.regularizers.l2(l=0.1)</p>&#13;
			<p class="source-code">ACTIVITY_REGULARIZER = tf.keras.regularizers.L1L2(l1=0.1,l2=0.1)</p>&#13;
			<p>This is followed by the addition of these <strong class="source-inline">regularizer</strong> definitions in the dense layer definition:</p>&#13;
			<p class="source-code">tf.keras.layers.Dense(NUM_CLASSES </p>&#13;
			<p class="source-code">                      ,activation='softmax'</p>&#13;
			<p class="source-code">                      ,kernel_regularizer=KERNEL_REGULARIZER</p>&#13;
			<p class="source-code">                      ,activity_regularizer = </p>&#13;
			<p class="source-code">                                           ACTIVITY_REGULARIZER</p>&#13;
			<p class="source-code">                      ,name = 'custom_class')</p>&#13;
			<p>These are the only changes that are required to the code used in the previous section. </p>&#13;
			<h2 id="_idParaDest-140"><a id="_idTextAnchor247"/>Adversarial regularization</h2>&#13;
			<p>An interesting<a id="_idIndexMarker485"/> technique known as adversarial learning emerged in 2014 (if interested, read the seminal paper published by <em class="italic">Goodfellow et al</em>., <em class="italic">2014</em>). This idea stems from the fact that a machine learning model's accuracy can be greatly compromised, and will produce incorrect predictions, if the inputs are slightly noisier than expected. Such noise is known as adversarial perturbation. Therefore, if the training dataset is augmented with some random variation in the data, then we can use this technique to make our model more robust. </p>&#13;
			<p>TensorFlow's <strong class="source-inline">AdversarialRegularization</strong> API is designed to complement the <strong class="source-inline">tf.keras</strong> API and simplify model building and training processes. We are going to reuse the TFRecord dataset downloaded as the original training data. Then we will apply a data augmentation technique to this dataset, and finally we will train the model. To do so follow the given steps:</p>&#13;
			<ol>&#13;
				<li value="1">Download and unzip the training data (if you didn't do so at the start of this chapter). You need to download flower_tfrecords.zip, the TFRecord dataset that we will use from Harvard Dataverse (<a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1ECTVN</a>). Put it in the compute node you intend to use. It may be your local compute environment or a cloud-based environment such as JupyterLab in Google AI Platform, or Google Colab. Unzip the file once you have downloaded it, and make a note of its path. We will refer to this path as <strong class="source-inline">&lt;PATH_TO_TFRECORD&gt;</strong>. In this path, you will see these TFRecord datasets:<p class="source-code">image_classification_builder-train.tfrecord-00000-of-00002</p><p class="source-code">image_classification_builder-train.tfrecord-00001-of-00002</p><p class="source-code">image_classification_builder-validation.tfrecord-00000-of-00001</p><p class="source-code">image_classification_builder-test.tfrecord-00000-of-00001</p></li>&#13;
				<li>Install the library. We need to make sure that the neural structured learning module is available in our environment. If you haven't done so yet, you should install this module using the following <strong class="source-inline">pip</strong> command:<p class="source-code"><strong class="bold">!pip install --quiet neural-structured-learning</strong></p></li>&#13;
				<li>Create a file pattern object for the data pipeline. There are multiple files (two). Therefore, we may leverage the file naming convention and wildcard <strong class="source-inline">*</strong> qualifier during the data<a id="_idIndexMarker486"/> ingestion process:<p class="source-code">import tensorflow as tf</p><p class="source-code">import neural_structured_learning as nsl</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">root_dir = './tfrecord-dataset/flowers'</p><p class="source-code">train_file_pattern = "{}/image_classification_builder-train*.tfrecord*".format(root_dir)</p><p class="source-code">val_file_pattern = "{}/image_classification_builder-validation*.tfrecord*".format(root_dir)</p><p class="source-code">test_file_pattern = "{}/image_classification_builder-test*.tfrecord*".format(root_dir)</p><p>For convenience, the path to these TFRecord files is designated as the following variables: <strong class="source-inline">train_file_pattern</strong>, <strong class="source-inline">val_file_pattern</strong>, and <strong class="source-inline">test_file_pattern</strong>. These paths are represented as text strings. The wildcard symbol <strong class="source-inline">*</strong> is used to handle multiple file parts, in case there are any.</p></li>&#13;
				<li>Take an inventory of all the filenames. We may use the <strong class="source-inline">glob</strong> API to create a dataset object that tracks all parts of the file:<p class="source-code">train_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(train_file_pattern))</p><p class="source-code">val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))</p><p class="source-code">test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_file_pattern))</p><p>Here, we use the <strong class="source-inline">tf.io</strong> API to refer to the file paths indicated in the previous step. The filenames referred to by the <strong class="source-inline">glob</strong> API of <strong class="source-inline">tf.io</strong> are then encoded in a list of filenames by the <strong class="source-inline">list_files</strong> API of <strong class="source-inline">tf.data</strong>.</p></li>&#13;
				<li>Establish the loading pipeline. Now we may establish the reference to our data source via <strong class="source-inline">TFRecordDataset</strong>:<p class="source-code">train_all_ds = tf.data.TFRecordDataset(train_all_files, num_parallel_reads = AUTOTUNE)</p><p class="source-code">val_all_ds = tf.data.TFRecordDataset(val_all_files, num_parallel_reads = AUTOTUNE)</p><p class="source-code">test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads = AUTOTUNE)</p><p>Here, we use<a id="_idIndexMarker487"/> the <strong class="source-inline">TFRecordDataset</strong> API to create respective datasets from our source.</p></li>&#13;
				<li>To check whether we have total visibility of the data, we will count the sample sizes in each dataset:<p class="source-code">train_sample_size = sum(1 for _ in tf.data.TFRecordDataset(train_all_files))</p><p class="source-code">validation_sample_size = sum(1 for _ in tf.data.TFRecordDataset(val_all_files))</p><p class="source-code">test_sample_size = sum(1 for _ in tf.data.TFRecordDataset(test_all_files))</p><p class="source-code">print("Sample size for training: {0}".format(train_sample_size)</p><p class="source-code">     ,'\n', "Sample size for validation: {0}".format(validation_sample_size)</p><p class="source-code">     ,'\n', "Sample size for test: {0}".format(test_sample_size))</p><p>Currently, the way to find out how many samples are in a TFRecord file is by iterating through it. In the code:</p><p class="source-code">sum(1 for _ in tf.data.TFRecordDataset(train_all_files))</p><p>We use the <strong class="source-inline">for</strong> loop to<a id="_idIndexMarker488"/> iterate through the dataset, and sum up the iteration count to obtain the final count as the sample size. This coding pattern is also used to determine validation and test dataset sample sizes. The sizes of these datasets are then stored as variables.</p><p>The output of the preceding code will look like this:</p><p class="source-code">Sample size for training: 3540 </p><p class="source-code">Sample size for validation: 80 </p><p class="source-code">Sample size for test: 50</p></li>&#13;
				<li>As regards data transformation, we need to transform all images to the same size, which is <strong class="source-inline">224</strong> pixels in height and <strong class="source-inline">224</strong> pixels in width. The intensity level of each pixel should be in the range [<strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>]. Therefore, we need to divide each pixel's value by <strong class="source-inline">255</strong>. We need these two functions for these transformation operations:<p class="source-code">def decode_and_resize(serialized_example):</p><p class="source-code">    # resized image should be [224, 224, 3] and 	 	    # normalized to value range [0, 255]</p><p class="source-code">    # label is integer index of class.</p><p class="source-code">   </p><p class="source-code">    parsed_features = tf.io.parse_single_example(</p><p class="source-code">      serialized_example,</p><p class="source-code">      features = {</p><p class="source-code">    'image/channels' :  tf.io.FixedLenFeature([], 	  	                                                tf.int64),</p><p class="source-code">    'image/class/label' :  tf.io.FixedLenFeature([], 	 	                                                tf.int64),</p><p class="source-code">    'image/class/text' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/colorspace' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/encoded' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/filename' : tf.io.FixedLenFeature([], 	 	                                               tf.string),</p><p class="source-code">    'image/format' : tf.io.FixedLenFeature([], 	 	    	                                               tf.string),</p><p class="source-code">    'image/height' : tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image/width' : tf.io.FixedLenFeature([], tf.int64)</p><p class="source-code">    })</p><p class="source-code">    image = tf.io.decode_jpeg(parsed_features[</p><p class="source-code">                            'image/encoded'], channels=3)</p><p class="source-code">    label = tf.cast(parsed_features['image/class/label'], </p><p class="source-code">                                                 tf.int32)</p><p class="source-code">    label_txt = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/text'], tf.string)</p><p class="source-code">    label_one_hot = tf.one_hot(label, depth = 5)</p><p class="source-code">    resized_image = tf.image.resize(image, [224, 224], </p><p class="source-code">                                         method='nearest')</p><p class="source-code">return resized_image, label_one_hot</p><p>The <strong class="source-inline">decode_and_resize</strong> function takes a dataset in TFRecord format, parses it, extracts the metadata and actual image, and returns the image and its label. At a more detailed level, inside this function, the TFRecord dataset is parsed with <strong class="source-inline">parsed_feature</strong>. This is how we extract different metadata from the dataset. The image is decoded by the <strong class="source-inline">decode_jpeg</strong> API, and it is resized to <strong class="source-inline">224</strong> by <strong class="source-inline">224</strong> pixels. As for the label, it is extracted and one-hot encoded. </p></li>&#13;
				<li>Finally, the function returns <a id="_idIndexMarker489"/>the resized image and the corresponding one-hot label.<p class="source-code">def normalize(image, label):</p><p class="source-code">    #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p><p class="source-code">    image = tf.cast(image, tf.float32) / 255.</p><p class="source-code">    return image, label</p><p>This function takes a JPEG image and normalizes pixel values (dividing each pixel by <strong class="source-inline">255</strong>) in the range of [<strong class="source-inline">0</strong>, <strong class="source-inline">1.0</strong>], and casts it to <strong class="source-inline">tf.float32</strong> to represent floating-point values. It returns the normalized image with its corresponding label.</p></li>&#13;
				<li>Execute data transformation. We will use the <strong class="source-inline">map</strong> function to apply the preceding transformation routines to each element in our dataset:<p class="source-code">resized_train_ds = train_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_val_ds = val_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_test_ds = test_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_normalized_train_ds = resized_train_ds.map(normalize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_normalized_val_ds = resized_val_ds.map(normalize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">resized_normalized_test_ds = resized_test_ds.map(normalize, num_parallel_calls=AUTOTUNE)</p><p>In the preceding code, we apply <strong class="source-inline">decode_and_resize</strong> to each dataset, and then we rescale it by applying the <strong class="source-inline">normalize</strong> function to each pixel in the dataset.</p></li>&#13;
				<li>Define the parameters <a id="_idIndexMarker490"/>for training. We need to specify the batch size for our dataset, as well as the parameters that define the epochs:<p class="source-code">pixels =224</p><p class="source-code">IMAGE_SIZE = (pixels, pixels)</p><p class="source-code">TRAIN_BATCH_SIZE = 32</p><p class="source-code">VAL_BATCH_SIZE = validation_sample_size</p><p class="source-code">TEST_BATCH_SIZE = test_sample_size</p><p class="source-code">NUM_EPOCHS = 5</p><p class="source-code">SHUFFLE_BUFFER_SIZE = 1000</p><p class="source-code">FINE_TUNING_CHOICE = False</p><p class="source-code">NUM_CLASSES = 5</p><p class="source-code">prepped_test_ds = resized_normalized_test_ds.batch(TEST_BATCH_SIZE).prefetch(buffer_size = AUTOTUNE)</p><p class="source-code">prepped_train_ds = resized_normalized_train_ds.repeat(100).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)</p><p class="source-code">prepped_train_ds = prepped_train_ds.batch(TRAIN_BATCH_SIZE)</p><p class="source-code">prepped_train_ds = prepped_train_ds.prefetch(buffer_size = AUTOTUNE)</p><p class="source-code">prepped_val_ds = resized_normalized_val_ds.repeat(NUM_EPOCHS).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)</p><p class="source-code">prepped_val_ds = prepped_val_ds.batch(80)</p><p class="source-code">prepped_val_ds = prepped_val_ds.prefetch(buffer_size = AUTOTUNE)</p><p>In the preceding code, we defined the parameters required to set up the training process. Datasets are also batched and fetched for consumption.</p><p>Now we have built our<a id="_idIndexMarker491"/> dataset pipeline that will fetch a batch of data at a time to ingest into the model training process.</p></li>&#13;
				<li>Build your model. We will build an image classification model using the ResNet feature vector:<p class="source-code">mdl = tf.keras.Sequential([</p><p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),</p><p class="source-code">    hub.KerasLayer("https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4",trainable=FINE_TUNING_CHOICE), </p><p class="source-code">    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name = 'custom_class')</p><p class="source-code">])</p><p class="source-code">mdl.build([None, 224, 224, 3])</p><p>We use the <strong class="source-inline">tf.keras</strong> sequential API to build an image classification model. It first uses the input layer to accept the training data as <strong class="source-inline">224</strong> by <strong class="source-inline">224</strong> by <strong class="source-inline">3</strong> pixels. Then we leverage the feature vector of <strong class="source-inline">ResNet_V2_50</strong> as the middle layer. We will use it as is (<strong class="source-inline">trainable</strong> = <strong class="source-inline">FINE_TUNING_CHOICE</strong>. <strong class="source-inline">FINE_TUNING_CHOICE</strong> is set to <strong class="source-inline">False</strong> in the previous step. If you wish, you may set it to <strong class="source-inline">True</strong>. However, this would increase your training time significantly). Finally, the output layer is represented by a dense layer with five nodes (<strong class="source-inline">NUM_CLASSES = 5</strong>). Each node represents a probability value for the respective flower type.</p><p>So far, there is nothing<a id="_idIndexMarker492"/> specific to adversarial regularization. Starting with the next step, we will begin by building a configuration object that specifies adversarial training data and launch the training process.</p></li>&#13;
				<li>Convert the training samples to a dictionary. A particular requirement for adversarial regularization is to have training data and labels combined as a dictionary and then streamed into the training process. This can easily be accomplished with the following function:<p class="source-code">def examples_to_dict(image, label):</p><p class="source-code">  return {'image_input': image, 'label_output': label}</p><p>This function accepts the image and corresponding label, and then reformats these as key-value pairs in a dictionary.</p></li>&#13;
				<li>Convert the data and label collection into a dictionary. For the batched dataset, we may use the <strong class="source-inline">map</strong> function again to apply <strong class="source-inline">examples_to_dict</strong> to each element in the dataset:<p class="source-code">train_set_for_adv_model = prepped_train_ds.map(examples_to_dict)</p><p class="source-code">val_set_for_adv_model = prepped_val_ds.map(examples_to_dict)</p><p class="source-code">test_set_for_adv_model = prepped_test_ds.map(examples_to_dict)</p><p>In this code, each sample in the dataset is also converted to a dictionary. This is done via the <strong class="source-inline">map</strong> function. The <strong class="source-inline">map</strong> function applies the <strong class="source-inline">examples_to_dict</strong> function to each element (sample) in the dataset.</p></li>&#13;
				<li>Create an adversarial regularization object. Now we are ready to create an <strong class="source-inline">adv_config</strong> object that specifies adversarial configuration. Then we wrap the <strong class="source-inline">mdl</strong> base model we created in a previous step with <strong class="source-inline">adv_config</strong>:<p class="source-code">adv_config = nsl.configs.make_adv_reg_config()</p><p class="source-code">adv_mdl = nsl.keras.AdversarialRegularization(mdl,</p><p class="source-code">label_keys=['label_output'],</p><p class="source-code">adv_config=adv_config)</p><p>Now we have a model, <strong class="source-inline">adv_mdl</strong>, that contains the base model structure as defined by <strong class="source-inline">mdl</strong>. <strong class="source-inline">adv_mdl</strong> includes knowledge of the adversarial configuration, <strong class="source-inline">adv_config</strong>, which will<a id="_idIndexMarker493"/> be used to create adversarial images during the training process.</p></li>&#13;
				<li>Compile and train the model. This part is similar to what we did previously. It is no different to training the base model, except for the input dataset:<p class="source-code">adv_mdl.compile(optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), </p><p class="source-code">    loss=tf.keras.losses.CategoricalCrossentropy(</p><p class="source-code">                  from_logits=True, label_smoothing=0.1),</p><p class="source-code">    metrics=['accuracy'])</p><p class="source-code">adv_mdl.fit(</p><p class="source-code">    train_set_for_adv_model,</p><p class="source-code">    epochs=5, steps_per_epoch=100,</p><p class="source-code">    validation_data=val_set_for_adv_model,</p><p class="source-code">    validation_steps=1)</p><p>Notice now the input to the <strong class="source-inline">fit</strong> function for training is <strong class="source-inline">train_set_for_adv_model</strong> and <strong class="source-inline">val_set_for_adv_model</strong>, which is a dataset that <a id="_idIndexMarker494"/>streams each sample as a dictionary into the training process.</p></li>&#13;
			</ol>&#13;
			<p>It doesn't take a lot of work to set up adversarial regularization with <strong class="source-inline">tf.keras</strong> and adversarial regularization APIs. Basically, an extra step is required to reformat the sample and label into a dictionary. Then, we wrap our model using the <strong class="source-inline">nsl.keras.AdversarialRegularization</strong> API, which encapsulates the model architecture and adversarial regularization object. This makes it very easy to implement this type of regularization.</p>&#13;
			<h1 id="_idParaDest-141"><a id="_idTextAnchor248"/>Summary</h1>&#13;
			<p>This chapter presented some common practices for enhancing and improving your model building and training processes. One of the most common issues in dealing with training data handling is to stream or fetch training data in an efficient and scalable manner. In this chapter, you have seen two methods to help you build such an ingestion pipeline: generators and datasets. Each has its strengths and purposes. Generators manage data transformation and batching quite well, while a dataset API is designed where a TPU is the target.</p>&#13;
			<p>We also learned how to implement various regularization techniques using the traditional L1 and L2 regularization, as well as a modern regularization technique known as adversarial regularization, which is applicable to image classification. Adversarial regularization also manages data transformation and augmentation on your behalf to save you the effort of generating noisy images. These new APIs and capabilities enhance TensorFlow Enterprise's user experience and help save on development time.</p>&#13;
			<p>In the next chapter, we are going to see how to serve a TensorFlow model with TensorFlow Serving.</p>&#13;
		</div>&#13;
	</div></body></html>