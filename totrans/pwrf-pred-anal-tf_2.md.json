["```\n    import numpy as np\n    import pandas as pd\n    from scipy import stats\n    import sklearn\n    from sklearn.model_selection import train_test_split\n    import tensorflow as tf\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    ```", "```\n    df = pd.read_csv('fandango_score_comparison.csv')\n    print(df.head())\n    ```", "```\n    df.rename(columns={'Metacritic_user_nom':'Metacritic_user_norm'}, inplace=True)\n    ```", "```\n     'Fandango_Stars',\n    'RT_user_norm',\n    'RT_norm',\n    'IMDB_norm',\n    'Metacritic_user_norm',\n    'Metacritic_norm'\n    ```", "```\n    rankings_lst = ['Fandango_Stars',\n                    'RT_user_norm',\n                    'RT_norm',\n                    'IMDB_norm',\n                    'Metacritic_user_norm',\n                    'Metacritic_norm']\n    ```", "```\n    def my_heatmap(df):    \n        import seaborn as sns    \n        fig, axes = plt.subplots()\n        sns.heatmap(df, annot=True)\n        plt.show()\n        plt.close()\n    ```", "```\n    my_heatmap(df[rankings_lst].corr(method='pearson'))\n    ```", "```\n    RT_lst = df['RT_norm'] >= 4.\n    my_heatmap(df[RT_lst][rankings_lst].corr(method='pearson'))\n    >>>\n    ```", "```\n    feature_cols = ['Fandango_Stars', 'RT_user_norm', 'RT_norm', 'Metacritic_user_norm', 'Metacritic_norm']\n    X = df.loc[:, feature_cols]\n    ```", "```\n    y = df['IMDB_norm']\n    ```", "```\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=43)\n    ```", "```\n    dim = len(feature_cols)\n    ```", "```\n    dim += 1\n\n    ```", "```\n    X_train = X_train.assign( independent = pd.Series([1] * len(y_train), index=X_train.index))\n    X_test = X_test.assign( independent = pd.Series([1] * len(y_train), index=X_test.index))\n    ```", "```\n    P_train = X_train.as_matrix(columns=None)\n    P_test = X_test.as_matrix(columns=None)\n\n    q_train = np.array(y_train.values).reshape(-1,1)\n    q_test = np.array(y_test.values).reshape(-1,1)\n    ```", "```\n    P = tf.placeholder(tf.float32,[None,dim])\n    q = tf.placeholder(tf.float32,[None,1])\n    T = tf.Variable(tf.ones([dim,1]))\n    ```", "```\n    bias = tf.Variable(tf.constant(1.0, shape = [n_dim]))\n    q_ = tf.add(tf.matmul(P, T),bias)\n    ```", "```\n    cost = tf.reduce_mean(tf.square(q_ - q))\n    learning_rate = 0.0001\n    training_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n    ```", "```\n    init_op = tf.global_variables_initializer()\n    cost_history = np.empty(shape=[1],dtype=float)\n    ```", "```\n    training_epochs = 50000\n    with tf.Session() as sess:\n        sess.run(init_op)\n        cost_history = np.empty(shape=[1], dtype=float)\n        t_history = np.empty(shape=[dim, 1], dtype=float)\n        for epoch in range(training_epochs):\n            sess.run(training_op, feed_dict={P: P_train, q: q_train})\n            cost_history = np.append(cost_history, sess.run(cost, feed_dict={P: P_train, q: q_train}))\n            t_history = np.append(t_history, sess.run(T, feed_dict={P: P_train, q: q_train}), axis=1)\n        q_pred = sess.run(q_, feed_dict={P: P_test})[:, 0]\n        mse = tf.reduce_mean(tf.square(q_pred - q_test))\n        mse_temp = mse.eval()\n        sess.close()\n    ```", "```\n    print(mse_temp)\n    RMSE = math.sqrt(mse_temp)\n    print(RMSE)\n    >>> \n    0.425983107542\n    0.6526738140461913\n    ```", "```\n    feature_cols = ['RT_user_norm', 'RT_norm', 'Metacritic_user_norm', 'Metacritic_norm']\n    ```", "```\n    0.426362842426\n    0.6529646563375979\n    ```", "```\n    fig, axes = plt.subplots()\n    plt.plot(range(len(cost_history)), cost_history)\n    axes.set_xlim(xmin=0.95)\n    axes.set_ylim(ymin=1.e-2)\n    axes.set_xscale(\"log\", nonposx='clip')\n    axes.set_yscale(\"log\", nonposy='clip')\n    axes.set_ylabel('Training cost')\n    axes.set_xlabel('Iterations')\n    axes.set_title('Learning rate = ' + str(learning_rate))\n    plt.show()\n    plt.close()\n    >>>\n\n    ```", "```\n    predictedDF = X_test.copy(deep=True)\n    predictedDF.insert(loc=0, column='IMDB_norm_predicted', value=pd.Series(data=q_pred, index=predictedDF.index))\n    predictedDF.insert(loc=0, column='IMDB_norm_actual', value=q_test)\n\n    print('Predicted vs actual rating using LR with TensorFlow')\n    print(predictedDF[['IMDB_norm_actual', 'IMDB_norm_predicted']].head())print(predictedDF[['IMDB_norm_actual', 'IMDB_norm_predicted']].tail())\n    >>>\n    ```", "```\n              IMDB_norm_actual  IMDB_norm_predicted\n    45              3.30              3.232061\n    50              3.35              3.381659\n    98              3.05              2.869175\n    119             3.60              3.796200\n    133             2.15              2.521702\n    140             4.30              4.033006\n    143             3.70              3.816177\n    42              4.10              3.996275\n    90              3.05              3.226954\n    40              3.45              3.509809\n    ```", "```\n    How the LR fit with the predicted data points:\n    plt.scatter(q_test, q_pred, color='blue', alpha=0.5)\n    plt.plot([q_test.min(), q_test.max()], [q_test.min(), q_test.max()], '--', lw=1)\n    plt.title('Predicted vs Actual')\n    plt.xlabel('Actual')\n    plt.ylabel('Predicted')\n    plt.show()\n    plt.show()\n\n    >>>\n    ```", "```\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport shutil\n\n```", "```\ntrain = pd.read_csv(os.path.join('input', 'train.csv'))\ntest = pd.read_csv(os.path.join('input', 'test.csv'))\nprint(\"Information about the data\")\nprint(train.info())\n>>> \nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\n```", "```\nprint(\"How many have survived?\")\nprint(train.Survived.value_counts(normalize=True))\ncount_plot = sns.countplot(train.Survived)\ncount_plot.get_figure().savefig(\"survived_count.png\")\n>>>\n```", "```\n0    0.616162\n1    0.383838\n```", "```\ntrain['Name_Title'] = train['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\nprint('Title count')\nprint(train['Name_Title'].value_counts())\nprint('Survived by title')\nprint(train['Survived'].groupby(train['Name_Title']).mean())\n>>> \t\nTitle      count\nMr.          517\nMiss.        182\nMrs.         125\nMaster.       40\nDr.            7\nRev.           6\nMlle.          2\nCol.           2\nMajor.         2\nSir.           1\nJonkheer.      1\nLady.          1\nCapt.          1\nthe            1\nDon.           1\nMs.            1\nMme.           1\n```", "```\ntrain['Name_Len'] = train['Name'].apply(lambda x: len(x))\nprint('Survived by name length')\nprint(train['Survived'].groupby(pd.qcut(train['Name_Len'],5)).mean())\n>>>\nSurvived by name length \n(11.999, 19.0]    0.220588\n(19.0, 23.0]      0.301282\n(23.0, 27.0]      0.319797\n(27.0, 32.0]      0.442424\n(32.0, 82.0]      0.674556\n```", "```\nprint('Survived by sex')\nprint(train['Survived'].groupby(train['Sex']).mean())\n>>> \nSurvived by sex\nSex\nfemale    0.742038\nmale      0.188908\n```", "```\ntrain['Cabin_Letter'] = train['Cabin'].apply(lambda x: str(x)[0])\nprint('Survived by Cabin_Letter')\nprint(train['Survived'].groupby(train['Cabin_Letter']).mean())\n>>>\nSurvived by Cabin_Letter\nA    0.466667\nB    0.744681\nC    0.593220\nD    0.757576\nE    0.750000\nF    0.615385\nG    0.500000\nT    0.000000\nn    0.299854\n```", "```\nprint('Survived by Embarked')\nprint(train['Survived'].groupby(train['Embarked']).mean())\ncount_plot = sns.countplot(train['Embarked'], hue=train['Pclass'])\ncount_plot.get_figure().savefig(\"survived_count_by_embarked.png\")\n\n>>> \nSurvived by Embarked\nC    0.553571\nQ    0.389610\nS    0.336957\n```", "```\n    for i in [train, test]:\n        i['Name_Len'] = i['Name'].apply(lambda x: len(x))\n        i['Name_Title'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n        del i['Name']\n    return train, test\n```", "```\ndef age_impute(train, test):\n    for i in [train, test]:\n        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n    return train, test\n```", "```\ndef fam_size(train, test):\n    for i in [train, test]:\n        i['Fam_Size'] = np.where((i['SibSp']+i['Parch']) == 0, 'One',\n                                 np.where((i['SibSp']+i['Parch']) <= 3, 'Small', 'Big'))\n        del i['SibSp']\n        del i['Parch']\n    return train, test\nWe are using the Ticket column to create Ticket_Letr, which indicates the first letter of each ticket and Ticket_Len, which indicates the length of the Ticket field:\n```", "```\ndef ticket_grouped(train, test):\n    for i in [train, test]:\n        i['Ticket_Letr'] = i['Ticket'].apply(lambda x: str(x)[0])\n        i['Ticket_Letr'] = i['Ticket_Letr'].apply(lambda x: str(x))\n        i['Ticket_Letr'] = np.where((i['Ticket_Letr']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']),\n                                    i['Ticket_Letr'],\n                                    np.where((i['Ticket_Letr']).isin(['W', '4', '7', '6', 'L', '5', '8']),'Low_ticket', 'Other_ticket'))\n        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))\n        del i['Ticket']\n    return train, test\n```", "```\ndef cabin(train, test):\n    for i in [train, test]:\n        i['Cabin_Letter'] = i['Cabin'].apply(lambda x: str(x)[0])\n        del i['Cabin']\n    return train, test\n```", "```\ndef embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test\n```", "```\ndef dummies(train, test,\n            columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Letr', 'Cabin_Letter', 'Name_Title', 'Fam_Size']):\n    for column in columns:\n        train[column] = train[column].apply(lambda x: str(x))\n        test[column] = test[column].apply(lambda x: str(x))\n        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n        train = pd.concat((train, pd.get_dummies(train[column], prefix=column)[good_cols]), axis=1)\n        test = pd.concat((test, pd.get_dummies(test[column], prefix=column)[good_cols]), axis=1)\n        del train[column]\n        del test[column]\n    return train, test\n```", "```\ndef PrepareTarget(data):\n    return np.array(data.Survived, dtype='int8').reshape(-1, 1)\n```", "```\nnn = tf.contrib.learn.Estimator(model_fn=model_fn, params=model_params)\n```", "```\nnn = tf.contrib.learn.Estimator(model_fn=model_fn, params=model_params)\n```", "```\nmodel_params = {\"learning_rate\": LEARNING_RATE}\n```", "```\n    import os\n    import shutil\n    import random\n    import pandas as pd\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import confusion_matrix\n    from feature import *\n    import tensorflow as tf\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\n    from tensorflow.contrib import learn\n    ```", "```\n    random.seed(12345) # For the reproducibility \n    train = pd.read_csv(os.path.join('input', 'train.csv'))\n    test = pd.read_csv(os.path.join('input', 'test.csv'))\n    ```", "```\n    train, test = create_name_feat(train, test)\n    train, test = age_impute(train, test)\n    train, test = cabin(train, test)\n    train, test = embarked_impute(train, test)\n    train, test = fam_size(train, test)\n    test['Fare'].fillna(train['Fare'].mean(), inplace=True)\n    train, test = ticket_grouped(train, test)\n    ```", "```\n    train, test = dummies(train, test, columns=['Pclass', 'Sex', 'Embarked', 'Ticket_Letr', 'Cabin_Letter', 'Name_Title', 'Fam_Size'])\n    ```", "```\n    TEST = True\n    if TEST:\n        train, test = train_test_split(train, test_size=0.25, random_state=10)\n        train = train.sort_values('PassengerId')\n        test = test.sort_values('PassengerId')\n\n    X_train = train.iloc[:, 1:]\n    x_test = test.iloc[:, 1:]\n    ```", "```\n    x_train = np.array(x_train.iloc[:, 1:], dtype='float32')\n    if TEST:\n     x_test = np.array(x_test.iloc[:, 1:], dtype='float32')\n    else:\n        x_test = np.array(x_test, dtype='float32')\n    ```", "```\n    y_train = PrepareTarget(train)\n    ```", "```\n    feature_count = x_train.shape[1]\n\n    ```", "```\n    def build_lr_estimator(model_dir, feature_count):\n        return estimator.SKCompat(learn.LinearClassifier(\n            feature_columns=[tf.contrib.layers.real_valued_column(\"\", dimension=feature_count)],\n            n_classes=2, model_dir=model_dir))\n    ```", "```\n    print(\"Training...\")\n    try:\n        shutil.rmtree('lr/')\n    except OSError:\n        pass\n    lr = build_lr_estimator('lr/', feature_count)\n    lr.fit(x_train, y_train, steps=1000)\n    lr_pred = lr.predict(x_test)\n    lr_pred = lr_pred['classes']\n    ```", "```\n    if TEST:\n     target_names = ['Not Survived', 'Survived']\n     print(\"Logistic Regression Report\")\n     print(classification_report(test['Survived'], lr_pred, target_names=target_names))\n     print(\"Logistic Regression Confusion Matrix\")\n\n    >>>\n    Logistic Regression Report\n     precision    recall  f1-score   support\n    Not Survived       0.90         0.88      0.89       147\n    Survived           0.78         0.80      0.79        76---------------------------------------------------------\n     avg / total       0.86         0.86       0.86       223\n    ```", "```\n    cm = confusion_matrix(test['Survived'], lr_pred)\n        df_cm = pd.DataFrame(cm, index=[i for i in ['Not Survived', 'Survived']],\n                             columns=[i for i in ['Not Survived', 'Survived']])\n        print(df_cm)\n\n    >>> \n    Logistic Regression Confusion Matrix\n                  Not Survived  Survived\n    Not Survived           130        17\n    Survived               15         61\n    ```", "```\n    print(\"Predicted Counts\")\n    print(sol.Survived.value_counts())\n\n    >>> \n    Predicted Counts\n    0    145\n    1     78\n    ```", "```\n    sol = pd.DataFrame()\n    sol['PassengerId'] = test['PassengerId']\n    sol['Survived'] = pd.Series(lr_pred.reshape(-1)).map({True:1, False:0}).values\n    sns.plt.suptitle(\"Predicted Survived LR\")\n    count_plot = sns.countplot(sol.Survived)\n    count_plot.get_figure().savefig(\"survived_count_lr_prd.png\")\n\n    >>>\n    ```", "```\n    import os\n    import shutil\n    import random\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import confusion_matrix\n    from feature import *\n    import tensorflow as tf\n    from tensorflow.contrib.learn.python.learn.estimators import svm\n    ```", "```\n    train['PassengerId'] = train['PassengerId'].astype(str)\n    test['PassengerId'] = test['PassengerId'].astype(str)\n    ```", "```\n    def train_input_fn():\n        continuous_cols = {k: tf.expand_dims(tf.constant(train[k].values), 1)\n                           for k in list(train) if k not in ['Survived', 'PassengerId']}\n        id_col = {'PassengerId' : tf.constant(train['PassengerId'].values)}\n        feature_cols = continuous_cols.copy()\n        feature_cols.update(id_col)\n        label = tf.constant(train[\"Survived\"].values)\n        return feature_cols, label\n    ```", "```\n    def predict_input_fn():\n        continuous_cols = {k: tf.expand_dims(tf.constant(test[k].values), 1)\n                           for k in list(test) if k not in ['Survived', 'PassengerId']}\n        id_col = {'PassengerId' : tf.constant(test['PassengerId'].values)}\n        feature_cols = continuous_cols.copy()\n        feature_cols.update(id_col)\n        return feature_cols\n    ```", "```\n    svm_model = svm.SVM(example_id_column=\"PassengerId\",\n                        feature_columns=[tf.contrib.layers.real_valued_column(k) for k in list(train)\n                                         if k not in ['Survived', 'PassengerId']], \n                        model_dir=\"svm/\")\n    svm_model.fit(input_fn=train_input_fn, steps=10000)\n    svm_pred = list(svm_model.predict_classes(input_fn=predict_input_fn))\n    ```", "```\n    target_names = ['Not Survived', 'Survived']\n    print(\"SVM Report\")\n    print(classification_report(test['Survived'], svm_pred, target_names=target_names))\n    >>>\n    SVM Report\n                           precision    recall  f1-score   support\n    Not Survived       0.94        0.72      0.82       117\n    Survived           0.63        0.92      0.75        62--------------------------------------------------------\n     avg / total       0.84         0.79      0.79       179\n    ```", "```\n    print(\"SVM Confusion Matrix\")\n    cm = confusion_matrix(test['Survived'], svm_pred)\n    df_cm = pd.DataFrame(cm, index=[i for i in ['Not Survived', 'Survived']],\n                            columns=[i for i in ['Not Survived', 'Survived']])\n    print(df_cm)\n    >>> \n    SVM Confusion Matrix\n                  Not Survived  Survived\n    Not Survived            84        33\n    Survived                    5         57\n    ```", "```\n    sol = pd.DataFrame()\n    sol['PassengerId'] = test['PassengerId']\n    sol['Survived'] = pd.Series(svm_pred).values\n    sns.plt.suptitle(\"Titanic Survival prediction using SVM with TensorFlow\")\n    count_plot = sns.countplot(sol.Survived)\n    ```", "```\n    print(\"Predicted Counts\")\n    print(sol.Survived.value_counts())\n\n    >>> \n    Predicted Counts\n    1    90\n    0    89\n    ```", "```\n    import os\n    import shutil\n    import random\n    import pandas as pd\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import confusion_matrix\n    from feature import *\n    import tensorflow as tf\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\n    from tensorflow.contrib.tensor_forest.client import random_forest\n    from tensorflow.contrib.tensor_forest.python import tensor_forest\n    ```", "```\n    def build_rf_estimator(model_dir, feature_count):\n        params = tensor_forest.ForestHParams(\n            num_classes=2,\n            num_features=feature_count,\n            num_trees=1000,\n            max_nodes=1000,\n            min_split_samples=10)\n        graph_builder_class = tensor_forest.RandomForestGraphs\n        return estimator.SKCompat(random_forest.TensorForestEstimator(\n            params, graph_builder_class=graph_builder_class,\n            model_dir=model_dir))\n    ```", "```\n    rf = build_rf_estimator('rf/', feature_count)\n    rf.fit(x_train, y_train, batch_size=100)\n    rf_pred = rf.predict(x_test)\n    rf_pred = rf_pred['classes']\n    ```", "```\n        target_names = ['Not Survived', 'Survived']\n        print(\"RandomForest Report\")\n        print(classification_report(test['Survived'], rf_pred, target_names=target_names))\n\n    >>>\n    RandomForest Report\n                             precision    recall  f1-score   support\n    ------------------------------------------------------\n    Not Survived       0.92         0.85       0.88            117\n    Survived           0.76         0.85       0.80            62\n    ------------------------------------------------------\n    avg / total        0.86         0.85       0.86            179\n    ```", "```\n        print(\"Random Forest Confusion Matrix\")\n        cm = confusion_matrix(test['Survived'], rf_pred)\n        df_cm = pd.DataFrame(cm, index=[i for i in ['Not Survived', 'Survived']],\n                             columns=[i for i in ['Not Survived', 'Survived']])\n        print(df_cm)\n    >>> \n    Random Forest Confusion Matrix\n                           Not Survived  Survived\n    -----------------------------------------------------\n    Not Survived            100             17\n    Survived                 9              53\n    ```", "```\n    sol = pd.DataFrame()\n    sol['PassengerId'] = test['PassengerId']\n    sol['Survived'] = pd.Series(svm_pred).values\n    sns.plt.suptitle(\"Titanic Survival prediction using RF with TensorFlow\")\n    count_plot = sns.countplot(sol.Survived)\n    ```", "```\n    print(\"Predicted Counts\")\n    print(sol.Survived.value_counts())\n    >>>  Predicted Counts\n    -------------------------\n    0   109\n    1    70\n    ```"]