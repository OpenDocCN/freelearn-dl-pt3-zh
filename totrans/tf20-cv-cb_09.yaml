- en: '*Chapter 9*: Localizing Elements in Images with Object Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection is one of the most common yet challenging tasks in computer
    vision. It's a natural evolution of image classification, where our goal is to
    work out what is in an image. On the other hand, object detection is not only
    concerned with the content of an image but also with the location of elements
    of interest in a digital image.
  prefs: []
  type: TYPE_NORMAL
- en: As with many other well-known tasks in computer vision, object detection has
    long been addressed with a wide array of techniques, ranging from naïve solutions
    (such as object matching) to machine learning-based ones (such as Haar Cascades).
    Nonetheless, the most effective detectors nowadays are powered by deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing state-of-the-art object detectors (such as **You Only Look Once**
    (**YOLO**) and **Fast Region-based Convolutional Neural Network** (**Fast R-CNN**)
    from scratch is a very challenging task. However, there are many pre-trained solutions
    we can leverage, not only to make predictions but also to train our own models
    from zero, as we'll discover in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of the recipes we''ll be working on in no time:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an object detector with image pyramids and sliding windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects with YOLOv3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your own object detector with TensorFlow's Object Detection **Application
    Programming Interface** (**API**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects using **TensorFlow Hub** (**TFHub**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the complexity of object detectors, having access to a **Graphics Processing
    Unit** (**GPU**) is a great idea. There are many cloud providers you can use to
    run the recipes in this chapter, my favorite being FloydHub, but you can use whichever
    you like the most! Of course, do keep in mind of the fees if you don't want any
    surprises! In the *Getting ready* sections, you'll find the preparatory steps
    for each recipe. The code for this chapter is available at [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/39wInla](https://bit.ly/39wInla).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an object detector with image pyramids and sliding windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, object detectors have worked following an iterative algorithm
    whereby a window is slid across the image, at different scales, in order to detect
    potential objects at every location and perspective. Although this approach is
    outdated due to its noticeable drawbacks (which we'll talk more about in the *How
    it works…* section), it has the great advantage of being agnostic about the type
    of image classifier we use, meaning we can use it as a framework to turn any classifier
    into an object detector. This is precisely what we'll do in this first recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to install a couple of external libraries, such as `OpenCV`, `Pillow`,
    and `imutils`, which can easily be accomplished with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We'll use a pre-trained model to power our object detector; therefore, we don't
    need any data for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define our `ObjectDetector()` class, starting with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `classifier` is just a trained network we'll use to classify each window,
    while `preprocess_fn` is the function used to process each window prior to passing
    it to the classifier. `confidence` is the minimum probability we'll allow detections
    to have in order to consider them valid. The remaining parameters will be explained
    in the next steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s define a `sliding_window()` method, which extracts portions of
    the input image, with dimensions equal to `self.roi_size`. It''s going to be slid
    across the image, both horizontally and vertically, at a rate of `self.window_step_size`
    pixels at a time (notice the use of `yield` instead of `return`—that''s because
    this is a generator):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the `pyramid()` method, which generates smaller and smaller copies
    of the input image, until a minimum size is met (akin to the levels of a pyramid):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because sliding a window across the same image at different scales is very
    prone to producing many detections related to the same object, we need a way to
    keep duplicates at a minimum. That''s the purpose of our next method, `non_max_suppression()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We start by computing the area of all bounding boxes, and also sort them by
    their probability, in increasing order. Now, we''ll pick the index of the bounding
    box with the highest probability, and add it to our final selection (`pick`) until
    we have `indexes` left to trim down:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compute the overlap between the picked bounding box and the other ones,
    and then get rid of those boxes where the overlap is higher than `self.nms_threshold`,
    which means that they probably refer to the same object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Return the picked bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `detect()` method ties the object detection algorithm together. We start
    by defining a list of `rois`) and their corresponding `locations` (coordinates
    in the original image):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll generate different copies of the input image at several scales
    using the `pyramid()` generator, and at each level, we''ll slide a window (with
    the `sliding_windows()` generator) to extract all possible ROIs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass all ROIs through the classifier at once:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a `dict` to map each label produced by the classifier to all the bounding
    boxes and their probabilities (notice we only keep those bounding boxes with a
    probability of at least `self.confidence`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate an `InceptionResnetV2` network trained on ImageNet to use as our
    classifier and pass it to a new `ObjectDetector`. Notice that we''re also passing
    the `preprocess_function` as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the input image, resize it to a width of 600 pixels maximum (the height
    will be computed accordingly to preserve the aspect ratio), and run it through
    the object detector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Go over all the detections corresponding to each label, and first draw all
    the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, use **Non-Maximum Suppression** (**NMS**) to get rid of duplicates and
    draw the surviving bounding boxes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the result without NMS:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Overlapping detections of the same dog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Overlapping detections of the same dog
  prefs: []
  type: TYPE_NORMAL
- en: 'And here''s the result after applying NMS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – With NMS, we got rid of the redundant detections'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – With NMS, we got rid of the redundant detections
  prefs: []
  type: TYPE_NORMAL
- en: Although we successfully detected the dog in the previous photos, we notice
    that the bounding box doesn't tightly wrap the object as nicely as we might have
    expected. Let's talk about this and other issues regarding old-school object detection
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a reusable class that easily allows us to turn
    any image classifier into an object detector, by leveraging the iterative approach
    of extracting ROIs (sliding windows) at different levels of perspective (image
    pyramid) and passing them to such a classifier to determine where objects are
    in a photo, and what they are. Also, we used NMS to reduce the amount of non-informative,
    duplicate detections that are characteristic of this strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this a great first attempt at creating an object detector, it has
    its flaws:'
  prefs: []
  type: TYPE_NORMAL
- en: It's incredibly slow, which makes it unusable in real-time situations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy of the bounding boxes depends heavily on the parameter selection
    for the image pyramid, the sliding window, and the ROI size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture is not end-to-end trainable, which means that errors in bounding-box
    predictions are not backpropagated through the network in order to produce better,
    more accurate detections in the future, by updating its weights. Instead, we're
    stuck with pre-trained models that limit themselves to infer but not to learn
    because the framework does not allow them to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, don't rule out this approach yet! If you're working with images that
    present very little variation in size and perspective, and your application definitely
    doesn't operate in a real-time context, the strategy implemented in this recipe
    can work wonders for your project!
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about NMS here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c)'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects with YOLOv3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Creating an object detector with image pyramids and sliding windows*
    recipe, we learned how to turn any image classifier into an object detector, by
    embedding it in a traditional framework that relies on image pyramids and sliding
    windows. However, we also learned that this approach isn't ideal because it doesn't
    allow the network to learn from its mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why deep learning has conquered the field of object detection is
    due to its end-to-end approach. The network not only figures out how to classify
    an object, but also discovers how to produce the best bounding box possible to
    locate each element in the image.
  prefs: []
  type: TYPE_NORMAL
- en: On top of this, thanks to this end-to-end strategy, a network can detect a myriad
    objects in a single pass! Of course, this makes such object detectors incredibly
    efficient!
  prefs: []
  type: TYPE_NORMAL
- en: One of the seminal end-to-end object detectors is YOLO, and in this recipe,
    we'll learn how to detect objects with a pre-trained YOLOv3 model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, install `tqdm`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Our implementation is heavily inspired by the amazing `keras-yolo3` repository
    implemented by *Huynh Ngoc Anh (on GitHub as experiencor)*, which you can consult
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we''ll use a pre-trained YOLO model, we need to download the weights.
    They''re available here: [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights).
    For the purposes of this tutorial, we assume they''re inside the `ch9/recipe2/resources`
    folder, in the companion repository, as `yolov3.weights`. These weights are the
    same ones used by the original authors of YOLO. Refer to the *See also* section
    to learn more about YOLO.'
  prefs: []
  type: TYPE_NORMAL
- en: We are good to go!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the relevant dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a `WeightReader()` class that automatically loads the YOLO weights in
    whichever format the original authors used. Notice that this is a very low-level
    solution, but we don''t need to understand it fully in order to leverage it. Let''s
    begin with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a method to read a given number of bytes from the `weights` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `load_weights()` method loads the weights for each of the 106 layers that
    comprise the YOLO architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the weights of the convolutional layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to reset the offset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a `BoundBox()` class that encapsulates the vertices of a bounding box,
    along with the confidence that the enclosed elements are an object (`objness`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a `YOLO()` class that encapsulates both the construction of the network
    and the detection logic. Let''s begin with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of YOLO is a set of encoded bounding boxes defined in the context
    of anchor boxes that were carefully chosen by the authors of YOLO. This is based
    on an analysis of the size of objects in the `COCO` dataset. That's why we store
    the anchors in `self.anchors`, and `COCO`'s labels in `self.labels`. Also, we
    rely on the `self._load_yolo()` method (defined later) to build the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'YOLO is comprised of a series of convolutional blocks and optional skip connections.
    The `_conv_block()` helper method allows us to instantiate such blocks easily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check if we need to add batch normalization, leaky ReLU activations, and skip
    connections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_make_yolov3_architecture()` method, defined as follows, builds the YOLO
    network by stacking a series of convolutional blocks, using the `_conv_block()`
    method defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Because this method is quite large, please refer to the companion repository
    for the full implementation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `_load_yolo()` method creates the architecture, loads the weights, and
    instantiates a trained YOLO model in a format TensorFlow understands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a static method to compute the Sigmoid value of a tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_decode_net_output()` method decodes the candidate bounding boxes and
    class predictions produced by YOLO:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We skip those bounding boxes that don''t confidently describe an object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We extract the coordinates and classes from the network output, and use them
    to create `BoundBox()` instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_correct_yolo_boxes()` method rescales the bounding boxes to the dimensions
    of the original image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll perform NMS in a bit, in order to reduce the number of redundant detections.
    For that matter, we need a way to compute the amount of overlap between two intervals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can calculate the `_interval_overlap()` method defined before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Armed with these methods, we can apply NMS to the bounding boxes in order to
    keep the number of duplicate detections to a minimum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_get_boxes()` method keeps only those boxes with a confidence score higher
    than the `self.class_threshold` method defined in the constructor (0.6 or 60%
    by default):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`_draw_boxes()` plots the most confident detections in an input image, which
    means that each bounding box is accompanied by its class label and its probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The only public method in the `YOLO()` class is `detect()`, which implements
    the end-to-end logic to detect objects in an input image. First, it passes the
    image through the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, it decodes the outputs of the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, it corrects the boxes so that they have proper proportions in relation
    to the input image. It also applies NMS to get rid of redundant detections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, it gets the valid bounding boxes and draws them in the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `YOLO()` class defined, we can instantiate it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final step is to iterate over all test images and run the model on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the first example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3 – YOLO detected the dog, with a very high confidence score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – YOLO detected the dog, with a very high confidence score
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe that YOLO confidently detected my dog as such, with a confidence
    score of 94.5%! Awesome! Let''s look at the second test image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – YOLO detected multiple objects at varying scales in a single
    pass'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – YOLO detected multiple objects at varying scales in a single pass
  prefs: []
  type: TYPE_NORMAL
- en: Even though the result is crowded, a quick glance reveals the network was able
    to identify both cars in the foreground, as well as the people in the background.
    This is an interesting example because it demonstrates the incredible power of
    YOLO as an end-to-end object detector, which in a single pass was capable of classifying
    and localizing many different objects, at varying scales. Impressive, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: Let's head to the *How it works…* section to connect the dots.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we discovered the immense power of end-to-end object detectors—
    particularly, one of the most famous and impressive of all: YOLO.'
  prefs: []
  type: TYPE_NORMAL
- en: Although YOLO was originally implemented in C++, we leveraged the fantastic
    Python adaptation by *Huynh Ngoc Anh* to perform object detection in our own images
    using a pre-trained version (specifically, version 3) of this architecture on
    the seminal `COCO` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have noticed, YOLO and many other end-to-end object detectors are
    very complex networks, but their advantage over traditional approaches such as
    image pyramids and sliding windows is evident. Not only are the results way better,
    but they also come through faster thanks to the ability of YOLO to look once at
    the input image in order to produce all the relevant detections.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you want to train an end-to-end object detector on your own data?
    Are you doomed to rely on out-of-the-box solutions? Do you need to spend hours
    deciphering cryptic papers in order to implement such networks?
  prefs: []
  type: TYPE_NORMAL
- en: Well, that's one option, but there's another one, which we'll explore in the
    next recipe, and it entails the TensorFlow Object Detection API, an experimental
    repository of state-of-the-art architectures that will ease and boost your object
    detection endeavors!
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLO is a milestone when it comes to deep learning and object detection, so
    reading the paper is a pretty smart time investment. You can find it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about YOLO directly from the author''s website, here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in exploring `keras-yolo3`, the tool we based our implementation
    on, refer to this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)'
  prefs: []
  type: TYPE_NORMAL
- en: Training your own object detector with TensorFlow's Object Detection API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's no secret that modern object detectors rank among the most complex and
    challenging architectures to implement and get it right! However, that doesn't
    mean we can't take advantage of the most recent advancements in this domain in
    order to train object detectors on our own datasets. *How?*, you ask. Enter TensorFlow's
    Object Detection API!
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll install this API, prepare a custom dataset for training,
    tweak a couple of configuration files, and use the resulting model to localize
    objects on test images. This recipe is a bit different from the ones you've worked
    on so far, because we'll be switching back and forth between Python and the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Then let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several dependencies we need to install for this recipe to work.
    Let''s begin with the most important one: the TensorFlow Object Detection API.
    First, `cd` to a location of your preference and clone the `tensorflow/models`
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, install the TensorFlow Object Detection API, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'For the purposes of this recipe, we''ll assume it''s installed at the same
    level as the `ch9` folder (https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9).
    Now, we must install `pandas` and `Pillow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset we will use is `Fruit Images for Object Detection`, hosted on Kaggle,
    which you can access here: [https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection).
    Log in or sign up and download the data to a location of your preference as `fruits.zip`
    (the data is available in the `ch9/recipe3` folder in the companion repository
    for this book). Finally, decompress it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Sample images of the three classes in the dataset: apple, orange,
    and banana'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.5 – Sample images of the three classes in the dataset: apple, orange,
    and banana'
  prefs: []
  type: TYPE_NORMAL
- en: The labels in this dataset are in **Pascal VOC** format, where **VOC** stands
    for **Visual Object Classes**. Refer to the *See also…* section to learn more
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we're all set! Let's begin implementing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of these steps, you''ll have trained your own state-of-the-art object
    detector using the TensorFlow Object Detection API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll work with two files in this recipe: the first one is used to prepare
    the data (you can find it as `prepare.py` in the repository), and the second one
    is used to make inferences with the object detector (`inference.py` in the repository).
    Open `prepare.py` and import all the needed packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `encode_class()` function, which maps the text labels to their integer
    counterparts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to split a dataframe of labels (which we''ll create later)
    into groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The TensorFlow Object Detection API works with a data structure known as `tf.train.Example`.
    The next function takes the path to an image and its label (which is the set of
    bounding boxes and the ground-truth classes of all objects contained in it) and
    creates the corresponding `tf.train.Example`. First, load the image and its properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, store the dimensions of the bounding boxes, along with the classes of
    each object contained in the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `tf.train.Features` object that will contain relevant information
    about the image and its objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Return a `tf.train.Example` structure initialized with the features created
    previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to transform an **Extensible Markup Language** (**XML**)
    file—with information about the bounding boxes in an image—to an equivalent one
    in **Comma-Separated Values** (**CSV**) format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate over the `test` and `train` subsets in the `fruits` folder, converting
    the labels from CSV to XML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, use the same labels to produce the `tf.train.Examples` corresponding
    to the current subset of data being processed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After running the `prepare.py` script implemented in *Step 1* through *Step
    10*, you'll have the data in the necessary shape for the TensorFlow Object Detection
    API to train on it. The next step is to download the weights of `EfficientDet`,
    a state-of-the-art architecture we'll fine-tune shortly. Download the weights
    from this `Desktop` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a file to map the classes to integers. Name it `label_map.txt` and place
    it inside `ch9/recipe3/resources`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must change the configuration file for this network to adapt it to
    our dataset. You can either locate it in `models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config`
    (assuming you installed the TensorFlow Object Detection API at the same level
    of the `ch9` folder in the companion repository), or download it directly from
    this URL: [https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config](https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config).
    Whichever option you choose, place a copy inside `ch9/recipe3/resources` and modify
    *line 13* to reflect the number of classes in our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, modify *line 140* to point to the `EfficientDet` weights we downloaded
    in *Step 7*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change `fine_tune_checkpoint_type` from `classification` to `detection` on
    *line 143*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify *line 180* to point to the `label_map.txt` file created in *Step 8*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify *line 182* to point to the `train.record` file created in *Step 11*,
    corresponding to the prepared training data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify *line 193* to point to the `label_map.txt` file created in *Step 12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify *line 197* to point to the `test.record` file created in *Step 11*,
    corresponding to the prepared test data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Time to train the model! First, assuming you''re at the root level of the companion
    repository, `cd` into the `object_detection` folder in the TensorFlow Object Detection
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, train the model with this command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are training the model for `10000` steps. Also, we'll save the results
    in the `training` folder inside `ch9/recipe3`. Finally, we're specifying the location
    of the configuration file with the `--pipeline_config_path` option. This step
    will take several hours.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the network has been fine-tuned, we must export it as a frozen graph in
    order to use it for inference. For that matter, `cd` once again to the `object_detection`
    folder in the TensorFlow Object Detection API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, execute the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `trained_checkpoint_dir` parameter is used to point to the location where
    the trained model is, while `pipeline_config_path` points to the model's configuration
    file. Finally, the frozen inference graph will be saved inside the `ch9/recipe3/resources/inference_graph`
    folder, as stated by the `output_directory` flag.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open a file named `inference.py`, and import all the relevant dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load an image from disk as a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to run the model on a single image. First, convert the image
    into a tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the tensor to the network, extract the number of detections, and keep
    as many values in the resulting dictionary as there are detections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If there are detection masks present, reframe them to image masks and return
    the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a category index from the `label_map.txt` file we created in *Step 12*,
    and also load the model from the frozen inference graph produced in *Step 15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pick three random test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the model over the sample images, and save the resulting detections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see the results in *Figure 9.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.6 – EfficientDet detection results on a random sample of test images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – EfficientDet detection results on a random sample of test images
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 9.6* that our fine-tuned network produced fairly accurate
    and confident detections. Considering we only concerned ourselves with data preparation
    and inference, and that regarding the architecture itself we just adapted a configuration
    file to our needs, the results are pretty impressive!
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we discovered that training an object detector is a hard and
    challenging feat. The good news, however, is that we have the TensorFlow Object
    Detection API at our disposal to train a wide range of vanguardist networks.
  prefs: []
  type: TYPE_NORMAL
- en: Because the TensorFlow Object Detection API is an experimental tool, it uses
    different conventions than regular TensorFlow, and therefore in order to use it,
    we need to perform a little bit of processing work on the input data to put it
    into a shape that the API understands. This is done by converting the labels in
    the `Fruits for Object Detection` dataset (originally in XML format) to CSV and
    then into serialized `tf.train.Example` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Then, to use the trained model, we exported it as an inference graph using the
    `exporter_main_v2.py` script and leveraged some of the visualization tools in
    the API to display the detections on the sample test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about the training? This is arguably the easiest part, entailing three
    major steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a mapping from text labels to integers (*Step 12*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying the configuration file corresponding to the model to fine-tune it
    in all the relevant places (*Step 13*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the `model_main_tf2.py` file to train the network, passing it the proper
    parameters (*Step 14*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This recipe provides you with a template you can tweak and adapt to train virtually
    any modern object detector (supported by the API) on any dataset of your choosing.
    Pretty cool, right?
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about the TensorFlow Object Detection API here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, I encourage you to read this great article to learn more about `EfficientDet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73](https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn a great deal about the **Pascal VOC** format, then you
    must watch this video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=-f6TJpHcAeM](https://www.youtube.com/watch?v=-f6TJpHcAeM)'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects using TFHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TFHub is a cornucopia of state-of-the-art models when it comes to object detection.
    As we'll discover in this recipe, using them to spot elements of interest in our
    images is a fairly straightforward task, especially considering they've been trained
    on the gigantic `COCO` dataset, which make them an excellent choice for out-of-the-box
    object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install `Pillow` and TFHub, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, because some visualization tools we''ll use live in the TensorFlow Object
    Detection API, we must install it. First, `cd` to a location of your preference
    and clone the `tensorflow/models` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, install the TensorFlow Object Detection API, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn how to use TFHub to detect objects in your own
    photos:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the packages we''ll need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load an image into a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to make predictions with a model, and save the results to
    disk. Start by loading the image and passing it through the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the results to NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a visualization of the detections with their boxes, scores, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the result to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `COCO`''s category index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load Faster R-CNN from TFHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Faster R-CNN over all test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a while, the labeled images should be in the `output` folder. The first
    example showcases the power of the network, which detected with 100% confidence
    the two elephants in the photo:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Both elephants were detected, with a perfect score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Both elephants were detected, with a perfect score
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are instances where the model makes some mistakes, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – The network mistakenly detected a person in the tablecloth'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_09_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – The network mistakenly detected a person in the tablecloth
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the network detected a person in the tablecloth, with 42% certainty,
    although it correctly identified my dog as a Pug, with 100% accuracy. This, and
    other false positives, can be prevented by increasing the `min_score_thresh` value
    passed to the `visualize_boxes_and_labels_on_image_array()` method in *Step 5*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's head to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we leveraged the ease of use of the powerful models that live
    in TFHub to perform out-of-the-box object detection with fairly good results.
  prefs: []
  type: TYPE_NORMAL
- en: Why should we consider TFHub a viable option to satisfy our object detection
    needs? Well, the vast majority of the models there are really challenging to implement
    when starting from scratch, let alone training them to achieve decent results.
    On top of this, these complex architectures have been trained on `COCO`, a massive
    corpus of images tailored for object detection and image segmentation tasks. Nevertheless,
    we must keep in mind that we cannot retrain these networks and, therefore, they
    will work best on images containing objects that exist in `COCO`. If we need to
    create our own custom object detectors, the other strategies covered in this chapter
    should suffice.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can access the list of all available object detectors in TFHub here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://tfhub.dev/tensorflow/collections/object_detection/1](https://tfhub.dev/tensorflow/collections/object_detection/1)'
  prefs: []
  type: TYPE_NORMAL
