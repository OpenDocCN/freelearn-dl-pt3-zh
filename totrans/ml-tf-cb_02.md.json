["```\nimport TensorFlow as tf\nimport NumPy as np \n```", "```\nx_vals = np.array([1., 3., 5., 7., 9.])\nx_data = tf.Variable(x_vals, dtype=tf.float32)\nm_const = tf.constant(3.)\noperation = tf.multiply(x_data, m_const)\nfor result in operation:\n    print(result.NumPy()) \n```", "```\n3.0 \n9.0 \n15.0 \n21.0 \n27.0 \n```", "```\nimport TensorFlow as tf\nimport NumPy as np \n```", "```\n    my_array = np.array([[1., 3., 5., 7., 9.], \n                         [-2., 0., 2., 4., 6.], \n                         [-6., -3., 0., 3., 6.]]) \n    x_vals = np.array([my_array, my_array + 1])\n    x_data = tf.Variable(x_vals, dtype=tf.float32) \n    ```", "```\n    m1 = tf.constant([[1.], [0.], [-1.], [2.], [4.]]) \n    m2 = tf.constant([[2.]]) \n    a1 = tf.constant([[10.]]) \n    ```", "```\n    def prod1(a, b):\n        return tf.matmul(a, b)\n    def prod2(a, b):\n        return tf.matmul(a, b) \n    def add1(a, b):\n        return tf.add(a, b) \n    ```", "```\n    result = add1(prod2(prod1(x_data, m1), m2), a1)\n    print(result.NumPy()) \n    [[ 102.] \n     [  66.] \n     [  58.]] \n    [[ 114.] \n     [  78.] \n     [  70.]] \n    ```", "```\nclass Operations():  \n    def __init__(self, a):\n        self.result = a\n    def apply(self, func, b):\n        self.result = func(self.result, b)\n        return self\n\noperation = (Operations(a=x_data)\n             .apply(prod1, b=m1)\n             .apply(prod2, b=m2)\n             .apply(add1, b=a1))\nprint(operation.result.NumPy()) \n```", "```\nv = tf.Variable(initial_value=tf.random.normal(shape=(1, 5)),\n                shape=tf.TensorShape((None, 5)))\nv.assign(tf.random.normal(shape=(10, 5))) \n```", "```\nimport TensorFlow as tf\nimport NumPy as np \n```", "```\nbatch_size = [1]\nx_shape = [4, 4, 1]\nx_data = tf.random.uniform(shape=batch_size + x_shape) \n```", "```\ndef mov_avg_layer(x):\n    my_filter = tf.constant(0.25, shape=[2, 2, 1, 1]) \n    my_strides = [1, 2, 2, 1] \n    layer = tf.nn.conv2d(x, my_filter, my_strides, \n                         padding='SAME', name='Moving_Avg_Window')\n    return layer \n```", "```\n def custom_layer(input_matrix): \n        input_matrix_sqeezed = tf.squeeze(input_matrix) \n        A = tf.constant([[1., 2.], [-1., 3.]]) \n        b = tf.constant(1., shape=[2, 2]) \n        temp1 = tf.matmul(A, input_matrix_sqeezed) \n        temp = tf.add(temp1, b) # Ax + b \n        return tf.sigmoid(temp) \n```", "```\nfirst_layer = mov_avg_layer(x_data) \nsecond_layer = custom_layer(first_layer) \n```", "```\nprint(second_layer)\n\ntf.Tensor(\n[[0.9385519  0.90720266]\n [0.9247799  0.82272065]], shape=(2, 2), dtype=float32) \n```", "```\nimport matplotlib.pyplot as plt \nimport TensorFlow as tf \n```", "```\nx_vals = tf.linspace(-1., 1., 500) \ntarget = tf.constant(0.) \n```", "```\ndef l2(y_true, y_pred):\n    return tf.square(y_true - y_pred) \n```", "```\ndef l1(y_true, y_pred):\n    return tf.abs(y_true - y_pred) \n```", "```\ndef phuber1(y_true, y_pred):\n    delta1 = tf.constant(0.25) \n    return tf.multiply(tf.square(delta1), tf.sqrt(1\\. +  \n                        tf.square((y_true - y_pred)/delta1)) - 1.) \ndef phuber2(y_true, y_pred):\n    delta2 = tf.constant(5.) \n    return tf.multiply(tf.square(delta2), tf.sqrt(1\\. +  \n                        tf.square((y_true - y_pred)/delta2)) - 1.) \n```", "```\nx_vals = tf.linspace(-3., 5., 500) \ntarget = tf.fill([500,], 1.) \n```", "```\ndef hinge(y_true, y_pred):\n    return tf.maximum(0., 1\\. - tf.multiply(y_true, y_pred)) \n```", "```\ndef xentropy(y_true, y_pred):\n    return (- tf.multiply(y_true, tf.math.log(y_pred)) -   \n          tf.multiply((1\\. - y_true), tf.math.log(1\\. - y_pred))) \n```", "```\ndef xentropy_sigmoid(y_true, y_pred):\n    return tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,  \n                                                   logits=y_pred) \n```", "```\ndef xentropy_weighted(y_true, y_pred):\n    weight = tf.constant(0.5) \n    return tf.nn.weighted_cross_entropy_with_logits(labels=y_true,\n                                                    logits=y_pred,  \n                                                pos_weight=weight) \n```", "```\ndef softmax_xentropy(y_true, y_pred):\n    return tf.nn.softmax_cross_entropy_with_logits(labels=y_true,                                                    logits=y_pred)\n\nunscaled_logits = tf.constant([[1., -3., 10.]]) \ntarget_dist = tf.constant([[0.1, 0.02, 0.88]])\nprint(softmax_xentropy(y_true=target_dist,                        y_pred=unscaled_logits))\n[ 1.16012561] \n```", "```\ndef sparse_xentropy(y_true, y_pred):\n    return tf.nn.sparse_softmax_cross_entropy_with_logits(\n                                                    labels=y_true,\n                                                    logits=y_pred) \nunscaled_logits = tf.constant([[1., -3., 10.]]) \nsparse_target_dist = tf.constant([2]) \nprint(sparse_xentropy(y_true=sparse_target_dist,  \n                      y_pred=unscaled_logits))\n[ 0.00012564] \n```", "```\nx_vals = tf.linspace(-1., 1., 500) \ntarget = tf.constant(0.) \nfuncs = [(l2, 'b-', 'L2 Loss'),\n         (l1, 'r--', 'L1 Loss'),\n         (phuber1, 'k-.', 'P-Huber Loss (0.25)'),\n         (phuber2, 'g:', 'P-Huber Loss (5.0)')]\nfor func, line_type, func_name in funcs:\n    plt.plot(x_vals, func(y_true=target, y_pred=x_vals), \n             line_type, label=func_name)\nplt.ylim(-0.2, 0.4) \nplt.legend(loc='lower right', prop={'size': 11}) \nplt.show() \n```", "```\nx_vals = tf.linspace(-3., 5., 500)  \ntarget = tf.fill([500,], 1.)\nfuncs = [(hinge, 'b-', 'Hinge Loss'),\n         (xentropy, 'r--', 'Cross Entropy Loss'),\n         (xentropy_sigmoid, 'k-.', 'Cross Entropy Sigmoid Loss'),\n         (xentropy_weighted, 'g:', 'Weighted Cross Enropy Loss            (x0.5)')]\nfor func, line_type, func_name in funcs:\n    plt.plot(x_vals, func(y_true=target, y_pred=x_vals), \n             line_type, label=func_name)\nplt.ylim(-1.5, 3) \nplt.legend(loc='lower right', prop={'size': 11}) \nplt.show() \n```", "```\nimport NumPy as np \nimport TensorFlow as tf \n```", "```\nnp.random.seed(0)\nx_vals = np.random.normal(1, 0.1, 100).astype(np.float32) \ny_vals = (x_vals * (np.random.normal(1, 0.05, 100) - 0.5)).astype(np.float32)\nplt.scatter(x_vals, y_vals)\nplt.show() \n```", "```\ndef my_output(X, weights, biases):\n    return tf.add(tf.multiply(X, weights), biases) \n```", "```\ndef loss_func(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_pred - y_true)) \n```", "```\nmy_opt = tf.optimizers.SGD(learning_rate=0.02) \n```", "```\ntf.random.set_seed(1)\nnp.random.seed(0)\nweights = tf.Variable(tf.random.normal(shape=[1])) \nbiases = tf.Variable(tf.random.normal(shape=[1])) \nhistory = list() \n```", "```\nfor i in range(100): \n    rand_index = np.random.choice(100) \n    rand_x = [x_vals[rand_index]] \n    rand_y = [y_vals[rand_index]]\n    with tf.GradientTape() as tape:\n        predictions = my_output(rand_x, weights, biases)\n        loss = loss_func(rand_y, predictions)\n    history.append(loss.NumPy())\n    gradients = tape.gradient(loss, [weights, biases])\n    my_opt.apply_gradients(zip(gradients, [weights, biases]))\n    if (i + 1) % 25 == 0: \n        print(f'Step # {i+1} Weights: {weights.NumPy()} Biases: {biases.NumPy()}')\n        print(f'Loss = {loss.NumPy()}') \nStep # 25 Weights: [-0.58009654] Biases: [0.91217995]\nLoss = 0.13842473924160004\nStep # 50 Weights: [-0.5050226] Biases: [0.9813488]\nLoss = 0.006441597361117601\nStep # 75 Weights: [-0.4791306] Biases: [0.9942327]\nLoss = 0.01728087291121483\nStep # 100 Weights: [-0.4777394] Biases: [0.9807473]\nLoss = 0.05371852591633797 \n```", "```\nplt.plot(history)\nplt.xlabel('iterations')\nplt.ylabel('loss')\nplt.show() \n```", "```\nnp.random.seed(0)\nx_vals = np.concatenate((np.random.normal(-3, 1, 50), \n                         np.random.normal(3, 1, 50))\n                    ).astype(np.float32) \ny_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50))).astype(np.float32) \nplt.hist(x_vals[y_vals==1], color='b')\nplt.hist(x_vals[y_vals==0], color='r')\nplt.show() \n```", "```\ndef loss_func(y_true, y_pred):\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, \n                                                logits=y_pred)) \n```", "```\ntf.random.set_seed(1)\nnp.random.seed(0)\nweights = tf.Variable(tf.random.normal(shape=[1])) \nbiases = tf.Variable(tf.random.normal(shape=[1])) \nhistory = list() \n```", "```\nfor i in range(100):    \n    rand_index = np.random.choice(100) \n    rand_x = [x_vals[rand_index]] \n    rand_y = [y_vals[rand_index]]\n    with tf.GradientTape() as tape:\n        predictions = my_output(rand_x, weights, biases)\n        loss = loss_func(rand_y, predictions)\n    history.append(loss.NumPy())\n    gradients = tape.gradient(loss, [weights, biases])\n    my_opt.apply_gradients(zip(gradients, [weights, biases]))\n    if (i + 1) % 25 == 0: \n        print(f'Step {i+1} Weights: {weights.NumPy()} Biases: {biases.NumPy()}')\n        print(f'Loss = {loss.NumPy()}')\nStep # 25 Weights: [-0.01804185] Biases: [0.44081175]\nLoss = 0.5967269539833069\nStep # 50 Weights: [0.49321094] Biases: [0.37732077]\nLoss = 0.3199256658554077\nStep # 75 Weights: [0.7071932] Biases: [0.32154965]\nLoss = 0.03642747551202774\nStep # 100 Weights: [0.8395616] Biases: [0.30409005]\nLoss = 0.028119442984461784 \n```", "```\nplt.plot(history)\nplt.xlabel('iterations')\nplt.ylabel('loss')\nplt.show() \n```", "```\nimport matplotlib as plt \nimport NumPy as np \nimport TensorFlow as tf \n```", "```\nbatch_size = 20 \n```", "```\nnp.random.seed(0)\nx_vals = np.random.normal(1, 0.1, 100).astype(np.float32) \ny_vals = (x_vals * (np.random.normal(1, 0.05, 100) - 0.5)).astype(np.float32)\ndef loss_func(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_pred - y_true))\ntf.random.set_seed(1)\nnp.random.seed(0)\nweights = tf.Variable(tf.random.normal(shape=[1])) \nbiases = tf.Variable(tf.random.normal(shape=[1])) \nhistory_batch = list()\nfor i in range(50):    \n    rand_index = np.random.choice(100, size=batch_size) \n    rand_x = [x_vals[rand_index]] \n    rand_y = [y_vals[rand_index]]\n    with tf.GradientTape() as tape:\n        predictions = my_output(rand_x, weights, biases)\n        loss = loss_func(rand_y, predictions)\n    history_batch.append(loss.NumPy())\n    gradients = tape.gradient(loss, [weights, biases])\n    my_opt.apply_gradients(zip(gradients, [weights, biases]))\n    if (i + 1) % 25 == 0: \n        print(f'Step # {i+1} Weights: {weights.NumPy()} \\\n              Biases: {biases.NumPy()}')\n        print(f'Loss = {loss.NumPy()}') \n```", "```\ntf.random.set_seed(1)\nnp.random.seed(0)\nweights = tf.Variable(tf.random.normal(shape=[1])) \nbiases = tf.Variable(tf.random.normal(shape=[1])) \nhistory_stochastic = list()\nfor i in range(50):    \n    rand_index = np.random.choice(100, size=1) \n    rand_x = [x_vals[rand_index]] \n    rand_y = [y_vals[rand_index]]\n    with tf.GradientTape() as tape:\n        predictions = my_output(rand_x, weights, biases)\n        loss = loss_func(rand_y, predictions)\n    history_stochastic.append(loss.NumPy())\n    gradients = tape.gradient(loss, [weights, biases])\n    my_opt.apply_gradients(zip(gradients, [weights, biases]))\n    if (i + 1) % 25 == 0: \n        print(f'Step # {i+1} Weights: {weights.NumPy()} \\\n              Biases: {biases.NumPy()}')\n        print(f'Loss = {loss.NumPy()}') \n```", "```\nplt.plot(history_stochastic, 'b-', label='Stochastic Loss') \nplt.plot(history_batch, 'r--', label='Batch Loss') \nplt.legend(loc='upper right', prop={'size': 11}) \nplt.show() \n```", "```\nimport matplotlib.pyplot as plt \nimport NumPy as np \nimport TensorFlow as tf \nimport TensorFlow_datasets as tfds \n```", "```\nbatch_size = 20 \n```", "```\niris = tfds.load('iris', split='train[:90%]', W)\niris_test = tfds.load('iris', split='train[90%:]', as_supervised=True)\ndef iris2d(features, label):\n    return features[2:], tf.cast((label == 0), dtype=tf.float32)\ntrain_generator = (iris\n                   .map(iris2d)\n                   .shuffle(buffer_size=100)\n                   .batch(batch_size)\n                  )\ntest_generator = iris_test.map(iris2d).batch(1) \n```", "```\ndef linear_model(X, A, b):\n    my_output = tf.add(tf.matmul(X, A), b) \n    return tf.squeeze(my_output) \n```", "```\ndef xentropy(y_true, y_pred):\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, \n                                                logits=y_pred)) \n```", "```\nmy_opt = tf.optimizers.SGD(learning_rate=0.02) \n```", "```\ntf.random.set_seed(1)\nnp.random.seed(0)\nA = tf.Variable(tf.random.normal(shape=[2, 1])) \nb = tf.Variable(tf.random.normal(shape=[1]))\nhistory = list()\nfor i in range(300):\n    iteration_loss = list()\n    for features, label in train_generator:\n        with tf.GradientTape() as tape:\n            predictions = linear_model(features, A, b)\n            loss = xentropy(label, predictions)\n        iteration_loss.append(loss.NumPy())\n        gradients = tape.gradient(loss, [A, b])\n        my_opt.apply_gradients(zip(gradients, [A, b]))\n    history.append(np.mean(iteration_loss))\n    if (i + 1) % 30 == 0:\n        print(f'Step # {i+1} Weights: {A.NumPy().T} \\\n              Biases: {b.NumPy()}')\n        print(f'Loss = {loss.NumPy()}')\nStep # 30 Weights: [[-1.1206311  1.2985772]] Biases: [1.0116111]\nLoss = 0.4503694772720337\nâ€¦\nStep # 300 Weights: [[-1.5611029   0.11102282]] Biases: [3.6908474]\nLoss = 0.10326375812292099 \n```", "```\nplt.plot(history)\nplt.xlabel('iterations')\nplt.ylabel('loss')\nplt.show() \n```", "```\npredictions = list()\nlabels = list()\nfor features, label in test_generator:\n    predictions.append(linear_model(features, A, b).NumPy())\n    labels.append(label.NumPy()[0])\n\ntest_loss = xentropy(np.array(labels), np.array(predictions)).NumPy()\nprint(f\"test cross-entropy is {test_loss}\")\ntest cross-entropy is 0.10227929800748825 \n```", "```\ncoefficients = np.ravel(A.NumPy())\nintercept = b.NumPy()\n# Plotting batches of examples\nfor j, (features, label) in enumerate(train_generator):\n    setosa_mask = label.NumPy() == 1\n    setosa = features.NumPy()[setosa_mask]\n    non_setosa = features.NumPy()[~setosa_mask]\n    plt.scatter(setosa[:,0], setosa[:,1], c='red', label='setosa')\n    plt.scatter(non_setosa[:,0], non_setosa[:,1], c='blue', label='Non-setosa')\n    if j==0:\n        plt.legend(loc='lower right')\n# Computing and plotting the decision function\na = -coefficients[0] / coefficients[1]\nxx = np.linspace(plt.xlim()[0], plt.xlim()[1], num=10000)\nyy = a * xx - intercept / coefficients[1]\non_the_plot = (yy > plt.ylim()[0]) & (yy < plt.ylim()[1])\nplt.plot(xx[on_the_plot], yy[on_the_plot], 'k--')\nplt.xlabel('Petal Length') \nplt.ylabel('Petal Width') \nplt.show() \n```"]