<html><head></head><body>
  <div id="_idContainer3310">
    <h1 id="_idParaDest-497" class="chapterTitle">Appendix 2 – Assessments</h1>
    <p class="normal">The following are the answers to the questions mentioned at the end of each chapter.</p>
    <h1 id="_idParaDest-498" class="title">Chapter 1 – Fundamentals of Reinforcement Learning</h1>
    <ol>
      <li class="numbered">In supervised and unsupervised learning, the model (agent) learns based on the given training dataset, whereas, in <strong class="keyword">reinforcement learning</strong> (<strong class="keyword">RL</strong>), the agent learns by directly interacting with the environment. Thus RL is essentially an interaction between the agent and its environment.</li>
      <li class="numbered">The environment is the world of the agent. The agent stays within the environment. For instance, in the chess game, the chessboard is the environment since the chess player (agent) learns to play chess within the chessboard (environment). Similarly, in the Super Mario Bros game, the world of Mario is called the environment.</li>
      <li class="numbered">The deterministic policy maps the state to one particular action, whereas the stochastic policy maps the state to the probability distribution over an action space.</li>
      <li class="numbered">The agent interacts with the environment by performing actions, starting from the initial state until they reach the final state. This agent-environment interaction starting from the initial state until the final state is called an episode.</li>
      <li class="numbered">The discount factor helps us in preventing the return reaching up to infinity by deciding how much importance we give to future rewards and immediate rewards. </li>
      <li class="numbered">The value function (value of a state) is the expected return of the trajectory starting from that state whereas the Q function (the Q value of a state-action pair) is the expected return of the trajectory starting from that state and action.</li>
      <li class="numbered">In a deterministic environment, we can be sure that when an agent performs an action <em class="italic">a</em> in state <em class="italic">s</em>, then it always reaches state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/>. In a stochastic environment, we cannot say that by performing some action <em class="italic">a</em> in state <em class="italic">s</em>, the agent always reaches state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/> because there will be some randomness associated with the stochastic environment.</li>
    </ol>
    <h1 id="_idParaDest-499" class="title">Chapter 2 – A Guide to the Gym Toolkit</h1>
    <ol>
      <li class="numbered">The Gym toolkit provides a variety of environments for training the RL agent ranging from classic control tasks to Atari game environments. We can train our RL agent to learn in these simulated environments using various RL algorithms. </li>
      <li class="numbered">We can create a Gym environment using the <code class="Code-In-Text--PACKT-">make</code> function. The <code class="Code-In-Text--PACKT-">make</code> function requires the environment ID as a parameter. </li>
      <li class="numbered">We learned that the action space consists of all the possible actions in the environment. We can obtain the action space by using <code class="Code-In-Text--PACKT-">env.action_space</code>.</li>
      <li class="numbered">We can visualize the Gym environment using the <code class="Code-In-Text--PACKT-">render()</code> function.</li>
      <li class="numbered">Some classic control environments offered by Gym include the cart pole balancing environment, the pendulum, and the mountain car environment. </li>
      <li class="numbered">We can generate an episode by selecting an action in each state using the <code class="Code-In-Text--PACKT-">step()</code> function. </li>
      <li class="numbered">The state space of the Atari environment will be either the game screen's pixel values or the RAM of the Atari machine. </li>
      <li class="numbered">We can record the agent's gameplay using the Monitor wrapper. It takes three parameters—the environment, the directory where we want to save our recordings, and the force option.</li>
    </ol>
    <h1 id="_idParaDest-500" class="title">Chapter 3 – The Bellman Equation and Dynamic Programming</h1>
    <ol>
      <li class="numbered">The Bellman equation states that the value of a state can be obtained as a sum of the immediate reward and the discounted value of the next state. Similar to the Bellman equation of the value function, the Bellman equation of the Q function states that the Q value of a state-action pair can be obtained as a sum of the immediate reward and the discounted Q value of the next state-action pair.</li>
      <li class="numbered">The Bellman expectation equation gives the Bellman value and Q functions whereas the Bellman optimality equation gives the optimal Bellman value and Q functions. </li>
      <li class="numbered">The value function can be derived from the Q function as <img src="../Images/B15558_19_003.png" alt="" style="height: 1.58em;"/>.</li>
      <li class="numbered">The Q function can be derived from the value function as <img src="../Images/B15558_19_004.png" alt="" style="height: 2.69em;"/>.</li>
      <li class="numbered">In the value iteration method, we perform the following steps:<ol>
          <li class="numbered-l2">Compute the optimal value function by taking maximum over Q function, that is, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Extract the optimal policy from the computed optimal value function</li>
        </ol>
      </li>
      <li class="numbered">In the policy iteration method, we perform the following steps:<ol>
          <li class="numbered-l2">Initialize the random policy</li>
          <li class="numbered-l2">Compute the value function using the given policy</li>
          <li class="numbered-l2">Extract a new policy using the value function obtained from <em class="italic">step 2</em></li>
          <li class="numbered-l2">If the extracted policy is the same as the policy used in <em class="italic">step 2</em> then stop, else send the extracted new policy to <em class="italic">step 2</em> and repeat <em class="italic">steps 2</em> to <em class="italic">4</em></li>
        </ol>
      </li>
      <li class="numbered">In the value iteration method, first, we compute the optimal value function by taking the maximum over the Q function iteratively. Once we find the optimal value function then we will use it to extract the optimal policy. In the policy iteration method, we will try to compute the optimal value function using the policy iteratively. Once we have found the optimal value function then the policy that was used to create the optimal value function will be extracted as the optimal policy.</li>
    </ol>
    <h1 id="_idParaDest-501" class="title">Chapter 4 – Monte Carlo Methods</h1>
    <ol>
      <li class="numbered">In the Monte Carlo method, we approximate the value of a state by taking the average return of a state across <em class="italic">N</em> episodes instead of taking the expected return.</li>
      <li class="numbered">To compute the value function using the dynamic programming method, we need to know the model dynamics, and when we don't know the model dynamics, we use model-free methods. The Monte Carlo method is a model-free method meaning that it doesn't require the model dynamics (transition probability) to compute the value function.</li>
      <li class="numbered">In a prediction task, we evaluate the given policy by predicting the value function or Q function, which helps us to understand the expected return an agent would get if it used the given policy. However, in a control task, our goal is to find the optimal policy and are not given any policy as input, so we start by initializing a random policy and try to find the optimal policy iteratively. </li>
      <li class="numbered">In the MC prediction method, the value of a state and value of a state-action pair can be computed by just taking the average return of the state and an average return of state-action pair across several episodes respectively. </li>
      <li class="numbered">In first-visit MC, we compute the return only for the first time the state is visited in the episode. In every-visit MC, we compute the return every time the state is visited in the episode. </li>
      <li class="numbered">When the environment is non-stationary, we don't have to take the return of the state from all the episodes and compute the average. As the environment is non-stationary, we can ignore returns from earlier episodes and use only the returns from the latest episodes for computing the average. Thus, we can compute the value of the state using the incremental mean.</li>
      <li class="numbered">In the on-policy method, we generate episodes using one policy and also improve the same policy iteratively to find the optimal policy, while with the off-policy Monte Carlo control method, we use two different policies for generating the episode (the behavior policy) and for finding the optimal policy (the target policy).</li>
      <li class="numbered">An epsilon-greedy policy is one where we select a random action (exploration) with probability epsilon, and we select the best action (exploitation) with probability 1-epsilon.</li>
    </ol>
    <h1 id="_idParaDest-502" class="title">Chapter 5 – Understanding Temporal Difference Learning</h1>
    <ol>
      <li class="numbered">Unlike the Monte Carlo method, the <strong class="keyword">Temporal Difference</strong> (<strong class="keyword">TD</strong>) learning method makes use of bootstrapping so that we don't have to wait until the end of the episode to compute the value of a state. </li>
      <li class="numbered">The TD learning algorithm takes the benefits of both the dynamic programming and the Monte Carlo methods into account. That is, just like the dynamic programming method, we perform bootstrapping so that we don't have to wait till the end of an episode to compute the state value or Q value and just like the Monte Carlo method, it is a model-free method, and so it does not require the model dynamics of the environment to compute the state value or Q value.</li>
      <li class="numbered">The TD error can be defined as the difference between the target value and predicted value.</li>
      <li class="numbered">The TD learning update rule is given as <img src="../Images/B15558_05_010.png" alt="" style="height: 1.2em;"/>.</li>
      <li class="numbered">In a TD prediction task, given a policy, we estimate the value function using the given policy. So, we can say what the expected return an agent can obtain in each state if it acts according to the given policy. </li>
      <li class="numbered"><strong class="keyword">SARSA</strong> is an on-policy TD control algorithm and it stands for <strong class="keyword">State-Action-Reward-State-Action</strong>. The update rule for computing the Q function using SARSA is given as <img src="../Images/B15558_18_045.png" alt="" style="height: 1.2em;"/>.</li>
      <li class="numbered">SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy policy for selecting an action in the environment and also to compute the Q value of the next state-action pair, whereas Q learning is an off-policy algorithm meaning that we use an epsilon-greedy policy for selecting an action in the environment, but to compute the Q value of the next state-action pair we use a greedy policy.</li>
    </ol>
    <h1 id="_idParaDest-503" class="title">Chapter 6 – Case Study – The MAB Problem</h1>
    <ol>
      <li class="numbered">The <strong class="keyword">Multi-Armed Bandit</strong> (<strong class="keyword">MAB</strong>) problem is one of the classic problems in RL. A MAB is a slot machine where we pull the arm (lever) and get a payout (reward) based on some probability distribution. A single slot machine is called a one-armed bandit, and when there are multiple slot machines, it is called a MAB or <em class="italic">k</em>-armed bandit, where <em class="italic">k</em> denotes the number of slot machines. </li>
      <li class="numbered">With the epsilon-greedy policy, we select the best arm with probability 1-epsilon, and we select the random arm with probability epsilon. </li>
      <li class="numbered">In softmax exploration, the arm will be selected based on the probability. However, in the initial rounds we will not know the correct average reward of each arm, so selecting the arm based on the probability of average reward will be inaccurate in the initial rounds. So to avoid this we introduce a new parameter called <em class="italic">T</em>. <em class="italic">T</em> is called the temperature parameter.</li>
      <li class="numbered">The upper confidence bound is computed as <img src="../Images/B15558_19_009.png" alt="" style="height: 3.51em;"/>.</li>
      <li class="numbered">When the value of <img src="../Images/B15558_19_010.png" alt="" style="height: 0.93em;"/> is higher than <img src="../Images/B15558_19_011.png" alt="" style="height: 1.11em;"/>, then we will have a high probability closer to 1 than 0. </li>
      <li class="numbered">The steps involved in the Thomson sampling method are as follows:<ol>
          <li class="numbered-l2">Initialize the beta distribution with alpha and beta set to equal values for all the <em class="italic">k</em> arms</li>
          <li class="numbered-l2">Sample a value from the beta distribution of all the <em class="italic">k</em> arms</li>
          <li class="numbered-l2">Pull the arm whose sampled value is high</li>
          <li class="numbered-l2">If we win the game then update the alpha value of the distribution as <img src="../Images/B15558_19_012.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">If we lose the game then update the beta value of the distribution as <img src="../Images/B15558_19_013.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Repeat <em class="italic">steps 2</em> to <em class="italic">5</em> for several numbers of rounds</li>
        </ol>
      </li>
      <li class="numbered">With contextual bandits, we take actions based on the state of the environment and the state holds the context. Contextual bandits are widely used for personalizing content according to the user's behavior. They are also used to solve the cold-start problems faced in recommendation systems.</li>
    </ol>
    <h1 id="_idParaDest-504" class="title">Chapter 7 – Deep Learning Foundations</h1>
    <ol>
      <li class="numbered">The activation function is used to introduce non-linearity to neural networks.</li>
      <li class="numbered">The softmax function is basically a generalization of the sigmoid function. It is usually applied to the final layer of the network and while performing multi-class classification tasks. It gives the probabilities of each class being output and thus, the sum of softmax values will always equal 1.</li>
      <li class="numbered">The epoch specifies the number of times the neural network sees our whole training data. So, we can say one epoch is equal to one forward pass and one backward pass for all training samples.</li>
      <li class="numbered">RNNs are widely applied for use cases that involve sequential data, such as time series, text, audio, speech, video, weather, and much more. They have been greatly used in various <strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>) tasks, such as language translation, sentiment analysis, text generation, and so on.</li>
      <li class="numbered">While backpropagating the RNN, we multiply the weights and derivative of the tanh function at every time step. When we multiply smaller numbers at every step while moving backward, our gradient becomes infinitesimally small and leads to a number that the computer can't handle; this is called the vanishing gradient problem.</li>
      <li class="numbered">The pooling layer reduces spatial dimensions by keeping only the important features. The different types of pooling operation include max pooling, average pooling, and sum pooling.</li>
      <li class="numbered">Suppose, we want our GAN to generate handwritten digits. First, we will take a dataset containing a collection of handwritten digits; say, the MNIST dataset. The generator learns the distribution of images in our dataset. It learns the distribution of handwritten digits in our training set. We feed random noise to the generator and it will convert the random noise into a new handwritten digit similar to the one in our training set. The goal of the discriminator is to perform a classification task. Given an image, it classifies it as real or fake; that is, whether the image is from the training set or has been generated by the generator.</li>
    </ol>
    <h1 id="_idParaDest-505" class="title">Chapter 8 – A Primer on TensorFlow</h1>
    <ol>
      <li class="numbered">A TensorFlow session is used to execute computational graphs with operations on the node and tensors to its edges.</li>
      <li class="numbered">Variables are the containers used to store values. Variables will be used as input to several other operations in the computational graph. We can think of placeholders as variables, where we only define the type and dimension, but will not assign the value. Values for the placeholders will be fed at runtime. We feed the data to the computational graphs using placeholders. Placeholders are defined with no values.</li>
      <li class="numbered">TensorBoard is TensorFlow's visualization tool that can be used to visualize the computational graph. It can also be used to plot various quantitative metrics and the results of several intermediate calculations. When we are training a really deep neural network, it would become confusing when we have to debug the model. As we can visualize the computational graph in TensorBoard, we can easily understand, debug, and optimize such complex models. It also supports sharing.</li>
      <li class="numbered">Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping. Unlike the graph mode, where we need to construct a graph every time to perform any operation, eager execution follows the imperative programming paradigm, where any operation can be performed immediately without having to create a graph, just like we do in Python.</li>
      <li class="numbered">Building a model in Keras involves four important steps:<ol>
          <li class="numbered-l2">Defining the model</li>
          <li class="numbered-l2">Compiling the model</li>
          <li class="numbered-l2">Fitting the model</li>
          <li class="numbered-l2">Evaluating the model</li>
        </ol>
      </li>
      <li class="numbered">A functional model provides more flexibility than a sequential model. For instance, in a functional model, we can easily connect any layer to another layer, whereas, in a sequential model, each layer is in a stack of one above another.</li>
    </ol>
    <h1 id="_idParaDest-506" class="title">Chapter 9 – Deep Q Network and Its Variants</h1>
    <ol>
      <li class="numbered">When the environment consists of a large number of states and actions, it will be very expensive to compute the Q value of all possible state-action pairs in an exhaustive fashion. So, we use a deep Q network for approximating the Q function.</li>
      <li class="numbered">We use a buffer called the replay buffer to collect the agent's experience and based on this experience, we train our network. The replay buffer is usually implemented as a queue structure (first in, first out) rather than a list. So, if the buffer is full and the new experience comes in, we remove the old experience and add the new experience into the buffer. </li>
      <li class="numbered">When the target and predicted values depend on the same parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>, it will cause instability in the mean squared error and the network will learn poorly. It also causes a lot of divergence during training. So, we use a target network.</li>
      <li class="numbered">Unlike with DQNs, in double DQNs, we compute the target value using two Q functions. One Q function parameterized by the main network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> selects the action that has the maximum Q value, and the other Q function parameterized by the target network parameter <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/> computes the Q value using the action selected by the main network.</li>
      <li class="numbered">A transition with a high TD error implies that the transition is not correct and so we need to learn more about that transition to minimize the error. A transition with a low TD error implies that the transition is already good. We can always learn more from our mistakes rather than only focusing on what we are already good at, right? Similarly, we can learn more from the transitions with a high TD error than those with a low TD error. Thus, we can assign a higher priority to the transitions with a high TD error and a lower priority to transitions that got a low TD error.</li>
      <li class="numbered">The advantage function can be defined as the difference between the Q function and the value function.</li>
      <li class="numbered">The LSTM layer is in the DQN so that we can retain information about the past states as long as it is required. Retaining information about the past states helps us when we have the problem of <strong class="keyword">Partially Observable Markov Decision Processes</strong> (<strong class="keyword">POMDPs</strong>).</li>
    </ol>
    <h1 id="_idParaDest-507" class="title">Chapter 10 – Policy Gradient Method</h1>
    <ol>
      <li class="numbered">In the value-based method, we extract the optimal policy from the optimal Q function (Q values).</li>
      <li class="numbered">It is difficult to compute optimal policy using the value-based method when our action space is continuous. So, we use the policy-based method. In the policy-based method, we compute the optimal policy without the Q function. </li>
      <li class="numbered">In the policy gradient method, we select actions based on the action probability distribution given by the network and if we win the episode, that is, if we get a high return, then we assign high probabilities to all the actions of the episode, else we assign low probabilities to all the actions of the episode. </li>
      <li class="numbered">The policy gradient is computed as <img src="../Images/B15558_19_017.png" alt="" style="height: 3.33em;"/>.</li>
      <li class="numbered">Reward-to-go is basically the return of the trajectory starting from the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>. It is computed as <img src="../Images/B15558_10_126.png" alt="" style="height: 3.42em;"/>.</li>
      <li class="numbered">The policy gradient with the baseline function is a policy gradient method that uses the baseline function to reduce the variance in the return.</li>
      <li class="numbered">The baseline function <em class="italic">b</em> gives us the expected return from the state the agent is in, then subtracting <em class="italic">b</em> on every step will reduce the variance in the return.</li>
    </ol>
    <h1 id="_idParaDest-508" class="title">Chapter 11 – Actor-Critic Methods – A2C and A3C</h1>
    <ol>
      <li class="numbered">The actor-critic method is one of the most popular algorithms in deep RL. Several modern deep RL algorithms are designed based on the actor-critic method. The actor-critic method lies at the intersection of value-based and policy-based methods. That is, it takes advantage of both value-based and policy-based methods.</li>
      <li class="numbered">In the actor-critic method, the actor computes the optimal policy and the critic evaluates the policy computed by the actor network by estimating the value function. </li>
      <li class="numbered">In the policy gradient method with baseline, first, we generate complete episodes (trajectories), and then we update the parameter of the network, whereas, in the actor-critic method, we update the network parameter at every step of the episode. </li>
      <li class="numbered">In the actor network, we compute the gradient as <img src="../Images/B15558_19_019.png" alt="" style="height: 1.4em;"/><strong class="keyword">.</strong></li>
      <li class="numbered">In <strong class="keyword">advantage actor-critic</strong> (<strong class="keyword">A2C</strong>), we compute the policy gradient with the advantage function and the advantage function is the difference between the Q function and the value function, that is, <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) – <em class="italic">V</em>(<em class="italic">s</em>).</li>
      <li class="numbered">The word asynchronous implies the way A3C works. That is, instead of having a single agent that tries to learn the optimal policy, here, we have multiple agents that interact with the environment. Since we have multiple agents interacting with the environment at the same time, we provide copies of the environment to every agent so that each agent can interact with its own copy of the environment. So, all these multiple agents are called worker agents and we have a separate agent called the global agent. All the worker agents report to the global agent asynchronously and the global agent aggregates the learning.</li>
      <li class="numbered">In A2C, we can have multiple worker agents, each interacting with its own copies of the environment, and all the worker agents perform the synchronous updates, unlike A3C where the worker agents perform asynchronous updates.</li>
    </ol>
    <h1 id="_idParaDest-509" class="title">Chapter 12 – Learning DDPG, TD3, and SAC</h1>
    <ol>
      <li class="numbered">DDPG consists of an actor and critic. The actor is a policy network and uses the policy gradient method for learning the optimal policy. The critic is a DQN and it evaluates the action produced by the actor.</li>
      <li class="numbered">The critic is basically a DQN. The goal of the critic is to evaluate the action produced by the actor network. The critic evaluates the action produced by the actor using the Q value computed by the DQN.</li>
      <li class="numbered">The key features of TD3 includes clipped double Q learning, delayed policy updates, and target policy smoothing.</li>
      <li class="numbered">Instead of using one critic network, we use two main critic networks for computing the Q value and we use two target critic networks for computing the target value. We compute two target Q values using two target critic networks and use the minimum value out of these two while computing the loss. This helps in preventing the overestimation of the target Q value. </li>
      <li class="numbered">The DDPG method produces different target values even for the same action, thus the variance of the target value will be high even for the same action, so we reduce this variance by adding some noise to the target action.</li>
      <li class="numbered">In the SAC method, we use a slightly modified version of the objective function with the entropy term as <img src="../Images/B15558_19_020.png" alt="" style="height: 3.33em;"/> and it is often called <strong class="keyword">maximum entropy RL</strong> or <strong class="keyword">entropy regularized RL</strong>. Adding an entropy term is also often referred to as an entropy bonus.</li>
      <li class="numbered">The role of the critic network is to evaluate the policy produced by the actor. Instead of using only the Q function to evaluate the actor's policy, the critic in SAC uses both the Q function and the value function.</li>
    </ol>
    <h1 id="_idParaDest-510" class="title">Chapter 13 – TRPO, PPO, and ACKTR Methods</h1>
    <ol>
      <li class="numbered">The trust region implies the region where our actual function <em class="italic">f</em>(<em class="italic">x</em>) and approximated function <img src="../Images/B15558_13_038.png" alt="" style="height: 1.29em;"/> are close together. So, we can say that our approximation will be accurate if our approximated function <img src="../Images/B15558_13_038.png" alt="" style="height: 1.29em;"/> is in the trust region.</li>
      <li class="numbered">TRPO is a policy gradient algorithm, and it acts as an improvement to policy gradient with baseline. TRPO tries to make a large policy update while imposing a KL constraint that the old policy and the new policy should not vary from each other too much. TRPO guarantees monotonic policy improvement, guaranteeing that there will always be a policy improvement on every iteration. </li>
      <li class="numbered">Just like gradient descent, conjugate gradient descent also tries to find the minimum of the function; however, the search direction of conjugate gradient descent will be different from gradient descent and conjugate gradient descent attains convergence in <em class="italic">N</em> iterations. </li>
      <li class="numbered">The update rule of TRPO is given as <img src="../Images/B15558_13_240.png" alt="" style="height: 3.51em;"/>.</li>
      <li class="numbered">PPO acts as an improvement to the TRPO algorithm and is simple to implement. Similar to TRPO, PPO ensures that the policy updates are in the trust region. But unlike TRPO, PPO does not use any constraint in the objective function. </li>
      <li class="numbered">In the PPO clipped method, in order to ensure that the policy updates are in the trust region, that is, the new policy is not far away from the old policy, PPO adds a new function called the clipping function, which ensures that the new and old policies are not far away from each other.</li>
      <li class="numbered">K-FAC approximates the Fisher information matrix as a block diagonal matrix where each block contains the derivatives. Then each block is approximated as a Kronecker product of two matrices, which is known as Kronecker factorization.</li>
    </ol>
    <h1 id="_idParaDest-511" class="title">Chapter 14 – Distributional Reinforcement Learning</h1>
    <ol>
      <li class="numbered">In a distributional RL, instead of selecting an action based on the expected return, we select the action based on the distribution of the return, which is often called the value distribution or return distribution.</li>
      <li class="numbered">In categorical DQN, we feed the state and support of the distribution as the input and the network returns the probabilities of the value distribution.</li>
      <li class="numbered">The authors of the categorical DQN suggest that it will be efficient to choose the number of support <em class="italic">N</em> as 51 and so the categorical DQN is also known as the C51 algorithm.</li>
      <li class="numbered">Inverse CDF is also known as the quantile function. Inverse CDF as the name suggests is the inverse of the cumulative distribution function. That is, in CDF, given the support <em class="italic">x</em>, we obtain the cumulative probability <img src="../Images/B15558_12_056.png" alt="" style="height: 0.84em;"/>, whereas in inverse CDF, given cumulative probability <img src="../Images/B15558_12_056.png" alt="" style="height: 0.84em;"/>, we obtain the support <em class="italic">x</em>.</li>
      <li class="numbered">In a categorical DQN, along with the state, we feed the fixed support at equally spaced intervals as an input to the network, and it returns the non-uniform probabilities. However, in a QR-DQN, along with the state, we feed the fixed uniform probabilities as an input to the network and it returns the support at variable locations (unequally spaced support).</li>
      <li class="numbered">The D4PG is similar to DDPG except for the following:<ol>
          <li class="numbered-l2">We use a distributional DQN in the critic network instead of using the regular DQN to estimate the Q values.</li>
          <li class="numbered-l2">We calculate <em class="italic">N</em>-step returns in the target instead of calculating one-step returns.</li>
          <li class="numbered-l2">We use prioritized experience replay and add importance to the gradient updates in the critic network.</li>
          <li class="numbered-l2">Instead of using one actor, we use <em class="italic">L</em> independent actors, each of which acts in parallel, collecting experience and storing the experience in the replay buffer.</li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-512" class="title">Chapter 15 – Imitation Learning and Inverse RL</h1>
    <ol>
      <li class="numbered">One of the simplest and most naive ways to perform imitation learning is by treating an imitation learning task as a supervised learning task. First, we collect a set of expert demonstrations, then we train a classifier to perform the same action performed by the expert in a particular state. We can view this as a big multiclass classification problem and train our agent to perform the action performed by the expert in the respective state.</li>
      <li class="numbered">In DAgger, we aggregate the dataset over a series of iterations and train the classifier on the aggregated dataset.</li>
      <li class="numbered">In DQfD, we fill the replay buffer with expert demonstrations and pre-train the agent. Note that these expert demonstrations are used only for pretraining the agent. Once the agent is pre-trained, the agent will interact with the environment and gather more experience and make use of it for learning. Thus DQfD consists of two phases, which are pre-training and training. </li>
      <li class="numbered">IRL is used when it is hard to design the reward function. In RL, we try to find the optimal policy given the reward function but in IRL, we try to learn the reward function given the expert demonstrations. Once we have derived the reward function from the expert demonstrations using IRL, then we can use the reward function to train our agent to learn the optimal policy using any RL algorithm. </li>
      <li class="numbered">We can represent the state with a feature vector <em class="italic">f</em>. Let's say we have a state <em class="italic">s</em>; then its feature vector can be defined as <em class="italic">f</em><sub class="" style="font-style: italic;">s</sub>.</li>
      <li class="numbered">In GAIL, the role of the generator is to generate a policy by learning the occupancy measure of the expert policy, and the role of the discriminator is to classify whether the generated policy is from the expert policy or the agent policy. So, we train the generator using TRPO. The discriminator is basically a neural network that tells us whether the policy generated by the generator is the expert policy or the agent policy. </li>
    </ol>
    <h1 id="_idParaDest-513" class="title">Chapter 16 – Deep Reinforcement Learning with Stable Baselines</h1>
    <ol>
      <li class="numbered">Stable Baselines is an improved implementation of OpenAI Baselines. Stable Baselines is a high-level library that is easier to use than OpenAI Baselines, and it also includes state-of-the-art deep RL algorithms along with offering several useful features. </li>
      <li class="numbered">We can save the agent as <code class="Code-In-Text--PACKT-">agent.save()</code> and load the trained agent as <code class="Code-In-Text--PACKT-">agent.load()</code>.<code class="Code-In-Text--PACKT-"> </code></li>
      <li class="numbered">We generally train our agent in a single environment per step but with Stable Baselines, we can train our agent in multiple environments per step. This helps our agent to learn quickly. Now, our states, actions, reward, and done will be in the form of a vector since we are training our agent in multiple environments. So, we call this a vectorized environment. </li>
      <li class="numbered">In SubprocVecEnv, we run each environment in a different process, whereas in DummyVecEnv, we run each environment in the same process. </li>
      <li class="numbered">With Stable Baselines, it is easier to view the computational graph of our model in TensorBoard. In order to do that, we just need to pass the directory where we need to store our log files while instantiating the agent.</li>
      <li class="numbered">With Stable Baselines, we can easily record a video of our agent using the <code class="Code-In-Text--PACKT-">VecVideoRecorder</code> module.</li>
    </ol>
    <h1 id="_idParaDest-514" class="title">Chapter 17 – Reinforcement Learning Frontiers</h1>
    <ol>
      <li class="numbered">Meta learning produces a versatile AI model that can learn to perform various tasks without having to train them from scratch. We train our meta-learning model on various related tasks with a few data points, so for a new but related task, it can make use of the learning obtained from the previous tasks and we don't have to train it from scratch. </li>
      <li class="numbered"><strong class="keyword">Model-Agnostic Meta Learning</strong> (<strong class="keyword">MAML</strong>) is one of the most popularly used meta-learning algorithms and it has created a major breakthrough in meta-learning research. The basic idea of MAML is to find a better initial model parameter so that with good initial parameters, the model can learn quickly on new tasks with fewer gradient steps.</li>
      <li class="numbered">In the outer loop of MAML, we update the model parameter as <img src="../Images/B15558_19_026.png" alt="" style="height: 2.78em;"/> and it is known as a meta objective. </li>
      <li class="numbered">The meta training set basically acts as a training set in the outer loop and is used to update the model parameter in the outer loop.</li>
      <li class="numbered">In hierarchical RL, we decompose a large problem into small subproblems in a hierarchy. The different methods used in hierarchical RL include state-space decomposition, state abstraction, and temporal abstraction. </li>
      <li class="numbered">With an imagination augmented agent, before taking any action in an environment, the agent imagines the consequences of taking the action and if they think the action will provide a good reward, they will perform the action.</li>
    </ol>
  </div>
</body></html>