- en: 'Chapter 2:'
  prefs: []
  type: TYPE_NORMAL
- en: Running TensorFlow Enterprise in Google AI Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, the TensorFlow Enterprise distribution is only available through
    Google Cloud AI Platform. This chapter will demonstrate how to launch AI Platform
    for use with TensorFlow Enterprise. In AI Platform, TensorFlow Enterprise can
    interact with Cloud Storage and BigQuery via their respective command-line tools
    as well as simple APIs to load data from the source. In this chapter, we are going
    to take a look at how to launch AI Platform and how easy it is to start using
    the TensorFlow Enterprise distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a notebook environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy parameterized data extraction from BigQuery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a notebook environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow Enterprise is exclusively available in the JupyterLab environment
    hosted by Google Cloud. There are three ways to consume the JupyterLab with this
    TensorFlow distribution: **Google Cloud** **AI Platform Notebook**, **Google Cloud**
    **Deep Learning Virtual Machine Images (DLVM)**, and **Google Cloud** **Deep Learning
    Containers** (**Docker image**) running on your local machine. No matter which
    one you choose, you will see the same interface of a standard JupyterLab environment
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – JupyterLab portal](img/Figure_2.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – JupyterLab portal
  prefs: []
  type: TYPE_NORMAL
- en: So let's take a look at how to get started.
  prefs: []
  type: TYPE_NORMAL
- en: AI Platform Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the easiest and least complicated way to start using TensorFlow Enterprise
    and get it running in Google Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply go to the Google Cloud portal, select **AI Platform** in the left panel,
    then select the **Notebooks** option:![Figure 2.2 – AI Platform starting portal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.2 – AI Platform starting portal
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then click on **NEW INSTANCE**, and you'll be offered choices for TensorFlow
    Enterprise, which is available for **1.15** as well as **2.1** and **2.3**. You
    also have the option to use one **Tesla K4** GPU:![Figure 2.3 – Creating a new
    notebook instance in AI Platform
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.3 – Creating a new notebook instance in AI Platform
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For our examples in this chapter, we don't need to use a GPU. Selecting **Without
    GPUs** will suffice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then click on **CREATE** to accept the default node choice, or **CUSTOMIZE**
    to see all the setup options available:![Figure 2.4 – Customizing a compute instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.4 – Customizing a compute instance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These are the available machine configuration choices when using the notebook
    option in AI Platform:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Available options for the machine instance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_2.5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.5 – Available options for the machine instance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The Notebook instance will be available within a few minutes after clicking
    **CREATE**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Instance going live and ready'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_2.6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.6 – Instance going live and ready
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When the instance is ready, **OPEN JUPYTERLAB** will be activated and you may
    click on it. Clicking on it will lead you to a JupyterLab notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.7 – JupyterLab environment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – JupyterLab environment
  prefs: []
  type: TYPE_NORMAL
- en: We will use Python 3 for all our examples.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Virtual Machine Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you wish to have more options, such as different GPU choices, then DLVM
    is a better choice. You may find these references helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-cli](https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-cli)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-marketplace](https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-marketplace)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Follow these steps to choose DLVM:'
  prefs: []
  type: TYPE_NORMAL
- en: Click **Marketplace** in the left panel:![Figure 2.8 – Google Cloud Platform
    Marketplace
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.8.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.8 – Google Cloud Platform Marketplace
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Search for `Deep Learning VM` in the query box and you will see the following:![Figure
    2.9 – Enabling DLVM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.9.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.9 – Enabling DLVM
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is where you can launch a DLVM deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **LAUNCH**, and you will see many options available, including **Machine
    Type**, **GPU type**, and **Number of GPUs**:![Figure 2.10 – DLVM configuration
    portal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.10 – DLVM configuration portal
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Also, DLVM has many more frameworks besides TensorFlow Enterprise:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.11 – DLVM and options for frameworks'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_2.11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.11 – DLVM and options for frameworks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you choose one of the two TensorFlow Enterprise frameworks, then click
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CREATE**, you will be able to reach JupyterLab as you did previously:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.12 – JupyterLab entry point'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – JupyterLab entry point
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a suggestion. In order to minimize your cost, it is important to stop
    your instances after you are done. The quickest way to see what you have running
    is to choose **Compute Engine** in the left panel, and then select **VM instances**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Compute instances in a subscription'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – Compute instances in a subscription
  prefs: []
  type: TYPE_NORMAL
- en: 'From there you will see all the instances you have created. Stop them when
    you are done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Listing VM instances and managing their use'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 – Listing VM instances and managing their use
  prefs: []
  type: TYPE_NORMAL
- en: It is the user's responsibility to be aware of the instances that are running.
    As a good practice, when you are finished with your work, download or check in
    your notebook to save your work, and delete the instance when not in use.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Container (DLC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a relatively more complicated way of using TensorFlow Enterprise. An
    important reason for using this approach is for cases where data is not stored
    in Google Cloud, and you wish to run TensorFlow Enterprise on-premises or in your
    local machine. Another reason is that for enterprise use, you may want to use
    DLC as a base Docker image to build your own Docker image for a specific use or
    distribution amongst your team. This is the way to run TensorFlow Enterprise outside
    of Google Cloud. Since it is a Docker image, it requires the Docker Engine installed,
    and the daemon running. It would be extremely helpful to have some basic understanding
    of Docker. You will find a full list of currently available DLCs at [https://console.cloud.google.com/gcr/images/deeplearning-platform-release](https://console.cloud.google.com/gcr/images/deeplearning-platform-release).
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is running a TensorFlow Enterprise JupyterLab. But since it is in
    a local machine, the URL to the JupyterLab is in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '`localhost:<LOCAL_PORT>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we can accomplish this (for reference, see [https://cloud.google.com/ai-platform/deep-learning-containers/docs/getting-started-local](https://cloud.google.com/ai-platform/deep-learning-containers/docs/getting-started-local)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming a Docker daemon is running, we will execute the following command
    to run the TensorFlow Enterprise container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s understand the parts of the preceding command with the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Explaining the objects of the command to run the TensorFlow
    Enterprise container'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_2.15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.15 – Explaining the objects of the command to run the TensorFlow Enterprise
    container
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the preceding table, note the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`<LOCAL_PORT>` refers to the port number in the local machine to host this
    Docker image instance. It may be `8080`, or any other available port number that
    you wish to use, should `8080` be in use by another program or process already.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<LOCAL_DIR>` is the path to the top-level directory where the training data
    and assets can be found. For a Windows machine, it may be `C:\Users\XXXX\Documents`.
    For Linux or Mac machine, it may be `/home/XXXX/Documents`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<CONTAINER_REGISTRY>` is where the Docker image can be found on the internet,
    and for the Docker container of our interest, it is in `gcr.io/deeplearning-platform-release/tf2-cpu.2-1`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Put these together in a command and run it from a terminal of a local machine
    (such as Windows Command Prompt):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now you may access the local port through your browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`localhost:8080`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And you will see the JupyterLab running as a Docker container, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Docker image of JupyterLab running in a local or on-premises
    environment](img/Figure_2.16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 2.16 – Docker image of JupyterLab running in a local or on-premises environment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's take a look at the left panel first. The left panel shows all the local
    files and folders that you designated as `<LOCAL_DIR>`. In this case, it is `/temp/chapter2`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `-v` (or `--volume`) option maps the local directory to the `/home` directory
    of your Docker container instance. This is how local contents become accessible
    to your Docker container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can click on the `/home`:![Figure 2.17 – Docker image of JupyterLab reading
    local data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.17 – Docker image of JupyterLab reading local data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also write data to the local directory:![Figure 2.18 – Docker image
    of JupyterLab writing data locally
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_2.18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since you mapped `/home` with a local directory, you will also find the file
    in the local file explorer:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Local data written by JupyterLab running in a Docker image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_2.19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.19 – Local data written by JupyterLab running in a Docker image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you are done, in order to shut down the Docker image, you need to know
    the container ID assigned to this instance by your local Docker daemon. The command
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And it will return an output similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a note of the `CONTAINER ID` value. Then use the following command to
    shut it down:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Suggestions for selecting workspaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All three methods discussed in the previous section lead you to a JupyterLab
    that runs TensorFlow Enterprise. There are some differences and consequences to
    consider for each method:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker image running locally is preferred for local data access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DLC can serve as a base image for creating a new enterprise-specific image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the Docker image running locally, its advantage lies in its direct access
    to the local environment or data sources. We have seen how it can easily read
    and write data on a local node. This obviously cannot be easily achieved with
    the AI Platform environment. Therefore, if the training data and output are to
    stay on-premises or in the local environment, then this is the most sensible choice.
    The downside of this method is the overhead of setting up and managing your own
    Docker environment. Another reason for using the DLC is that big enterprises often
    need to have customizable environments. They may want to create their own Docker
    container on top of the DLC and later ask everyone in the company to use that
    container with Cloud AI Platform Notebook. Notebook supports the custom container
    mode, as long as that container is based on the DLC.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use DLVM if you want to customize compute instance cores, memory, and disk resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to configure the CPU, GPU, memory, or disk resources for the workload,
    then DLVM is the method of choice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the default notebook environment for most general needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With AI Platform, the notebook environment obviously has direct access to cloud
    storage such as bucket containers or BigQuery tables. If it is not essential to
    pick and choose your CPU or GPU configurations, then the AI Platform Notebook
    would definitely suffice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What we have learned so far are the three different environments for users to
    start using Google's AI Platform and consume the TensorFlow Enterprise distribution.
    By and large, these methods all provide a consistent user experience and runtime
    distribution of the TensorFlow Enterprise library. The rationale for choosing
    a method is grounded in your need for data access and compute resource configurations.
    If the training data is on-premises or on your local disks, then the Docker image
    is the preferred method. If compute resources and speed are the primary concerns,
    then DLVM is the preferred choice.
  prefs: []
  type: TYPE_NORMAL
- en: Now, having arrived at AI Platform and its notebook environment, as a starter,
    we are going to take a closer look at a common example of using AI Platform to
    access data in BigQuery and build your own training data.
  prefs: []
  type: TYPE_NORMAL
- en: Easy parameterized data extraction from BigQuery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Very often, your enterprise data warehouse contains the sources for you to build
    your own training data, and simple SQL query commands would meet your requirements
    for row and column selection and feature transformation. So let's take a look
    at a convenient, flexible, and fast way of selecting and manipulating original
    data through SQL queries, where the result of the query is a pandas DataFrame.
    We have already seen how to use the `%%bigquery` interpreter to execute a query
    and return the result as a pandas DataFrame. We now will look at how to pass in
    query parameters so users may explore and select data suitable for model training.
    The following example uses one of the public datasets, `covid19_juh_csse`, and
    its `summary` table.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – A table’s schema as shown using BigQuery'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.20 – A table's schema using BigQuery
  prefs: []
  type: TYPE_NORMAL
- en: 'In the JupyterLab provided by any of the three methods discussed earlier, you
    may execute the following steps to perform parameterized queries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a set of parameters in a JSON-compatible format, that is, a key-value
    pair as in a Python dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the query and assign it to a DataFrame by name. Notice how each key
    in the parameter is referenced in the query with a preceding `@`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the following is the output of the aggregation command, which demonstrates
    the total results by country:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Output of the aggregation command from the notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21 – Output of the aggregation command from the notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'We may confirm the data structure of our data object using Python''s `type`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And it confirms the object being a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22 – Output from type(myderiveddata)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16070_02_021.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.22 – Output from type(myderiveddata)
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame may be serialized as a pickle file for future use. Once converted
    to the pickle format, you may persist it in the cloud storage as demonstrated
    in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Parameterized querying enables quick and easy data selection and manipulation
    for building training data as a pandas DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters are wrapped in a Python dictionary and can be passed into the query
    string during execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The query string can refer to the parameter with the `@` operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the complete code snippet for the quick example we just worked
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in *Figure 2.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23 – Output from BigQuery and compatibility with pandas DataFrame
    format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.23 – Output from BigQuery and compatibility with pandas DataFrame format
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding steps demonstrate, the notebook environment is integrated closely
    with BigQuery. As a result, an inline query with SQL produces a DataFrame that
    is ready for use in Python. This further demonstrates the flexibility of the Google
    Cloud AI Platform Notebook environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you have learned how to launch the JupyterLab environment
    to run TensorFlow Enterprise. TensorFlow Enterprise is available in three different
    forms: AI Platform Notebook, DLVM, and a Docker container. The computing resources
    used by these methods can be found in the Google Cloud Compute Engine panel. These
    compute nodes do not shut down on their own, therefore it is important to stop
    or delete them once you are done using them.'
  prefs: []
  type: TYPE_NORMAL
- en: The BigQuery command tool is seamlessly integrated with the TensorFlow Enterprise
    environment. Parameterized data extraction via the use of a SQL query string enables
    the quick and easy creation of a derived dataset and feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Enterprise works even when your data is not yet in Google Cloud storage.
    By pulling and running the TensorFlow Enterprise Docker container, you can use
    it with on-premises or local data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have seen how to leverage data availability and accessibility for
    TensorFlow Enterprise consumption, in the next chapter, we are going to examine
    some common data transformation, serialization, and storage techniques optimized
    for TensorFlow Enterprise consumption and model training pipelines.
  prefs: []
  type: TYPE_NORMAL
