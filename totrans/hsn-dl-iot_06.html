<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Audio/Speech/Voice Recognition in IoT</h1>
                </header>
            
            <article>
                
<p>Automatic audio/speech/voice recognition is becoming a common, convenient way for people to interact with their devices, including smartphones, wearables, and other smart devices. Machine learning and DL algorithms are useful for audio/speech/voice recognition and decision making. Consequently, they are very promising for IoT applications, which rely on audio/speech/voice recognition for their activity and decisions. This chapter will present DL-based speech/voice data analysis and recognition in IoT applications in general.</p>
<p>The first part of this chapter will briefly describe different IoT applications and their speech/voice recognition-based decision making. In addition, it will briefly discuss two IoT applications and their speech/voice recognition-based implementations in a real-world scenario. In the second part of the chapter, we will present a hands-on speech/voice detection implementation of the applications using DL algorithms. We will cover the following topics:</p>
<ul>
<li>IoT applications and audio/speech recognition</li>
<li>Use case one – voice-controlled smart light</li>
<li>Implementing a <span>voice-controlled smart light</span></li>
<li>Use case two – voice-controlled home access</li>
<li>Implementing <span>voice-controlled home access</span></li>
<li>DL for audio/speech recognition in IoT</li>
<li>DL algorithms for audio/speech recognition in IoT applications</li>
<li>Different deployment options for DL-based audio/speech recognition in IoT</li>
<li> Data collection and preprocessing</li>
<li>Model training</li>
<li>Evaluating models</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech/voice recognition for IoT</h1>
                </header>
            
            <article>
                
<p>Like image recognition, the <span>speech/voice </span>recognition landscape in IoT applications is rapidly changing. In recent years, consumers have become depending on voice command features and this has been fueled by Amazon, Google, Xiomi, and other companies' voice-enabled search and/or devices. This technology is becoming an extremely useful technology for users. Statistics show that around 50% of households (<a href="https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/">https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/</a>) in the United States use voice-activated commands for accessing online content. Thus, IoT, machine learning, and DL-supported <span>speech/voice </span>recognition has revolutionized the focus of businesses and consumer expectations. Many industries—including home automation, healthcare, automobiles, and entertainment—are adopting voice-enabled IoT applications. As shown in the following diagram, these applications use one or more of the following <span>speech/voice</span> recognition services:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-985 image-border" src="assets/8cdd501c-ab06-4b16-b800-936f0e920c28.png" style="width:38.33em;height:17.25em;"/></p>
<ul>
<li><strong>Speech/command Recognition:</strong> Voice-controlled IoT applications are gaining popularity in many application domains, such as smart home/office, smart hospital, and smart cars, because of their convenience. For example, a mobility <span>disabled</span> person may find difficulty in switching on their TV or light. A voice-controlled/commanded TV/light can ease this difficulty by turning on the TV/light simply by listening to a voice. This will offer independent living to many disabled individuals and/or people with special needs. Voice-activated smart microwave ovens can revolutionize cooking. Moreover, a voice enabled smart speaker can assist with and answer many common questions in many public service areas, such as hospitals, airports, and train stations. For example, a smart voice-enabled speaker can answer patients' common questions in hospital, such as when the visiting time <span>is</span> and who the ward doctor is.</li>
<li><strong>Person/Speaker Identification:</strong> <span>Speaker/person recognition is the second important service provided by IoT applications that has received the spotlight in recent years. The key applications that are utilizing DL/machine learning-based speaker recognition services include personalized voice-controlled assistants, smart home appliances, biometric authentication in security services, criminal investigations, and smart cars [1,2]. Voice-controlled home/office access is an example of biometric authentication.</span></li>
<li><strong>Sentiment Analysis/Emotion Detection:</strong> <span>User emotion detection or sentiment analysis can be useful in providing personalized and effective services to the user. IoT applications, such as smart healthcare [3], smart education, and security and safety, can improve their services through DL-based emotion detection or sentiment analysis. For example, in a smart classroom, a teacher can analyze the students' sentiments in real time or quasi real time to offer personalized and/or group-wise teaching. This will improve their learning experience.</span></li>
<li><strong>Language Translation:</strong> T<span>here are</span> 6,500 (<a href="https://www.infoplease.com/askeds/how-many-spoken-languages">https://www.infoplease.com/askeds/how-many-spoken-languages</a>) <span>active spoken languages worldwide, and this is a challenge to</span> <span>effective communication and interoperability. Many public services, such as the immigration office, can use a translator instead of a paid interpreter. Tourists can use smart devices, such as</span> <strong>ILI</strong> (<a href="https://iamili.com/us/">https://iamili.com/us/</a>)<span>, to effectively communicate with others.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case one – voice-controlled smart light</h1>
                </header>
            
            <article>
                
<p>According to the <strong>World Health Organisation</strong> (<strong>WHO</strong>), more than one billion people in the world live with some form of disability. Almost 20% of them are experiencing considerable difficulties in functioning and living independently. In the future, disability will be an even bigger concern because of its increasing prevalence. IoT applications, such as smart home, with the support of machine learning/DL, can offer support to this community and improve their quality of life through independence. One of these applications is a voice-activated smart light/fan control.</p>
<p>An individual facing a disability such as mobility impairment faces various difficulties in living their day-to-day life. One of these difficulties is switching on/off home or office lights/fans/other devices. Voice-activated smart control of home/office lights/fans/other devices is an IoT application. However, voice recognition and the correct detection of a given command is not an easy job. A person's accent, pronunciation, and ambient noises can make the person's voice recognition difficult. An appropriate DL algorithm trained on a significantly large voice dataset can be useful in addressing these issues and can make a working voice-controlled smart light application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing use case one</h1>
                </header>
            
            <article>
                
<p>The following diagram presents the key components needed for the implementation of a voice-activated light (in a room):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-896 image-border" src="assets/98aa3b44-3195-4593-a2c6-56f5c37c2678.png" style="width:44.17em;height:40.17em;"/></p>
<p>As shown in the preceding diagram, the implementation of the use case will need the following components:</p>
<ul>
<li><strong>Sensors and a Computing Platform</strong>: For this use case, we are considering two omnidirectional microphones that are installed on the walls of the room. These microphones are wirelessly connected to a computing platform. In this use case, we are <span>using</span> a Raspberry Pi 3 as the computing platform, and this can work as the smart home’s edge-computing device to control the IoT devices deployed in the home. We need two more devices: a 433 MHz wireless transmitter, connected to the Raspberry Pi, to transmit the processed commands to the switch, and a 433 MHz remote control or wirelessly controlled mains socket to control the light or target device.</li>
<li><strong>Voice-Activated Command Detection and Control</strong>: In this phase, the edge-computing device will be installed with one app. The installed app on the Raspberry Pi will be loaded with a pre-trained voice command detection and classification model. Once one of the microphones receives a “switch off the light” <span>command</span> or similar, it sends the received commands to the Raspberry Pi for processing and detection using the DL model. Finally, the <span>Raspberry Pi</span> transmits detected commands to the wirelessly controlled mains socket for the necessary action to be taken on the light.</li>
<li><strong>Desktop or Server for Model Learning</strong>: We also need a desktop/server or access to a cloud computing platform in order to learn the model for voice detection and classification using reference datasets. This learned model will be preinstalled in the Raspberry Pi.</li>
</ul>
<p>The second part (in the sections starting from <em>DL for Sound/Audio Recognition in IoT</em>) of the chapter will describe the implementation of the DL-based anomaly detection of the preceding use case. All the necessary code is available in the chapter's code folder.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case two – voice-controlled home access</h1>
                </header>
            
            <article>
                
<p>Creating secure and friendly access to homes, offices, and any other premises is a challenging task, as it may need keys or an access card (such as a hotel room access card) that a user may not always remember to carry with them. The use of smart devices, including IoT solutions, can offer secure and friendly access to many premises. A potential approach to smart and secure access to homes/offices is image recognition-based identification of people and the opening of a door/gate accordingly. However, one problem with this approach is that any intruder can collect a photograph of one or more permitted persons and present the photo to the installed camera to access the office/home. One solution to this problem is to use a combination of image recognition and voice recognition or only voice recognition to allow access to the home/office.</p>
<p>A voice biometric (or voiceprint) is unique to every individual, and mimicking this is a challenging task. However, detection of this unique property is not an easy job. DL-based speech recognition can identify unique properties and the corresponding person, and allow access only to that person.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing use case two</h1>
                </header>
            
            <article>
                
<p>As shown in the following diagram, the implementation of the voice-activated light (in a room) use case consists of three main elements:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-897 image-border" src="assets/a239f740-81e4-43d7-ac66-b7eefd4c15b8.png" style="width:49.42em;height:19.67em;"/></p>
<ul>
<li><strong>Sensors and computing platform</strong>: For this use case, we are considering one omnidirectional microphone installed in the entrance of the home and connected to the computing platform wirelessly or concealed in the walls. For the computing platform, we are using a Raspberry Pi , and this will work as the smart home's edge-computing device to control the IoT devices deployed in the home. Also, the door is installed with a digital lock system that can be controlled through a computer.</li>
<li><strong>V</strong><strong>oice-activated command detection and control</strong>: In this phase, the edge-computing device will be installed with one app. The installed app on the Raspberry Pi will be loaded with a pre-trained speaker or person detection and classification model. Once an authentic user talks to the door microphone, it gathers the audio signals and sends the received speech signal to the Raspberry Pi for processing and person detection using the DL model. If the detected person is on the <strong>white list</strong> (the list of occupants of the home) of the smart home controller (Raspberry Pi, in this case), the controller will command the door to be unlocked, otherwise it won't.</li>
<li><strong>Desktop or server for model learning:</strong> We also need a desktop/server or access to a cloud computing platform in order to learn the model for voice detection and classification using reference datasets. This learned model will be preinstalled in the Raspberry Pi.</li>
</ul>
<p>All the following sections describe the implementation of the DL-based command/speaker recognition needed for the aforementioned use cases. All the necessary code is available in the chapter's code folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL for sound/audio recognition in IoT</h1>
                </header>
            
            <article>
                
<p>It is important to understand the working principle of an <strong>Automatic Speech Recognition</strong> (<strong>ASR</strong>) system before discussing the useful DL models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ASR system model</h1>
                </header>
            
            <article>
                
<p>An <strong>Automatic Speech Recognition</strong> (<strong>ASR</strong>) system needs three main sources of knowledge. These sources are known as an <strong>acoustic model</strong>, a <strong>phonetic lexicon</strong>, and a <strong>language model</strong> [4]. Generally, an acoustic model deals with the sounds of language, including the phonemes and extra sounds (such as pauses, breathing, background noise, and so on). On the other hand, a phonetic lexicon model or dictionary includes the words that can be understood by the system, with their possible pronunciations. Finally, a language model includes knowledge about the <span>potential</span> <span>word sequences of a language. In recent years, DL approaches have been extensively used in acoustic and language models of ASR.</span></p>
<p>The following diagram presents a system model for <strong>automatic speech recognition</strong> (<strong>ASR</strong>). The model consists of three main stages:</p>
<ul>
<li>Data gathering</li>
<li>Signal analysis and feature extraction (also known as <strong>preprocessing</strong>)</li>
<li>Decoding/identification/classification. As shown in the following diagram, DL will be used in the identification stage:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-986 image-border" src="assets/eff5f6b3-cac2-47d8-9b2c-897050ff357f.png" style="width:62.75em;height:25.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Features extraction in ASR</h1>
                </header>
            
            <article>
                
<p><strong>Features extraction</strong> is an important preprocessing stage in a DL pipeline of ASR. This stage consists of an analyzer and the extraction of audio fingerprints or features. This stage also mainly computes a sequence of feature vectors, which provides a compact representation of a gathered speech signal. Generally, this task can be performed in three key steps. The first step is known as speech analysis. This step carries out a spectra-temporal analysis of the speech signal and generates raw features describing the envelope of the power spectrum of short speech intervals. The second step extracts an extended feature vector that consists of static and dynamic features. The final step converts these extended feature vectors into more compact and robust vectors. Importantly, these vectors are the input for a DL-based command/speaker/language recognizer.</p>
<p>A number of feature extraction methods are available for ASR, and <strong>Linear Predictive Codes</strong> (<strong>LPC</strong>), <strong>Perceptual Linear Prediction</strong> (<strong>PLP</strong>), and <strong>Mel Frequency Cepstral Coefficients</strong> (<strong>MFCC</strong>) are widely used ones. MFCC is the most widely used <span><span>method</span></span> for feature extraction. The following diagram presents the key components of MFCC:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-898 image-border" src="assets/e1502bfa-3932-49b9-a2b3-bffe7a5d0e4d.png" style="width:67.08em;height:24.75em;"/></p>
<p>The key steps of the MFCC are as follows:</p>
<ol>
<li>Inputting sound files and <span>converting</span> them to original sound data (a time domain signal).</li>
<li>Converting time domain signals into frequency domain signals through short-time Fourier transforms, windowing, and framing.</li>
<li>Turning frequency into a linear relationship that humans can perceive through Mel spectrum transformation.</li>
</ol>
<ol start="4">
<li>Separating the DC component from the sine component by adopting DCT Transform through Mel cepstrum analysis.</li>
<li>Extracting sound spectrum feature vectors and converting them into images.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL models for ASR</h1>
                </header>
            
            <article>
                
<p>A number of DL algorithms or models have been used in ASR. A <strong>Deep Belief Network</strong> (<strong>DBN</strong>) is one of the early implementations of DL in ASR. Generally, it has been used as a pre-training layer with a single supervised layer of a <strong>Deep Neural Network</strong> (<strong>DNN</strong>). <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) has been used for large-scale acoustic modeling. <strong>Time Delay Neural Network</strong> (<strong>TDNN</strong>) architectures have been used for audio signal processing. CNN, which has popularized DL, is also used as DL architecture for ASR. Use of DL architectures has significantly improved the speech recognition accuracy of ASRs. However, not all DL architectures have shown improvements, especially in different types of audio signals and environments, such as noisy and reverberant environments. CNNs can be used to reduce spectral variations and model the spectral correlation that exists in a speech signal.</p>
<p><strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>) and LSTM are widely used in continuous and/or natural language processing because of the capability to incorporate temporal features of input during evolution. On the contrary, CNNs are good for short and non-continuous audio signals because of their translation invariance, such as the skill of discovering structure patterns, regardless of the position. In addition, CNNs show the best performance for speech recognition in noisy and reverberant environments, and LSTMs are better in clean conditions. The reason for this could be CNNs' emphasis on local correlations as opposed to global ones. In this context, we will use CNNs for the implementation of use cases, as voices used for light control and speech used for door access are short and non-continuous. In addition, their environments can be noisy and reflective.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNNs and transfer learning for speech recognition in IoT applications</h1>
                </header>
            
            <article>
                
<p>A CNN is a very widely used DL algorithm for image recognition. Recently, this has become popular in audio/speech/speaker recognition, as these signals can be converted into images. A CNN has different implementations, including two versions of Mobilenets, and Incentive V3. An overview of Mobilenets and Incentive V3 are presented in <a href="b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml">Chapter 3</a>, <em>Image Recognition in IoT</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting data</h1>
                </header>
            
            <article>
                
<p>Data collection for ASR is a challenging task for many reasons, including privacy. Consequently, open source datasets are limited in number. Importantly, these datasets may not be easy to access, may have insufficient data/speakers, or may be noisy. In this context, we decided to use two different datasets for the two use cases. For the voice-driven <span>controlled</span> <span>smart light, we are using Google’s speech command datasets, and for use case two, we can scrap data from one of three popular open data sources,</span> LibriVox<span>,</span> LibriSpeech ASR, <span>corpus, voxceleb, and</span> YouTube<span>.</span></p>
<p>Google's speech command dataset includes 65,000 one-second long utterances of 30 short words, contributed to <span>by thousands of different</span> <span>members of the public through the AIY website. The dataset offers basic audio data on common words such as <kbd>On</kbd>, <kbd>Off</kbd>, <kbd>Yes</kbd>, digits, and directions, but this can be useful in testing the first use case. For example, the <kbd>switch on the light</kbd> command can be represented by <kbd>On</kbd> while <kbd>switch off the light</kbd> can be represented by <kbd>Off</kbd> data in the dataset. Similarly, data gathered on an individual's speech through scrapping can represent the occupants of a home. The second use case will consider a typical home with three to five occupants. These occupants will be the white list for the home and will be granted access if they are identified. Any people other than the listed ones will not be granted automated access to the home. We tested CNN on Google’s speech commands dataset and a smaller version of it. The following screenshots show a hierarchical view of the smaller dataset used for use case one:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5af7802d-765a-4872-b10b-a641843da7b0.png" style="width:30.08em;height:21.17em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For use case two, we scrapped data from LibriVox and also downloaded audio files from the LibriSpeech ASR corpus. We wrote a web scrapper using BeautifulSoup and Selenium for the scrapping. You can write a similar scrapper using other Python modules or even other languages, such as Node.js, C, C++, and PHP. The scrapper will parse the LibriVox website or any other given link and download the listed audio books/files we want. In the following code, we briefly present the scrapper's script, which consists of three main parts:</p>
<p><strong>Part 1</strong>: <span>Import the necessary Python modules for audio file scrapping:</span></p>
<pre># Import the required modules<br/>import urllib<br/>from bs4 import BeautifulSoup<br/>from selenium import webdriver<br/>import os, os.path<br/>import simplejson<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC</pre>
<p><strong>Part 2</strong>: Prepare the links for the audio books to be downloaded. Please note that the links may include repeated readers, which will be cleaned up to produce a non-repeated reader/speaker/home occupants dataset:</p>
<pre># Create links book for the audio data to be downloaded: this may include repeated readers<br/>book_links = []<br/>browser = webdriver.PhantomJS(executable_path = '/usr/local/bin/phantomjs')<br/><br/>for i in range(1): ## testing first 0-1 (2) pages of the site : to minimise the time require to downloads   <br/>    url = ("https://librivox.org/search?title=&amp;author=&amp;reader=&amp;keywords=&amp;genre_id=0&amp;status=all&amp;project_type=solo&amp;recorded_language=&amp;sort_order=catalog_date&amp;search_page={}&amp;search_form=advanced").format(i)<br/>    print(url)<br/>    browser.get(url)<br/>    element = WebDriverWait(browser, 100).until(<br/>    EC.presence_of_element_located((By.CLASS_NAME , "catalog-result")))<br/>    html = browser.page_source<br/>    soup = BeautifulSoup(html, 'html.parser')<br/>    ul_tag = soup.find('ul', {'class': 'browse-list'})   <br/>    for li_tag in ul_tag.find_all('li', {'class': 'catalog-result'}):<br/>        result_data = li_tag.find('div', {'class': 'result-data'})<br/>        book_meta = result_data.find('p', {'class': 'book-meta'})<br/>        link = result_data.a["href"]<br/>        print(link)<br/>        if str(book_meta).find("Complete") and link not in book_links:<br/>            book_links.append(link)      <br/>    print(len(book_links)) # links per page could be different from regular browsers<br/>browser.quit()</pre>
<p><strong>Part 3</strong>: Download the audio files from the listed books and form a dataset of non-repeatable readers/speakers:</p>
<pre>#  List of Links or pages for the audio books to be downloaded<br/>f = open('audiodownload_links.txt', 'w')<br/>simplejson.dump(download_links, f)<br/>f.close()<br/><br/># Record the file size of each reader's file<br/>f = open('audiodownload_sizes.txt', 'w')<br/>simplejson.dump(download_sizes, f)<br/>f.close()<br/><br/># Download the audio files and save them in local directory<br/> def count_files():<br/>    dir = 'audio_files_downloaded'<br/>    list = [file for file in os.listdir(dir) if file.endswith('.zip')] # dir is your directory path<br/>    number_files = len(list)<br/>    return number_files<br/>counter = 100 # this is for naming each downloaded file<br/>for link, size in zip(download_links, download_sizes):<br/>    if size &gt;= 50 and size &lt;= 100:<br/>        localDestination = 'audio_files_downloaded/audio{}.zip'.format(counter)<br/>        resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>        counter += 1<br/>cnt2 =  0<br/>num = count_files()<br/>if num &lt; 200:<br/>    for link, size in zip(download_links, download_sizes):<br/>        if size &gt; 100 and size &lt;= 150:<br/>            localDestination = 'audio_files_downloaded/audio{}.zip'.format(counter)<br/>            resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>            counter += 1<br/>        cnt2 += 1<br/>num = count_files()<br/>if num &lt; 200:<br/>    for link, size in zip(download_links, download_sizes):        if size &gt; 150 and size &lt;= 200:<br/>            localDestination = 'audio_files_downloaded/audio{}.zip'.format(counter)<br/>            resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>            counter += 1<br/>num = count_files()<br/>if num &lt; 200:<br/>    for link, size in zip(download_links, download_sizes):<br/>        if size &gt; 200 and size &lt;= 250:<br/>            localDestination = 'audio_files_downloaded/audio{}.zip'.format(counter)<br/>            resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>            counter += 1<br/>num = count_files()<br/>if num &lt; 200:<br/>    for link, size in zip(download_links, download_sizes):<br/>        if size &gt; 250 and size &lt;= 300:<br/>            localDestination = 'audio_files_downloaded/audio{}.zip'.format(counter)<br/>            resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>            counter += 1<br/>num = count_files()<br/>if num &lt; 200:<br/>    for link, size in zip(download_links, download_sizes):<br/>        if size &gt; 300 and size &lt;= 350:<br/>            localDestination = <br/>audio_files_downloaded/audio{}.zip'.format(counter)<br/>            resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>            counter += 1<br/>num = count_files()<br/>if num &lt; 200:<br/>    for link, size in zip(download_links, download_sizes):<br/>        if size &gt; 350 and size &lt;= 400:<br/>            localDestination = 'audio_files_downloaded/audio{}.zip'.format(counter)<br/>            resultFilePath, responseHeaders = urllib.request.urlretrieve(link, localDestination)<br/>            counter += 1</pre>
<p>After downloading the desired number of reader's/speaker's audio files or <kbd>.mp3</kbd> files (such as five speakers or home occupants), we process the <kbd>.mp3</kbd> files and convert them into fixed-size five-second audio files (<kbd>.wav</kbd>). We can do this through a shell script using tools such as ffmpeg, sox, and mp3splt, or we can do it manually (if there are not many readers/occupants and files).</p>
<p>As the implementations are based on CNNs, we need to convert the WAV audio files into images. The process of converting audio files into images varies according to the input data format. We can use <span><kbd>convert_wav2spect.sh</kbd></span> (available in the <a href="ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml">Chapter 4</a>, <em>Audio/Speech/Voice Recognition in IoT</em> code folder) to convert the WAV files into fixed-size (503 x 800) spectrogram color images:</p>
<pre>#!/bin/bash<br/>#for file in test/*/*.wav<br/>for file in train/*/*.wav<br/>do<br/>    outfile=${file%.*}<br/>          sox "$file" -n spectrogram -r -o ${outfile}.png<br/>done</pre>
<p class="graf" style="background: white;margin: 4.5pt 0cm 0.0001pt"><span>Generally, sox, the tool in the preceding script, supports the <kbd>.png</kbd> format, and if we need to convert the images, we can do this through the batch renaming of the files from Windows or Command Prompt.</span> <span>The following screenshot shows a hierarchical view of the dataset used for use case 2:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c3b3cb1e-f0da-434b-a61a-65db268db505.png" style="width:42.17em;height:23.67em;"/></p>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring data</h1>
                </header>
            
            <article>
                
<p>It is essential to explore a dataset before applying DL algorithms on the data. To explore, firstly, we can run the audio signal (<kbd>.wav</kbd>) to the image converter, <kbd>wav2image.py</kbd> (available in <a href="ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml">Chapter 4</a>, <em>Audio/Speech/Voice Recognition in IoT</em> code directory), to see how the spectrum image looks. This will produce images, as shown. The following screenshot shows converted images for an <kbd>on</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c3981384-dab4-49e0-ba8b-649b84db6b39.png" style="width:67.42em;height:38.92em;"/></p>
<p><span>The following screenshot shows converted images for an <kbd>off</kbd> command. As we can see from the screenshots, their color distributions are different, which will be exploited by the DL algorithms in order to recognize them:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1ba84f18-1ca5-4fb4-b862-2fd84139a358.png" style="width:67.58em;height:39.00em;"/></p>
<p>We can <span>also</span> carry out<span> group-wise exploration of data, and for this we can run</span> <kbd>image_explorer.py</kbd> <span>on the dataset we want to explore, as shown:</span></p>
<pre><strong>python image_explorer.py</strong></pre>
<p class="mce-root"/>
<p>The following screenshot presents a snapshot of the data exploration process of the spectrum image data <span><span>in the</span></span> speech commands dataset. Interestingly, the colors of the images are different than the individual images presented earlier. This could be because of the tools we used for them. For the group ones, we used the sox tool; while we used <kbd>ffmpegf</kbd> for the individual ones:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-901 image-border" src="assets/bfcc2321-4038-4716-a4c9-5466e3f98088.png" style="width:101.42em;height:59.92em;"/></p>
<p>As shown in the preceding screenshot of data exploration, the differences between four different speech commands in spectrum images may not always <span>be</span> <span>significant. This is a challenge in audio signal recognition.</span></p>
<p>The following screenshot presents a snapshot of the data exploration process of the spectrum image data based on a speaker’s/occupant’s speech (5-second) dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-902 image-border" src="assets/b7842bb0-a9af-45ce-8275-ac00b08b795a.png" style="width:88.08em;height:40.25em;"/></p>
<p>As shown in the preceding screenshot, each occupant’s short speech spectrum images present a pattern that will help to classify the occupants and grant access accordingly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preprocessing</h1>
                </header>
            
            <article>
                
<p><strong>Data preprocessing</strong> is an essential step for a DL pipeline. The speech commands dataset consists of 1-second <kbd>.wav</kbd> files for each short speech command, and these files only need to be converted into a spectrum image. However, the downloaded audio files for the second use case are not uniform in length; hence, they require two-step preprocessing:</p>
<ul>
<li><kbd>.mp3</kbd> to uniform length (such as a 5-second length) WAV file conversion</li>
<li><kbd>.wav</kbd> file to spectrum image conversion.</li>
</ul>
<p>The preprocessing of the datasets is discussed in the data collection section. A few issues to be noted during the training image set preparation are as follows:</p>
<ul>
<li><strong>Data Size</strong>: We need to collect at least a hundred images for each class in order to train a model that works well. The more we can gather, the better the accuracy of the trained model is likely to be. Each of the categories in the use case one dataset has more than 3,000 sample images. However, one-shot learning (learning with fewer samples) works well with fewer than 100 training samples. We also made sure that the images are a good representation of what our application will actually face in a real implementation.</li>
<li><strong>Data heterogeneity</strong>: <span>Data collected for</span> training <span>should be heterogeneous. For example, audio or speech signals about a speaker need to be taken in as wide a variety of situations as possible, at different conditions of their voice, and with different devices.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Models training</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, we are using transfer learning for both use cases, which does not require training from scratch; retraining the models with a new dataset will sufficiently work in many cases. In addition, in <a href="b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml">Chapter 3</a>, <em>Image Recognition in IoT</em> , we found that Mobilenet V1 is a lightweight (low-memory footprint and lower training time) CNN architecture. Consequently, we are implementing both uses using the <span>Mobilenet V1 network. Importantly, we will use TensorFlow's <kbd>retrain.py</kbd> module as it is specially designed for CNNs (such as Mobilenet V1) based transfer learning).</span></p>
<p>We need to understand the list of key arguments of <kbd>retrain.py</kbd> before retraining <span>Mobilenet V1 on the datase</span>ts. For the retraining, if we type in our Terminal (in Linux or macOS) or Command Prompt (Windows) <kbd>python retrain.py -h</kbd>, we will see a window like the following screenshot with additional information (such as an overview of each argument):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ac1fd4ce-9554-46c5-93dd-f976765ca767.png" style="width:46.25em;height:20.50em;"/></p>
<p>As shown in the preceding screenshot, the compulsory argument is the <kbd>-–image directory</kbd>, and it needs to be a dataset directory in which we want to train or retrain the models. In the case of Mobilenet V1, we must explicitly mention the CNN architecture, such as <kbd>--architecture mobilenet_1.0_224</kbd>. For the rest of the arguments, including data split ratio among training, validation, and test, we used the default values. The default split of data is to put 80% of the images into the main training set, keep 10% aside to run frequently as validation during training, and the final 10% of the data is for testing the real-world performance of the classifier.</p>
<p><span>The following is the command for running the retraining model for the Mobilenet v1 model:<br/></span></p>
<pre>python retrain.py \<br/>--output_graph=trained_model_mobilenetv1/retrained_graph.pb \<br/>--output_labels=trained_model_mobilenetv1/retrained_labels.txt   \<br/>--architecture mobilenet_1.0_224 \<br/>--image_dir= your dataset directory</pre>
<p>Once we run the preceding commands, they will generate the retrain models (<kbd>retrained_graph.pb</kbd>) and labels text (<kbd>retrained_labels.txt</kbd>) in the given directory and the summary directory consists of training and validation summary information for the models. The summary information (<kbd>--summaries_dir argument with default value retrain_logs)</kbd>) can be used by TensorBoard to visualize different aspects of the models, including the networks and their performance graphs. If we type the following command in the Terminal or Command Prompt, it will run <kbd>tensorboard</kbd>:</p>
<pre>tensorboard --logdir retrain_logs</pre>
<p>Once TensorBoard is running, navigate your web browser to <kbd>localhost:6006</kbd> to view TensorBoard and view the network of the corresponding model. The following diagram presents the network of the Mobilnet V1 <span>used:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-904 image-border" src="assets/9529247b-18f8-40bb-83c5-6ef78a80144d.png" style="width:37.92em;height:45.75em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating models</h1>
                </header>
            
            <article>
                
<p>We can evaluate the models from three different aspects:</p>
<ul>
<li>Learning/(re)training time</li>
<li>Storage requirement </li>
<li>Performance (accuracy)</li>
</ul>
<p>The Mobilnet V1's <span>retraining and validation process using the <kbd>retrain.py</kbd> module</span> took less than an hour on a desktop (Intel Xenon CPU E5-1650 v3@3.5GHz and 32 GB RAM) with GPU support.</p>
<p>The storage/memory requirement of a model is an essential consideration for resource-constrained IoT devices. To evaluate the <span>storage/memory footprint</span> <span>of the Mobilenet V1, we compared its storage requirement to another two similar networks' (the Incentive V3 and CIFAR-10 CNN) storage requirements. The f</span>ollowing screenshot presents the storage requirements for the three models. As shown, Mobilenet V1 requires only 17.1 MB, less than one-fifth of the Incentive V3 (87.5 MB) and CIFAR-10 CNN (91.1 MB). In terms of storage requirements, <span>Mobilenet V1 is a better choice for</span> many resource-constrained IoT devices, including the Raspberry Pi and smartphones:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-905 image-border" src="assets/df8c7fd0-c88c-4399-9848-540d8e283d4c.png" style="width:82.17em;height:24.08em;"/></p>
<p>Finally, we have evaluated the performance of the models. Two levels of performance evaluation have been carried out for the use cases:</p>
<ul>
<li>Dataset-wide evaluation or testing has been done during the retraining phase on the desktop PC platform/server side</li>
<li>Individual audio and a group of home occupants, samples were tested or evaluated in the Raspberry Pi 3 environment. All the evaluation performances are presented in the following figures.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance (use case 1)</h1>
                </header>
            
            <article>
                
<p>The following screenshots present the evaluation results of Mobilenet V1 on a speech command dataset (customized to only five commands, including <kbd>on</kbd>, <kbd>no</kbd>, <kbd>off</kbd>, <kbd>yes</kbd>, and <kbd>stop</kbd>). Note that <kbd>on</kbd> is considered to be <em>switch on the light</em> for use case one due to the lack of a real dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-906 image-border" src="assets/95f50be3-73bc-4066-b89a-f6e152d10def.png" style="width:67.17em;height:34.92em;"/></p>
<p>The following screenshot was generated from the TensorBoard log files. The orange line represents the training and the blue one represents the validation accuracy of the <span>Mobilenet V1 on the command dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b9e9d8c2-8965-43e4-b3ee-f0fbce31e3b2.png"/></p>
<p>As we can see from the preceding two screenshots, the performance of the Mobilenet V1 is not great, but it will be sufficient for detecting commands by adding more information to the commands, such as <em>switch on the main light</em> instead of only <em>on</em>. Furthermore, we can <span><span>use</span></span> a better audio file to image converter to improve the image quality and recognition accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance (use case 2)</h1>
                </header>
            
            <article>
                
<p>The following screenshots represents the evaluation results of Mobilenet V1 on a <kbd>three occupants</kbd> dataset. As we can see, the performance of the dataset is reasonably good. It can successfully detect occupants more than 90% of the time:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d5dbedef-8761-444e-9534-31fbcfefaf1d.png"/></p>
<p><span>The following screenshot was generated from the TensorBoard log files. The orange line represents the training and the blue one represents the validation accuracy of the</span> <span>Mobilenet V1 on the <kbd>three occupants</kbd> dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-907 image-border" src="assets/bbed2591-7daf-4640-9ed3-0653a6c3d26e.png" style="width:92.92em;height:28.00em;"/></p>
<p><span>We also tested the Mobilenet V1 on a <kbd>five occupants</kbd> dataset, and this consistently showed accuracy in the range of 85-94%. Finally, we can export the trained model detail (such as <kbd>retrained_mobilenet_graph.pb</kbd> and <kbd>retrained_labels.txt</kbd>) to an IoT device, including a smartphone or Raspberry Pi, and we can test the model on new data from both use cases using the provided <kbd>label_image.py</kbd> code or something similar.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Automatic audio/speech/voice recognition is becoming a popular means for people to interact with their devices, including smartphones, wearables, and other smart devices. Machine learning and DL algorithms are essential in audio/speech/voice-based decision making.</p>
<p>In the first part of this chapter, we briefly described different IoT applications and their audio/speech/voice detection-based decision making. We also briefly discussed two potential use cases of IoT where DL algorithms can be useful in speech/command-based decision making. The first use case considered an IoT application to make a home smart using voice-controlled lighting. The second use case also made a home or office smart, where a DL-based IoT solution offered automated access control to the smart home or office. In the second part of the chapter, we briefly discussed the data collection process for the use cases, and discussed the rationale behind selecting a CNN, especially the Mobilenet V1. The rest of the sections of the chapter describe all the necessary components of the DL pipeline for these models and their results.</p>
<p>Many IoT devices and/or users are mobile. Localization of the devices and users is essential for offering them services when they are on the move. GPS can support outdoor localization, but it does not work in indoor environments. Consequently, alternative technologies are necessary for indoor localization. Different indoor technologies, including WiFi-fingerprinting, are available, and generally they work based on a device's communication signal analysis. In the next chapter (<a href="3426d6f2-8913-4585-b04b-f0b3a8bd235d.xhtml">Chapter 5</a>, <em>Indoor Localization in IoT</em>), we will discuss and demonstrate how DL models can be used for indoor localization in IoT applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Assistive technology: <a href="http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology">http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology</a></li>
<li><em>Smart and Robust Speaker Recognition for Context-Aware In-Vehicle Applications</em>, <span>I Bisio, C Garibotto, A Grattarola, F Lavagetto, and A Sciarrone,</span> in IEEE Transactions on Vehicular Technology, vol. 67, no. 9, pp. 8,808-8,821, September, 2018.</li>
<li><em>Emotion-Aware Connected Healthcare Big Data Towards 5G</em>, <span>M S Hossain and G Muhammad,</span> in IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2,399-2,406, August, 2018.</li>
<li><em>Machine Learning Paradigms for Speech Recognition</em>, <span>L Deng, X Li (2013).</span> IEEE Transactions on Audio, Speech, and Language Processing, vol. 2, # 5.</li>
<li><em>On Comparison of Deep Learning Architectures for Distant Speech Recognition</em>, <span>R Sustika, A R Yuliani, E Zaenudin, and H F Pardede,</span> <em>2017 Second International Conferences on Information Technology, Information Systems and Electrical Engineering (ICITISEE)</em>, Yogyakarta, 2017, pp. 17-21.</li>
<li><em>Deep Neural Networks for Acoustic Modeling in Speech Recognition</em>, <span>G Hinton, L Deng, D Yu, G E Dahl, A R Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, T N Sainath, and B Kingsbury,</span> IEEE Signal Processing Magazine, vol. 29, # 6, pp. 82–97, 2012.</li>
<li><em>Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</em>, <span>H Sak, A Senior, and F Beaufays,</span> in Fifteenth Annual Conference of the International Speech Communication Association, 2014.</li>
<li><em>Phoneme recognition using time delay neural network</em>, IEEE Transaction on Acoustics, Speech, and Signal Processing, <span>G. H. K. S. K. J. L. Alexander Waibel, Toshiyuki Hanazawa, vol.</span> 37, # 3, 1989.</li>
<li><em>A Time Delay Neural Network Architecture for Efficient Modeling of Long Temporal Contexts</em>, <span>V Peddinti, D Povey, and S Khudanpur,</span> in Proceedings of Interspeech. ISCA, 2005.</li>
<li><em>Deep Convolutional Neural Network for lvcsr</em>, <span>B. K. B. R. Tara N Sainath and Abdel Rahman Mohamed,</span> in International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 8614–8618.</li>
<li><em>Mel Frequency Cepstral Coefficients for Music Modeling</em>, <span>Logan, Beth and others,</span> ISMIR,vol. 270, 2000.</li>
<li><span><em>Launching the Speech Commands Dataset</em>,</span> <span>Pete Warden:</span> <a href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html">https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>