- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Conversational AI Applications with Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The art of conversation is considered a uniquely human trait. The ability of
    machines to have a dialog with humans has been a research topic for many years.
    Alan Turing proposed the now-famous Turing Test to see if a human could converse
    with another human and a machine through written messages, and identify each participant
    as machine or human correctly. In recent times, digital assistants such as Alexa
    by Amazon and Siri by Apple have made considerable strides in conversational AI.
    This chapter discusses different conversational agents and puts the techniques
    learned in the previous chapters into context. While there are several approaches
    to building conversational agents, we''ll focus on the more recent deep learning
    approaches and cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of conversational agents and their general architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An end-to-end pipeline for building a conversational agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of different types of conversational agents, such as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question-answering bots
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Slot-filling or task-oriented bots
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: General conversation bots
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start with an overview of the general architecture of conversational agents.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of conversational agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A conversational agent interacts with people using speech or text. Facebook
    Messenger would be an example of a text-based agent while Alexa and Siri are examples
    of agents that interact through speech. In either case, the agent needs to understand
    the user''s intent and respond accordingly. Hence, the core part of the agent
    would be a **natural language understanding** (**NLU**) module. This module would
    interface with a **natural** **language generation** (**NLG**) module to supply
    a response back to the user. Voice agents differ from text-based agents in having
    an additional module that converts voice to text and vice versa. We can imagine
    the system having the following logical structure for a voice-activated agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Conceptual architecture of a conversational AI system'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between a speech-based system and a text-based system is
    how the users communicate with the system. All the other parts to the right of
    the Speech Recognition and Generation section shown in *Figure 9.1* above are
    identical in both types of conversational AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: The user communicates with the agent using speech. The agent first converts
    speech to text. Many advancements have been made in the past few years in this
    area, and it is generally considered a solved problem for major languages like
    English.
  prefs: []
  type: TYPE_NORMAL
- en: 'English is spoken in many countries across the globe, resulting in many different
    pronunciations and dialects. Consequently, companies like Apple develop various
    models for different accents, such as British English, Indian English, and Australian
    English. *Figure 9.2* below shows some English and French accents from the Siri
    control panel on an iPhone 11 running iOS 13.6\. French, German, and some other
    languages also have multiple variants. Another way to do this could be by putting
    an accent and language classification model as the first step and then processing
    the input through the appropriate speech recognition model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Language variants in Siri for speech recognition'
  prefs: []
  type: TYPE_NORMAL
- en: For virtual assistants, there are specific models for wake word detection. The
    model's objective is to start the bot once it detects a wake word or phrase such
    as "OK Google." The wake word triggers the bot to listen to the utterances until
    the conversation is completed. Once the user's speech has been converted into
    words, it is easy to apply to various NLP techniques that we have seen in multiple
    chapters in this book. The breakdown of the elements shown inside the NLP box
    in *Figure 9.1* can be considered conceptual. Depending on the system and the
    task, these components may be different models or one end-to-end model. However,
    it is useful to think of the logical breakdown, as shown in the figure.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the user's commands and the intent is a crucial part. Intent identification
    is essential for general-purpose systems like Amazon's Alexa or Apple's Siri,
    which serve multiple purposes. Specific dialogue management systems may be invoked
    based on the intent identified. The dialog management may invoke APIs provided
    by a fulfillment system. In a banking bot, the command may be to get the latest
    balances, and the fulfillment may be a banking system that retrieves the latest
    balance. The dialogue manager would process the balance and use an NLG system
    to convert the balance into a proper sentence. Note that some of these systems
    are built on rules-based systems and others use end-to-end deep learning. A question-answering
    system is an example of an end-to-end deep learning system where dialog management,
    and NLU are a single unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of conversational AI applications. The most common
    ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: Task-oriented or slot-filling systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question-answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine reading comprehension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social or chit-chat bots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these types is described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Task-oriented or slot-filling systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Task-oriented systems are purpose-built to satisfy a specific task. Some examples
    of tasks are ordering a pizza, getting the latest balance of a bank account, calling
    a person, sending a text message, turning a light on, and so on. Most of the capabilities
    exposed by virtual assistants can be classified into this category. Once the user''s
    intent has been identified, control is transferred to the model managing a specific
    intent to gather all the information to perform the task and manage the dialog
    with the user. NER and POS detection models form a crucial part of such systems.
    Imagine that the user needs to fill a form with some information, and the bot
    interacts with the user to find the required information to fulfill the task.
    Let''s take the example of ordering a pizza. The table below shows a simplified
    example of the choices in this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Crust | Toppings | Delivery | Quantity |'
  prefs: []
  type: TYPE_TB
- en: '| SmallMediumLargeXL | ThinRegularDeep dishGluten-free | CheeseJalapenoPineapplePepperoni
    | Take-outDelivery | 12… |'
  prefs: []
  type: TYPE_TB
- en: 'Here is a made-up example of a conversation with a bot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B16252_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: A possible pizza-ordering bot conversation'
  prefs: []
  type: TYPE_NORMAL
- en: The bot tracks the information needed and keeps marking the information it has
    received from the person as the conversation progresses. Once the bot has all
    the information needed to complete the task, it can execute the task. Note that
    some steps, such as confirming the order or the customer asking for options for
    toppings, have been excluded for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In today''s world, solutions like Dialogflow, part of Google Cloud, and LUIS,
    part of Azure, simplify building such conversational agents to just the configuration.
    Let''s see how a simple bot that implements a portion of the pizza-ordering task
    above can be implemented with Dialogflow. Note that this example has been kept
    small to simplify configuration and use the free tier of Dialogflow. The first
    step is to navigate to [https://cloud.google.com/dialogflow](https://cloud.google.com/dialogflow),
    which is the home page for this service. There are two version of Dialogflow –
    Essentials or ES, and CX. CX is the advanced version with a lot more features
    and controls. Essentials is a simplified version with a free tier that is perfect
    for a bot''s trial build. Scroll down on the page so that you can see the Dialogflow
    Essentials section and click on the **Go to console** link, as shown in *Figure
    9.4* below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B16252_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Dialogflow console access'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the console may require the authorization of the service, and you
    may need to log in with your Google Cloud account. Alternatively, you may navigate
    to [dialogflow.cloud.google.com/#/agents](http://dialogflow.cloud.google.com/#/agents)
    to see a list of configured agents. This screen is shown in *Figure 9.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B16252_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Agents configuration in Dialogflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'A new agent can be created by clicking on the blue **CREATE AGENT** button
    on the top right. If you see a different interface, please check that you are
    using Dialogflow Essentials. You can also use this URL to get to the agents section:
    [https://dialogflow.cloud.google.com/#/agents](https://dialogflow.cloud.google.com/#/agents).
    This brings up the new agent configuration screen, shown in *Figure 9.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, Teams  Description automatically
    generated](img/B16252_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Creating a new agent'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that this is not a comprehensive tutorial of Dialogflow, so we
    will be using several default values to illustrate the concept of building slot-filling
    bots. Hitting **CREATE** will build a new bot and load a screen, as shown in *Figure
    9.7*. The main part of building the bot is to define intent. The main intent of
    our bot is to order pizza. Before we create an intent, we will configure a few
    entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B16252_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: A barebones agent ready for configuration'
  prefs: []
  type: TYPE_NORMAL
- en: 'These entities are the slots that the bot will fill out in conversation with
    the user. In this case, we will define two entities – the crust of the pizza and
    the size of the pizza. Click on the **+** sign next to Entities on the left in
    the previous screenshot, and you''ll see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B16252_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Configuring options for the crust entity in Dialogflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'The values on the left represent the values for the crust entity, and the multiple
    options or synonyms on the right are the terms the user can input or speak corresponding
    to each choice. We will configure four options corresponding to the table above.
    Another entity will be created for the size of the pizza. The configured entity
    looks like *Figure 9.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B16252_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Configuration of the size entity'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to build the intent. Click on the **+** sign next to the **Intents**
    section on the left navigation bar. We will name this intent `order`, as this
    intent will get the options for crust and size from the user. First, we need to
    specify a set of training phrases that will trigger this intent. Some examples
    of such training phrases can be "I would like to order pizza" or "Can I get a
    pizza?". *Figure 9.10* shows some of the configured training phrases for the intent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B16252_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Training phrases that trigger the ordering intent'
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of hidden machine learning and deep learning happening in this
    picture, simplified by Dialogflow. For example, the platform can process text
    input as well as speech. These training examples are indicative, and the actual
    phrasing does not need to match any of these expressions directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to define the parameters we need from the user. We add an
    action with two parameters – size and crust. Note that the **ENTITY** column links
    the parameter with the defined entities and their values. The **VALUE** column
    defines a variable name that can be used in future dialogue or for integration
    with APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, table  Description automatically generated](img/B16252_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Required parameters for the order intent'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each parameter, we need to specify some prompts that the agent will use
    to ask the user for the information. *Figure 9.12* below shows some example prompts
    for the size parameter. You may choose to configure your phrasings for the prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B16252_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Prompt options for the size parameter'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step in configuring the intent is configuring a response once the
    information is collected. This configuration is done in the **Responses** section
    and is shown in *Figure 9.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B16252_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: Response configuration for the order intent'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the use of `$size.original` and `$crust.original` in the response text.
    It uses the original terms used by the user while ordering when it repeats the
    order back. Finally, note that we set this intent as the end of the conversation
    as we have obtained all the information we needed to get. Our bot is ready to
    be trained and tested. Hit the blue **Save** button at the top of the page after
    you have configured the training phrases, action and parameters, and the responses.
    There is another section at the bottom called fulfilment. This allows connecting
    the intent with a web service to complete the intent. The bot can be tested using
    the right side. Note that though we configured only text, Dialogflow enables both
    text and voice interfaces. While we demonstrate the text interface here, you are
    encouraged to try the voice interface as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_09_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: An example of dialog showing the response processing and the variable
    being set'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud-based solutions have made it quite easy to build task-oriented conversational
    agents for general uses. However, building an agent for a specialized domain like
    medical uses may require custom builds. Let''s look at options for specific parts
    of such a system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intent identification**: The simplest way to identify intent is to treat
    it as a classification problem. Given an utterance or input text, the model needs
    to classify it into several intents. Standard RNN-based architectures, like those
    seen in earlier chapters, can be used and adapted for this task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slot tagging**: Tagging slots used in a sentence to correspond to inputs
    can be treated as a sequence classification problem. This is similar to the approach
    used in the second chapter, where named entities were tagged in a sequence of
    text. Bi-directional RNN models are quite effective in this part.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different models can be developed for these parts, or they can be combined
    in one end-to-end model with a dialog manager. Dialog state tracking systems can
    be built by using a set of rules generated by experts or by using CRFs (see *Chapter
    2*, *Understanding Sentiment in Natural Language with BiLSTMs*, for a detailed
    explanation). Recent approaches include a Neural Belief Tracker proposed by Mrkšić
    et al. in 2017 in their paper titled *Neural Belief Tracker: Data-Driven Dialogue
    State Tracking*. This system takes three inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The last system output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last user utterance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A slot-value pair from the possible candidates for slots
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These three inputs are combined through the content model and semantic decoding
    model and fed to a binary decision (softmax) layer to produce a final output.
    Deep reinforcement learning is being used to optimize the dialog policy overall.
  prefs: []
  type: TYPE_NORMAL
- en: In the NLG part, the most common approach is to define a set of templates that
    can be dynamically populated. This approach was shown in the preceding figure
    *Figure 9.13*. Neural methods, such as semantically controlled LSTM, as proposed
    by Wen et al. in their paper *Semantically Conditioned LSTM-based Natural Language
    Generation for Spoken Dialogue Systems* in 2015, are being actively researched.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to another interesting area of conversational agents – question-answering
    and machine reading comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: Question-answering and MRC conversational agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bots can be trained to answer questions based on information contained in a
    **knowledge base** (**KB**). This setting is called the question-answering setting.
    Another related area is **machine reading comprehension** or **MRC**. In MRC,
    questions need to be answered with respect to a set of passages or documents provided
    with the query. Both of these areas are seeing a lot of startup activity and innovation.
    A very large number of business use cases can be enabled with both of these types
    of conversational agents. Passing the financial report to a bot and answering
    questions such as the increase in revenue given the financial report would be
    an example of MRC. Organizations have large digital caches of information, with
    new information pouring in every day. Building such agents empowers knowledge
    workers to process and parse large amounts of information quickly. Startups like
    Pryon are delivering conversational AI agents that merge, ingest, and adapt a
    myriad of structured and unstructured data into unified knowledge domains that
    end users can ask natural language questions as a way to discover information.
  prefs: []
  type: TYPE_NORMAL
- en: KBs typically consist of subject-predicate-object triples. The subject and object
    are entities, while the predicate indicates a relationship between them. The KB
    can be represented as a knowledge graph, where objects and subjects are nodes
    connected by predicate edges. A big challenge is the maintenance of such knowledge
    bases and graphs in real life. Most deep NLP approaches are focused on determining
    whether a given subject-predicate-object triplet is true or not. The problem is
    reduced to a binary classification through this reformulation. There are several
    approaches, including the use of BERT models, which can solve the classification
    problem. The key here is to learn an embedding of the KB and then frame queries
    on top of this embedding. Dat Nguyen's survey paper, titled *A survey of embedding
    models of entities and relationships for knowledge graph completion*, provides
    an excellent overview of various topics for a deeper dive. We focus on MRC for
    the rest of this section now.
  prefs: []
  type: TYPE_NORMAL
- en: MRC is a challenging task as the objective is to answer any set of questions
    about a given set of passages or documents. These passages are not known in advance
    and may be of variable length. The most common research dataset used for evaluating
    models is the **Stanford Question Answering Dataset** or **SQuAD**, as it is commonly
    called. The dataset has 100,000 questions for different Wikipedia articles. The
    objective of the model is to output the span of text from the article that answers
    the question. A more challenging dataset has been published by Microsoft based
    on Bing queries. This dataset is called the **MAchine Reading COmprehension**
    or **MARCO** dataset. This dataset has over 1 million anonymized questions, with
    over 8.8 million passages extracted from over 3.5 million documents. Some of the
    questions in this dataset may not be answerable based on the passages, which is
    not the case with the SQuAD dataset, which makes this a challenging dataset. The
    second challenging aspect of MARCO as compared to SQuAD is that MARCO requires
    the generation of an answer by combining information from multiple passages, whereas
    SQuAD requires marking the span from the given passage.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT and its variants such as *ALBERT: A Lite BERT for Self-supervised Learning
    of Language Representations* published at ICLR 2020 form the basis of most competitive
    baselines today. BERT architecture is well suited to this task as it allows passing
    in two pieces of input text separated by a [SEP] token. The BERT paper evaluated
    their language model on a number of tasks, including performance on the SQuAD
    task. Question tokens formed the first part of the pair, and the passage/document
    formed the second part of the pair. The output tokens corresponding to the second
    part, the passage, are scored to represent whether the token represents the start
    of the span or the end of the span.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A high-level depiction of the architecture is shown in *Figure 9.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: BERT fine-tuning approach for SQuAD question answering'
  prefs: []
  type: TYPE_NORMAL
- en: A multi-modal aspect of question answering is Visual QA, which was briefly introduced
    in *Chapter 7*, *Multi-modal Networks and Image Captioning with ResNets and Transformer*.
    Analogous architectures to the one proposed for image captioning, which can take
    images as well as text tokens, are used for solving this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The setting for QA above is called single turn because the user presents a
    question with a passage from where the question needs to be answered. However,
    people have conversations with a back and forth dialog. Such a setting is called
    multi-turn dialog. A follow-up question may have context from a previous question
    or answer in the conversation. One of the challenges in a multi-turn dialog is
    coreference resolution. Consider the following dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Person**: Can you tell me the balance in my account #XYZ?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bot**: Your balance is $NNN.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Person**: Can you transfer $MM to account #ABC from *that* account?'
  prefs: []
  type: TYPE_NORMAL
- en: '"that" in the second instruction refers to account #XYZ, which was mentioned
    in the first question from the person. This is called coreference resolution.
    In a multi-turn conversation, resolving references could be quite complicated
    based on the distance between the references. Several strides have been made in
    this area with respect to general conversation bots, which we''ll cover next.'
  prefs: []
  type: TYPE_NORMAL
- en: General conversational agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Seq2seq models provide the best inspiration for learning multi-turn general
    conversations. A useful mental model is that of machine translation. Similar to
    the machine translation problem, the response to the previous question can be
    thought of as a translation of that input into a different language – the response.
    Encoding more context into a conversation can be achieved by passing in a sliding
    window of the previous conversation turns instead of just the last question/statement.
    The term open-domain is often used to describe bots in this area as the domain
    of the conversation is not fixed. The bot should be able to discuss a wide variety
    of topics. There are several issues that are their own research topics.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of personality or blandness is one such problem. The dialog is very dry.
    As an example, we have seen the use of a temperature hyperparameter to adjust
    the predictability of the response in previous chapters. Conversational agents
    have a high propensity to generate "I don't know" responses due to a lack of specificity
    in the dialog. A variety of techniques, including GANs, can be used to address
    this. The *Personalizing Dialogue Agents* paper authored by Zhang et al. from
    Facebook outlines some of the approaches used to address this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Two recent examples that highlight the state of the art of writing human-like
    comments come from Google and Facebook. Google published a paper titled *Towards
    a Human-like Open-Domain Chatbot*, with a chatbot named Meena with over 2.6 billion
    parameters. The core model is a seq2seq model using an **Evolved Transformer**
    (**ET**) block for encoding and decoding. The model architecture has one ET block
    in the encoder and 13 ET block in the decoder. ET block was discovered through
    **neural architecture search** (**NAS**) on top of the Transformer architecture.
    A new human evaluation metric called **Sensibleness and Specificity Average**
    (**SSA**) was proposed in the paper. The current literature has a variety of different
    metrics being proposed for the evaluation of such open-domain chatbots with little
    standardization.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of an open-domain chatbot is described by Facebook on [https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/).
    This paper builds on several years of research and combines the work on personalization,
    empathy, and KBs into a blended model called BlenderBot. Similar to Google's research,
    different datasets and benchmarks are used to train this chatbot. The code for
    the bot has been shared on [https://parl.ai/projects/recipes/](https://parl.ai/projects/recipes/).
    ParlAI, by Facebook research, provides several models for chatbots through [https://github.com/facebookresearch/ParlAI](https://github.com/facebookresearch/ParlAI).
  prefs: []
  type: TYPE_NORMAL
- en: This is a very hot area of active research with a lot of action happening in
    it. Comprehensive coverage of this topic would take a book of its own. Hopefully,
    you have learned many techniques in this book that can be combined to build amazing
    conversational agents. Let's wrap up.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed the various types of conversational agents, such as task-oriented,
    question-answering, machine reading comprehension, and general chit-chat bots.
    Building a conversational AI system is a very challenging task with many layers,
    and it is an area of active research and development. The material covered earlier
    in the book can also help in building various parts of chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Epilogue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let me congratulate you on reaching the end of the book. I hope this
    book helped you get a grounding in advanced NLP models. The main challenge facing
    a book such as this is that it will likely be obsolete by the time it reaches
    the press. The key thing is that new developments are based on past developments;
    for example, the Evolved Transformer is based on the Transformer architecture.
    Knowledge of all the models presented in the book will give you a solid foundation
    and significantly cut down the amount of time you need to spend to understand
    a new development. A set of influential and important papers for each chapter
    have also been made available in the GitHub repository. I am excited to see what
    you will discover and build next!
  prefs: []
  type: TYPE_NORMAL
