["```\nlspci | grep -i nvidia\n```", "```\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt-get update\nsudo apt-get install nvidia-375\n```", "```\ncat /proc/driver/nvidia/version\n```", "```\nsudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb\nsudo apt-get update\nsudo apt-get install cuda\n```", "```\necho 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n```", "```\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n```", "```\nsource ~/.bashrc\n```", "```\nnvcc -V\n```", "```\ncd ~/Downloads/\n```", "```\ntar xvf cudnn*.tgz\n```", "```\ncd cuda\n```", "```\nsudo cp */*.h /usr/local/cuda/include/\n```", "```\nsudo cp */libcudnn* /usr/local/cuda/lib64/\n```", "```\nsudo chmod a+r /usr/local/cuda/lib64/libcudnn*\n```", "```\nsudo apt-get update && apt-get install -y python-numpy python-scipy python-nose python-h5py python-skimage python-matplotlib python-pandas python-sklearn python-sympy\n```", "```\nsudo apt-get clean && sudo apt-get autoremove\n```", "```\nsudo rm -rf /var/lib/apt/lists/*\n```", "```\nsudo apt-get update\n```", "```\nsudo apt-get install git python-dev python3-dev python-numpy python3-numpy build-essential  python-pip python3-pip python-virtualenv swig python-wheel libcurl3-dev\n```", "```\nsudo apt-get install -y libfreetype6-dev libpng12-dev\n```", "```\npip3 install -U matplotlib ipython[all] jupyter pandas scikit-image\n```", "```\npip3 install --upgrade tensorflow-gpu\n```", "```\npython3\n>>> import tensorflow as tf\n>>> a = tf.constant(5)\n>>> b = tf.constant(6)\n>>> sess = tf.Session()\n>>> sess.run(a+b)\n// this should print bunch of messages showing device status etc. // If everything goes well, you should see gpu listed in device\n>>> sess.close()\n```", "```\nsudo apt-get update && apt-get install -y python-numpy python-scipy python-nose python-h5py python-skimage python-matplotlib python-pandas python-sklearn python-sympy\n```", "```\nsudo apt-get clean && sudo apt-get autoremove\n```", "```\nsudo rm -rf /var/lib/apt/lists/*\n```", "```\nsudo apt-get update\n```", "```\nsudo apt-get install git python-dev python3-dev python-numpy python3-numpy build-essential  python-pip python3-pip python-virtualenv swig python-wheel libcurl3-dev\n```", "```\nsudo apt-get install -y libfreetype6-dev libpng12-dev\n```", "```\npip3 install -U matplotlib ipython[all] jupyter pandas scikit-image\n```", "```\npip3 install --upgrade tensorflow\n```", "```\npython3\n>>> import tensorflow as tf\n>>> a = tf.constant(5)\n>>> b = tf.constant(6)\n>>> sess = tf.Session()\n>>> sess.run(a+b)\n>> sess.close()\n```", "```\nsudo easy_install pip\n```", "```\nsudo pip install --upgrade virtualenv\n```", "```\nvirtualenv --system-site-packages targetDirectory # for Python 2.7\n```", "```\nvirtualenv --system-site-packages -p python3 targetDirectory # for Python 3.n\n```", "```\nsource ~/tensorflow/bin/activate \n```", "```\ndeactivate\n```", "```\nsource bin/activate\n```", "```\n(tensorflow)$ pip install --upgrade tensorflow      # for Python 2.7\n```", "```\n(tensorflow)$ pip3 install --upgrade tensorflow     # for Python 3.n\n```", "```\nC:\\> pip3 install --upgrade tensorflow-gpu\n```", "```\nC:\\> pip3 install --upgrade tensorflow\n```", "```\nvar = tf.Variable(tf.random_normal((0,1)),name='random_values')\n```", "```\nph_var1 = tf.placeholder(tf.float32,shape=(2,3))\nph_var2 = tf.placeholder(tf.float32,shape=(3,2))\nresult = tf.matmul(ph_var1,ph_var2)\n```", "```\n# import TensorFlow package\nimport tensorflow as tf\n# build a TensorFlow variable b taking in initial zeros of size 100\n# ( a vector of 100 values)\nb  = tf.Variable(tf.zeros((100,)))\n# TensorFlow variable uniformly distributed values between -1 and 1\n# of shape 784 by 100\nW = tf.Variable(tf.random_uniform((784, 100),-1,1))\n# TensorFlow placeholder for our input data that doesn't take in\n# any initial values, it just takes a data type 32 bit floats as\n# well as its shape\nx = tf.placeholder(tf.float32, (100, 784))\n# express h as Tensorflow ReLU of the TensorFlow matrix\n#Multiplication of x and W and we add b\nh = tf.nn.relu(tf.matmul(x,W) + b )\nh and see its value until we run this graph. So, this code snippet is just for building a backbone for our model. If you try to print the value of *W* or *b* in the preceding code, you should get the following output in Python:\n```", "```\nsess.run(fetches, feeds)\n```", "```\n# importing the numpy package for generating random variables for\n# our placeholder x\nimport numpy as np\n# build a TensorFlow session object which takes a default execution\n# environment which will be most likely a CPU\nsess = tf.Session()\n# calling the run function of the sess object to initialize all the\n# variables.\nsess.run(tf.global_variables_initializer())\n# calling the run function on the node that we are interested in,\n# the h, and we feed in our second argument which is a dictionary\n# for our placeholder x with the values that we are interested in.\nsess.run(h, {x: np.random.random((100,784))})   \n```", "```\nlazy evaluation. It means that the evaluation of your graph only ever happens at runtime, and runtime in TensorFlow means the session. So, calling this function, global_variables_initializer(), will actually initialize anything called variable in your graph, such as *W* and *b* in our case.\n```", "```\nph_var1 = tf.placeholder(tf.float32,shape=(2,3))\nph_var2 = tf.placeholder(tf.float32,shape=(3,2))\nresult = tf.matmul(ph_var1,ph_var2)\nwith tf.Session() as sess:\n    print(sess.run([result],feed_dict={ph_var1:[[1.,3.,4.],[1.,3.,4.]],ph_var2:[[1., 3.],[3.,1.],[.1,4.]]}))\n\nOutput:\n[array([[10.4, 22\\. ],\n       [10.4, 22\\. ]], dtype=float32)]\n\n```", "```\nimport tensorflow as tf\n\n# Using TensorFlow helper function to get the MNIST dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist_dataset = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nOutput:\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n```", "```\n# hyperparameters of the the model (you don't have to understand the functionality of each parameter)\nlearning_rate = 0.01\nnum_training_epochs = 25\ntrain_batch_size = 100\ndisplay_epoch = 1\nlogs_path = '/tmp/tensorflow_tensorboard/'\n\n# Define the computational graph input which will be a vector of the image pixels\n# Images of MNIST has dimensions of 28 by 28 which will multiply to 784\ninput_values = tf.placeholder(tf.float32, [None, 784], name='input_values')\n\n# Define the target of the model which will be a classification problem of 10 classes from 0 to 9\ntarget_values = tf.placeholder(tf.float32, [None, 10], name='target_values')\n\n# Define some variables for the weights and biases of the model\nweights = tf.Variable(tf.zeros([784, 10]), name='weights')\nbiases = tf.Variable(tf.zeros([10]), name='biases')\n```", "```\n# Create the computational graph and encapsulating different operations to different scopes\n# which will make it easier for us to understand the visualizations of TensorBoard\nwith tf.name_scope('Model'):\n # Defining the model\n predicted_values = tf.nn.softmax(tf.matmul(input_values, weights) + biases)\n\nwith tf.name_scope('Loss'):\n # Minimizing the model error using cross entropy criteria\n model_cost = tf.reduce_mean(-tf.reduce_sum(target_values*tf.log(predicted_values), reduction_indices=1))\n\nwith tf.name_scope('SGD'):\n # using Gradient Descent as an optimization method for the model cost above\n model_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_cost)\n\nwith tf.name_scope('Accuracy'):\n #Calculating the accuracy\n model_accuracy = tf.equal(tf.argmax(predicted_values, 1), tf.argmax(target_values, 1))\n model_accuracy = tf.reduce_mean(tf.cast(model_accuracy, tf.float32))\n\n# TensorFlow use the lazy evaluation strategy while defining the variables\n# So actually till now none of the above variable got created or initialized\ninit = tf.global_variables_initializer()\n```", "```\n# Create a summary to monitor the model cost tensor\ntf.summary.scalar(\"model loss\", model_cost)\n\n# Create another summary to monitor the model accuracy tensor\ntf.summary.scalar(\"model accuracy\", model_accuracy)\n\n# Merging the summaries to single operation\nmerged_summary_operation = tf.summary.merge_all()\n```", "```\n# kick off the training process\nwith tf.Session() as sess:\n\n # Intialize the variables \n sess.run(init)\n\n # operation to feed logs to TensorBoard\n summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n # Starting the training cycle by feeding the model by batch at a time\n for train_epoch in range(num_training_epochs):\n\n average_cost = 0.\n total_num_batch = int(mnist_dataset.train.num_examples/train_batch_size)\n\n # iterate through all training batches\n for i in range(total_num_batch):\n batch_xs, batch_ys = mnist_dataset.train.next_batch(train_batch_size)\n\n # Run the optimizer with gradient descent and cost to get the loss\n # and the merged summary operations for the TensorBoard\n _, c, summary = sess.run([model_optimizer, model_cost, merged_summary_operation],\n feed_dict={input_values: batch_xs, target_values: batch_ys})\n\n # write statistics to the log et every iteration\n summary_writer.add_summary(summary, train_epoch * total_num_batch + i)\n\n # computing average loss\n average_cost += c / total_num_batch\n\n # Display logs per epoch step\n if (train_epoch+1) % display_epoch == 0:\n print(\"Epoch:\", '%03d' % (train_epoch+1), \"cost=\", \"{:.9f}\".format(average_cost))\n\n print(\"Optimization Finished!\")\n\n # Testing the trained model on the test set and getting the accuracy compared to the actual labels of the test set\n print(\"Accuracy:\", model_accuracy.eval({input_values: mnist_dataset.test.images, target_values: mnist_dataset.test.labels}))\n\n print(\"To view summaries in the Tensorboard, run the command line:\\n\" \\\n \"--> tensorboard --logdir=/tmp/tensorflow_tensorboard \" \\\n\"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n```", "```\nEpoch: 001 cost= 1.183109128\nEpoch: 002 cost= 0.665210275\nEpoch: 003 cost= 0.552693334\nEpoch: 004 cost= 0.498636444\nEpoch: 005 cost= 0.465516675\nEpoch: 006 cost= 0.442618381\nEpoch: 007 cost= 0.425522513\nEpoch: 008 cost= 0.412194222\nEpoch: 009 cost= 0.401408134\nEpoch: 010 cost= 0.392437336\nEpoch: 011 cost= 0.384816745\nEpoch: 012 cost= 0.378183398\nEpoch: 013 cost= 0.372455584\nEpoch: 014 cost= 0.367275238\nEpoch: 015 cost= 0.362772711\nEpoch: 016 cost= 0.358591895\nEpoch: 017 cost= 0.354892231\nEpoch: 018 cost= 0.351451424\nEpoch: 019 cost= 0.348337946\nEpoch: 020 cost= 0.345453095\nEpoch: 021 cost= 0.342769080\nEpoch: 022 cost= 0.340236065\nEpoch: 023 cost= 0.337953151\nEpoch: 024 cost= 0.335739001\nEpoch: 025 cost= 0.333702818\nOptimization Finished!\nAccuracy: 0.9146\nTo view summaries in the Tensorboard, run the command line:\n--> tensorboard --logdir=/tmp/tensorflow_tensorboard \nThen open http://0.0.0.0:6006/ into your web browser\n```", "```\ntensorboard --logdir=/tmp/tensorflow_tensorboard\n```"]