<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer814">
<h1 class="chapterNumber">16</h1>
<h1 class="chapterTitle" id="_idParaDest-412">Other Useful Deep Learning Libraries</h1>
<p class="normal">TensorFlow from Google is not the only framework available for deep learning tasks. There is a good range of libraries and frameworks available, each with its special features, capabilities, and use cases. In this chapter, we will explore some of the popular deep learning libraries and compare their features. </p>
<p class="normal">The chapter will include:</p>
<ul>
<li class="bulletList">Hugging Face</li>
<li class="bulletList">H2O</li>
<li class="bulletList">PyTorch</li>
<li class="bulletList">ONNX</li>
<li class="bulletList">Open AI </li>
</ul>
<div class="note">
<p class="normal"> All the code files for this chapter can be found at <a href="https://packt.link/dltfchp16"><span class="url">https://packt.link/dltfchp16</span></a>.</p>
</div>
<p class="normal">Let’s begin!</p>
<h1 class="heading-1" id="_idParaDest-413">Hugging Face</h1>
<p class="normal">Hugging Face <a id="_idIndexMarker1495"/>is not new for us; <em class="chapterRef">Chapter 6</em>, <em class="italic">Transformers</em>, introduced us to the library. Hugging Face is an NLP-centered startup, founded by Delangue and Chaumond in 2016. It has, in a short time, established itself as one of the best tools for all NLP-related tasks. The AutoNLP and accelerated inference API are available for a price. However, its core NLP libraries datasets, tokenizers, Accelerate, and transformers (<em class="italic">Figure 16.1</em>) are available for free. It has built a cool community-driven open-source platform.</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated with medium confidence" height="350" src="../Images/B18331_16_01.png" width="396"/></figure>
<p class="packt_figref">Figure 16.1: NLP libraries from Hugging Face </p>
<p class="normal">The core of the Hugging<a id="_idIndexMarker1496"/> Face ecosystem is its transformers library. The Tokenizers and Datasets libraries support the Transformers library. To use these libraries, we need to install them first. Transformers can be installed using a simple <code class="inlineCode">pip install</code> command:</p>
<pre class="programlisting con"><code class="hljs-con">pip install transformers
</code></pre>
<p class="normal">Some of the out-of-the-box models available with Hugging Face are text summarization, question answering, text classification, audio classification, automatic speech recognition, feature extraction, image classification, and translation. In <em class="italic">Figure 16.2</em>, we can see the result of the out-of-the-box summarization model available with Hugging Face.</p>
<figure class="mediaobject"><img alt="A picture containing graphical user interface  Description automatically generated" height="467" src="../Images/B18331_16_02.png" width="879"/></figure>
<p class="packt_figref">Figure 16.2: Out-of-the-box text summarization using Hugging Face</p>
<p class="normal">Besides these <a id="_idIndexMarker1497"/>out-of-the-box models, we can use the large number of models and datasets available at Hugging Face Hub and can use them with PyTorch, TensorFlow, and JAX to build customized models. </p>
<h1 class="heading-1" id="_idParaDest-414">OpenAI</h1>
<p class="normal">OpenAI is<a id="_idIndexMarker1498"/> another well-known name for people working in the field of reinforcement learning. Their Gym module is a standard toolkit used by developers across the globe for developing and comparing reinforcement learning algorithms. In <em class="chapterRef">Chapter 11</em>, <em class="italic">Reinforcement Learning</em>, we have already covered the Gym module in detail. In this chapter, we will explore two more offerings by OpenAI.</p>
<h2 class="heading-2" id="_idParaDest-415">OpenAI GPT-3 API</h2>
<p class="normal">“OpenAI GPT3 is a <a id="_idIndexMarker1499"/>machine learning platform that allows developers to build custom algorithms for deep learning. This platform was released in December of 2017 and has been widely used by businesses and individuals in the field of artificial intelligence. One of the primary reasons that GPT3 has been so successful is because it is easy to use and has a wide range of features. This platform is able to learn from data and can be used for a variety of tasks, including deep learning, natural language processing, and image recognition. GPT3 is also popular because it is open source and can be used by anyone. This makes it an ideal platform for anyone who wants to learn about deep learning and the various ways that it can be used. Overall, GPT3 is a powerful and easy-to-use machine learning platform that has been widely used by businesses and individuals in the field of artificial intelligence.”</p>
<p class="normal">This is the text<a id="_idIndexMarker1500"/> generated by the OpenAI GPT-3 API, when asked to write <a id="_idIndexMarker1501"/>on GPT-3 itself (<a href="https://beta.openai.com/playground"><span class="url">https://beta.openai.com/playground</span></a>): </p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" height="496" src="../Images/B18331_16_03.png" width="879"/></figure>
<p class="packt_figref">Figure 16.3: Text generation using OpenAI GPT-3 API</p>
<p class="normal">The OpenAI GPT-3 API offers the following tasks:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Text Completion</strong>: Here, the<a id="_idIndexMarker1502"/> GPT-3 API is used to generate or manipulate text and even code. You can use it to write a tagline, an introduction, or an essay, or you can leave a sentence half-written and ask it to complete it. People have used it to generate stories and advertisement leads.</li>
<li class="bulletList"><strong class="keyWord">Semantic Search</strong>: This allows you to do a semantic search over a set of documents. For example, you can upload documents using the API; it can handle up to 200 documents, where each file can be a maximum of 150 MB in size, and the total limited to 1 GB at any given time. The API will take your query and rank the documents based on the semantic similarity score (ranges normally between 0-300). </li>
<li class="bulletList"><strong class="keyWord">Question Answering</strong>: This API uses the documents uploaded as the source of truth; the API first searches the documents for relevance to the question. Then it ranks them based on the semantic relevance and finally, answers the question. </li>
<li class="bulletList"><strong class="keyWord">Text Classification</strong>: The text classification endpoint of OpenAI GPT-3 takes as input a labeled set of examples and then uses the labels in it to label the query text. There<a id="_idIndexMarker1503"/> are a lot of examples where this feature has been used to perform sentiment analysis.</li>
</ul>
<p class="normal">Initially, the OpenAI GPT-3 was available only after applying for it, but now, anyone can use the API; there is no longer a waitlist.</p>
<h2 class="heading-2" id="_idParaDest-416">OpenAI DALL-E 2</h2>
<p class="normal">The GPT-3 API by OpenAI <a id="_idIndexMarker1504"/>deals with all things related to NLP; DALL-E 2 goes a step further. DALL-E was originally released by OpenAI in January 2021. It claims to produce photorealistic images based on the textual description provided to the model. It can also make realistic edits to existing images; you can use it to add or remove objects and elements from the image, and when it does so, it considers the effect on shadows, reflections, and texture. <em class="italic">Figure 16.4</em> shows some of the remarkable feats by DALL-E 2. In the figures on the top row, I gave DALL-E 2 a text describing what I want: “Albert Einstein flying on dinosaur over the Amazon Forest.” It generated a cartoon-like image. The images in the lower row are generated using the image-editor feature of DALL-E 2. I added the image on the left, and it generated four variations. The variations look very realistic if you ignore that the faces are blurred: </p>
<figure class="mediaobject"><img alt="" height="254" src="../Images/B18331_16_04_1.png" width="884"/></figure>
<figure class="mediaobject"><img alt="" height="194" src="../Images/B18331_16_04_2.png" width="879"/></figure>
<p class="packt_figref">Figure 16.4: On top is the image generated by DALL-E 2, and below are the images edited by DALL-E 2</p>
<p class="normal">At the time of writing this book (August 2022), DALL-E 2 is not available for public use. But imagine the <a id="_idIndexMarker1505"/>possibilities for artists and professionals working in creating digital media once the model is available as an API.</p>
<h2 class="heading-2" id="_idParaDest-417">OpenAI Codex</h2>
<p class="normal">When a student<a id="_idIndexMarker1506"/> starts with their first lessons of programming, as a teacher, I often recommend that they think of a program as a set of instructions – the only important thing to master is writing those instructions as clearly as possible in whatever language you know.</p>
<p class="normal">Well, Codex makes it happen, you just need to give it the instructions of what you want to achieve, and it will generate the respective code for you. </p>
<p class="normal">OpenAI launches it as a general-purpose programming model, and it has been trained on publicly available GitHub codes.</p>
<p class="normal">Below are a few snippets of the task and corresponding code generated by Codex:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">""" Ask the user for their name and say "Hello" in Python """</span> 
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Hello, world!"</span>) 
name = <span class="hljs-built_in">input</span>(<span class="hljs-string">"Please type your name: "</span>) 
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Hello, "</span> + name + <span class="hljs-string">"!"</span>)
<span class="hljs-string">""" Write a program in Python to find sum of n fibonacci sequences """</span> 
<span class="hljs-keyword">def</span> <span class="hljs-title">Fibonacci</span>(<span class="hljs-params">n</span>): 
<span class="hljs-keyword">    if</span> n &lt; <span class="hljs-number">0</span>: 
<span class="hljs-built_in">        print</span>(<span class="hljs-string">"</span><span class="hljs-string">Incorrect input"</span>) 
<span class="hljs-keyword">    elif</span> n == <span class="hljs-number">1</span>: 
       <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> 
<span class="hljs-keyword">    elif</span> n == <span class="hljs-number">2</span>: 
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> 
<span class="hljs-keyword">    else</span>: 
        <span class="hljs-keyword">return</span> Fibonacci(n - <span class="hljs-number">1</span>) + Fibonacci(n - <span class="hljs-number">2</span>) 
n = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">"Enter the number of terms: "</span>)) 
<span class="hljs-comment"># check if the number of terms is valid </span>
<span class="hljs-keyword">if</span> n &lt;= <span class="hljs-number">0</span>: 
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">Please enter a positive integer"</span>) 
<span class="hljs-keyword">else</span>: 
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Fibonacci sequence: "</span>) 
    <span class="hljs-keyword">for</span> I <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n + <span class="hljs-number">1</span>): 
        <span class="hljs-built_in">print</span>(Fibonacci(n), end=<span class="hljs-string">'</span><span class="hljs-string"> '</span>)
</code></pre>
<p class="normal">The first task, as you can see, is done flawlessly. In the second task, we asked it to find the sum of the Fibonacci sequence; instead, it generated the Fibonacci sequence, which is a more common <a id="_idIndexMarker1507"/>problem. This tells us that while it is great at doing run-of-the-mill jobs, the need for real programmers is still there.</p>
<h1 class="heading-1" id="_idParaDest-418">PyTorch</h1>
<p class="normal">Like TensorFlow, PyTorch<a id="_idIndexMarker1508"/> is a full-fledged deep learning framework. In AI-based social groups, you will often find die-hard fans of PyTorch and TensorFlow arguing that theirs is best. PyTorch, developed by Facebook (Meta now), is an open-source deep learning framework. Many researchers prefer it for its flexible and modular approach. PyTorch also has stable support for production deployment. Like TF, the core of PyTorch is its tensor processing library and its automatic differentiation engine. In a C++ runtime environment, it leverages TorchScript for an easy transition between graph and eager mode. The major feature that makes PyTorch popular is its ability to use dynamic computation, i.e., its ability to dynamically build the computational graph – this gives the programmer flexibility to modify and inspect the computational graphs anytime.</p>
<p class="normal">The PyTorch library consists of many modules, which are used as building blocks to make complex models. Additionally, PyTorch also provides convenient functions to transfer variables and models between different devices viz CPU, GPU, or TPU. Of special mention<a id="_idIndexMarker1509"/> are the following three powerful modules:</p>
<ul>
<li class="bulletList"><strong class="keyWord">NN Module</strong>: This is the<a id="_idIndexMarker1510"/> base class where all layers <a id="_idIndexMarker1511"/>and functions to build a deep learning network are. Below, you can see the code snippet where the NN module is used to build a network. The network can then be instantiated using the statement <code class="inlineCode">net = My_Net(1,10,5)</code>; this creates a network with one input channel, 10 output neurons, and a kernel of size <code class="inlineCode">5x5</code>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">class</span> <span class="hljs-title">My_Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_channel, output_neurons, kernel_size</span>):
        <span class="hljs-built_in">super</span>(My_Net, self).__init__()
        self.conv1 = nn.Conv2d(input_channel, <span class="hljs-number">6</span>, kernel_size)
        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)
        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)
        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)
        self.fc3 = nn.Linear(<span class="hljs-number">84</span>,output_neurons)
    <span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):
        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="hljs-number">2</span>)
        x = x.view(-<span class="hljs-number">1</span>, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        <span class="hljs-keyword">return</span> x
    <span class="hljs-keyword">def</span> <span class="hljs-title">num_flat_features</span>(<span class="hljs-params">self, x</span>):
        size = x.size()[<span class="hljs-number">1</span>:]  
        num_features = <span class="hljs-number">1</span>
        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> size:
            num_features *= s
        <span class="hljs-keyword">return</span> num_features
</code></pre>
<p class="normal">Here is a summary of the network:</p>
<pre class="programlisting con"><code class="hljs-con">My_Net(
    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1,
    1))
    (fc1): Linear(in_features=400, out_features=120,
    bias=True)
    (fc2): Linear(in_features=120, out_features=84,
    bias=True)
    (fc3): Linear(in_features=84, out_features=10,
    bias=True)
)
</code></pre>
</li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord">Autograd Module</strong>: This is<a id="_idIndexMarker1512"/> the heart of PyTorch. The<a id="_idIndexMarker1513"/> module provides classes and functions that are used for implementing automatic differentiation. The module creates an acyclic graph called the dynamic computational graph; the leaves of this graph are the input tensors, and the root is the output tensors. It calculates a gradient by tracing the root to the leaf and multiplying every gradient in the path using the chain rule. The following code snippet shows how to use the Autograd module for calculating gradients. The <code class="inlineCode">backward()</code> function computes the gradient of the loss with respect to all the tensors whose <code class="inlineCode">requires_grad</code> is set to <code class="inlineCode">True</code>. So suppose you have a variable <code class="inlineCode">w</code>, then after the call to <code class="inlineCode">backward(),</code> the tensor <code class="inlineCode">w.grad</code> will give us the gradient of the loss with respect to <code class="inlineCode">w</code>. 
    <p class="bulletList">We can then use this to update the variable <code class="inlineCode">w</code> as per the learning rule: </p>
<pre class="programlisting code"><code class="hljs-code">loss = (y_true – y_pred).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()
loss.backward()
<span class="hljs-comment"># Here the autograd is used to compute the backward pass. </span>
With torch.no_grad():
    W = w – lr_rate * w.grad
    w.grad = <span class="hljs-literal">None</span> <span class="hljs-comment"># Manually set to zero after updating</span>
</code></pre>
</li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord">Optim Module</strong>: The Optim <a id="_idIndexMarker1514"/>module implements <a id="_idIndexMarker1515"/>various optimization algorithms. Some of the optimizer algorithms available in Optim are SGD, AdaDelta, Adam, SparseAdam, AdaGrad, and LBFGS. One can also use the Optim module to create complex optimizers. To use the Optim module, one just needs to construct an optimizer object that will hold the current state and will update the parameters based on gradients.</li>
</ul>
<p class="normal">PyTorch is used by many companies for their AI solutions. <strong class="keyWord">Tesla </strong>uses PyTorch for <strong class="keyWord">AutoPilot</strong>. The Tesla Autopilot, which uses the footage from eight cameras around the vehicle, passes that footage through 48 neural networks for object detection, semantic segmentation, and monocular depth estimation. The system provides level 2 vehicle automation. They take video from all eight cameras to generate road layout, any static infrastructure (e.g., buildings and traffic/electricity poles), and 3D objects (other vehicles, persons on the road, and so on). The networks are trained iteratively in real time. While a little technical, this 2019 talk by Andrej Karpathy, Director of AI at Tesla, gives a bird’s-eye view of Autopilot and its capabilities: <a href="https://www.youtube.com/watch?v=oBklltKXtDE&amp;t=670s"><span class="url">https://www.youtube.com/watch?v=oBklltKXtDE&amp;t=670s</span></a>. Uber’s Pyro, a probabilistic deep learning library, and OpenAI are other examples of big AI companies using PyTorch for research and development.</p>
<h1 class="heading-1" id="_idParaDest-419">ONNX</h1>
<p class="normal"><strong class="keyWord">Open Neural Network Exchange</strong> (<strong class="keyWord">ONNX</strong>) provides an open-source format for AI models. It supports both<a id="_idIndexMarker1516"/> deep learning models and traditional machine learning models. It is a format designed to represent any type of model, and it achieves this by using an intermediate representation of the computational graph created by different frameworks. It supports PyTorch, TensorFlow, MATLAB, and many more deep learning frameworks. Thus, using ONNX, we can easily convert models from one framework to another. This helps in reducing the time from research to deployment. For example, you can use ONNX to convert a PyTorch model to ONNX.js form, which can then be directly deployed on the web.</p>
<h1 class="heading-1" id="_idParaDest-420">H2O.ai</h1>
<p class="normal">H2O is a fast, scalable<a id="_idIndexMarker1517"/> machine learning and deep learning framework developed by H2O.ai, released under the open-source Apache license. According to the company website, as of the time of writing this book, more than 20,000 organizations use H2O for their ML/deep learning needs. The company offers many products like H2O AI cloud, H2O Driverless AI, H2O wave, and Sparkling Water. In this section, we will explore its open-source product, H2O. </p>
<p class="normal">It works on big data infrastructure on Hadoop, Spark, or Kubernetes clusters and it can also work in standalone mode. It makes use of distributed systems and in-memory computing, which allows it to handle a large amount of data in memory, even with a small cluster of machines. It has an interface for R, Python, Java, Scala, and JavaScript, and even has a built-in web interface. </p>
<p class="normal">H2O includes a large number of statistical-based ML algorithms such as generalized linear modeling, Naive Bayes, random forest, gradient boosting, and all major deep learning algorithms. The best part of H2O is that one can build thousands of models, compare the results, and even do hyperparameter tuning with only a few lines of code. H2O also has better data pre-processing tools. </p>
<p class="normal">H2O requires Java, therefore, ensure that Java is installed on your system. You can install H2O to<a id="_idIndexMarker1518"/> work in Python using PyPi, as shown in the following code: </p>
<pre class="programlisting con"><code class="hljs-con">pip install h2o
</code></pre>
<h2 class="heading-2" id="_idParaDest-421">H2O AutoML </h2>
<p class="normal">One of the most <a id="_idIndexMarker1519"/>exciting features of H2O is <strong class="keyWord">AutoML</strong>, the automatic ML. It is an attempt to develop a user-friendly ML interface that can be used by beginners and non-experts. H2O AutoML automates the process of training and tuning a large selection of candidate models. Its interface is designed so that users just need to specify their dataset, input and output features, and any constraints they want on the number of total models trained, or time constraints. The rest of the work is done by the AutoML itself, in the specified time constraint; it identifies the best performing models and provides<a id="_idIndexMarker1520"/> a <strong class="keyWord">leaderboard</strong>. It has been observed that usually, the Stacked Ensemble model, the ensemble of all the previously trained models, occupies the top position on the leaderboard. There is a large number of options that advanced users can use; details of these options and their various features are available at <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml"><span class="url">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml</span></a>.</p>
<p class="normal">To learn more about H2O, visit <a id="_idIndexMarker1521"/>their website: <a href="http://h2o.ai"><span class="url">http://h2o.ai</span></a>. </p>
<h2 class="heading-2" id="_idParaDest-422">AutoML using H2O</h2>
<p class="normal">Let us try H2O <a id="_idIndexMarker1522"/>AutoML on<a id="_idIndexMarker1523"/> a synthetically created dataset. We use the scikit-learn <code class="inlineCode">make_circles</code> method to create the data and save it as a CSV file:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
X, y = make_circles(n_samples=<span class="hljs-number">1000</span>, noise=<span class="hljs-number">0.2</span>, factor=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">9</span>)
df = pd.DataFrame(X, columns=[<span class="hljs-string">'x1'</span>,<span class="hljs-string">'x2'</span>])
df[<span class="hljs-string">'y'</span>] = y
df.head()
df.to_csv(<span class="hljs-string">'circle.csv'</span>, index=<span class="hljs-literal">False</span>, header=<span class="hljs-literal">True</span>)
</code></pre>
<p class="normal">Before we can use H2O, we need to initiate its server, which is done using the <code class="inlineCode">init()</code> function:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> h2o
h2o.init()
</code></pre>
<p class="normal">The following shows the output we will receive after initializing the H2O server:</p>
<pre class="programlisting con"><code class="hljs-con">Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.
Attempting to start a local H2O server...
  Java Version: openjdk version "11.0.15" 2022-04-19; OpenJDK Runtime Environment (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1); OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1, mixed mode, sharing)
  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar
  Ice root: /tmp/tmpm2fsae68
  JVM stdout: /tmp/tmpm2fsae68/h2o_unknownUser_started_from_python.out
  JVM stderr: /tmp/tmpm2fsae68/h2o_unknownUser_started_from_python.err
  Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321 ... successful.
H2O_cluster_uptime:    05 secs
H2O_cluster_timezone:    Etc/UTC
H2O_data_parsing_timezone:    UTC
H2O_cluster_version:    3.36.1.1
H2O_cluster_version_age:    27 days 
H2O_cluster_name:    H2O_from_python_unknownUser_45enk6
H2O_cluster_total_nodes:    1
H2O_cluster_free_memory:    3.172 Gb
H2O_cluster_total_cores:    2
H2O_cluster_allowed_cores:    2
H2O_cluster_status:    locked, healthy
H2O_connection_url:    http://127.0.0.1:54321
H2O_connection_proxy:    {"http": null, "https": null}
H2O_internal_security:    False
Python_version:    3.7.13 final
</code></pre>
<p class="normal">We read the file containing the synthetic data that we created earlier. Since we want to treat the problem as a classification problem, whether the points lie in a circle or not, we redefine our label <code class="inlineCode">'y'</code> as <code class="inlineCode">asfactor()</code> – this will tell the H2O AutoML module to treat the variable <code class="inlineCode">y</code> as categorical, and thus the problem as classification. The dataset is split into training, validation, and<a id="_idIndexMarker1524"/> test <a id="_idIndexMarker1525"/>datasets in a ratio of 60:20:20: </p>
<pre class="programlisting code"><code class="hljs-code">class_df = h2o.import_file(<span class="hljs-string">"circle.csv"</span>,\
                           destination_frame=<span class="hljs-string">"circle_df"</span>)
class_df[<span class="hljs-string">'y'</span>] = class_df[<span class="hljs-string">'</span><span class="hljs-string">y'</span>].asfactor()
train_df,valid_df,test_df = class_df.split_frame(ratios=[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.2</span>],\
                                                 seed=<span class="hljs-number">133</span>)
</code></pre>
<p class="normal">And now we invoke the AutoML module from H2O and train on our training dataset. AutoML will search a maximum of 10 models, but you can change the parameter <code class="inlineCode">max_models</code> to increase or decrease the number of models to test:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> h2o.automl <span class="hljs-keyword">import</span> H2OAutoML <span class="hljs-keyword">as</span> AutoML
aml = AutoML(max_models = <span class="hljs-number">10</span>, max_runtime_secs=<span class="hljs-number">100</span>, seed=<span class="hljs-number">2</span>)
aml.train(training_frame= train_df, \
          validation_frame=valid_df, \
          y = <span class="hljs-string">'y'</span>, x=[<span class="hljs-string">'x1'</span>,<span class="hljs-string">'x2'</span>])
</code></pre>
<p class="normal">For each of the models, it gives a performance summary, for example, in <em class="italic">Figure 16.5</em>, you can see the evaluation summary for a binomial GLM:</p>
<figure class="mediaobject"><img alt="Text  Description automatically generated" height="462" src="../Images/B18331_16_05.png" width="710"/></figure>
<p class="packt_figref">Figure 16.5: Performance summary of one of the models by H2O AutoML</p>
<p class="normal">You can check <a id="_idIndexMarker1526"/>the <a id="_idIndexMarker1527"/>performance of all the models evaluated by H2O AutoML on a leaderboard:</p>
<pre class="programlisting code"><code class="hljs-code">lb = aml.leaderboard
lb.head()
</code></pre>
<p class="normal">Here is the snippet of the leaderboard:</p>
<pre class="programlisting con"><code class="hljs-con">model_id     auc    logloss    aucpr    mean_per_class_error    rmse    mse
StackedEnsemble_BestOfFamily_1_AutoML_2_20220511_61356    0.937598    0.315269    0.940757    0.117037    0.309796    0.0959735
StackedEnsemble_AllModels_1_AutoML_2_20220511_61356     0.934905    0.323695    0.932648    0.120348    0.312413    0.0976021
XGBoost_2_AutoML_2_20220511_61356     0.93281     0.322668    0.938299    0.122004    0.313339    0.0981811
XGBoost_3_AutoML_2_20220511_61356     0.932392    0.330866    0.929846    0.130168    0.319367    0.101995 
GBM_2_AutoML_2_20220511_61356     0.926839    0.353181    0.923751    0.141713    0.331589    0.109951 
XRT_1_AutoML_2_20220511_61356     0.925743    0.546718    0.932139    0.154774    0.331096    0.109625 
GBM_3_AutoML_2_20220511_61356     0.923935    0.358691    0.917018    0.143374    0.334959    0.112197 
DRF_1_AutoML_2_20220511_61356     0.922535    0.705418    0.921029    0.146669    0.333494    0.111218 
GBM_4_AutoML_2_20220511_61356     0.921954    0.36403     0.911036    0.151582    0.336908    0.113507 
XGBoost_1_AutoML_2_20220511_61356     0.919142    0.365454    0.928126    0.130227    0.336754    0.113403
</code></pre>
<h2 class="heading-2" id="_idParaDest-423">H2O model explainability</h2>
<p class="normal">H2O provides a<a id="_idIndexMarker1528"/> convenient wrapper for a number of explainability methods and their visualizations using a single function <code class="inlineCode">explain()</code> with a dataset and model. To get explainability on our test data for the models tested by AutoML, we will use <code class="inlineCode">aml.explain()</code>. Below, we use the <code class="inlineCode">explain</code> module for the <code class="inlineCode">StackedEnsemble_BestOfFamily</code> model – the topmost in the leaderboard (we are continuing with the same data that we created in the previous section):</p>
<pre class="programlisting code"><code class="hljs-code">exa = aml.leader.explain(test_df)
</code></pre>
<p class="normal">The results are:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" height="227" src="../Images/B18331_16_06.png" width="762"/></figure>
<p class="packt_figref">Figure 16.6: A confusion matrix on test dataset generated by H2O explain module</p>
<p class="normal">The ground truth is displayed in rows and the prediction by the model in columns. For our data, 0 was predicted correctly 104 times, and 1 was predicted correctly 88 times.</p>
<h3 class="heading-3" id="_idParaDest-424">Partial dependence plots</h3>
<p class="normal"><strong class="keyWord">Partial Dependence Plots</strong> (<strong class="keyWord">PDP</strong>) provide <a id="_idIndexMarker1529"/>a graphical<a id="_idIndexMarker1530"/> depiction of the marginal effect of a variable on the response of the model. It can tell us about the relationship between the output label and the input feature. <em class="italic">Figure 16.7</em> shows the PDP plots as obtained from the H2O <code class="inlineCode">explain</code> module on our synthetic dataset:</p>
<figure class="mediaobject"><img alt="A picture containing chart  Description automatically generated" height="279" src="../Images/B18331_16_07.png" width="826"/></figure>
<p class="packt_figref">Figure 16.7: PDP for input features x<sub class="subscript">1</sub> and x<sub class="subscript">2</sub></p>
<p class="normal">For building PDP plots for each feature, H2O considers the rest of the features as constant. So, in the PDP plot for x<sub class="subscript">1</sub> (x<sub class="subscript">2</sub>), the feature x<sub class="subscript">2</sub> (x<sub class="subscript">1</sub>) is kept constant and the mean response is measured, as x<sub class="subscript">1</sub> (x<sub class="subscript">2</sub>) is varied. The graph shows that both features play an important role in determining if the point is a circle or not, especially for values lying between [-0.5, 0.5].</p>
<h3 class="heading-3" id="_idParaDest-425">Variable importance heatmap</h3>
<p class="normal">We can <a id="_idIndexMarker1531"/>also check the importance of variables across different models: </p>
<figure class="mediaobject"><img alt="Icon  Description automatically generated" height="487" src="../Images/B18331_16_08.png" width="826"/></figure>
<p class="packt_figref">Figure 16.8: Variable importance heatmap for input features x<sub class="subscript">1</sub> and x<sub class="subscript">2</sub></p>
<p class="normal"><em class="italic">Figure 16.8</em> shows how much importance was given to the two input features by different algorithms. We can see that the models that gave almost equal importance to the two features are doing well on the leaderboard, while <strong class="screenText">GLM_1</strong>, which treated both features quite<a id="_idIndexMarker1532"/> differently, has only about 41% accuracy.</p>
<h3 class="heading-3" id="_idParaDest-426">Model correlation</h3>
<p class="normal">The prediction <a id="_idIndexMarker1533"/>between different models is correlated; we can check this correlation:</p>
<figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" height="702" src="../Images/B18331_16_09.png" width="826"/></figure>
<p class="packt_figref">Figure 16.9: Model correlation</p>
<p class="normal"><em class="italic">Figure 16.9</em> shows the model<a id="_idIndexMarker1534"/> correlation; it shows the correlation between the predictions on the test dataset for different models. It measures the frequency of identical predictions to calculate correlations. Again, we can see that except for <strong class="screenText">GLM_1</strong>, most other models perform almost equally, with accuracy ranging from 84-93% on the leaderboard.</p>
<p class="normal">What we have discussed here is just the tip of the iceberg; each of the frameworks listed here has entire books on their features and applications. Depending upon your use case, you should choose the respective framework. If you are building a model for production, TensorFlow is a better choice for both web-based and Edge applications. If you are building a model where you need better control of training and how the gradients are updated, then PyTorch is better suited. If you need to work cross-platform very often, ONNX can be useful. And finally, H2O and platforms like OpenAI GPT-3 and DALL-E 2 provide a low-threshold <a id="_idIndexMarker1535"/>entry into the field of artificial intelligence and deep learning.</p>
<h1 class="heading-1" id="_idParaDest-427">Summary</h1>
<p class="normal">In this chapter, we briefly covered the features and capabilities of some other popular deep learning frameworks, libraries, and platforms. We started with Hugging Face, a popular framework for NLP. Then we explored OpenAI’s GPT-3 and DALL-E 2, both very powerful frameworks. The GPT-3 API can be used for a variety of NLP-related tasks, and DALL-E 2 uses GPT-3 to generate images from textual descriptions. Next, we touched on the PyTorch framework. According to many people, PyTorch and TensorFlow are equal competitors, and PyTorch indeed has many features comparable to TensorFlow. In this chapter, we briefly talked about some important features like the NN module, Optim module, and Autograd module of PyTorch. We also discussed ONNX, the open-source format for deep learning models, and how we can use it to convert the model from one framework to another. Lastly, the chapter introduced H2O and its AutoML and <code class="inlineCode">explain</code> modules. </p>
<p class="normal">In the next chapter, we will learn about graph neural networks.</p>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>