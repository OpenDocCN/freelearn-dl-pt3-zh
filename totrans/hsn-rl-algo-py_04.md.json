["```\nInitialize ![](img/296b08e4-1bbe-4a7d-b82f-348344424f0b.png) and ![](img/569f8ffb-0948-4e8c-93a3-7fe12df6abc8.png) for every state \n\nwhile ![](img/9a940658-b350-4def-a52e-d8c73338da96.png) is not stable:\n\n    > policy evaluation\n   while ![](img/4d814211-461a-465d-8ca0-ddf80fb78834.png) is not stable:\n        for each state s:\n\n    > policy improvement\n    for each state s:\n\n```", "```\nenv = gym.make('FrozenLake-v0')\nenv = env.unwrapped\nnA = env.action_space.n\nnS = env.observation_space.n\nV = np.zeros(nS)\npolicy = np.zeros(nS)\n\n```", "```\npolicy_stable = False\nit = 0\nwhile not policy_stable:\n    policy_evaluation(V, policy)\n    policy_stable = policy_improvement(V, policy)\n    it += 1\n```", "```\nprint('Converged after %i policy iterations'%(it))\nrun_episodes(env, V, policy)\nprint(V.reshape((4,4)))\nprint(policy.reshape((4,4)))\n```", "```\ndef eval_state_action(V, s, a, gamma=0.99):\n    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n```", "```\ndef policy_evaluation(V, policy, eps=0.0001):\n    while True:\n        delta = 0\n        for s in range(nS):\n            old_v = V[s]\n            V[s] = eval_state_action(V, s, policy[s])\n            delta = max(delta, np.abs(old_v - V[s]))\n        if delta < eps:\n            break\n\n```", "```\ndef policy_improvement(V, policy):\n    policy_stable = True\n    for s in range(nS):\n        old_a = policy[s]\n        policy[s] = np.argmax([eval_state_action(V, s, a) for a in range(nA)])\n        if old_a != policy[s]: \n            policy_stable = False\n    return policy_stable\n```", "```\ndef run_episodes(env, V, policy, num_games=100):\n    tot_rew = 0\n    state = env.reset()\n    for _ in range(num_games):\n        done = False\n        while not done:\n            next_state, reward, done, _ = env.step(policy[state])\n            state = next_state\n            tot_rew += reward \n            if done:\n                state = env.reset()\n    print('Won %i of %i games!'%(tot_rew, num_games))\n```", "```\nInitialize  for every state \n\nwhile  is not stable:\n    > value iteration\n    for each state s:\n\n> compute the optimal policy:\n\n```", "```\ndef eval_state_action(V, s, a, gamma=0.99):\n    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n```", "```\ndef value_iteration(eps=0.0001):\n    V = np.zeros(nS)\n    it = 0\n    while True:\n        delta = 0\n        # update the value for each state\n        for s in range(nS):\n            old_v = V[s]\n            V[s] = np.max([eval_state_action(V, s, a) for a in range(nA)]) # equation 3.10\n            delta = max(delta, np.abs(old_v - V[s]))\n        # if stable, break the cycle\n        if delta < eps:\n            break\n        else:\n            print('Iter:', it, ' delta:', np.round(delta,5))\n        it += 1\n    return V\n```", "```\ndef run_episodes(env, V, num_games=100):\n    tot_rew = 0\n    state = env.reset()\n\n    for _ in range(num_games):\n        done = False\n\n        while not done:\n            # choose the best action using the value function\n            action = np.argmax([eval_state_action(V, state, a) for a in range(nA)]) #(11)\n            next_state, reward, done, _ = env.step(action)\n            state = next_state\n            tot_rew += reward \n            if done:\n                state = env.reset()\n\n    print('Won %i of %i games!'%(tot_rew, num_games))\n```", "```\nenv = gym.make('FrozenLake-v0')\nenv = env.unwrapped\n\nnA = env.action_space.n\nnS = env.observation_space.n\n\nV = value_iteration(eps=0.0001)\nrun_episodes(env, V, 100)\nprint(V.reshape((4,4)))\n```", "```\nIter: 0 delta: 0.33333\nIter: 1 delta: 0.1463\nIter: 2 delta: 0.10854\n...\nIter: 128 delta: 0.00011\nIter: 129 delta: 0.00011\nIter: 130 delta: 0.0001\nWon 86 of 100 games!\n[[0.54083394 0.49722378 0.46884941 0.45487071]\n [0.55739213 0\\.         0.35755091 0\\.        ]\n [0.5909355  0.64245898 0.61466487 0\\.        ]\n [0\\.         0.74129273 0.86262154 0\\.        ]]\n```"]